{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> ", "gt": "elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> ", "gt": "if not sil_tags :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> elapsed_time = time . time ( ) - t_start <EOL> if elapsed_time < elapsed_time_expected : <EOL> time . sleep ( elapsed_time_expected - elapsed_time ) <EOL> if not quiet : <EOL> pbar . close ( ) <EOL> if tmp_file : <EOL> f . close ( ) <EOL> ", "gt": "shutil . move ( tmp_file , output )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> ", "gt": "for py_file in py_files :"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> ", "gt": "with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_ivf_added = faiss . extract_index_ivf ( index_added )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> ", "gt": "index_trained = faiss . index_factory ("}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> ", "gt": "batch_size_add = <NUM_LIT>"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> ", "gt": "print ("}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> elapsed_time = time . time ( ) - t_start <EOL> if elapsed_time < elapsed_time_expected : <EOL> time . sleep ( elapsed_time_expected - elapsed_time ) <EOL> ", "gt": "if not quiet :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> ", "gt": "sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> ", "gt": "np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> ", "gt": "pos_l = ("}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_filepath_trained = os . path . join ( exp_dir , index_filename_trained )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> ", "gt": "pos_r = ("}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> ", "gt": "if resume and existing_tmp_files :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> ", "gt": "except FileURLRetrievalError as e :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> ", "gt": "for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> ", "gt": "silence_end = min ( total_frames , silence_start + self . max_sil_kept )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> ", "gt": "self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> ", "gt": "if not osp . exists ( output ) :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ", "gt": ")"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> ", "gt": "standard_file = \"<STR_LIT>\""}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> ", "gt": "index_trained . train ( big_npy )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "except Exception as error :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_ivf_trained = faiss . extract_index_ivf ( index_trained )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> ", "gt": "url_origin ,"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> ", "gt": "silence_start is not None"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> ", "gt": "n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> ", "gt": "if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> ", "gt": "chunks . append ("}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> ", "gt": "chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> ", "gt": "sil_tags . append ( ( <NUM_LIT> , pos_r ) )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "file = sys . stderr ,"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ", "gt": ") :"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> ", "gt": "return i18n_strings"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> ", "gt": "missing_keys = code_keys - standard_keys"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> ", "gt": "format = \"<STR_LIT>\" if format is None else format ,"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ", "gt": ")"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> for unused_key in unused_keys : <EOL> print ( \"<STR_LIT>\" , unused_key ) <EOL> print ( \"<STR_LIT>\" , len ( missing_keys ) ) <EOL> for missing_key in missing_keys : <EOL> print ( \"<STR_LIT>\" , missing_key ) <EOL> code_keys_dict = OrderedDict ( ( s , s ) for s in code_keys ) <EOL> ", "gt": "with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> ", "gt": "\"<STR_LIT>\" . format ("}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> ", "gt": "return chunks"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ", "gt": ") :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> ", "gt": "content_disposition = six . moves . urllib_parse . unquote ("}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> ", "gt": "f\"<STR_LIT>\""}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> ", "gt": "standard_data = json . load ( file , object_pairs_hook = OrderedDict )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> ", "gt": "print ( f\"<STR_LIT>\" )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> ", "gt": "faiss . write_index ( index_added , index_filepath_added )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> ", "gt": "index_ivf_trained . nprobe = <NUM_LIT>"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) <EOL> ", "gt": "y = np . pad ( y , padding , mode = pad_mode )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) <EOL> y = np . pad ( y , padding , mode = pad_mode ) <EOL> axis = - <NUM_LIT> <EOL> out_strides = y . strides + tuple ( [ y . strides [ axis ] ] ) <EOL> x_shape_trimmed = list ( y . shape ) <EOL> x_shape_trimmed [ axis ] -= frame_length - <NUM_LIT> <EOL> out_shape = tuple ( x_shape_trimmed ) + tuple ( [ frame_length ] ) <EOL> xw = np . lib . stride_tricks . as_strided ( y , shape = out_shape , strides = out_strides ) <EOL> if axis < <NUM_LIT> : <EOL> target_axis = axis - <NUM_LIT> <EOL> else : <EOL> ", "gt": "target_axis = axis + <NUM_LIT>"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> ", "gt": "if resume :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> elapsed_time = time . time ( ) - t_start <EOL> if elapsed_time < elapsed_time_expected : <EOL> time . sleep ( elapsed_time_expected - elapsed_time ) <EOL> if not quiet : <EOL> ", "gt": "pbar . close ( )"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> ", "gt": "url = tb . launch ( )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> ", "gt": "<NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\""}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> for unused_key in unused_keys : <EOL> print ( \"<STR_LIT>\" , unused_key ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , len ( missing_keys ) )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> ", "gt": "elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> ", "gt": "clip_start = max ( pos_r , pos )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> ", "gt": "index_filename_trained = ("}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> ", "gt": "faiss . write_index ( index_trained , index_filepath_trained )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> ", "gt": "index_added . train ( big_npy )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> ", "gt": "f\"<STR_LIT>\""}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> ", "gt": "code_keys . update ( strings )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> ", "gt": "total = res . headers . get ( \"<STR_LIT>\" )"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "while True :"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> ", "gt": "print ("}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> ", "gt": "hop_length = <NUM_LIT> ,"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) <EOL> y = np . pad ( y , padding , mode = pad_mode ) <EOL> ", "gt": "axis = - <NUM_LIT>"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ", "gt": ")"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> ", "gt": "message = ("}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> ", "gt": "standard_keys = set ( standard_data . keys ( ) )"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ", "gt": ")"}
