{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ", "gt": ")"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> ", "gt": "continue"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> ", "gt": "decorators = [ user_is_collection_creator ] ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> initial = True <EOL> dependencies = [ <EOL> ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = '<STR_LIT>' , <EOL> fields = [ <EOL> ( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> , unique = True ) ) , <EOL> ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT> ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> ) ) , <EOL> ( '<STR_LIT>' , models . BooleanField ( default = True ) ) , <EOL> ( '<STR_LIT>' , models . DateTimeField ( auto_now_add = True ) ) , <EOL> ] , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> { <EOL> \"<STR_LIT>\" : django_asgi_app , <EOL> ", "gt": "\"<STR_LIT>\" : AllowedHostsOriginValidator ("}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ", "gt": "( \"<STR_LIT>\" , \"<STR_LIT>\" ) ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> ", "gt": "pre_save = lambda request , instance : None ,"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> ", "gt": "res_re = re . search ( '<STR_LIT>' , res . text )"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> ", "gt": "batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ]"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> ", "gt": "doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> ", "gt": "with autocast ( ) :"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . FORBIDDEN , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . forbidden is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . forbidden , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_path_parameters_not_found ( <EOL> ", "gt": "self ,"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> ", "gt": "yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> ", "gt": "default_request_body = ItemIn"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> ", "gt": "msg = \"<STR_LIT>\""}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> ", "gt": "response = self . index . query ( modified_query_str )"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> ", "gt": "return list ( self . page )"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> markdown_sources = f\"<STR_LIT>\" <EOL> else : <EOL> markdown_sources = \"<STR_LIT>\" <EOL> formatted_response = f\"<STR_LIT>\" <EOL> ", "gt": "await self . send ( json . dumps ( { \"<STR_LIT>\" : formatted_response } , indent = <NUM_LIT> ) )"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> ", "gt": "channel = {"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> ", "gt": "options = {"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> markdown_sources = f\"<STR_LIT>\" <EOL> else : <EOL> markdown_sources = \"<STR_LIT>\" <EOL> ", "gt": "formatted_response = f\"<STR_LIT>\""}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( max_length = <NUM_LIT> , null = True ) , <EOL> ", "gt": ") ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> raise ValidationError ( <EOL> self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" <EOL> ) <EOL> return prompt_uuid <EOL> class DescribeImageApiForm ( ApiForm ) : <EOL> image_id = forms . CharField ( ) <EOL> maxlength = forms . IntegerField ( required = False , min_value = <NUM_LIT> , max_value = <NUM_LIT> ) <EOL> class DescribeImageForm ( BaseImageForm ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> if self . instance and self . instance . pk : <EOL> widget = self . fields [ \"<STR_LIT>\" ] . widget <EOL> widget . attrs [ \"<STR_LIT>\" ] = str ( self . instance . pk ) <EOL> widget . attrs [ \"<STR_LIT>\" ] = _ ( \"<STR_LIT>\" ) <EOL> ", "gt": "widget . template_name = \"<STR_LIT>\""}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> while len ( input_ids ) < max_input_length : <EOL> input_ids . append ( <NUM_LIT> ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> attention_masks . append ( <NUM_LIT> ) <EOL> output [ doc_index ] [ seq_index ] = torch . cat ( ( torch . LongTensor ( input_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> torch . LongTensor ( input_type_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> torch . LongTensor ( attention_masks ) . unsqueeze ( <NUM_LIT> ) ) , <EOL> dim = <NUM_LIT> ) <EOL> ", "gt": "max_seq_index = seq_index"}
{"input": "from dvadmin . system . models import LoginLog <EOL> from dvadmin . utils . serializers import CustomModelSerializer <EOL> from dvadmin . utils . viewset import CustomModelViewSet <EOL> class LoginLogSerializer ( CustomModelSerializer ) : <EOL> class Meta : <EOL> model = LoginLog <EOL> fields = \"<STR_LIT>\" <EOL> read_only_fields = [ \"<STR_LIT>\" ] <EOL> class LoginLogViewSet ( CustomModelViewSet ) : <EOL> queryset = LoginLog . objects . all ( ) <EOL> serializer_class = LoginLogSerializer <EOL> ", "gt": "extra_filter_class = [ ]"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> ", "gt": "id = getattr ( path_parameters , \"<STR_LIT>\" , None )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) , <EOL> channels_str ) <EOL> channels_j = json . loads ( channels_str ) <EOL> channels = [ ] <EOL> for li in channels_j : <EOL> name = li [ '<STR_LIT>' ] <EOL> name_en = li [ '<STR_LIT>' ] <EOL> href = li [ '<STR_LIT>' ] if '<STR_LIT>' in li else '<STR_LIT>' <EOL> id = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : name_en , <EOL> '<STR_LIT>' : [ id ] , <EOL> '<STR_LIT>' : href , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } <EOL> ", "gt": "channels . append ( channel )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> text_splitting = _get_text_splitter_config ( <EOL> backend_alias = alias , <EOL> config = backend_dict . get ( \"<STR_LIT>\" , { } ) , <EOL> ) <EOL> config = ai_backend_cls . config_cls . from_settings ( <EOL> backend_settings , <EOL> text_splitter_class = text_splitting . splitter_class , <EOL> text_splitter_length_calculator_class = text_splitting . splitter_length_calculator_class , <EOL> ) <EOL> return ai_backend_cls ( config = config ) <EOL> class BackendNotFound ( Exception ) : <EOL> pass <EOL> def get_backend ( feature : BackendFeature = BackendFeature . TEXT_COMPLETION ) -> AIBackend : <EOL> match feature : <EOL> case BackendFeature . TEXT_COMPLETION : <EOL> alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> case BackendFeature . IMAGE_DESCRIPTION : <EOL> alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" ) <EOL> case _ : <EOL> alias = None <EOL> if alias is None : <EOL> ", "gt": "raise BackendNotFound ( f\"<STR_LIT>\" )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) ,"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> markdown_sources = f\"<STR_LIT>\" <EOL> else : <EOL> markdown_sources = \"<STR_LIT>\" <EOL> formatted_response = f\"<STR_LIT>\" <EOL> await self . send ( json . dumps ( { \"<STR_LIT>\" : formatted_response } , indent = <NUM_LIT> ) ) <EOL> else : <EOL> ", "gt": "await self . send ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> ", "gt": "predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ", "gt": "( \"<STR_LIT>\" , \"<STR_LIT>\" ) ,"}
{"input": "from django . contrib import admin <EOL> from django . urls import include <EOL> from django . urls import path <EOL> from django . urls import reverse_lazy <EOL> from django . views . generic . base import RedirectView <EOL> urlpatterns = [ <EOL> ", "gt": "path ( \"<STR_LIT>\" , admin . site . urls ) ,"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> ", "gt": "yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> } , <EOL> } , <EOL> ] <EOL> DATABASES = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : BASE_DIR / \"<STR_LIT>\" , <EOL> } <EOL> ", "gt": "}"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> ", "gt": "if self . model_view . query_parameters"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> ", "gt": "def clean_prompt ( self ) :"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> ", "gt": "text = PromptTextField ( )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> ", "gt": "self . view_test_manager . test_view_ok ("}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> EVENTS_RETENTION_DAYS = <NUM_LIT> , <EOL> ) <EOL> ) <EOL> def test_clear_webhook_events ( ) : <EOL> now = timezone . now ( ) <EOL> retained_event = WebhookEventFactory ( ) <EOL> older_event = WebhookEventFactory ( ) <EOL> WebhookEvent . objects . filter ( id = older_event . id ) . update ( <EOL> created = now - timedelta ( days = <NUM_LIT> ) <EOL> ) <EOL> ", "gt": "clear_webhook_events . delay ( )"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ( '<STR_LIT>' , limit ) , <EOL> ", "gt": "( '<STR_LIT>' , total ) ,"}
{"input": "from django . db import models <EOL> from django . utils import timezone <EOL> class Product ( models . Model ) : <EOL> name = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> description = models . TextField ( ) <EOL> price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> ) <EOL> sku = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> created_at = models . DateTimeField ( default = timezone . now ) <EOL> @ property <EOL> ", "gt": "def slug ( self ) :"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> ", "gt": "else :"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> EVENTS_RETENTION_DAYS = <NUM_LIT> , <EOL> ) <EOL> ) <EOL> def test_clear_webhook_events ( ) : <EOL> now = timezone . now ( ) <EOL> ", "gt": "retained_event = WebhookEventFactory ( )"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> ", "gt": "with open ( data_file ) as fin :"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] , <EOL> max_length = <NUM_LIT> , <EOL> ) , <EOL> ", "gt": ") ,"}
{"input": "import django . contrib . postgres . fields <EOL> from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = django . contrib . postgres . fields . ArrayField ( <EOL> base_field = models . CharField ( max_length = <NUM_LIT> ) , null = True , size = None <EOL> ) , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> ", "gt": "text_splitting = _get_text_splitter_config ("}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> options = { <EOL> \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> raise ValidationError ( <EOL> self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" <EOL> ) <EOL> return prompt_uuid <EOL> class DescribeImageApiForm ( ApiForm ) : <EOL> image_id = forms . CharField ( ) <EOL> ", "gt": "maxlength = forms . IntegerField ( required = False , min_value = <NUM_LIT> , max_value = <NUM_LIT> )"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_multiple_signatures ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] <EOL> ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> h = responses . calls [ <NUM_LIT> ] . request . headers <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ", "gt": "( \"<STR_LIT>\" , \"<STR_LIT>\" ) ,"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> ", "gt": "images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) )"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> ", "gt": "import summarizer . routing"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> ", "gt": "self . plt_train_qwk = [ ]"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> ", "gt": "\"<STR_LIT>\" : {"}
{"input": "from dvadmin . system . models import LoginLog <EOL> from dvadmin . utils . serializers import CustomModelSerializer <EOL> from dvadmin . utils . viewset import CustomModelViewSet <EOL> class LoginLogSerializer ( CustomModelSerializer ) : <EOL> class Meta : <EOL> model = LoginLog <EOL> fields = \"<STR_LIT>\" <EOL> ", "gt": "read_only_fields = [ \"<STR_LIT>\" ]"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> options = { <EOL> \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> if len ( line_vec ) == <NUM_LIT> : <EOL> ids . append ( line_vec [ <NUM_LIT> ] ) <EOL> if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) <EOL> texts . append ( line_vec [ <NUM_LIT> ] ) <EOL> labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) <EOL> else : <EOL> ", "gt": "ids . append ( str ( sample_index ) )"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ", "gt": "] ,"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> return images , image_masks , labels , labels_masks <EOL> ", "gt": "class Words :"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> ", "gt": "assert event . url == webhook . url"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_multiple_signatures ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] <EOL> ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> ", "gt": "h = responses . calls [ <NUM_LIT> ] . request . headers"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> return images , image_masks , labels , labels_masks <EOL> class Words : <EOL> def __init__ ( self , words_path ) : <EOL> with open ( words_path ) as f : <EOL> words = f . readlines ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> self . words_dict = { words [ i ] . strip ( ) : i for i in range ( len ( words ) ) } <EOL> self . words_index_dict = { i : words [ i ] . strip ( ) for i in range ( len ( words ) ) } <EOL> def __len__ ( self ) : <EOL> return len ( self . words_dict ) <EOL> def encode ( self , labels ) : <EOL> ", "gt": "label_index = [ self . words_dict [ item ] for item in labels ]"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> ", "gt": "for c in s :"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> ", "gt": "yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> )"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> ", "gt": "USE_CACHE = False ,"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> ", "gt": "epgs . append ( epg )"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> def expect_modal_closed ( page : Page ) : <EOL> expect ( search_box ( page ) ) . not_to_be_visible ( ) <EOL> def open_modal ( page : Page , with_keyboard : bool = False ) : <EOL> ", "gt": "if with_keyboard :"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> ", "gt": "{"}
{"input": "from typing import List , Optional , Union <EOL> from ninja_crud . testing . core . components import utils <EOL> class Headers : <EOL> def __init__ ( <EOL> self , <EOL> ok : Union [ dict , List [ dict ] ] , <EOL> forbidden : Union [ dict , List [ dict ] , None ] = None , <EOL> unauthorized : Union [ dict , List [ dict ] , None ] = None , <EOL> ) -> None : <EOL> self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok ) <EOL> self . forbidden : Optional [ List [ dict ] ] = ( <EOL> utils . ensure_list_of_dicts ( forbidden ) if forbidden is not None else None <EOL> ) <EOL> self . unauthorized : Optional [ List [ dict ] ] = ( <EOL> utils . ensure_list_of_dicts ( unauthorized ) <EOL> if unauthorized is not None <EOL> else None <EOL> ", "gt": ")"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> ", "gt": "max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT>"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ", "gt": ")"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> ", "gt": "model_name = '<STR_LIT>' ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> ", "gt": "payload : dict ,"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] , <EOL> max_length = <NUM_LIT> , <EOL> ) , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> ", "gt": "obj_w = <NUM_LIT> * radius"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "if self . index is not None :"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> ", "gt": "tokens . append ( \"<STR_LIT>\" )"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> ", "gt": "query_str = text_data_json [ \"<STR_LIT>\" ]"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> ", "gt": "desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>'"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions <EOL> predictions = predictions . detach ( ) . numpy ( ) <EOL> for i in range ( len ( predictions ) ) : <EOL> if predictions [ i ] < <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> elif predictions [ i ] > <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> return predictions <EOL> if __name__ == '<STR_LIT>' : <EOL> ", "gt": "model = model ( )"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> initial = True <EOL> dependencies = [ <EOL> ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = '<STR_LIT>' , <EOL> fields = [ <EOL> ( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> , unique = True ) ) , <EOL> ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT> ) ) , <EOL> ", "gt": "( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> ) ) ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> ", "gt": "self ,"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> ", "gt": "re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) , <EOL> channels_str ) <EOL> channels_j = json . loads ( channels_str ) <EOL> channels = [ ] <EOL> for li in channels_j : <EOL> ", "gt": "name = li [ '<STR_LIT>' ]"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> ", "gt": "results_list . append ( str ( val ) )"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ", "gt": ")"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> ", "gt": "query_parameters_list = query_parameters . ok ,"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> def expect_modal_closed ( page : Page ) : <EOL> expect ( search_box ( page ) ) . not_to_be_visible ( ) <EOL> def open_modal ( page : Page , with_keyboard : bool = False ) : <EOL> if with_keyboard : <EOL> page . keyboard . press ( \"<STR_LIT>\" ) <EOL> else : <EOL> ", "gt": "search_button ( page ) . click ( )"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> ", "gt": "if len ( line_vec ) == <NUM_LIT> :"}
{"input": "from typing import List , Optional , Union <EOL> from ninja_crud . testing . core . components import utils <EOL> class Headers : <EOL> def __init__ ( <EOL> self , <EOL> ok : Union [ dict , List [ dict ] ] , <EOL> forbidden : Union [ dict , List [ dict ] , None ] = None , <EOL> unauthorized : Union [ dict , List [ dict ] , None ] = None , <EOL> ) -> None : <EOL> self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok ) <EOL> self . forbidden : Optional [ List [ dict ] ] = ( <EOL> utils . ensure_list_of_dicts ( forbidden ) if forbidden is not None else None <EOL> ) <EOL> self . unauthorized : Optional [ List [ dict ] ] = ( <EOL> utils . ensure_list_of_dicts ( unauthorized ) <EOL> if unauthorized is not None <EOL> ", "gt": "else None"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> ", "gt": "type = j [ '<STR_LIT>' ]"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> return images , image_masks , labels , labels_masks <EOL> class Words : <EOL> def __init__ ( self , words_path ) : <EOL> with open ( words_path ) as f : <EOL> words = f . readlines ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> self . words_dict = { words [ i ] . strip ( ) : i for i in range ( len ( words ) ) } <EOL> self . words_index_dict = { i : words [ i ] . strip ( ) for i in range ( len ( words ) ) } <EOL> ", "gt": "def __len__ ( self ) :"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ) , <EOL> } , <EOL> ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ) <EOL> list_display = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> ", "gt": "search_fields = [ \"<STR_LIT>\" ]"}
{"input": "from django . db import models <EOL> from django . utils import timezone <EOL> class Product ( models . Model ) : <EOL> name = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> description = models . TextField ( ) <EOL> ", "gt": "price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> ", "gt": "pass"}
{"input": "import django . contrib . postgres . fields <EOL> from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> ", "gt": "model_name = \"<STR_LIT>\" ,"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def export_data ( self , request : Request , * args , ** kwargs ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . export_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws = wb . active <EOL> header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] <EOL> hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ] <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( self . export_field_label ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> ", "gt": "else :"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> ", "gt": "input_type_ids . append ( <NUM_LIT> )"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> ", "gt": "User . objects . create ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> ", "gt": "self . bert_regression_by_word_document . eval ( )"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> ", "gt": "run_in_shell ( insert_a_post , eval_result = False )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> ", "gt": "if starttime . date ( ) < dt :"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) <EOL> list_tags = ListModelView ( <EOL> ", "gt": "path = \"<STR_LIT>\" ,"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] , <EOL> max_length = <NUM_LIT> , <EOL> ", "gt": ") ,"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> ", "gt": "batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ("}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> ", "gt": "model_name = '<STR_LIT>' ,"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> ", "gt": "labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) )"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> ", "gt": "migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\""}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> EVENTS_RETENTION_DAYS = <NUM_LIT> , <EOL> ) <EOL> ) <EOL> ", "gt": "def test_clear_webhook_events ( ) :"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> ", "gt": "raise ImproperlyConfigured ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> ", "gt": "self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] )"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> ", "gt": "makemigrations ( )"}
{"input": "from django . contrib import admin <EOL> from django . forms import BooleanField <EOL> from django . forms . widgets import CheckboxInput <EOL> from . models import ApiKey <EOL> @ admin . register ( ApiKey ) <EOL> class ApiKeyAdmin ( admin . ModelAdmin ) : <EOL> list_display = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "formfield_overrides = {"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> ", "gt": "assert len ( responses . calls ) == <NUM_LIT>"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> ", "gt": "webhook = WebhookFactory ( active = False )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_forbidden ( self ) : <EOL> self . view_test_manager . test_view_headers_forbidden ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_path_parameters_not_found ( self ) : <EOL> ", "gt": "self . view_test_manager . test_view_path_parameters_not_found ("}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> ", "gt": "def get_channels_4gtv ( ) :"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> ", "gt": "if key == h_item :"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ", "gt": ")"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> } , <EOL> } , <EOL> ] <EOL> DATABASES = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : BASE_DIR / \"<STR_LIT>\" , <EOL> ", "gt": "}"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> ", "gt": "return False"}
{"input": "from django . db import models <EOL> from django . utils import timezone <EOL> class Product ( models . Model ) : <EOL> name = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> description = models . TextField ( ) <EOL> price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> ) <EOL> sku = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> created_at = models . DateTimeField ( default = timezone . now ) <EOL> @ property <EOL> def slug ( self ) : <EOL> ", "gt": "return self . name . lower ( ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" )"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> ", "gt": "expect ( search_box ( page ) ) . to_be_visible ( )"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> ", "gt": "def export_data ( self , request : Request , * args , ** kwargs ) :"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> ", "gt": "parser = argparse . ArgumentParser ( )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_forbidden ( self ) : <EOL> self . view_test_manager . test_view_headers_forbidden ( <EOL> test_case = self . model_viewset_test_case , <EOL> ", "gt": "on_completion = self . on_failed_request ,"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> ", "gt": "try :"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> ", "gt": "migrations . RemoveField ("}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> if len ( line_vec ) == <NUM_LIT> : <EOL> ids . append ( line_vec [ <NUM_LIT> ] ) <EOL> ", "gt": "if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len :"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> ", "gt": "for line in fin :"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . FORBIDDEN , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . forbidden is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . forbidden , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> ", "gt": "test_case , on_completion = on_completion , status = status"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> ", "gt": "if c >= '<STR_LIT>' and c <= '<STR_LIT>' :"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> ", "gt": "def search_button ( page : Page ) -> Locator :"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( max_length = <NUM_LIT> , null = True ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> ", "gt": "field = models . DateTimeField ( blank = True , null = True ) ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> ", "gt": "get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) ,"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> ", "gt": "max_length = <NUM_LIT> ,"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> def expect_modal_closed ( page : Page ) : <EOL> expect ( search_box ( page ) ) . not_to_be_visible ( ) <EOL> def open_modal ( page : Page , with_keyboard : bool = False ) : <EOL> if with_keyboard : <EOL> ", "gt": "page . keyboard . press ( \"<STR_LIT>\" )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> ", "gt": "try :"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> ", "gt": "for j in js :"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> ", "gt": "def test_multiple_signatures ( responses ) :"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ", "gt": "ret = {"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> } , <EOL> } , <EOL> ] <EOL> DATABASES = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : BASE_DIR / \"<STR_LIT>\" ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> ", "gt": "test_case . skipTest ( reason = \"<STR_LIT>\" )"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> ", "gt": "\"<STR_LIT>\" : True ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> ", "gt": "is_next = self . page . has_next ( ) if self . page else False"}
{"input": "from typing import List , Optional , Union <EOL> from ninja_crud . testing . core . components import utils <EOL> class Headers : <EOL> def __init__ ( <EOL> self , <EOL> ok : Union [ dict , List [ dict ] ] , <EOL> forbidden : Union [ dict , List [ dict ] , None ] = None , <EOL> unauthorized : Union [ dict , List [ dict ] , None ] = None , <EOL> ) -> None : <EOL> self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok ) <EOL> self . forbidden : Optional [ List [ dict ] ] = ( <EOL> ", "gt": "utils . ensure_list_of_dicts ( forbidden ) if forbidden is not None else None"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : [ gtvid ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : logo , <EOL> '<STR_LIT>' : desc , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ", "gt": "EVENTS_RETENTION_DAYS = <NUM_LIT> ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . FORBIDDEN , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . forbidden is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . forbidden , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_path_parameters_not_found ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . NOT_FOUND , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if path_parameters . not_found is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . not_found , <EOL> ", "gt": "query_parameters_list = query_parameters . ok ,"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ", "gt": ")"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> options = { <EOL> \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> ", "gt": "bases = ( models . Model , ) ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> ", "gt": "default_response_body = ItemOut"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> ", "gt": "query_parameters_list = query_parameters . ok ,"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> ", "gt": "length = <NUM_LIT>"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ", "gt": "ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> ", "gt": "with open ( img_path , \"<STR_LIT>\" ) as fh :"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> ", "gt": "images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ]"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> ", "gt": "msg = '<STR_LIT>' % ( spidername , e )"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> ", "gt": "yolo_obj_list = [ ]"}
{"input": "from typing import List , Optional , Union <EOL> from ninja_crud . testing . core . components import utils <EOL> class Headers : <EOL> def __init__ ( <EOL> self , <EOL> ok : Union [ dict , List [ dict ] ] , <EOL> forbidden : Union [ dict , List [ dict ] , None ] = None , <EOL> unauthorized : Union [ dict , List [ dict ] , None ] = None , <EOL> ) -> None : <EOL> ", "gt": "self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = float , nargs = '<STR_LIT>' , default = None , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , nargs = '<STR_LIT>' , default = None , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , <EOL> default = '<STR_LIT>' , help = '<STR_LIT>' ) <EOL> args = parser . parse_args ( sys . argv [ <NUM_LIT> : ] ) <EOL> convertor = Labelme2YOLO ( args . json_dir ) <EOL> if args . json_name is None : <EOL> convertor . convert ( val_size = args . val_size ) <EOL> else : <EOL> ", "gt": "convertor . convert_one ( args . json_name )"}
{"input": "from django . contrib import admin <EOL> from django . urls import include <EOL> from django . urls import path <EOL> from django . urls import reverse_lazy <EOL> from django . views . generic . base import RedirectView <EOL> ", "gt": "urlpatterns = ["}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> with self . assertRaises ( ValueError ) : <EOL> utils . ensure_list_of_dicts ( data = data ) <EOL> def test_ensure_list_of_dicts_not_list_or_dict ( self ) : <EOL> data = <NUM_LIT> <EOL> ", "gt": "with self . assertRaises ( TypeError ) :"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> ", "gt": "with self . assertRaises ( ValueError ) :"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> { <EOL> \"<STR_LIT>\" : django_asgi_app , <EOL> \"<STR_LIT>\" : AllowedHostsOriginValidator ( <EOL> AuthMiddlewareStack ( URLRouter ( websocket_urlpatterns ) ) <EOL> ", "gt": ") ,"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> ", "gt": "f'<STR_LIT>'"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> field = models . CharField ( blank = True , max_length = <NUM_LIT> ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> ", "gt": "model_name = '<STR_LIT>' ,"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> ", "gt": "def _get_yolo_object_list ( self , json_data , img_path ) :"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ( '<STR_LIT>' , limit ) , <EOL> ( '<STR_LIT>' , total ) , <EOL> ( '<STR_LIT>' , is_next ) , <EOL> ( '<STR_LIT>' , is_previous ) , <EOL> ( '<STR_LIT>' , data ) <EOL> ", "gt": "] ) )"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> ", "gt": "allow_blank = True ,"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ", "gt": "ids = [ ]"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> ", "gt": "self . assertEqual ( result , data )"}
{"input": "from . crud import InstallCrudUtils <EOL> from . crud import ModelCRUD <EOL> from . htmx import Htmx <EOL> from . htmx_extension import HtmxExtension <EOL> from . reset_migrations import ResetMigrations <EOL> from . rm_migrations import RmMigrations <EOL> from . start_app import StartApp <EOL> from . start_project import StartProject <EOL> ", "gt": "from . sync_dotenv import SyncDotenv"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> ", "gt": "proper_items . append ( item )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> ", "gt": "list_items = ListModelView ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> ", "gt": "p = configparser . ConfigParser ( )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> text_splitting = _get_text_splitter_config ( <EOL> backend_alias = alias , <EOL> config = backend_dict . get ( \"<STR_LIT>\" , { } ) , <EOL> ) <EOL> config = ai_backend_cls . config_cls . from_settings ( <EOL> ", "gt": "backend_settings ,"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions <EOL> predictions = predictions . detach ( ) . numpy ( ) <EOL> for i in range ( len ( predictions ) ) : <EOL> if predictions [ i ] < <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> elif predictions [ i ] > <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> return predictions <EOL> if __name__ == '<STR_LIT>' : <EOL> model = model ( ) <EOL> text = <EOL> ", "gt": "valdata = [ '<STR_LIT>' , '<STR_LIT>' , text ]"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> ", "gt": "image_masks [ i ] [ : , : h , : w ] = <NUM_LIT>"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> ", "gt": "name = '<STR_LIT>' ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> ", "gt": "read_model = lambda request , path_parameters , _ : Item . objects . get ("}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> ", "gt": "splitter_class : type [ TextSplitterProtocol ]"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> ", "gt": "return min_score"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ", "gt": ") ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ", "gt": "headers : dict ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . FORBIDDEN , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . forbidden is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . forbidden , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_path_parameters_not_found ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . NOT_FOUND , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if path_parameters . not_found is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> ", "gt": "self . run_combinatorial_tests ("}
{"input": "import os <EOL> import sys <EOL> def main ( ) : <EOL> os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> try : <EOL> from django . core . management import execute_from_command_line <EOL> except ImportError as exc : <EOL> raise ImportError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) from exc <EOL> ", "gt": "execute_from_command_line ( sys . argv )"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( max_length = <NUM_LIT> , null = True ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> ", "gt": "name = \"<STR_LIT>\" ,"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> { <EOL> \"<STR_LIT>\" : django_asgi_app , <EOL> \"<STR_LIT>\" : AllowedHostsOriginValidator ( <EOL> AuthMiddlewareStack ( URLRouter ( websocket_urlpatterns ) ) <EOL> ) , <EOL> } <EOL> ", "gt": ")"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> ", "gt": "input_ids = tokenizer . convert_tokens_to_ids ( tokens )"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ", "gt": "last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> ", "gt": "@ django . test . tag ( \"<STR_LIT>\" )"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ", "gt": "( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) ,"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> ", "gt": "train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ( '<STR_LIT>' , limit ) , <EOL> ( '<STR_LIT>' , total ) , <EOL> ( '<STR_LIT>' , is_next ) , <EOL> ( '<STR_LIT>' , is_previous ) , <EOL> ", "gt": "( '<STR_LIT>' , data )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) , <EOL> channels_str ) <EOL> channels_j = json . loads ( channels_str ) <EOL> channels = [ ] <EOL> for li in channels_j : <EOL> name = li [ '<STR_LIT>' ] <EOL> name_en = li [ '<STR_LIT>' ] <EOL> href = li [ '<STR_LIT>' ] if '<STR_LIT>' in li else '<STR_LIT>' <EOL> id = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : name_en , <EOL> '<STR_LIT>' : [ id ] , <EOL> '<STR_LIT>' : href , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> ", "gt": "migrations . RemoveField ("}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> ", "gt": "proper_items = [ ]"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> ", "gt": "total = self . page . paginator . count if self . page else <NUM_LIT>"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> ", "gt": "num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True )"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> ", "gt": "name = cht_to_chs ( j [ '<STR_LIT>' ] )"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ", "gt": "field = models . BinaryField ( editable = True ) ,"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ) , <EOL> } , <EOL> ) , <EOL> ", "gt": "( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) ,"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ) , <EOL> ", "gt": "} ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_forbidden ( self ) : <EOL> self . view_test_manager . test_view_headers_forbidden ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> ", "gt": "@ django . test . tag ( \"<STR_LIT>\" )"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> run_in_shell ( insert_a_post , eval_result = False ) <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> assert count == <NUM_LIT> <EOL> assert count == <NUM_LIT> <EOL> change_model_attribute ( django_project ) <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> assert count_migrations ( django_project ) == <NUM_LIT> <EOL> runner . invoke ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> assert count_migrations ( django_project ) == <NUM_LIT> <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> ", "gt": "assert count == <NUM_LIT>"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> ", "gt": "status = http . HTTPStatus . OK ,"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> while len ( input_ids ) < max_input_length : <EOL> input_ids . append ( <NUM_LIT> ) <EOL> ", "gt": "input_type_ids . append ( <NUM_LIT> )"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = '<STR_LIT>' , <EOL> ", "gt": "name = '<STR_LIT>' ,"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> def expect_modal_closed ( page : Page ) : <EOL> expect ( search_box ( page ) ) . not_to_be_visible ( ) <EOL> ", "gt": "def open_modal ( page : Page , with_keyboard : bool = False ) :"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ", "gt": ") ,"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "} )"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> ", "gt": "name = '<STR_LIT>' ,"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> ", "gt": "obj_h = <NUM_LIT> * radius"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> ", "gt": "else :"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = float , nargs = '<STR_LIT>' , default = None , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , nargs = '<STR_LIT>' , default = None , <EOL> ", "gt": "help = '<STR_LIT>' )"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> ", "gt": "raise ValidationError ("}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = float , nargs = '<STR_LIT>' , default = None , <EOL> ", "gt": "help = '<STR_LIT>' )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> ", "gt": "type [ TextSplitterLengthCalculatorProtocol ] ,"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions <EOL> predictions = predictions . detach ( ) . numpy ( ) <EOL> for i in range ( len ( predictions ) ) : <EOL> if predictions [ i ] < <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> ", "gt": "elif predictions [ i ] > <NUM_LIT> :"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> ", "gt": "test_case : TestCase ,"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> run_in_shell ( insert_a_post , eval_result = False ) <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> assert count == <NUM_LIT> <EOL> assert count == <NUM_LIT> <EOL> change_model_attribute ( django_project ) <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> ", "gt": "assert count_migrations ( django_project ) == <NUM_LIT>"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ", "gt": ")"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> ", "gt": "if length_calculator_class_path is not None :"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> ", "gt": "header_data . append ( value . get ( \"<STR_LIT>\" ) )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> ", "gt": "update_item = UpdateModelView ("}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> ", "gt": "self . plt_test_qwk = [ ]"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> ", "gt": "on_completion = self . wrap_completion_with_status_check ("}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : ("}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> ", "gt": "continue"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def export_data ( self , request : Request , * args , ** kwargs ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . export_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws = wb . active <EOL> header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] <EOL> hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ] <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( self . export_field_label ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> ", "gt": "results_list . append ( \"<STR_LIT>\" )"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> EVENTS_RETENTION_DAYS = <NUM_LIT> , <EOL> ) <EOL> ) <EOL> def test_clear_webhook_events ( ) : <EOL> now = timezone . now ( ) <EOL> retained_event = WebhookEventFactory ( ) <EOL> ", "gt": "older_event = WebhookEventFactory ( )"}
{"input": "from . crud import InstallCrudUtils <EOL> from . crud import ModelCRUD <EOL> from . htmx import Htmx <EOL> from . htmx_extension import HtmxExtension <EOL> from . reset_migrations import ResetMigrations <EOL> from . rm_migrations import RmMigrations <EOL> from . start_app import StartApp <EOL> from . start_project import StartProject <EOL> from . sync_dotenv import SyncDotenv <EOL> ", "gt": "from . work import Work"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> run_in_shell ( insert_a_post , eval_result = False ) <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> assert count == <NUM_LIT> <EOL> assert count == <NUM_LIT> <EOL> change_model_attribute ( django_project ) <EOL> makemigrations ( ) <EOL> ", "gt": "migrate ( )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) <EOL> list_tags = ListModelView ( <EOL> path = \"<STR_LIT>\" , <EOL> get_queryset = lambda request , path_parameters : Tag . objects . filter ( <EOL> items__id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> response_body = List [ TagOut ] , <EOL> ", "gt": ")"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> ", "gt": "'<STR_LIT>' : <NUM_LIT> ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> ", "gt": "name = '<STR_LIT>' ,"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> ", "gt": "operations = ["}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> ", "gt": "yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> )"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> ", "gt": "code = <NUM_LIT>"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def export_data ( self , request : Request , * args , ** kwargs ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . export_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws = wb . active <EOL> header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] <EOL> hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ] <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( self . export_field_label ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> else : <EOL> results_list . append ( val ) <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> ", "gt": "showRowStripes = True ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> ", "gt": "return Response ( OrderedDict ( ["}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> markdown_sources = f\"<STR_LIT>\" <EOL> else : <EOL> markdown_sources = \"<STR_LIT>\" <EOL> formatted_response = f\"<STR_LIT>\" <EOL> await self . send ( json . dumps ( { \"<STR_LIT>\" : formatted_response } , indent = <NUM_LIT> ) ) <EOL> else : <EOL> await self . send ( <EOL> json . dumps ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , indent = <NUM_LIT> ) <EOL> ", "gt": ")"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> ", "gt": "name = '<STR_LIT>' ,"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> ", "gt": "migrate ( )"}
{"input": "from dvadmin . system . models import LoginLog <EOL> from dvadmin . utils . serializers import CustomModelSerializer <EOL> from dvadmin . utils . viewset import CustomModelViewSet <EOL> class LoginLogSerializer ( CustomModelSerializer ) : <EOL> class Meta : <EOL> model = LoginLog <EOL> ", "gt": "fields = \"<STR_LIT>\""}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ( '<STR_LIT>' , limit ) , <EOL> ( '<STR_LIT>' , total ) , <EOL> ( '<STR_LIT>' , is_next ) , <EOL> ", "gt": "( '<STR_LIT>' , is_previous ) ,"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> ", "gt": "def test_does_not_create_events_when_disabled ( responses ) :"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions <EOL> predictions = predictions . detach ( ) . numpy ( ) <EOL> for i in range ( len ( predictions ) ) : <EOL> ", "gt": "if predictions [ i ] < <NUM_LIT> :"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : [ gtvid ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : logo , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "}"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : [ gtvid ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : logo , <EOL> ", "gt": "'<STR_LIT>' : desc ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_forbidden ( self ) : <EOL> self . view_test_manager . test_view_headers_forbidden ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_path_parameters_not_found ( self ) : <EOL> self . view_test_manager . test_view_path_parameters_not_found ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ", "gt": ")"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> ", "gt": "def getscore ( self , valdata ) :"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> ", "gt": "channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( max_length = <NUM_LIT> , null = True ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . DateTimeField ( blank = True , null = True ) , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> ", "gt": "migrations . AlterField ("}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> ", "gt": "else :"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> field = models . BinaryField ( editable = True ) , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> markdown_sources = f\"<STR_LIT>\" <EOL> else : <EOL> markdown_sources = \"<STR_LIT>\" <EOL> formatted_response = f\"<STR_LIT>\" <EOL> await self . send ( json . dumps ( { \"<STR_LIT>\" : formatted_response } , indent = <NUM_LIT> ) ) <EOL> else : <EOL> await self . send ( <EOL> ", "gt": "json . dumps ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , indent = <NUM_LIT> )"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> ", "gt": "if not data :"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] , <EOL> ", "gt": "max_length = <NUM_LIT> ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> ", "gt": "prompt = PromptUUIDField ( )"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> ", "gt": "self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) )"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> options = { <EOL> \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "} ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> raise ValidationError ( <EOL> self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" <EOL> ) <EOL> return prompt_uuid <EOL> class DescribeImageApiForm ( ApiForm ) : <EOL> ", "gt": "image_id = forms . CharField ( )"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> if len ( line_vec ) == <NUM_LIT> : <EOL> ids . append ( line_vec [ <NUM_LIT> ] ) <EOL> if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) <EOL> texts . append ( line_vec [ <NUM_LIT> ] ) <EOL> labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) <EOL> else : <EOL> ids . append ( str ( sample_index ) ) <EOL> sample_index += <NUM_LIT> <EOL> if is_zh ( line_vec [ <NUM_LIT> ] ) and len ( line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] <EOL> elif len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) <EOL> ", "gt": "texts . append ( line_vec [ <NUM_LIT> ] )"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> ", "gt": "return max_score"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> ", "gt": "assert event . topic == \"<STR_LIT>\""}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ", "gt": "( '<STR_LIT>' , msg ) ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> ", "gt": "def test_view_payloads_conflict ("}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> while len ( input_ids ) < max_input_length : <EOL> input_ids . append ( <NUM_LIT> ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> attention_masks . append ( <NUM_LIT> ) <EOL> output [ doc_index ] [ seq_index ] = torch . cat ( ( torch . LongTensor ( input_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> torch . LongTensor ( input_type_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> ", "gt": "torch . LongTensor ( attention_masks ) . unsqueeze ( <NUM_LIT> ) ) ,"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> markdown_response = f\"<STR_LIT>\" <EOL> if response . source_nodes : <EOL> ", "gt": "markdown_sources = f\"<STR_LIT>\""}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> ", "gt": "all = [ name , gtvid , logo ]"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : <EOL> batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> with autocast ( ) : <EOL> batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , <EOL> device = self . args [ '<STR_LIT>' ] ) <EOL> batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) <EOL> batch_predictions = batch_doctok_predictions <EOL> if len ( batch_predictions . shape ) == <NUM_LIT> : <EOL> batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) <EOL> predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions <EOL> predictions = predictions . detach ( ) . numpy ( ) <EOL> for i in range ( len ( predictions ) ) : <EOL> if predictions [ i ] < <NUM_LIT> : <EOL> predictions [ i ] = <NUM_LIT> <EOL> elif predictions [ i ] > <NUM_LIT> : <EOL> ", "gt": "predictions [ i ] = <NUM_LIT>"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> ", "gt": "responses . post ( webhook . url )"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> ", "gt": "p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' )"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ", "gt": "( '<STR_LIT>' , limit ) ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> ", "gt": "if prompt_uuid . version != <NUM_LIT> :"}
{"input": "import os <EOL> import sys <EOL> def main ( ) : <EOL> os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> try : <EOL> from django . core . management import execute_from_command_line <EOL> except ImportError as exc : <EOL> raise ImportError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> ", "gt": "@ override_settings ("}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = float , nargs = '<STR_LIT>' , default = None , <EOL> help = '<STR_LIT>' ) <EOL> ", "gt": "parser . add_argument ( '<STR_LIT>' , type = str , nargs = '<STR_LIT>' , default = None ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ", "gt": ") ,"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> ", "gt": "data : List [ dict ] = [ ]"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> ", "gt": "test_case = self . model_viewset_test_case ,"}
{"input": "import django . contrib . postgres . fields <EOL> from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = django . contrib . postgres . fields . ArrayField ( <EOL> base_field = models . CharField ( max_length = <NUM_LIT> ) , null = True , size = None <EOL> ) , <EOL> ", "gt": ") ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) <EOL> list_tags = ListModelView ( <EOL> path = \"<STR_LIT>\" , <EOL> get_queryset = lambda request , path_parameters : Tag . objects . filter ( <EOL> items__id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> response_body = List [ TagOut ] , <EOL> ) <EOL> ", "gt": "ItemViewSet . register_routes ( router )"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> raise ValidationError ( <EOL> self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "return prompt_uuid"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> ", "gt": "while len ( input_ids ) < max_input_length :"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> initial = True <EOL> dependencies = [ <EOL> ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = '<STR_LIT>' , <EOL> fields = [ <EOL> ", "gt": "( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) ,"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> if len ( line_vec ) == <NUM_LIT> : <EOL> ids . append ( line_vec [ <NUM_LIT> ] ) <EOL> if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> ", "gt": "line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] )"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> ", "gt": "img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from dvadmin . system . models import LoginLog <EOL> from dvadmin . utils . serializers import CustomModelSerializer <EOL> from dvadmin . utils . viewset import CustomModelViewSet <EOL> class LoginLogSerializer ( CustomModelSerializer ) : <EOL> class Meta : <EOL> model = LoginLog <EOL> fields = \"<STR_LIT>\" <EOL> read_only_fields = [ \"<STR_LIT>\" ] <EOL> class LoginLogViewSet ( CustomModelViewSet ) : <EOL> queryset = LoginLog . objects . all ( ) <EOL> ", "gt": "serializer_class = LoginLogSerializer"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "] ,"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> } , <EOL> } , <EOL> ] <EOL> DATABASES = { <EOL> ", "gt": "\"<STR_LIT>\" : {"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> ", "gt": "channels_str = res_re . group ( <NUM_LIT> )"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def export_data ( self , request : Request , * args , ** kwargs ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . export_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws = wb . active <EOL> header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] <EOL> hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ] <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( self . export_field_label ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ", "gt": "ws . append ( header_data )"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ", "gt": ")"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> ", "gt": "query_parameters_schema ,"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\""}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : [ gtvid ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : logo , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } <EOL> ", "gt": "channels . append ( channel )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> ", "gt": "f'<STR_LIT>'"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> ", "gt": "self . bert_regression_by_word_document = mainplm ( self . args )"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> ", "gt": "elif score > max_score :"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_multiple_signatures ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] <EOL> ) <EOL> ", "gt": "WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" )"}
{"input": "from django . contrib import admin <EOL> from django . urls import include <EOL> from django . urls import path <EOL> from django . urls import reverse_lazy <EOL> from django . views . generic . base import RedirectView <EOL> urlpatterns = [ <EOL> path ( \"<STR_LIT>\" , admin . site . urls ) , <EOL> path ( \"<STR_LIT>\" , RedirectView . as_view ( url = reverse_lazy ( \"<STR_LIT>\" ) ) ) , <EOL> ", "gt": "path ( \"<STR_LIT>\" , include ( \"<STR_LIT>\" , namespace = \"<STR_LIT>\" ) ) ,"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> ", "gt": "decorators = [ user_is_collection_creator ] ,"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> ", "gt": "if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> :"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ", "gt": ")"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> return images , image_masks , labels , labels_masks <EOL> class Words : <EOL> def __init__ ( self , words_path ) : <EOL> with open ( words_path ) as f : <EOL> words = f . readlines ( ) <EOL> ", "gt": "print ( f'<STR_LIT>' )"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ", "gt": "ws1 [ f\"<STR_LIT>\" ] = ele"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> ", "gt": "]"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> ", "gt": "rand_value = random . random ( )"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> ", "gt": "headers = self . get_headers ( test_case )"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> initial = True <EOL> dependencies = [ <EOL> ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = '<STR_LIT>' , <EOL> fields = [ <EOL> ( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> , unique = True ) ) , <EOL> ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT> ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> ) ) , <EOL> ( '<STR_LIT>' , models . BooleanField ( default = True ) ) , <EOL> ( '<STR_LIT>' , models . DateTimeField ( auto_now_add = True ) ) , <EOL> ] , <EOL> ", "gt": ") ,"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> ", "gt": "result = utils . ensure_list_of_dicts ( data = data )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) <EOL> list_tags = ListModelView ( <EOL> path = \"<STR_LIT>\" , <EOL> get_queryset = lambda request , path_parameters : Tag . objects . filter ( <EOL> items__id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> ", "gt": "response_body = List [ TagOut ] ,"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_multiple_signatures ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] <EOL> ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> h = responses . calls [ <NUM_LIT> ] . request . headers <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> ", "gt": "== \"<STR_LIT>\""}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> ", "gt": "def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) :"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> ", "gt": "data = [ ]"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> return images , image_masks , labels , labels_masks <EOL> class Words : <EOL> def __init__ ( self , words_path ) : <EOL> with open ( words_path ) as f : <EOL> words = f . readlines ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> self . words_dict = { words [ i ] . strip ( ) : i for i in range ( len ( words ) ) } <EOL> self . words_index_dict = { i : words [ i ] . strip ( ) for i in range ( len ( words ) ) } <EOL> def __len__ ( self ) : <EOL> return len ( self . words_dict ) <EOL> def encode ( self , labels ) : <EOL> label_index = [ self . words_dict [ item ] for item in labels ] <EOL> return label_index <EOL> def decode ( self , label_index ) : <EOL> ", "gt": "label = '<STR_LIT>' . join ( [ self . words_index_dict [ int ( item ) ] for item in label_index ] )"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> ", "gt": "self . plt_x = [ ]"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> run_in_shell ( insert_a_post , eval_result = False ) <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> assert count == <NUM_LIT> <EOL> assert count == <NUM_LIT> <EOL> change_model_attribute ( django_project ) <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> assert count_migrations ( django_project ) == <NUM_LIT> <EOL> runner . invoke ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> ", "gt": "assert count_migrations ( django_project ) == <NUM_LIT>"}
{"input": "import django . contrib . postgres . fields <EOL> from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> ", "gt": "migrations . AlterField ("}
{"input": "from . crud import InstallCrudUtils <EOL> from . crud import ModelCRUD <EOL> from . htmx import Htmx <EOL> from . htmx_extension import HtmxExtension <EOL> from . reset_migrations import ResetMigrations <EOL> from . rm_migrations import RmMigrations <EOL> ", "gt": "from . start_app import StartApp"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> ", "gt": "result_column_width = self . get_string_len ( val )"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> ", "gt": "{"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> ", "gt": "url = '<STR_LIT>'"}
{"input": "from django . contrib import admin <EOL> from django . forms import BooleanField <EOL> from django . forms . widgets import CheckboxInput <EOL> from . models import ApiKey <EOL> @ admin . register ( ApiKey ) <EOL> class ApiKeyAdmin ( admin . ModelAdmin ) : <EOL> ", "gt": "list_display = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> f'<STR_LIT>' ) <EOL> return train_loader , eval_loader <EOL> def collate_fn ( batch_images ) : <EOL> max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> proper_items = [ ] <EOL> for item in batch_images : <EOL> if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : <EOL> continue <EOL> max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height <EOL> max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width <EOL> max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length <EOL> proper_items . append ( item ) <EOL> images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) <EOL> labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) <EOL> for i in range ( len ( proper_items ) ) : <EOL> _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape <EOL> images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> <EOL> l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] <EOL> labels_masks [ i ] [ : l ] = <NUM_LIT> <EOL> ", "gt": "return images , image_masks , labels , labels_masks"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . CharField ( <EOL> max_length = <NUM_LIT> , <EOL> verbose_name = \"<STR_LIT>\" , <EOL> validators = [ _simple_domain_name_validator ] , <EOL> ) , <EOL> ) , <EOL> ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , <EOL> ] , <EOL> options = { <EOL> \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_forbidden ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . FORBIDDEN , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . forbidden is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . forbidden , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_path_parameters_not_found ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . NOT_FOUND , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if path_parameters . not_found is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . not_found , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> ", "gt": "payload_list = payloads . ok ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ", "gt": "field = models . CharField ( blank = True , max_length = <NUM_LIT> ) ,"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> ", "gt": "type [ TextSplitterProtocol ] , import_string ( splitter_class_path )"}
{"input": "import django . contrib . sites . models <EOL> from django . contrib . sites . models import _simple_domain_name_validator <EOL> from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = \"<STR_LIT>\" , <EOL> fields = [ <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> models . AutoField ( <EOL> verbose_name = \"<STR_LIT>\" , <EOL> serialize = False , <EOL> auto_created = True , <EOL> primary_key = True , <EOL> ) , <EOL> ) , <EOL> ( <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "models . CharField ("}
{"input": "from django . contrib import admin <EOL> from django . urls import include <EOL> from django . urls import path <EOL> from django . urls import reverse_lazy <EOL> from django . views . generic . base import RedirectView <EOL> urlpatterns = [ <EOL> path ( \"<STR_LIT>\" , admin . site . urls ) , <EOL> path ( \"<STR_LIT>\" , RedirectView . as_view ( url = reverse_lazy ( \"<STR_LIT>\" ) ) ) , <EOL> path ( \"<STR_LIT>\" , include ( \"<STR_LIT>\" , namespace = \"<STR_LIT>\" ) ) , <EOL> ", "gt": "]"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ", "gt": ")"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> while len ( input_ids ) < max_input_length : <EOL> ", "gt": "input_ids . append ( <NUM_LIT> )"}
{"input": "from django . contrib import admin <EOL> from django . urls import include <EOL> from django . urls import path <EOL> from django . urls import reverse_lazy <EOL> from django . views . generic . base import RedirectView <EOL> urlpatterns = [ <EOL> path ( \"<STR_LIT>\" , admin . site . urls ) , <EOL> ", "gt": "path ( \"<STR_LIT>\" , RedirectView . as_view ( url = reverse_lazy ( \"<STR_LIT>\" ) ) ) ,"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> with self . assertRaises ( ValueError ) : <EOL> utils . ensure_list_of_dicts ( data = data ) <EOL> ", "gt": "def test_ensure_list_of_dicts_not_list_or_dict ( self ) :"}
{"input": "from django . contrib import admin <EOL> from django . forms import BooleanField <EOL> from django . forms . widgets import CheckboxInput <EOL> from . models import ApiKey <EOL> @ admin . register ( ApiKey ) <EOL> ", "gt": "class ApiKeyAdmin ( admin . ModelAdmin ) :"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( max_length = <NUM_LIT> , null = True ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . DateTimeField ( blank = True , null = True ) , <EOL> ", "gt": ") ,"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> ", "gt": "__get_dist = lambda int_list : max ( int_list ) - min ( int_list )"}
{"input": "import os <EOL> import sys <EOL> def main ( ) : <EOL> os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> try : <EOL> from django . core . management import execute_from_command_line <EOL> except ImportError as exc : <EOL> raise ImportError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) from exc <EOL> execute_from_command_line ( sys . argv ) <EOL> ", "gt": "if __name__ == \"<STR_LIT>\" :"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> ", "gt": "def test_ensure_list_of_dicts_empty_list ( self ) :"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> ", "gt": "continue"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> ", "gt": "DJANGO_WEBHOOK = dict ("}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> with self . assertRaises ( ValueError ) : <EOL> utils . ensure_list_of_dicts ( data = data ) <EOL> def test_ensure_list_of_dicts_not_list_or_dict ( self ) : <EOL> data = <NUM_LIT> <EOL> with self . assertRaises ( TypeError ) : <EOL> ", "gt": "utils . ensure_list_of_dicts ( data = data )"}
{"input": "from dvadmin . system . models import LoginLog <EOL> from dvadmin . utils . serializers import CustomModelSerializer <EOL> from dvadmin . utils . viewset import CustomModelViewSet <EOL> class LoginLogSerializer ( CustomModelSerializer ) : <EOL> class Meta : <EOL> model = LoginLog <EOL> fields = \"<STR_LIT>\" <EOL> read_only_fields = [ \"<STR_LIT>\" ] <EOL> ", "gt": "class LoginLogViewSet ( CustomModelViewSet ) :"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_tvb ( ) : <EOL> url = '<STR_LIT>' <EOL> res = requests . get ( url ) <EOL> res_re = re . search ( '<STR_LIT>' , res . text ) <EOL> channels_str = res_re . group ( <NUM_LIT> ) <EOL> channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) , <EOL> ", "gt": "channels_str )"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> ", "gt": "self . request = request"}
{"input": "from . crud import InstallCrudUtils <EOL> from . crud import ModelCRUD <EOL> from . htmx import Htmx <EOL> from . htmx_extension import HtmxExtension <EOL> from . reset_migrations import ResetMigrations <EOL> from . rm_migrations import RmMigrations <EOL> from . start_app import StartApp <EOL> ", "gt": "from . start_project import StartProject"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_headers_unauthorized ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . UNAUTHORIZED , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if headers . unauthorized is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . unauthorized , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> ", "gt": "def test_view_headers_forbidden ("}
{"input": "from django . contrib import admin <EOL> from django . forms import BooleanField <EOL> from django . forms . widgets import CheckboxInput <EOL> from . models import ApiKey <EOL> @ admin . register ( ApiKey ) <EOL> class ApiKeyAdmin ( admin . ModelAdmin ) : <EOL> list_display = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> formfield_overrides = { <EOL> ", "gt": "BooleanField : { '<STR_LIT>' : CheckboxInput } ,"}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) <EOL> def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : <EOL> makemigrations ( ) <EOL> migrate ( ) <EOL> run_in_shell ( insert_a_post , eval_result = False ) <EOL> count = run_in_shell ( count_nbr_of_posts , eval_result = True ) <EOL> ", "gt": "assert count == <NUM_LIT>"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> ", "gt": "application = ProtocolTypeRouter ("}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> EVENTS_RETENTION_DAYS = <NUM_LIT> , <EOL> ) <EOL> ) <EOL> def test_clear_webhook_events ( ) : <EOL> ", "gt": "now = timezone . now ( )"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : [ \"<STR_LIT>\" ] ,"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> ", "gt": "code = <NUM_LIT>"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> ", "gt": "else :"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_forbidden ( self ) : <EOL> self . view_test_manager . test_view_headers_forbidden ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> ", "gt": "def test_read_model_path_parameters_not_found ( self ) :"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> ", "gt": "delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] )"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> ", "gt": "path_parameters = self . get_path_parameters ( test_case )"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> ", "gt": "read_item = ReadModelView ("}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ) , <EOL> } , <EOL> ", "gt": ") ,"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> field = models . CharField ( blank = True , max_length = <NUM_LIT> ) , <EOL> ) , <EOL> ", "gt": "migrations . AddField ("}
{"input": "import subprocess <EOL> from cappa . testing import CommandRunner <EOL> from falco . utils import run_in_shell <EOL> def makemigrations ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def migrate ( ) : <EOL> subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) <EOL> def change_model_attribute ( django_project_dir ) : <EOL> models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> models_file_content = models_file . read_text ( ) <EOL> models_file . write_text ( <EOL> models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\" <EOL> ) <EOL> def insert_a_post ( ) : <EOL> from blog . models import Post <EOL> Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) <EOL> def count_nbr_of_posts ( ) -> int : <EOL> from blog . models import Post <EOL> return Post . objects . all ( ) . count ( ) <EOL> def count_migrations ( django_project_dir ) : <EOL> migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" <EOL> ", "gt": "return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> text_splitting = _get_text_splitter_config ( <EOL> backend_alias = alias , <EOL> config = backend_dict . get ( \"<STR_LIT>\" , { } ) , <EOL> ) <EOL> config = ai_backend_cls . config_cls . from_settings ( <EOL> backend_settings , <EOL> text_splitter_class = text_splitting . splitter_class , <EOL> ", "gt": "text_splitter_length_calculator_class = text_splitting . splitter_length_calculator_class ,"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> ", "gt": "return ret"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> ", "gt": "choices = ["}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> ", "gt": "MODELS = [ \"<STR_LIT>\" ] ,"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> ", "gt": "try :"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> ", "gt": "\"<STR_LIT>\" : ["}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> ", "gt": "self . page = [ ]"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> ", "gt": "if not empty :"}
{"input": "import json <EOL> from channels . generic . websocket import AsyncWebsocketConsumer <EOL> from delphic . utils . collections import load_collection_model <EOL> from delphic . utils . paths import extract_connection_id <EOL> class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : <EOL> async def connect ( self ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> try : <EOL> self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . index = await load_collection_model ( self . collection_id ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except ValueError as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await self . accept ( ) <EOL> await self . close ( code = <NUM_LIT> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> async def disconnect ( self , close_code ) : <EOL> pass <EOL> async def receive ( self , text_data ) : <EOL> text_data_json = json . loads ( text_data ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if self . index is not None : <EOL> query_str = text_data_json [ \"<STR_LIT>\" ] <EOL> modified_query_str = <EOL> response = self . index . query ( modified_query_str ) <EOL> ", "gt": "markdown_response = f\"<STR_LIT>\""}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> ", "gt": "attention_masks = [ <NUM_LIT> ] * len ( input_ids )"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> ", "gt": "'<STR_LIT>' : starttime ,"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> ", "gt": "{"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> line = line . strip ( ) <EOL> line_vec = line . split ( \"<STR_LIT>\" ) <EOL> if len ( line_vec ) == <NUM_LIT> : <EOL> ids . append ( line_vec [ <NUM_LIT> ] ) <EOL> if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) <EOL> texts . append ( line_vec [ <NUM_LIT> ] ) <EOL> labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) <EOL> else : <EOL> ids . append ( str ( sample_index ) ) <EOL> sample_index += <NUM_LIT> <EOL> if is_zh ( line_vec [ <NUM_LIT> ] ) and len ( line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] <EOL> elif len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : <EOL> line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) <EOL> texts . append ( line_vec [ <NUM_LIT> ] ) <EOL> labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) <EOL> ", "gt": "for id , text , label in zip ( ids , texts , labels ) :"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> def __get_object_desc ( obj_port_list ) : <EOL> __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) <EOL> x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] <EOL> return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) <EOL> obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) <EOL> yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) <EOL> yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) <EOL> yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) <EOL> yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) <EOL> label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] <EOL> return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h <EOL> def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : <EOL> txt_path = os . path . join ( label_dir_path , <EOL> target_dir , <EOL> json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> with open ( txt_path , '<STR_LIT>' ) as f : <EOL> for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : <EOL> yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj <EOL> f . write ( yolo_obj_line ) <EOL> def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : <EOL> img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> img_path = os . path . join ( image_dir_path , target_dir , img_name ) <EOL> if not os . path . exists ( img_path ) : <EOL> with open ( img_path , \"<STR_LIT>\" ) as fh : <EOL> fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) <EOL> return img_path <EOL> def _save_dataset_yaml ( self ) : <EOL> yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) <EOL> yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) <EOL> names_str = '<STR_LIT>' <EOL> for label , _ in self . _label_id_map . items ( ) : <EOL> names_str += \"<STR_LIT>\" % label <EOL> names_str = names_str . rstrip ( '<STR_LIT>' ) <EOL> yaml_file . write ( '<STR_LIT>' % names_str ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> ", "gt": "parser . add_argument ( '<STR_LIT>' , type = str ,"}
{"input": "import django . contrib . postgres . fields <EOL> from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = django . contrib . postgres . fields . ArrayField ( <EOL> ", "gt": "base_field = models . CharField ( max_length = <NUM_LIT> ) , null = True , size = None"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> text_splitting = _get_text_splitter_config ( <EOL> backend_alias = alias , <EOL> ", "gt": "config = backend_dict . get ( \"<STR_LIT>\" , { } ) ,"}
{"input": "from django . contrib import admin <EOL> from django . forms import BooleanField <EOL> from django . forms . widgets import CheckboxInput <EOL> from . models import ApiKey <EOL> @ admin . register ( ApiKey ) <EOL> class ApiKeyAdmin ( admin . ModelAdmin ) : <EOL> list_display = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> formfield_overrides = { <EOL> BooleanField : { '<STR_LIT>' : CheckboxInput } , <EOL> ", "gt": "}"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> ", "gt": "pass"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> ", "gt": "def expect_modal_closed ( page : Page ) :"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> TEMPLATES = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> } , <EOL> } , <EOL> ] <EOL> DATABASES = { <EOL> \"<STR_LIT>\" : { <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> ", "gt": "if '<STR_LIT>' not in li . attrs :"}
{"input": "from pathlib import Path <EOL> DEBUG = True <EOL> BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent <EOL> SECRET_KEY = \"<STR_LIT>\" <EOL> ALLOWED_HOSTS = [ ] <EOL> ROOT_URLCONF = \"<STR_LIT>\" <EOL> STATIC_URL = \"<STR_LIT>\" <EOL> WSGI_APPLICATION = \"<STR_LIT>\" <EOL> DEFAULT_AUTO_FIELD = \"<STR_LIT>\" <EOL> LANGUAGE_CODE = \"<STR_LIT>\" <EOL> TIME_ZONE = \"<STR_LIT>\" <EOL> USE_I18N = True <EOL> USE_TZ = True <EOL> INSTALLED_APPS = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> MIDDLEWARE = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> ", "gt": "TEMPLATES = ["}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> ", "gt": "if payloads . bad_request is None :"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> with self . assertRaises ( ValueError ) : <EOL> utils . ensure_list_of_dicts ( data = data ) <EOL> def test_ensure_list_of_dicts_not_list_or_dict ( self ) : <EOL> ", "gt": "data = <NUM_LIT>"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> ", "gt": "def search_box ( page : Page ) -> Locator :"}
{"input": "from functools import wraps <EOL> from typing import List <EOL> from django . core . exceptions import PermissionDenied <EOL> from ninja import Router <EOL> from ninja_crud . views import ( <EOL> DeleteModelView , <EOL> ListModelView , <EOL> ReadModelView , <EOL> UpdateModelView , <EOL> ) <EOL> from ninja_crud . viewsets import ModelViewSet <EOL> from tests . test_app . models import Item , Tag <EOL> from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut <EOL> router = Router ( ) <EOL> def user_is_collection_creator ( func ) : <EOL> @ wraps ( func ) <EOL> def wrapper ( request , * args , ** kwargs ) : <EOL> item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) <EOL> item = Item . objects . get ( id = item_id ) <EOL> if item . collection . created_by != request . auth : <EOL> raise PermissionDenied ( ) <EOL> return func ( request , * args , ** kwargs ) <EOL> return wrapper <EOL> class ItemViewSet ( ModelViewSet ) : <EOL> model = Item <EOL> default_request_body = ItemIn <EOL> default_response_body = ItemOut <EOL> list_items = ListModelView ( <EOL> query_parameters = OrderByFilterSchema , <EOL> get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , <EOL> ) <EOL> read_item = ReadModelView ( <EOL> read_model = lambda request , path_parameters , _ : Item . objects . get ( <EOL> id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ) , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> update_item = UpdateModelView ( <EOL> pre_save = lambda request , instance : None , <EOL> post_save = lambda request , instance : None , <EOL> decorators = [ user_is_collection_creator ] , <EOL> ) <EOL> delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) <EOL> list_tags = ListModelView ( <EOL> path = \"<STR_LIT>\" , <EOL> get_queryset = lambda request , path_parameters : Tag . objects . filter ( <EOL> items__id = getattr ( path_parameters , \"<STR_LIT>\" , None ) <EOL> ", "gt": ") ,"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> ", "gt": "print ( '<STR_LIT>' )"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> ", "gt": "@ django . test . tag ( \"<STR_LIT>\" )"}
{"input": "import re <EOL> import pytest <EOL> from playwright . sync_api import Locator , Page , expect <EOL> from dev . football . core . management . commands . testdata import PASSWORD , USERNAME <EOL> @ pytest . fixture <EOL> def page_admin ( page ) -> Page : <EOL> page . goto ( f\"<STR_LIT>\" ) <EOL> re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) <EOL> re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) <EOL> page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) <EOL> re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) <EOL> return page <EOL> def search_button ( page : Page ) -> Locator : <EOL> re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) <EOL> def search_box ( page : Page ) -> Locator : <EOL> re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) <EOL> return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) <EOL> def expect_modal_open ( page : Page ) : <EOL> expect ( search_box ( page ) ) . to_be_visible ( ) <EOL> def expect_modal_closed ( page : Page ) : <EOL> expect ( search_box ( page ) ) . not_to_be_visible ( ) <EOL> def open_modal ( page : Page , with_keyboard : bool = False ) : <EOL> if with_keyboard : <EOL> page . keyboard . press ( \"<STR_LIT>\" ) <EOL> else : <EOL> search_button ( page ) . click ( ) <EOL> ", "gt": "expect_modal_open ( page )"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if query_parameters . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . bad_request , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> ", "gt": "test_case , on_completion = on_completion , status = status"}
{"input": "from django . db import models <EOL> from django . utils import timezone <EOL> class Product ( models . Model ) : <EOL> name = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> description = models . TextField ( ) <EOL> price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> ) <EOL> sku = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> ", "gt": "created_at = models . DateTimeField ( default = timezone . now )"}
{"input": "from datetime import date , datetime , timedelta <EOL> import pytest <EOL> from django . core . serializers . json import DjangoJSONEncoder <EOL> from django . test import override_settings <EOL> from django . utils import timezone <EOL> from django_webhook . models import WebhookEvent <EOL> from django_webhook . tasks import clear_webhook_events <EOL> from django_webhook . test_factories import ( <EOL> WebhookEventFactory , <EOL> WebhookFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> from tests . model_data import TEST_USER <EOL> from tests . models import User <EOL> pytestmark = pytest . mark . django_db <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = True , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_creates_events_when_enabled ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> assert WebhookEvent . objects . count ( ) == <NUM_LIT> <EOL> event = WebhookEvent . objects . get ( ) <EOL> assert event . webhook == webhook <EOL> assert event . object == { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : TEST_USER , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : str ( webhook . uuid ) , <EOL> } <EOL> assert event . object_type == \"<STR_LIT>\" <EOL> assert event . topic == \"<STR_LIT>\" <EOL> assert event . status == \"<STR_LIT>\" <EOL> assert event . url == webhook . url <EOL> @ override_settings ( <EOL> DJANGO_WEBHOOK = dict ( <EOL> STORE_EVENTS = False , <EOL> PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , <EOL> MODELS = [ \"<STR_LIT>\" ] , <EOL> USE_CACHE = False , <EOL> ) <EOL> ) <EOL> def test_does_not_create_events_when_disabled ( responses ) : <EOL> webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) <EOL> responses . post ( webhook . url ) <EOL> User . objects . create ( <EOL> name = \"<STR_LIT>\" , <EOL> email = \"<STR_LIT>\" , <EOL> ", "gt": "join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ,"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> ", "gt": "getattr ( response , \"<STR_LIT>\" , None ) ,"}
{"input": "from typing import List , Optional , Union <EOL> from ninja_crud . testing . core . components import utils <EOL> class Headers : <EOL> def __init__ ( <EOL> self , <EOL> ok : Union [ dict , List [ dict ] ] , <EOL> forbidden : Union [ dict , List [ dict ] , None ] = None , <EOL> unauthorized : Union [ dict , List [ dict ] , None ] = None , <EOL> ) -> None : <EOL> self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok ) <EOL> self . forbidden : Optional [ List [ dict ] ] = ( <EOL> utils . ensure_list_of_dicts ( forbidden ) if forbidden is not None else None <EOL> ) <EOL> self . unauthorized : Optional [ List [ dict ] ] = ( <EOL> ", "gt": "utils . ensure_list_of_dicts ( unauthorized )"}
{"input": "from django import forms <EOL> from django . core . exceptions import ValidationError <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from wagtail . admin . staticfiles import versioned_static <EOL> from wagtail . images . forms import BaseImageForm <EOL> class PromptTextField ( forms . CharField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> } <EOL> class PromptUUIDField ( forms . UUIDField ) : <EOL> default_error_messages = { <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , <EOL> } <EOL> class ApiForm ( forms . Form ) : <EOL> def errors_for_json_response ( self ) -> str : <EOL> errors_for_response = [ ] <EOL> for _field , errors in self . errors . get_json_data ( ) . items ( ) : <EOL> for error in errors : <EOL> errors_for_response . append ( error [ \"<STR_LIT>\" ] ) <EOL> return \"<STR_LIT>\" . join ( errors_for_response ) <EOL> class PromptForm ( ApiForm ) : <EOL> text = PromptTextField ( ) <EOL> prompt = PromptUUIDField ( ) <EOL> def clean_prompt ( self ) : <EOL> prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] <EOL> if prompt_uuid . version != <NUM_LIT> : <EOL> raise ValidationError ( <EOL> self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" <EOL> ) <EOL> return prompt_uuid <EOL> class DescribeImageApiForm ( ApiForm ) : <EOL> image_id = forms . CharField ( ) <EOL> maxlength = forms . IntegerField ( required = False , min_value = <NUM_LIT> , max_value = <NUM_LIT> ) <EOL> class DescribeImageForm ( BaseImageForm ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> if self . instance and self . instance . pk : <EOL> ", "gt": "widget = self . fields [ \"<STR_LIT>\" ] . widget"}
{"input": "import http <EOL> import json <EOL> from typing import Optional , Type , cast <EOL> import django . http <EOL> import django . test <EOL> import django . utils . http <EOL> from ninja import Schema <EOL> from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager <EOL> from ninja_crud . testing . core . components import Headers , PathParameters <EOL> from ninja_crud . testing . views import AbstractModelViewTest <EOL> from ninja_crud . views import ReadModelView <EOL> class ReadModelViewTest ( AbstractModelViewTest ) : <EOL> model_view : ReadModelView <EOL> def __init__ ( <EOL> self , <EOL> path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> super ( ) . __init__ ( model_view_class = ReadModelView ) <EOL> self . view_test_manager = ViewTestManager ( <EOL> simulate_request = self . simulate_request , <EOL> path_parameters = path_parameters , <EOL> headers = headers , <EOL> ) <EOL> def on_successful_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> actual_output = json . loads ( response . content ) <EOL> expected_output = self . _get_expected_output ( <EOL> response = response , <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> ) <EOL> self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) <EOL> def _get_expected_output ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> ) -> dict : <EOL> path_parameters_schema : Optional [ Schema ] = ( <EOL> self . model_view . path_parameters ( ** path_parameters ) <EOL> if self . model_view . path_parameters <EOL> else None <EOL> ) <EOL> query_parameters_schema = ( <EOL> self . model_view . query_parameters ( ** query_parameters ) <EOL> if self . model_view . query_parameters <EOL> else None <EOL> ) <EOL> instance = self . model_view . read_model ( <EOL> getattr ( response , \"<STR_LIT>\" , None ) , <EOL> path_parameters_schema , <EOL> query_parameters_schema , <EOL> ) <EOL> schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) <EOL> return json . loads ( schema . json ( ) ) <EOL> def on_failed_request ( <EOL> self , <EOL> response : django . http . HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> pass <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_ok ( self ) : <EOL> self . view_test_manager . test_view_ok ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_successful_request , <EOL> status = http . HTTPStatus . OK , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> def test_read_model_headers_unauthorized ( self ) : <EOL> self . view_test_manager . test_view_headers_unauthorized ( <EOL> test_case = self . model_viewset_test_case , <EOL> on_completion = self . on_failed_request , <EOL> ) <EOL> @ django . test . tag ( \"<STR_LIT>\" ) <EOL> ", "gt": "def test_read_model_headers_forbidden ( self ) :"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ) , <EOL> } , <EOL> ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ) <EOL> ", "gt": "list_display = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ]"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ", "gt": "ws . add_table ( tab )"}
{"input": "import os <EOL> import sys <EOL> import argparse <EOL> import shutil <EOL> import math <EOL> from collections import OrderedDict <EOL> import json <EOL> import cv2 <EOL> from sklearn . model_selection import train_test_split <EOL> import base64 <EOL> import numpy as np <EOL> class Labelme2YOLO ( object ) : <EOL> def __init__ ( self , json_dir , output_path ) : <EOL> self . _json_dir = json_dir <EOL> self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) <EOL> self . output_path = output_path <EOL> def _make_train_val_dir ( self ) : <EOL> self . _label_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> self . _image_dir_path = os . path . join ( self . output_path , <EOL> '<STR_LIT>' ) <EOL> for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , <EOL> os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : <EOL> if os . path . exists ( yolo_path ) : <EOL> shutil . rmtree ( yolo_path ) <EOL> os . makedirs ( yolo_path ) <EOL> def _get_label_id_map ( self , json_dir ) : <EOL> label_set = set ( ) <EOL> for file_name in os . listdir ( json_dir ) : <EOL> if file_name . endswith ( '<STR_LIT>' ) : <EOL> json_path = os . path . join ( json_dir , file_name ) <EOL> data = json . load ( open ( json_path ) ) <EOL> for shape in data [ '<STR_LIT>' ] : <EOL> label_set . add ( shape [ '<STR_LIT>' ] ) <EOL> return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) <EOL> def _train_test_split ( self , folders , json_names , val_size ) : <EOL> if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : <EOL> train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] <EOL> val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) <EOL> val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] <EOL> return train_json_names , val_json_names <EOL> train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , <EOL> test_size = val_size ) <EOL> train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] <EOL> val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] <EOL> return train_json_names , val_json_names <EOL> def convert ( self , val_size ) : <EOL> json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] <EOL> folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] <EOL> train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) <EOL> self . _make_train_val_dir ( ) <EOL> for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( train_json_names , val_json_names ) ) : <EOL> for json_name in json_names : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) <EOL> img_path = self . _save_yolo_image ( json_data , <EOL> json_name , <EOL> self . output_path , <EOL> \"<STR_LIT>\" + target_dir ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , <EOL> self . _label_dir_path , <EOL> target_dir , <EOL> yolo_obj_list ) <EOL> print ( '<STR_LIT>' ) <EOL> self . _save_dataset_yaml ( ) <EOL> def convert_one ( self , json_name ) : <EOL> json_path = os . path . join ( self . _json_dir , json_name ) <EOL> json_data = json . load ( open ( json_path ) ) <EOL> print ( '<STR_LIT>' % json_name ) <EOL> img_path = self . _save_yolo_image ( json_data , json_name , <EOL> self . output_path , '<STR_LIT>' ) <EOL> yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) <EOL> self . _save_yolo_label ( json_name , self . output_path , <EOL> '<STR_LIT>' , yolo_obj_list ) <EOL> def _get_yolo_object_list ( self , json_data , img_path ) : <EOL> yolo_obj_list = [ ] <EOL> img_h , img_w , _ = cv2 . imread ( img_path ) . shape <EOL> for shape in json_data [ '<STR_LIT>' ] : <EOL> if shape [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) <EOL> else : <EOL> yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) <EOL> yolo_obj_list . append ( yolo_obj ) <EOL> return yolo_obj_list <EOL> def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : <EOL> obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + <EOL> ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) <EOL> obj_w = <NUM_LIT> * radius <EOL> obj_h = <NUM_LIT> * radius <EOL> yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) <EOL> ", "gt": "yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> )"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> ", "gt": "django_asgi_app = get_asgi_application ( )"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , <EOL> num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) <EOL> print ( f'<STR_LIT>' <EOL> ", "gt": "f'<STR_LIT>' )"}
{"input": "from http import HTTPStatus <EOL> from typing import Callable , List , Optional , TypeVar , Union , cast <EOL> from django . http import HttpResponse <EOL> from django . test import TestCase <EOL> from ninja_crud . testing . core . components import ( <EOL> Headers , <EOL> PathParameters , <EOL> Payloads , <EOL> QueryParameters , <EOL> ) <EOL> T = TypeVar ( \"<STR_LIT>\" ) <EOL> TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) <EOL> ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] <EOL> CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] <EOL> class ViewTestManager : <EOL> def __init__ ( <EOL> self , <EOL> simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , <EOL> path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , <EOL> query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , <EOL> headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , <EOL> payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , <EOL> ) -> None : <EOL> self . simulate_request = simulate_request <EOL> self . path_parameters = path_parameters or PathParameters ( ok = { } ) <EOL> self . query_parameters = query_parameters or QueryParameters ( ok = { } ) <EOL> self . headers = headers or Headers ( ok = { } ) <EOL> self . payloads = payloads or Payloads ( ok = { } ) <EOL> @ staticmethod <EOL> def _get_arg_or_callable ( <EOL> test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] <EOL> ) -> T : <EOL> if callable ( params ) : <EOL> return params ( test_case ) <EOL> elif isinstance ( params , property ) : <EOL> return cast ( Callable , params . fget ) ( test_case ) <EOL> else : <EOL> return params <EOL> def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : <EOL> path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) <EOL> if not isinstance ( path_parameters , PathParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return path_parameters <EOL> def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : <EOL> query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) <EOL> if not isinstance ( query_parameters , QueryParameters ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return query_parameters <EOL> def get_headers ( self , test_case : TestCase ) -> Headers : <EOL> headers = self . _get_arg_or_callable ( test_case , self . headers ) <EOL> if not isinstance ( headers , Headers ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return headers <EOL> def get_payloads ( self , test_case : TestCase ) -> Payloads : <EOL> payloads = self . _get_arg_or_callable ( test_case , self . payloads ) <EOL> if not isinstance ( payloads , Payloads ) : <EOL> raise TypeError ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> return payloads <EOL> @ staticmethod <EOL> def wrap_completion_with_status_check ( <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus , <EOL> ) -> CompletionCallback : <EOL> def on_completion_with_status_check ( <EOL> response : HttpResponse , <EOL> path_parameters : dict , <EOL> query_parameters : dict , <EOL> headers : dict , <EOL> payload : dict , <EOL> ) : <EOL> test_case . assertEqual ( response . status_code , status ) <EOL> on_completion ( response , path_parameters , query_parameters , headers , payload ) <EOL> return on_completion_with_status_check <EOL> def run_combinatorial_tests ( <EOL> self , <EOL> test_case : TestCase , <EOL> path_parameters_list : List [ dict ] , <EOL> query_parameters_list : List [ dict ] , <EOL> headers_list : List [ dict ] , <EOL> payload_list : List [ dict ] , <EOL> on_completion : CompletionCallback , <EOL> ) : <EOL> for path_parameters in path_parameters_list : <EOL> for query_parameters in query_parameters_list : <EOL> for headers in headers_list : <EOL> for payload in payload_list : <EOL> with test_case . subTest ( <EOL> path_parameters = path_parameters , <EOL> query_parameters = query_parameters , <EOL> headers = headers , <EOL> payload = payload , <EOL> ) : <EOL> response = self . simulate_request ( <EOL> path_parameters , query_parameters , headers , payload <EOL> ) <EOL> on_completion ( <EOL> response , <EOL> path_parameters , <EOL> query_parameters , <EOL> headers , <EOL> payload , <EOL> ) <EOL> def test_view_ok ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . OK , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . ok , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_bad_request ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . BAD_REQUEST , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . bad_request is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . bad_request , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_payloads_conflict ( <EOL> self , <EOL> test_case : TestCase , <EOL> on_completion : CompletionCallback , <EOL> status : HTTPStatus = HTTPStatus . CONFLICT , <EOL> ) : <EOL> path_parameters = self . get_path_parameters ( test_case ) <EOL> query_parameters = self . get_query_parameters ( test_case ) <EOL> headers = self . get_headers ( test_case ) <EOL> payloads = self . get_payloads ( test_case ) <EOL> if payloads . conflict is None : <EOL> test_case . skipTest ( reason = \"<STR_LIT>\" ) <EOL> else : <EOL> self . run_combinatorial_tests ( <EOL> test_case = test_case , <EOL> path_parameters_list = path_parameters . ok , <EOL> query_parameters_list = query_parameters . ok , <EOL> headers_list = headers . ok , <EOL> payload_list = payloads . conflict , <EOL> on_completion = self . wrap_completion_with_status_check ( <EOL> test_case , on_completion = on_completion , status = status <EOL> ) , <EOL> ) <EOL> def test_view_query_parameters_bad_request ( <EOL> ", "gt": "self ,"}
{"input": "from django . db import models <EOL> from django . utils import timezone <EOL> class Product ( models . Model ) : <EOL> name = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> description = models . TextField ( ) <EOL> price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> ) <EOL> sku = models . CharField ( max_length = <NUM_LIT> , unique = True ) <EOL> created_at = models . DateTimeField ( default = timezone . now ) <EOL> ", "gt": "@ property"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> <EOL> is_next = self . page . has_next ( ) if self . page else False <EOL> is_previous = self . page . has_previous ( ) if self . page else False <EOL> if not data : <EOL> code = <NUM_LIT> <EOL> msg = \"<STR_LIT>\" <EOL> data = [ ] <EOL> return Response ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , code ) , <EOL> ( '<STR_LIT>' , msg ) , <EOL> ( '<STR_LIT>' , page ) , <EOL> ( '<STR_LIT>' , limit ) , <EOL> ( '<STR_LIT>' , total ) , <EOL> ", "gt": "( '<STR_LIT>' , is_next ) ,"}
{"input": "from django . db import models , migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AlterField ( <EOL> model_name = \"<STR_LIT>\" , <EOL> name = \"<STR_LIT>\" , <EOL> field = models . CharField ( <EOL> choices = [ <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> ", "gt": "( \"<STR_LIT>\" , \"<STR_LIT>\" ) ,"}
{"input": "from django . db import migrations <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ) , <EOL> migrations . RemoveField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ", "gt": ") ,"}
{"input": "import torch , math <EOL> from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer <EOL> import logging <EOL> log = logging . getLogger ( ) <EOL> def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : <EOL> tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] <EOL> max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) <EOL> output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) <EOL> document_seq_lengths = [ ] <EOL> for doc_index , tokenized_document in enumerate ( tokenized_documents ) : <EOL> max_seq_index = <NUM_LIT> <EOL> for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : <EOL> raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] <EOL> tokens = [ ] <EOL> input_type_ids = [ ] <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> for token in raw_tokens : <EOL> tokens . append ( token ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> tokens . append ( \"<STR_LIT>\" ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> input_ids = tokenizer . convert_tokens_to_ids ( tokens ) <EOL> attention_masks = [ <NUM_LIT> ] * len ( input_ids ) <EOL> while len ( input_ids ) < max_input_length : <EOL> input_ids . append ( <NUM_LIT> ) <EOL> input_type_ids . append ( <NUM_LIT> ) <EOL> attention_masks . append ( <NUM_LIT> ) <EOL> output [ doc_index ] [ seq_index ] = torch . cat ( ( torch . LongTensor ( input_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> torch . LongTensor ( input_type_ids ) . unsqueeze ( <NUM_LIT> ) , <EOL> torch . LongTensor ( attention_masks ) . unsqueeze ( <NUM_LIT> ) ) , <EOL> dim = <NUM_LIT> ) <EOL> max_seq_index = seq_index <EOL> ", "gt": "document_seq_lengths . append ( max_seq_index + <NUM_LIT> )"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) <EOL> except ImportError as e : <EOL> raise InvalidAIBackendError ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> backend_settings = backend_dict [ \"<STR_LIT>\" ] <EOL> text_splitting = _get_text_splitter_config ( <EOL> backend_alias = alias , <EOL> config = backend_dict . get ( \"<STR_LIT>\" , { } ) , <EOL> ) <EOL> config = ai_backend_cls . config_cls . from_settings ( <EOL> backend_settings , <EOL> text_splitter_class = text_splitting . splitter_class , <EOL> text_splitter_length_calculator_class = text_splitting . splitter_length_calculator_class , <EOL> ) <EOL> return ai_backend_cls ( config = config ) <EOL> class BackendNotFound ( Exception ) : <EOL> pass <EOL> def get_backend ( feature : BackendFeature = BackendFeature . TEXT_COMPLETION ) -> AIBackend : <EOL> match feature : <EOL> case BackendFeature . TEXT_COMPLETION : <EOL> alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> case BackendFeature . IMAGE_DESCRIPTION : <EOL> alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" ) <EOL> ", "gt": "case _ :"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def export_data ( self , request : Request , * args , ** kwargs ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . export_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws = wb . active <EOL> header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] <EOL> ", "gt": "hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ]"}
{"input": "import configparser <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoModel <EOL> import configargparse <EOL> from transformers import AutoTokenizer <EOL> from scoreblocks . MSPLM . encoder import encode_documents <EOL> from scoreblocks . MSPLM . plms import mainplm <EOL> from torch . cuda . amp import autocast <EOL> def _initialize_arguments ( p ) : <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> p . add_section ( '<STR_LIT>' ) <EOL> config_dict = { } <EOL> for section in p . sections ( ) : <EOL> section_dict = { } <EOL> for option in p . options ( section ) : <EOL> section_dict [ option ] = p . get ( section , option ) <EOL> config_dict [ section ] = section_dict <EOL> config_dict = config_dict [ '<STR_LIT>' ] <EOL> print ( config_dict ) <EOL> if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> config_dict [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return config_dict <EOL> def init_weights ( m ) : <EOL> if isinstance ( m , nn . Linear ) : <EOL> torch . nn . init . xavier_uniform ( m . weight ) <EOL> m . bias . data . fill_ ( <NUM_LIT> ) <EOL> class model : <EOL> def __init__ ( self ) : <EOL> p = configparser . ConfigParser ( ) <EOL> p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) <EOL> self . args = _initialize_arguments ( p ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) <EOL> self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) <EOL> self . chunk_sizes = [ ] <EOL> self . bert_batch_sizes = [ ] <EOL> self . bert_regression_by_word_document = mainplm ( self . args ) <EOL> self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) <EOL> self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) <EOL> self . bert_regression_by_word_document . eval ( ) <EOL> self . plt_x = [ ] <EOL> self . plt_train_qwk = [ ] <EOL> self . plt_val_qwk = [ ] <EOL> self . plt_test_qwk = [ ] <EOL> self . best_val_qwk = <NUM_LIT> <EOL> def getscore ( self , valdata ) : <EOL> with torch . no_grad ( ) : <EOL> target_scores = None <EOL> doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( <EOL> valdata , self . tokenizer , max_input_length = <NUM_LIT> ) <EOL> predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) <EOL> acculation_loss = <NUM_LIT> <EOL> ", "gt": "for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) :"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> dependencies = [ <EOL> ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ] <EOL> operations = [ <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> field = models . CharField ( blank = True , max_length = <NUM_LIT> ) , <EOL> ) , <EOL> migrations . AddField ( <EOL> model_name = '<STR_LIT>' , <EOL> name = '<STR_LIT>' , <EOL> ", "gt": "field = models . CharField ( blank = True , max_length = <NUM_LIT> ) ,"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : django_asgi_app ,"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text <EOL> title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> ", "gt": "'<STR_LIT>' : msg ,"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ", "gt": ") from e"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> ", "gt": "if splitter_class_path is not None :"}
{"input": "import torch <EOL> import time <EOL> import pickle as pkl <EOL> from torch . utils . data import DataLoader , Dataset , RandomSampler <EOL> class HMERDataset ( Dataset ) : <EOL> def __init__ ( self , params , image_path , label_path , words , is_train = True ) : <EOL> super ( HMERDataset , self ) . __init__ ( ) <EOL> if image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> self . images = pkl . load ( f ) <EOL> elif image_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( image_path , '<STR_LIT>' ) as f : <EOL> lines = f . readlines ( ) <EOL> self . images = { } <EOL> print ( f'<STR_LIT>' ) <EOL> for line in lines : <EOL> name = line . strip ( ) <EOL> print ( f'<STR_LIT>' ) <EOL> start = time . time ( ) <EOL> with open ( name , '<STR_LIT>' ) as f : <EOL> images = pkl . load ( f ) <EOL> self . images . update ( images ) <EOL> print ( f'<STR_LIT>' ) <EOL> with open ( label_path , '<STR_LIT>' ) as f : <EOL> self . labels = f . readlines ( ) <EOL> self . words = words <EOL> self . is_train = is_train <EOL> self . params = params <EOL> def __len__ ( self ) : <EOL> assert len ( self . images ) == len ( self . labels ) <EOL> return len ( self . labels ) <EOL> def __getitem__ ( self , idx ) : <EOL> name , * labels = self . labels [ idx ] . strip ( ) . split ( ) <EOL> name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name <EOL> image = self . images [ name ] <EOL> image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> <EOL> image = image . unsqueeze ( <NUM_LIT> ) <EOL> labels . append ( '<STR_LIT>' ) <EOL> words = self . words . encode ( labels ) <EOL> words = torch . LongTensor ( words ) <EOL> return image , words <EOL> def get_crohme_dataset ( params ) : <EOL> words = Words ( params [ '<STR_LIT>' ] ) <EOL> params [ '<STR_LIT>' ] = len ( words ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) <EOL> eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) <EOL> train_sampler = RandomSampler ( train_dataset ) <EOL> eval_sampler = RandomSampler ( eval_dataset ) <EOL> train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , <EOL> ", "gt": "num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True )"}
{"input": "from urllib . parse import quote <EOL> from django . db import transaction <EOL> from django . http import HttpResponse <EOL> from openpyxl import Workbook <EOL> from openpyxl . worksheet . datavalidation import DataValidation <EOL> from openpyxl . utils import get_column_letter , quote_sheetname <EOL> from openpyxl . worksheet . table import Table , TableStyleInfo <EOL> from rest_framework . decorators import action <EOL> from rest_framework . request import Request <EOL> from dvadmin . utils . import_export import import_to_data <EOL> from dvadmin . utils . json_response import DetailResponse <EOL> from dvadmin . utils . request_util import get_verbose_name <EOL> class ImportSerializerMixin : <EOL> import_field_dict = { } <EOL> import_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> return False <EOL> def get_string_len ( self , string ) : <EOL> length = <NUM_LIT> <EOL> if string is None : <EOL> return length <EOL> if self . is_number ( string ) : <EOL> return length <EOL> for char in string : <EOL> length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> <EOL> return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width <EOL> @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) <EOL> @ transaction . atomic <EOL> def import_data ( self , request : Request , * args , ** kwargs ) : <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> if request . method == \"<STR_LIT>\" : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ <EOL> \"<STR_LIT>\" <EOL> ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> header_data = [ <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> validation_data_dict = { } <EOL> for index , ele in enumerate ( self . import_field_dict . values ( ) ) : <EOL> if isinstance ( ele , dict ) : <EOL> header_data . append ( ele . get ( \"<STR_LIT>\" ) ) <EOL> choices = ele . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( ele ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> ws . append ( header_data ) <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> else : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> m2m_fields = [ <EOL> ele . name <EOL> for ele in queryset . model . _meta . get_fields ( ) <EOL> if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True <EOL> ] <EOL> import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } <EOL> data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) <EOL> for ele in data : <EOL> filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } <EOL> instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) <EOL> serializer = self . import_serializer_class ( instance , data = ele , request = request ) <EOL> serializer . is_valid ( raise_exception = True ) <EOL> serializer . save ( ) <EOL> return DetailResponse ( msg = f\"<STR_LIT>\" ) <EOL> @ action ( methods = [ '<STR_LIT>' ] , detail = False ) <EOL> def update_template ( self , request ) : <EOL> queryset = self . filter_queryset ( self . get_queryset ( ) ) <EOL> assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ <EOL> data = self . import_serializer_class ( queryset , many = True , request = request ) . data <EOL> response = HttpResponse ( content_type = \"<STR_LIT>\" ) <EOL> response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" <EOL> response [ \"<STR_LIT>\" ] = f'<STR_LIT>' <EOL> wb = Workbook ( ) <EOL> ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) <EOL> ws1 . sheet_state = \"<STR_LIT>\" <EOL> ws = wb . active <EOL> import_field_dict = { } <EOL> header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> validation_data_dict = { } <EOL> for index , item in enumerate ( self . import_field_dict . items ( ) ) : <EOL> items = list ( item ) <EOL> key = items [ <NUM_LIT> ] <EOL> value = items [ <NUM_LIT> ] <EOL> if isinstance ( value , dict ) : <EOL> header_data . append ( value . get ( \"<STR_LIT>\" ) ) <EOL> hidden_header . append ( value . get ( '<STR_LIT>' ) ) <EOL> choices = value . get ( \"<STR_LIT>\" , { } ) <EOL> if choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = [ ] <EOL> data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list <EOL> elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : <EOL> data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) <EOL> validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) <EOL> else : <EOL> continue <EOL> column_letter = get_column_letter ( len ( validation_data_dict ) ) <EOL> dv = DataValidation ( <EOL> type = \"<STR_LIT>\" , <EOL> formula1 = f\"<STR_LIT>\" , <EOL> allow_blank = True , <EOL> ) <EOL> ws . add_data_validation ( dv ) <EOL> dv . add ( f\"<STR_LIT>\" ) <EOL> else : <EOL> header_data . append ( value ) <EOL> hidden_header . append ( key ) <EOL> ws1 . append ( list ( validation_data_dict . keys ( ) ) ) <EOL> for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : <EOL> for inx , ele in enumerate ( validation_data ) : <EOL> ws1 [ f\"<STR_LIT>\" ] = ele <EOL> df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] <EOL> row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) <EOL> column = <NUM_LIT> <EOL> ws . append ( header_data ) <EOL> for index , results in enumerate ( data ) : <EOL> results_list = [ ] <EOL> for h_index , h_item in enumerate ( hidden_header ) : <EOL> for key , val in results . items ( ) : <EOL> if key == h_item : <EOL> if val is None or val == \"<STR_LIT>\" : <EOL> results_list . append ( \"<STR_LIT>\" ) <EOL> elif isinstance ( val , list ) : <EOL> results_list . append ( str ( val ) ) <EOL> else : <EOL> results_list . append ( val ) <EOL> if isinstance ( val , str ) : <EOL> result_column_width = self . get_string_len ( val ) <EOL> if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : <EOL> df_len_max [ h_index ] = result_column_width <EOL> ws . append ( [ index + <NUM_LIT> , * results_list ] ) <EOL> column += <NUM_LIT> <EOL> for index , width in enumerate ( df_len_max ) : <EOL> ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width <EOL> tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) <EOL> style = TableStyleInfo ( <EOL> name = \"<STR_LIT>\" , <EOL> showFirstColumn = True , <EOL> showLastColumn = True , <EOL> showRowStripes = True , <EOL> showColumnStripes = True , <EOL> ) <EOL> tab . tableStyleInfo = style <EOL> ws . add_table ( tab ) <EOL> wb . save ( response ) <EOL> return response <EOL> class ExportSerializerMixin : <EOL> export_field_label = [ ] <EOL> export_serializer_class = None <EOL> export_column_width = <NUM_LIT> <EOL> def is_number ( self , num ) : <EOL> try : <EOL> float ( num ) <EOL> return True <EOL> except ValueError : <EOL> pass <EOL> try : <EOL> import unicodedata <EOL> unicodedata . numeric ( num ) <EOL> return True <EOL> except ( TypeError , ValueError ) : <EOL> pass <EOL> ", "gt": "return False"}
{"input": "import requests , datetime , os , re , json <EOL> from utils . general import headers <EOL> from bs4 import BeautifulSoup as bs <EOL> def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> dt_str = dt . strftime ( '<STR_LIT>' ) <EOL> url = '<STR_LIT>' % ( dt_str , channel_id ) <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_j = res . json ( ) <EOL> epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> for li in epg_list : <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) <EOL> title = li [ '<STR_LIT>' ] <EOL> title_en = li [ '<STR_LIT>' ] <EOL> desc = li [ '<STR_LIT>' ] <EOL> desc_en = li [ '<STR_LIT>' ] <EOL> url = li [ '<STR_LIT>' ] <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , <EOL> '<STR_LIT>' : title_en , <EOL> '<STR_LIT>' : desc , <EOL> '<STR_LIT>' : desc_en , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , <EOL> } <EOL> epgs . append ( epg ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % channel_id <EOL> try : <EOL> res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> lis = soup . select ( '<STR_LIT>' ) <EOL> for li in lis : <EOL> if '<STR_LIT>' not in li . attrs : <EOL> continue <EOL> starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) <EOL> ", "gt": "em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text"}
{"input": "import random <EOL> asap_ranges = { <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , <EOL> } <EOL> asap_essay_lengths = { <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> , <EOL> <NUM_LIT> : <NUM_LIT> <EOL> } <EOL> def fix_score ( score , prompt ) : <EOL> if prompt == <NUM_LIT> : <EOL> int_part = float ( int ( score ) ) <EOL> float_part = score - int_part <EOL> result = int_part <EOL> if float_part < <NUM_LIT> : <EOL> result = int_part <EOL> elif float_part < <NUM_LIT> : <EOL> result = int_part + <NUM_LIT> <EOL> else : <EOL> result = int_part + <NUM_LIT> <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if result < min_score : <EOL> return min_score <EOL> elif result > max_score : <EOL> return max_score <EOL> else : <EOL> return result <EOL> elif prompt <= <NUM_LIT> : <EOL> min_score , max_score = asap_ranges [ prompt ] <EOL> if score < min_score : <EOL> return min_score <EOL> elif score > max_score : <EOL> return max_score <EOL> else : <EOL> return round ( score ) <EOL> else : <EOL> return score <EOL> def is_zh ( s ) : <EOL> for c in s : <EOL> if c >= '<STR_LIT>' and c <= '<STR_LIT>' : <EOL> return True <EOL> return False <EOL> def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : <EOL> ids = [ ] <EOL> texts = [ ] <EOL> labels = [ ] <EOL> sample_index = <NUM_LIT> <EOL> with open ( data_file ) as fin : <EOL> for line in fin : <EOL> rand_value = random . random ( ) <EOL> if rand_value > data_sample_rate : <EOL> continue <EOL> ", "gt": "line = line . strip ( )"}
{"input": "import unittest <EOL> from typing import List <EOL> from ninja_crud . testing . core . components import utils <EOL> class TestUtils ( unittest . TestCase ) : <EOL> def test_ensure_list_of_dicts_single_dict ( self ) : <EOL> data = { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , [ data ] ) <EOL> def test_ensure_list_of_dicts_list_of_dicts ( self ) : <EOL> data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] <EOL> result = utils . ensure_list_of_dicts ( data = data ) <EOL> self . assertEqual ( result , data ) <EOL> def test_ensure_list_of_dicts_empty_list ( self ) : <EOL> data : List [ dict ] = [ ] <EOL> with self . assertRaises ( ValueError ) : <EOL> ", "gt": "utils . ensure_list_of_dicts ( data = data )"}
{"input": "from . crud import InstallCrudUtils <EOL> from . crud import ModelCRUD <EOL> from . htmx import Htmx <EOL> from . htmx_extension import HtmxExtension <EOL> from . reset_migrations import ResetMigrations <EOL> ", "gt": "from . rm_migrations import RmMigrations"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> channels = [ ] <EOL> for j in js : <EOL> id = str ( j [ '<STR_LIT>' ] ) <EOL> type = j [ '<STR_LIT>' ] <EOL> name = cht_to_chs ( j [ '<STR_LIT>' ] ) <EOL> gtvid = j [ '<STR_LIT>' ] <EOL> logo = j [ '<STR_LIT>' ] <EOL> desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' <EOL> all = [ name , gtvid , logo ] <EOL> channel = { <EOL> '<STR_LIT>' : name , <EOL> '<STR_LIT>' : [ gtvid ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>' ,"}
{"input": "import os <EOL> import sys <EOL> def main ( ) : <EOL> os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> try : <EOL> from django . core . management import execute_from_command_line <EOL> except ImportError as exc : <EOL> raise ImportError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) from exc <EOL> execute_from_command_line ( sys . argv ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> ", "gt": "main ( )"}
{"input": "import pytest <EOL> from freezegun import freeze_time <EOL> from django_webhook . tasks import fire_webhook <EOL> from django_webhook . test_factories import ( <EOL> WebhookFactory , <EOL> WebhookSecretFactory , <EOL> WebhookTopicFactory , <EOL> ) <EOL> pytestmark = pytest . mark . django_db <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_fire ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> secrets__token = \"<STR_LIT>\" , <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , <EOL> ) <EOL> responses . post ( webhook . url ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> req = responses . calls [ <NUM_LIT> ] . request <EOL> assert req . body == b'<STR_LIT>' <EOL> h = req . headers <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> assert ( <EOL> h [ \"<STR_LIT>\" ] <EOL> == \"<STR_LIT>\" <EOL> ) <EOL> assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" <EOL> def test_does_not_fire_inactive ( responses ) : <EOL> webhook = WebhookFactory ( active = False ) <EOL> fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) <EOL> assert len ( responses . calls ) == <NUM_LIT> <EOL> @ freeze_time ( \"<STR_LIT>\" ) <EOL> def test_multiple_signatures ( responses ) : <EOL> webhook = WebhookFactory ( <EOL> topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] <EOL> ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) <EOL> ", "gt": "responses . post ( webhook . url )"}
{"input": "import os <EOL> from channels . auth import AuthMiddlewareStack <EOL> from channels . routing import ProtocolTypeRouter , URLRouter <EOL> from channels . security . websocket import AllowedHostsOriginValidator <EOL> from django . core . asgi import get_asgi_application <EOL> from channels . routing import get_default_application <EOL> import django <EOL> os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> django . setup ( ) <EOL> from summarizer . routing import websocket_urlpatterns <EOL> django_asgi_app = get_asgi_application ( ) <EOL> import summarizer . routing <EOL> print ( '<STR_LIT>' ) <EOL> application = ProtocolTypeRouter ( <EOL> { <EOL> \"<STR_LIT>\" : django_asgi_app , <EOL> \"<STR_LIT>\" : AllowedHostsOriginValidator ( <EOL> ", "gt": "AuthMiddlewareStack ( URLRouter ( websocket_urlpatterns ) )"}
{"input": "from django . db import migrations , models <EOL> class Migration ( migrations . Migration ) : <EOL> initial = True <EOL> dependencies = [ <EOL> ] <EOL> operations = [ <EOL> migrations . CreateModel ( <EOL> name = '<STR_LIT>' , <EOL> fields = [ <EOL> ( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> , unique = True ) ) , <EOL> ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT> ) ) , <EOL> ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> ) ) , <EOL> ", "gt": "( '<STR_LIT>' , models . BooleanField ( default = True ) ) ,"}
{"input": "from django . contrib import admin <EOL> from django . contrib . auth import admin as auth_admin <EOL> from django . contrib . auth import get_user_model <EOL> from django . utils . translation import gettext_lazy as _ <EOL> from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm <EOL> User = get_user_model ( ) <EOL> @ admin . register ( User ) <EOL> class UserAdmin ( auth_admin . UserAdmin ) : <EOL> form = UserAdminChangeForm <EOL> add_form = UserAdminCreationForm <EOL> fieldsets = ( <EOL> ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , <EOL> ( <EOL> _ ( \"<STR_LIT>\" ) , <EOL> { <EOL> \"<STR_LIT>\" : ( <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import os <EOL> import sys <EOL> def main ( ) : <EOL> os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> try : <EOL> from django . core . management import execute_from_command_line <EOL> except ImportError as exc : <EOL> raise ImportError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ", "gt": ") from exc"}
{"input": "import warnings <EOL> from collections . abc import Mapping <EOL> from dataclasses import dataclass <EOL> from typing import NotRequired , Required , TypedDict , cast <EOL> from django . conf import settings <EOL> from django . core . exceptions import ImproperlyConfigured <EOL> from django . utils . module_loading import import_string <EOL> from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter <EOL> from . . text_splitters . length import NaiveTextSplitterCalculator <EOL> from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol <EOL> from . . utils import deprecation <EOL> from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings <EOL> class TextSplittingSettingsDict ( TypedDict ) : <EOL> SPLITTER_CLASS : NotRequired [ str ] <EOL> SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] <EOL> class InvalidAIBackendError ( ImproperlyConfigured ) : <EOL> def __init__ ( self , alias ) : <EOL> super ( ) . __init__ ( f\"<STR_LIT>\" ) <EOL> class AIBackendSettingsDict ( TypedDict ) : <EOL> CONFIG : Required [ BaseAIBackendConfigSettings ] <EOL> CLASS : Required [ str ] <EOL> TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] <EOL> def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : <EOL> try : <EOL> return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] <EOL> except AttributeError : <EOL> pass <EOL> try : <EOL> legacy_settings = settings . WAGTAIL_AI_BACKENDS <EOL> except AttributeError : <EOL> pass <EOL> else : <EOL> warnings . warn ( <EOL> '<STR_LIT>' , <EOL> deprecation . WagtailAISettingsDeprecationWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> return legacy_settings <EOL> return { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } , <EOL> } <EOL> } <EOL> def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : <EOL> if \"<STR_LIT>\" not in settings : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : <EOL> raise ImproperlyConfigured ( <EOL> '<STR_LIT>' <EOL> ) <EOL> def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : <EOL> backends_settings = get_ai_backends_settings ( ) <EOL> try : <EOL> return backends_settings [ alias ] <EOL> except ( KeyError , ImportError ) as e : <EOL> raise InvalidAIBackendError ( alias ) from e <EOL> _validate_backend_settings ( settings = backends_settings , alias = alias ) <EOL> def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : <EOL> return LangchainRecursiveCharacterTextSplitter <EOL> def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] : <EOL> return NaiveTextSplitterCalculator <EOL> @ dataclass ( kw_only = True ) <EOL> class _TextSplitterConfig : <EOL> splitter_class : type [ TextSplitterProtocol ] <EOL> splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] <EOL> def _get_text_splitter_config ( <EOL> * , backend_alias : str , config : TextSplittingSettingsDict <EOL> ) -> _TextSplitterConfig : <EOL> splitter_class_path = config . get ( \"<STR_LIT>\" ) <EOL> length_calculator_class_path = config . get ( \"<STR_LIT>\" ) <EOL> if splitter_class_path is not None : <EOL> try : <EOL> splitter_class = cast ( <EOL> type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> splitter_class = _get_default_text_splitter_class ( ) <EOL> if length_calculator_class_path is not None : <EOL> try : <EOL> length_calculator_class = cast ( <EOL> type [ TextSplitterLengthCalculatorProtocol ] , <EOL> import_string ( length_calculator_class_path ) , <EOL> ) <EOL> except ImportError as e : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) from e <EOL> else : <EOL> length_calculator_class = _get_default_text_splitter_length_class ( ) <EOL> return _TextSplitterConfig ( <EOL> splitter_class = splitter_class , <EOL> splitter_length_calculator_class = length_calculator_class , <EOL> ) <EOL> def get_ai_backend ( alias : str ) -> AIBackend : <EOL> backend_dict = get_ai_backend_settings ( alias ) <EOL> if \"<STR_LIT>\" not in backend_dict : <EOL> raise ImproperlyConfigured ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> try : <EOL> ", "gt": "ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) )"}
{"input": "import requests , datetime , os <EOL> from utils . general import headers , cht_to_chs <EOL> def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : <EOL> epgs = [ ] <EOL> msg = '<STR_LIT>' <EOL> success = <NUM_LIT> <EOL> url = '<STR_LIT>' % ( channel_id ) <EOL> try : <EOL> res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) <EOL> res . encoding = '<STR_LIT>' <EOL> res_json = res . json ( ) <EOL> for j in res_json : <EOL> title = j [ '<STR_LIT>' ] <EOL> start_date = j [ '<STR_LIT>' ] <EOL> start_time = j [ '<STR_LIT>' ] <EOL> end_date = j [ '<STR_LIT>' ] <EOL> end_time = j [ '<STR_LIT>' ] <EOL> starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) <EOL> endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) <EOL> if starttime . date ( ) < dt : <EOL> continue <EOL> epg = { '<STR_LIT>' : channel . id , <EOL> '<STR_LIT>' : starttime , <EOL> '<STR_LIT>' : endtime , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : starttime . date ( ) , <EOL> } <EOL> epgs . append ( epg ) <EOL> epglen = len ( epgs ) <EOL> except Exception as e : <EOL> success = <NUM_LIT> <EOL> spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> msg = '<STR_LIT>' % ( spidername , e ) <EOL> ret = { <EOL> '<STR_LIT>' : success , <EOL> '<STR_LIT>' : epgs , <EOL> '<STR_LIT>' : msg , <EOL> '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> } <EOL> return ret <EOL> def get_channels_4gtv ( ) : <EOL> url = '<STR_LIT>' <EOL> headers . update ( { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } ) <EOL> res = requests . get ( url , headers = headers ) <EOL> res . encoding = '<STR_LIT>' <EOL> js = res . json ( ) [ '<STR_LIT>' ] <EOL> ", "gt": "channels = [ ]"}
{"input": "from collections import OrderedDict <EOL> from django . core import paginator <EOL> from django . core . paginator import Paginator as DjangoPaginator , InvalidPage <EOL> from rest_framework . pagination import PageNumberPagination <EOL> from rest_framework . response import Response <EOL> class CustomPagination ( PageNumberPagination ) : <EOL> page_size = <NUM_LIT> <EOL> page_size_query_param = \"<STR_LIT>\" <EOL> max_page_size = <NUM_LIT> <EOL> django_paginator_class = DjangoPaginator <EOL> def paginate_queryset ( self , queryset , request , view = None ) : <EOL> empty = True <EOL> page_size = self . get_page_size ( request ) <EOL> if not page_size : <EOL> return None <EOL> paginator = self . django_paginator_class ( queryset , page_size ) <EOL> page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) <EOL> if page_number in self . last_page_strings : <EOL> page_number = paginator . num_pages <EOL> try : <EOL> self . page = paginator . page ( page_number ) <EOL> except InvalidPage as exc : <EOL> empty = False <EOL> if paginator . num_pages > <NUM_LIT> and self . template is not None : <EOL> self . display_page_controls = True <EOL> self . request = request <EOL> if not empty : <EOL> self . page = [ ] <EOL> return list ( self . page ) <EOL> def get_paginated_response ( self , data ) : <EOL> code = <NUM_LIT> <EOL> msg = '<STR_LIT>' <EOL> page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> <EOL> total = self . page . paginator . count if self . page else <NUM_LIT> <EOL> ", "gt": "limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT>"}
