{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> ", "gt": "batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , ["}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> ", "gt": "'<STR_LIT>' : feed_url ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> ", "gt": "'<STR_LIT>' : ctype } , priority = <NUM_LIT> )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ <EOL> '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) ) <EOL> ", "gt": "batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> ", "gt": "op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> ", "gt": "models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> ", "gt": "return flask . redirect ( masto_app . auth_redirect_url ( ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> server_default = None , <EOL> ", "gt": "nullable = True )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , nullable = False ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> ", "gt": "if meta_tag :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> ", "gt": "batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> ", "gt": "@ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> table , <EOL> [ { \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : generate_password_hash ( \"<STR_LIT>\" ) } ] )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) )"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> ", "gt": "def run_migrations_online ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> ", "gt": "existing_type = sa . String ( ) ,"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> ", "gt": "document . dump ( file )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True , server_default = \"<STR_LIT>\" ) )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> ", "gt": "def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> ", "gt": "entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> document . dump ( file ) <EOL> def add_if_not_exists ( feed ) : <EOL> query = db . select ( db . exists ( models . Feed ) <EOL> . where ( models . Feed . name == feed . name , models . Feed . user_id == feed . user_id ) ) <EOL> if db . session . execute ( query ) . scalar ( ) : <EOL> app . logger . info ( '<STR_LIT>' , feed . name ) <EOL> return <EOL> db . session . add ( feed ) <EOL> db . session . commit ( ) <EOL> feed . load_icon ( ) <EOL> db . session . commit ( ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ user_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> @ click . password_option ( ) <EOL> def user_add ( email , password ) : <EOL> ", "gt": "user = models . User ( email = email )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> ", "gt": "target = folder_outlines [ feed . folder ]"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True , server_default = \"<STR_LIT>\" ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> ", "gt": "display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] )"}
{"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> DELETE_AFTER_DAYS = <NUM_LIT> <EOL> RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT> <EOL> MASTODON_FETCH_LIMIT = <NUM_LIT> <EOL> ", "gt": "HUEY_POOL_SIZE = <NUM_LIT>"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> ", "gt": "def make_absolute ( url , path ) :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> ", "gt": "entry . id ( )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "pass"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> ", "gt": "return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ]"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> ", "gt": "'<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> ", "gt": "for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = \"<STR_LIT>\" ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( \"<STR_LIT>\" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> ", "gt": "masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) )"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> ", "gt": "prefix = \"<STR_LIT>\" ,"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> ", "gt": "context . run_migrations ( )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> ", "gt": "def all_meta ( soup ) :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> ", "gt": "feed = cls . from_valuelist ( * values )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> def extract_entry_ids ( response ) : <EOL> ", "gt": "entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> ", "gt": "return flask . render_template ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) <EOL> ", "gt": "with op . batch_alter_table ( \"<STR_LIT>\" ) as batch_op :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> ", "gt": "@ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> ", "gt": "@ login_required"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> def extract_entry_ids ( response ) : <EOL> entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text ) <EOL> entry_ids = [ ] <EOL> ", "gt": "for e in entry_ids_with_duplicates :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> def extract_entry_ids ( response ) : <EOL> entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text ) <EOL> entry_ids = [ ] <EOL> for e in entry_ids_with_duplicates : <EOL> ", "gt": "if e not in entry_ids :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , <EOL> sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> ", "gt": "sa . UniqueConstraint ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> ", "gt": "def extract ( url = None , html = None ) :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> ", "gt": "week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> ", "gt": "verifier = flask . request . form . get ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> ", "gt": "batch_op . drop_index ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> ", "gt": "connectable = engine_from_config ("}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> ", "gt": "op . bulk_insert ("}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> ", "gt": "masto_app = models . MastodonApp . get_or_create ( base_url )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> ", "gt": "mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> ", "gt": "@ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Boolean ( ) , nullable = True ) )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( \"<STR_LIT>\" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) <EOL> ", "gt": "if not masto_app :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , <EOL> ", "gt": "sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : json . dumps ( metadata ) , <EOL> ", "gt": "'<STR_LIT>' : icon_url }"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> ", "gt": "display_date = datetime . datetime . utcnow ( )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> ", "gt": "entry_url = f'<STR_LIT>'"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> ", "gt": "soup = BeautifulSoup ( html , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> ", "gt": "server_default = None ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . String ( ) , <EOL> ", "gt": "nullable = False )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> ", "gt": "op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> document . dump ( file ) <EOL> def add_if_not_exists ( feed ) : <EOL> query = db . select ( db . exists ( models . Feed ) <EOL> . where ( models . Feed . name == feed . name , models . Feed . user_id == feed . user_id ) ) <EOL> if db . session . execute ( query ) . scalar ( ) : <EOL> app . logger . info ( '<STR_LIT>' , feed . name ) <EOL> return <EOL> db . session . add ( feed ) <EOL> db . session . commit ( ) <EOL> feed . load_icon ( ) <EOL> db . session . commit ( ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> ", "gt": "@ user_cli . command ( '<STR_LIT>' )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> ", "gt": "app . logger . info ( '<STR_LIT>' , base_url )"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = \"<STR_LIT>\" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> target_metadata = target_metadata , <EOL> render_as_batch = True <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> if context . is_offline_mode ( ) : <EOL> run_migrations_offline ( ) <EOL> ", "gt": "else :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> ", "gt": "sa . UniqueConstraint ( '<STR_LIT>' )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> ", "gt": "html = requests . get ( url ) . content"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True , server_default = \"<STR_LIT>\" ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> document . dump ( file ) <EOL> def add_if_not_exists ( feed ) : <EOL> query = db . select ( db . exists ( models . Feed ) <EOL> . where ( models . Feed . name == feed . name , models . Feed . user_id == feed . user_id ) ) <EOL> if db . session . execute ( query ) . scalar ( ) : <EOL> app . logger . info ( '<STR_LIT>' , feed . name ) <EOL> return <EOL> db . session . add ( feed ) <EOL> db . session . commit ( ) <EOL> feed . load_icon ( ) <EOL> db . session . commit ( ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ user_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> ", "gt": "@ click . password_option ( )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> ", "gt": "title = feed . name ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> document . dump ( file ) <EOL> ", "gt": "def add_if_not_exists ( feed ) :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> def extract_entry_ids ( response ) : <EOL> entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text ) <EOL> entry_ids = [ ] <EOL> for e in entry_ids_with_duplicates : <EOL> if e not in entry_ids : <EOL> ", "gt": "entry_ids . append ( e )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> ", "gt": "img . attrs = { '<STR_LIT>' : img [ data_attr ] }"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> ", "gt": "existing_type = sa . INTEGER ( ) ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> ", "gt": "return flask . redirect ( flask . url_for ( '<STR_LIT>' ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) <EOL> with op . batch_alter_table ( \"<STR_LIT>\" ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> ", "gt": "op . create_index ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> ", "gt": "'<STR_LIT>' : ctype } , priority = <NUM_LIT> )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ <EOL> '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) ) <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> pass <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> ", "gt": "meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True )"}
{"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> DELETE_AFTER_DAYS = <NUM_LIT> <EOL> RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT> <EOL> ", "gt": "MASTODON_FETCH_LIMIT = <NUM_LIT>"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> ", "gt": "config . get_section ( config . config_ini_section , { } ) ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> ", "gt": "folder = folder ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> pass <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "pass"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> ", "gt": "batch_op . drop_index ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) <EOL> with op . batch_alter_table ( \"<STR_LIT>\" ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> ", "gt": "'<STR_LIT>' : url ,"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = \"<STR_LIT>\" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> target_metadata = target_metadata , <EOL> render_as_batch = True <EOL> ", "gt": ")"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> ", "gt": "for item in items :"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : url , <EOL> ", "gt": "'<STR_LIT>' : url ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> ", "gt": "@ app . get ( \"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) ) <EOL> ", "gt": "batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . BOOLEAN ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = \"<STR_LIT>\" ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> ", "gt": "base_url = flask . request . form . get ( '<STR_LIT>' )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> ", "gt": "return path"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Boolean ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> ", "gt": "sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Boolean ( ) , nullable = True ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> ", "gt": "entry . unbacklog ( )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> ", "gt": "fg . title ( f'<STR_LIT>' )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : url , <EOL> ", "gt": "'<STR_LIT>' : json . dumps ( metadata ) ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' , nullable = False )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> ", "gt": "fg . description ( f'<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( \"<STR_LIT>\" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) <EOL> if not masto_app : <EOL> app . logger . error ( \"<STR_LIT>\" , base_url ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> app . logger . info ( \"<STR_LIT>\" , current_user . id , base_url ) <EOL> account = masto_app . create_account ( current_user . id , code ) <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> ", "gt": "return flask . redirect ( flask . url_for ( '<STR_LIT>' , masto_acct = account . id ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> ", "gt": "op . execute ("}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> ", "gt": "batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> img . attrs = { '<STR_LIT>' : img [ data_attr ] } <EOL> for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : <EOL> del iframe [ '<STR_LIT>' ] <EOL> article [ '<STR_LIT>' ] = str ( soup ) <EOL> return article <EOL> def compress ( outfilename , article ) : <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> with zipfile . ZipFile ( outfilename , '<STR_LIT>' , compression = zipfile . ZIP_DEFLATED ) as zip : <EOL> for img in soup . findAll ( '<STR_LIT>' ) : <EOL> img_url = img [ '<STR_LIT>' ] <EOL> img_filename = '<STR_LIT>' + img [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> img [ '<STR_LIT>' ] = img_filename <EOL> ", "gt": "with requests . get ( img_url , stream = True ) as img_src , zip . open ( img_filename , mode = '<STR_LIT>' ) as img_dest :"}
{"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> DELETE_AFTER_DAYS = <NUM_LIT> <EOL> RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT> <EOL> MASTODON_FETCH_LIMIT = <NUM_LIT> <EOL> HUEY_POOL_SIZE = <NUM_LIT> <EOL> ", "gt": "DEFAULT_AUTH_USER = '<STR_LIT>'"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> ", "gt": "httpretty . register_uri ( httpretty . HEAD , url , adding_headers = {"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> ", "gt": "@ app . get ( \"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> table , <EOL> [ { \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : generate_password_hash ( \"<STR_LIT>\" ) } ] ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_table ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> ", "gt": "sa . PrimaryKeyConstraint ( '<STR_LIT>' ) ,"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> ", "gt": "@ app . post ( \"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> ", "gt": "table ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> ", "gt": "nullable = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> ", "gt": "def load_user_arg ( _ctx , _param , email ) :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> ", "gt": "links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> ", "gt": "entry . title ( item [ '<STR_LIT>' ] )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> DELETE_AFTER_DAYS = <NUM_LIT> <EOL> ", "gt": "RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT>"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> ", "gt": "if url :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> ", "gt": "base_url = flask . request . args . get ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> table , <EOL> [ { \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : generate_password_hash ( \"<STR_LIT>\" ) } ] ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> ", "gt": "batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> ", "gt": "def extract_entry_ids ( response ) :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> ", "gt": "article = json . loads ( r . stdout )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> table , <EOL> ", "gt": "[ { \"<STR_LIT>\" : <NUM_LIT> ,"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> ", "gt": "capture_output = True , check = True )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , <EOL> sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_table ( '<STR_LIT>' )"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> ", "gt": "entry . updated ( item [ '<STR_LIT>' ] )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ <EOL> '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> ", "gt": "sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) ,"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> img . attrs = { '<STR_LIT>' : img [ data_attr ] } <EOL> for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : <EOL> del iframe [ '<STR_LIT>' ] <EOL> article [ '<STR_LIT>' ] = str ( soup ) <EOL> return article <EOL> def compress ( outfilename , article ) : <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> with zipfile . ZipFile ( outfilename , '<STR_LIT>' , compression = zipfile . ZIP_DEFLATED ) as zip : <EOL> for img in soup . findAll ( '<STR_LIT>' ) : <EOL> img_url = img [ '<STR_LIT>' ] <EOL> img_filename = '<STR_LIT>' + img [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> img [ '<STR_LIT>' ] = img_filename <EOL> with requests . get ( img_url , stream = True ) as img_src , zip . open ( img_filename , mode = '<STR_LIT>' ) as img_dest : <EOL> shutil . copyfileobj ( img_src . raw , img_dest ) <EOL> ", "gt": "zip . writestr ( '<STR_LIT>' , str ( soup ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ", "gt": ")"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> ", "gt": "'<STR_LIT>' : datetime . datetime . utcnow ( ) ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> ", "gt": "def mastodon_oauth_submit ( ) :"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> ", "gt": "if outline . outlines :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : json . dumps ( metadata ) , <EOL> '<STR_LIT>' : icon_url } <EOL> ", "gt": "return entry"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> ", "gt": "'<STR_LIT>' )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( \"<STR_LIT>\" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) <EOL> if not masto_app : <EOL> app . logger . error ( \"<STR_LIT>\" , base_url ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> app . logger . info ( \"<STR_LIT>\" , current_user . id , base_url ) <EOL> account = masto_app . create_account ( current_user . id , code ) <EOL> ", "gt": "app . logger . info ( \"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) <EOL> with op . batch_alter_table ( \"<STR_LIT>\" ) as batch_op : <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> server_default = None , <EOL> nullable = True ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "pass"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> ", "gt": "add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . String ( ) , <EOL> nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> \"<STR_LIT>\" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> ", "gt": "base_url = f'<STR_LIT>'"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = \"<STR_LIT>\" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> target_metadata = target_metadata , <EOL> render_as_batch = True <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> if context . is_offline_mode ( ) : <EOL> run_migrations_offline ( ) <EOL> else : <EOL> ", "gt": "run_migrations_online ( )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f\"<STR_LIT>\" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> ", "gt": "entry = {"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> ", "gt": "batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False )"}
{"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> ", "gt": "DELETE_AFTER_DAYS = <NUM_LIT>"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> ", "gt": "app . logger . error ( \"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = \"<STR_LIT>\" ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> img . attrs = { '<STR_LIT>' : img [ data_attr ] } <EOL> for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : <EOL> del iframe [ '<STR_LIT>' ] <EOL> ", "gt": "article [ '<STR_LIT>' ] = str ( soup )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = \"<STR_LIT>\" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> target_metadata = target_metadata , <EOL> render_as_batch = True <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> ", "gt": "context . run_migrations ( )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> img . attrs = { '<STR_LIT>' : img [ data_attr ] } <EOL> for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : <EOL> del iframe [ '<STR_LIT>' ] <EOL> article [ '<STR_LIT>' ] = str ( soup ) <EOL> return article <EOL> def compress ( outfilename , article ) : <EOL> ", "gt": "soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> \"<STR_LIT>\" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( \"<STR_LIT>\" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( \"<STR_LIT>\" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> \"<STR_LIT>\" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> \"<STR_LIT>\" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( \"<STR_LIT>\" ) <EOL> ", "gt": "@ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> ", "gt": "op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> \"<STR_LIT>\" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( \"<STR_LIT>\" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( \"<STR_LIT>\" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> \"<STR_LIT>\" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> ", "gt": "soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> ", "gt": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}
{"input": "import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( \"<STR_LIT>\" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> \"<STR_LIT>\" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( \"<STR_LIT>\" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( \"<STR_LIT>\" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) <EOL> if not masto_app : <EOL> ", "gt": "app . logger . error ( \"<STR_LIT>\" , base_url )"}
{"input": "from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( \"<STR_LIT>\" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = \"<STR_LIT>\" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> ", "gt": "target_metadata = target_metadata ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_column ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , <EOL> sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
