{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : item [ '<STR_LIT>' ] ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> ", "gt": "'<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> continue <EOL> article_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( article ) , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : article_url , <EOL> ", "gt": "} )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> ", "gt": "nullable = True )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . execute ( \"<STR_LIT>\" ) <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> ", "gt": "'<STR_LIT>' : date ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> ", "gt": "continue"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . execute ( \"<STR_LIT>\" ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . drop_index ( '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> nullable = True ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> ", "gt": "nullable = False )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : item [ '<STR_LIT>' ] ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> ", "gt": "} )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> continue <EOL> article_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( article ) , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ", "gt": "'<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . execute ( \"<STR_LIT>\" ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> nullable = True ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> ", "gt": "return entry_values"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> continue <EOL> article_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( article ) , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : pub_date , <EOL> '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : article_url , <EOL> } ) <EOL> ", "gt": "return entry_values"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . execute ( \"<STR_LIT>\" )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> ", "gt": "'<STR_LIT>' : content_url ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> ", "gt": "pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( \"<STR_LIT>\" ) , nullable = True ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : content_url ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> ", "gt": "entry_values = [ ]"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : None ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> ", "gt": "entry_values . append ( {"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> ", "gt": "'<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> ", "gt": "content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ]"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> ", "gt": "continue"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> continue <EOL> article_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( article ) , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : article [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : pub_date , <EOL> ", "gt": "'<STR_LIT>' : pub_date ,"}
