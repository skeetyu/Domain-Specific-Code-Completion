{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> ", "gt": "pos_l = ("}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> ", "gt": "mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> ", "gt": "if args . no_wandb :"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> ", "gt": "text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT>"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ] <EOL> banner = '<STR_LIT>' . join ( banner_strings ) <EOL> ", "gt": "REPL ( ) . interact ( banner = banner )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> video_info , <EOL> ) <EOL> for video_info in row [ self . data_key ] <EOL> ] <EOL> videos_enc = self . module . processor ( <EOL> videos = [ list ( video ) for video in video_arrays ] , <EOL> text = [ \"<STR_LIT>\" ] , <EOL> ", "gt": "return_tensors = \"<STR_LIT>\" ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> ", "gt": "model : CLIPModel ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> ", "gt": "def run_dense_cap_on_model ("}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> ", "gt": "<NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\""}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" <EOL> ", "gt": "} , {"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> ", "gt": "if not osp . exists ( output ) :"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> ", "gt": "yield _write_convo ( k , audio_dataset [ k ] )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> ", "gt": "raise NotImplementedError ( \"<STR_LIT>\""}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> ", "gt": "i = <NUM_LIT>"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> ", "gt": "for k , v in sess . cookies . items ( )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> return result <EOL> async def aw_all ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> all_list = [ ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> ", "gt": "result = await coro"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ", "gt": ")"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ", "gt": "( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> ", "gt": "client = Client ( cookie , isproxy )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> ", "gt": "def name ( self ) -> str :"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] <EOL> print ( matriz3 ) <EOL> lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> print ( lista1 . pop ( <NUM_LIT> ) ) <EOL> ", "gt": "print ( lista1 )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> ", "gt": "terminate_after_contacts_on = [ \"<STR_LIT>\" ]"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> ", "gt": "async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response :"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> ", "gt": "@ property"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> token_count += toks <EOL> return token_count , caption_count , full_token_count , full_caption_count <EOL> def get_clip_token_lengths_for_localized_narratives ( ) : <EOL> toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "if __name__ == '<STR_LIT>' :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> ", "gt": "else :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> return result <EOL> async def aw_all ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> all_list = [ ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> all_list . append ( result ) <EOL> return all_list <EOL> def main ( title , artist , album ) : <EOL> return asyncio . run ( aw_main ( title , artist , album ) ) <EOL> def allin ( title , artist , album ) : <EOL> return asyncio . run ( aw_all ( title , artist , album ) ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> ", "gt": "print ( allin ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) )"}
{"input": "from setuptools import setup , find_packages <EOL> setup ( name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> author_email = '<STR_LIT>' , <EOL> license = \"<STR_LIT>\" , <EOL> packages = find_packages ( ) , <EOL> description = '<STR_LIT>' , <EOL> python_requires = '<STR_LIT>' , <EOL> install_requires = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> ", "gt": "<NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' ,"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> ", "gt": "WhisperAudioModality ("}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> ", "gt": "pickle . dump ( self . id , f )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> ", "gt": "self . num_tokens_output = num_tokens_output"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> ", "gt": "logger . info ( arg )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> ", "gt": "\"<STR_LIT>\" : {"}
{"input": "from unittest . mock import Mock <EOL> import pytest <EOL> from fastapi . testclient import TestClient <EOL> from simple_fastapi_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> yield client <EOL> def test_db_goes_boom ( client ) : <EOL> ", "gt": "db = Mock ( spec_set = Database )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> ", "gt": "'<STR_LIT>' ,"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> ", "gt": "discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) )"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_NO_CONTENT : \"<STR_LIT>\" ,"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> ", "gt": "\"<STR_LIT>\" : lambda : ["}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> ", "gt": "def set_id ( self , id ) :"}
{"input": "from . base import ma <EOL> from . user import UserSchema <EOL> from app . models import Message <EOL> class MessageSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = Message <EOL> ordered = True <EOL> author = ma . Nested ( UserSchema ) <EOL> ", "gt": "time = ma . auto_field ( )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> ", "gt": "if isinstance ( file_spec , mutagen . FileType ) :"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> pool . apply_async ( start_process , args = [ type_item , args . config ] ) <EOL> if terminal : <EOL> start_process ( terminal , args . config ) <EOL> pool . close ( ) <EOL> pool . join ( ) <EOL> ", "gt": "except Exception as e :"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> ", "gt": "def main ( args ) :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> ", "gt": "async def api_2 ( title , artist , album ) :"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> conn . commit ( ) <EOL> mi_cursor . close ( ) <EOL> ", "gt": "conn . close ( )"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_BAD_REQUEST : \"<STR_LIT>\" ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> ", "gt": "'<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> ", "gt": "dataset . processor = processor"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> ", "gt": "svc_type2 : type [ T2 ] ,"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> ", "gt": "cookie = get_cookie ( )"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> logger . warning ( arg . format ( * args ) ) <EOL> def error ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . error ( arg ) <EOL> else : <EOL> logger . error ( arg . format ( * args ) ) <EOL> def exception ( e ) : <EOL> ", "gt": "logger . exception ( e )"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> ", "gt": "logger . warning ( arg . format ( * args ) )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( other ) ) <EOL> return overlap / size_1 > thresh or overlap / size_2 > thresh <EOL> def near_equivalent_to ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> if size_1 / size_2 < thresh or size_2 / size_1 < thresh : <EOL> return False <EOL> difference = mask_size ( self . _get_xor_submask ( other ) ) <EOL> if ( difference / size_1 ) > ( <NUM_LIT> - thresh ) or ( difference / size_2 ) > ( <NUM_LIT> - thresh ) : <EOL> return False <EOL> return True <EOL> def union ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> new_mask = self . mask * <NUM_LIT> <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> ", "gt": "new_mask [ t2 : b2 , l2 : r2 ] += other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT>"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' , '<STR_LIT>' ) )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> ", "gt": "for row in registros :"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> ", "gt": "penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ]"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> ", "gt": "def group_correct ( result ) :"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> ", "gt": "size_1 = self . get_size ( ) + <NUM_LIT>"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> ", "gt": "@ pytest . mark . asyncio"}
{"input": "from flask import request , jsonify <EOL> from app import app , mongo , photos <EOL> import os <EOL> import uuid <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload ( ) : <EOL> if '<STR_LIT>' in request . files : <EOL> filename = photos . save ( request . files [ '<STR_LIT>' ] , name = str ( uuid . uuid4 ( ) ) + '<STR_LIT>' ) <EOL> ", "gt": "return jsonify ( { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : filename } )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } <EOL> def _split ( it , i ) : <EOL> return it [ : i ] , it [ i : ] <EOL> def parse_picture_block ( dat ) : <EOL> head , rest = _split ( dat , <NUM_LIT> * <NUM_LIT> ) <EOL> typeid , mime_len = struct . unpack ( '<STR_LIT>' , head ) <EOL> mime , rest = _split ( rest , mime_len ) <EOL> mime = mime . decode ( '<STR_LIT>' ) . lower ( ) <EOL> head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) <EOL> descr_len , = struct . unpack ( '<STR_LIT>' , head ) <EOL> ", "gt": "descr , rest = _split ( rest , descr_len )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> ", "gt": "e_context . action = EventAction . CONTINUE"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , image_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , group_correct_count / denominator ) <EOL> ", "gt": "return {"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> ", "gt": "} , {"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> ", "gt": "subtests = {"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> ", "gt": "mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } )"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" <EOL> ] <EOL> for question in list_questions : <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> ", "gt": "size_in = self . get_size ( ) + <NUM_LIT>"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> entry_points = { <EOL> ", "gt": "'<STR_LIT>' : ["}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> storage = IDStorage ( ) <EOL> id_value = storage . get_id ( ) <EOL> storage . set_id ( \"<STR_LIT>\" ) <EOL> if id_value is not None : <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> ", "gt": "return i18n_strings"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "except Exception as error :"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> ", "gt": "client = Client ( cookie , isproxy )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> ", "gt": "svc_type9 : type [ T9 ] ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> ", "gt": "if ( channel_type == const . WECHAT ) :"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> ", "gt": "query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( )"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> pool . apply_async ( start_process , args = [ type_item , args . config ] ) <EOL> if terminal : <EOL> start_process ( terminal , args . config ) <EOL> pool . close ( ) <EOL> pool . join ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" ) <EOL> ", "gt": "log . exception ( e )"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> ", "gt": "return await Database . connect ( config [ \"<STR_LIT>\" ] )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> if args . no_wandb : <EOL> ", "gt": "mode = \"<STR_LIT>\""}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ", "gt": ")"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> ", "gt": "svc_type2 : type [ T2 ] ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> ", "gt": "channel . _do_send_img ("}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , <EOL> self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , <EOL> self . HTTP_FORBIDDEN : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_METHOD_NOT_ALLOWED : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_INTERNAL_SERVER_ERROR : \"<STR_LIT>\" ,"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> ", "gt": "if ret is None and err == '<STR_LIT>' :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> ", "gt": "return None"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> ", "gt": "url = f\"<STR_LIT>\""}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> ", "gt": "CALLBACK_SUBMIT ,"}
{"input": "from flask import request , jsonify <EOL> from app import app , mongo , photos <EOL> import os <EOL> import uuid <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload ( ) : <EOL> if '<STR_LIT>' in request . files : <EOL> filename = photos . save ( request . files [ '<STR_LIT>' ] , name = str ( uuid . uuid4 ( ) ) + '<STR_LIT>' ) <EOL> return jsonify ( { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : filename } ) <EOL> ", "gt": "return jsonify ( { \"<STR_LIT>\" : \"<STR_LIT>\" } ) , <NUM_LIT>"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> if not data or not data . p : <EOL> raise Exception ( \"<STR_LIT>\" ) <EOL> horoscope = data . p . text <EOL> if lang and lang != NATIVE_LANG : <EOL> horoscope = text_translate ( data . p . text , lang ) <EOL> ", "gt": "return horoscope"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> ", "gt": "'<STR_LIT>' : '<STR_LIT>'"}
{"input": "import sys <EOL> try : <EOL> archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) <EOL> texto = archivo . readline ( ) <EOL> i = int ( texto . strip ( ) ) <EOL> except OSError as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> except ValueError : <EOL> \"<STR_LIT>\" <EOL> ", "gt": "except Exception as err :"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> ", "gt": "args . cols = <NUM_LIT>"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : <EOL> continue <EOL> ", "gt": "article_url = f'<STR_LIT>'"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> def run_dense_cap_on_lora ( lora_weight_path : str ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> run_dense_cap_on_model ( loaded , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> ", "gt": "from transformers import CLIPProcessor , CLIPModel"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> ", "gt": "token_count += toks"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> ", "gt": "if __name__ == \"<STR_LIT>\" :"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> app = Starlette ( <EOL> ", "gt": "lifespan = lifespan ,"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> if args . no_wandb : <EOL> mode = \"<STR_LIT>\" <EOL> wandb . init ( project = args . proj_name , name = args . exptid , entity = \"<STR_LIT>\" , group = args . exptid [ : <NUM_LIT> ] , mode = mode , dir = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> env , env_cfg = task_registry . make_env ( name = args . task , args = args ) <EOL> ", "gt": "ppo_runner , train_cfg = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> ", "gt": "if python_file is None :"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } <EOL> def _split ( it , i ) : <EOL> return it [ : i ] , it [ i : ] <EOL> def parse_picture_block ( dat ) : <EOL> head , rest = _split ( dat , <NUM_LIT> * <NUM_LIT> ) <EOL> typeid , mime_len = struct . unpack ( '<STR_LIT>' , head ) <EOL> mime , rest = _split ( rest , mime_len ) <EOL> mime = mime . decode ( '<STR_LIT>' ) . lower ( ) <EOL> head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) <EOL> descr_len , = struct . unpack ( '<STR_LIT>' , head ) <EOL> descr , rest = _split ( rest , descr_len ) <EOL> descr = descr . decode ( '<STR_LIT>' ) <EOL> ", "gt": "head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> )"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ) <EOL> ] , <EOL> ", "gt": "\"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> ", "gt": "else :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> ", "gt": "def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool :"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> ", "gt": "print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> ", "gt": "for row in registros :"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> while True : <EOL> ", "gt": "time . sleep ( <NUM_LIT> )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> ", "gt": "discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ]"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , <EOL> self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , <EOL> self . HTTP_FORBIDDEN : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_METHOD_NOT_ALLOWED : \"<STR_LIT>\" , <EOL> self . HTTP_INTERNAL_SERVER_ERROR : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_NOT_IMPLEMENTED : \"<STR_LIT>\" ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . execute ( \"<STR_LIT>\" ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from . base import db <EOL> class Message ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> content = db . Column ( db . Text , nullable = False ) <EOL> time = db . Column ( db . DateTime , nullable = False ) <EOL> author_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) <EOL> ", "gt": "author = db . relationship ( '<STR_LIT>' , lazy = True )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> ", "gt": "isproxy = get_proxy ( )"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> if isinstance ( check_prefix , str ) : <EOL> ", "gt": "e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ", "gt": "ds = Dataset . from_generator ( gen )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> ", "gt": "images += f\"<STR_LIT>\""}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> elapsed_time = time . time ( ) - t_start <EOL> ", "gt": "if elapsed_time < elapsed_time_expected :"}
{"input": "from flask import request , jsonify <EOL> from app import app , mongo , photos <EOL> import os <EOL> import uuid <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload ( ) : <EOL> if '<STR_LIT>' in request . files : <EOL> ", "gt": "filename = photos . save ( request . files [ '<STR_LIT>' ] , name = str ( uuid . uuid4 ( ) ) + '<STR_LIT>' )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> ", "gt": "date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> ", "gt": "/ ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> ", "gt": "svc_type2 : type [ T2 ] ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> ", "gt": "branch_labels : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> def call_reliable_openai ( ) : <EOL> ", "gt": "nonlocal error_count , failure_count"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ] <EOL> ", "gt": "banner = '<STR_LIT>' . join ( banner_strings )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> ", "gt": "'<STR_LIT>' . join ( str ( i ) for i in discnumber ) ,"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> ", "gt": "if len ( args ) == <NUM_LIT> :"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> ", "gt": "return self . id"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> video_info , <EOL> ) <EOL> for video_info in row [ self . data_key ] <EOL> ] <EOL> videos_enc = self . module . processor ( <EOL> videos = [ list ( video ) for video in video_arrays ] , <EOL> text = [ \"<STR_LIT>\" ] , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> padding = True , <EOL> ", "gt": ")"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> logger . warning ( arg . format ( * args ) ) <EOL> def error ( arg , * args ) : <EOL> ", "gt": "if len ( args ) == <NUM_LIT> :"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> images += f\"<STR_LIT>\" <EOL> e_context [ \"<STR_LIT>\" ] = images <EOL> return e_context <EOL> def send_images ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> method = self . handles . get ( type ( channel ) , None ) <EOL> if ( method ) : <EOL> ", "gt": "e_context = method ( e_context )"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , <EOL> self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , <EOL> self . HTTP_FORBIDDEN : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_METHOD_NOT_ALLOWED : \"<STR_LIT>\" , <EOL> self . HTTP_INTERNAL_SERVER_ERROR : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_IMPLEMENTED : \"<STR_LIT>\" , <EOL> ", "gt": "} . get ( status_code , \"<STR_LIT>\" )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ,"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> ", "gt": "async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response :"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> ", "gt": "'<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) ,"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> storage = IDStorage ( ) <EOL> ", "gt": "id_value = storage . get_id ( )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> ", "gt": "file = \"<STR_LIT>\""}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> token_count += toks <EOL> return token_count , caption_count , full_token_count , full_caption_count <EOL> def get_clip_token_lengths_for_localized_narratives ( ) : <EOL> toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> ", "gt": "maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : <NUM_LIT> ,"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> ", "gt": "for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) :"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> ", "gt": "@ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> import torch <EOL> def test_env ( args ) : <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> ) <EOL> env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) <EOL> for i in range ( int ( <NUM_LIT> * env . max_episode_length ) ) : <EOL> actions = <NUM_LIT> * torch . ones ( env . num_envs , env . num_actions , device = env . device ) <EOL> obs , _ , rew , done , info = env . step ( actions ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> ", "gt": "return [ waveform ]"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> ", "gt": "else :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> ", "gt": "}"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_ivf_added = faiss . extract_index_ivf ( index_added )"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> ", "gt": "full_token_count += toks"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> ", "gt": "print ( f\"<STR_LIT>\" )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> ", "gt": "'<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] ,"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> ", "gt": "+ i"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : None ,"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> ", "gt": "svc_type4 : type [ T4 ] ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> ", "gt": "json . dump ( cookies , f , indent = <NUM_LIT> )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> ", "gt": "\"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] ,"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "url += \"<STR_LIT>\""}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> def run_dense_cap_on_lora ( lora_weight_path : str ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> run_dense_cap_on_model ( loaded , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> ", "gt": "clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" )"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "from . base import ma <EOL> from . user import UserSchema <EOL> from app . models import Message <EOL> class MessageSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = Message <EOL> ", "gt": "ordered = True"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> if isinstance ( check_prefix , str ) : <EOL> e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> break <EOL> log . debug ( f\"<STR_LIT>\" ) <EOL> e_context . action = EventAction . CONTINUE <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = model <EOL> ", "gt": "return e_context"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> ", "gt": "print ("}
{"input": "from unittest . mock import Mock <EOL> import pytest <EOL> from fastapi . testclient import TestClient <EOL> from simple_fastapi_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> yield client <EOL> def test_db_goes_boom ( client ) : <EOL> db = Mock ( spec_set = Database ) <EOL> ", "gt": "db . get_user . side_effect = Exception ( \"<STR_LIT>\" )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> ", "gt": "} ,"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> import torch <EOL> def test_env ( args ) : <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> ) <EOL> ", "gt": "env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> ", "gt": "filename = mfile . filename"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> ", "gt": "} ,"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> app = Starlette ( <EOL> lifespan = lifespan , <EOL> middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , <EOL> routes = [ Route ( \"<STR_LIT>\" , get_user ) ] , <EOL> ", "gt": ")"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> def run_dense_cap_on_lora ( lora_weight_path : str ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> run_dense_cap_on_model ( loaded , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> ", "gt": "clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> ", "gt": "{ '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( other ) ) <EOL> ", "gt": "return overlap / size_1 > thresh or overlap / size_2 > thresh"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> ", "gt": "print ( f\"<STR_LIT>\" )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> ", "gt": "} )"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ XCLIPVideoModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ ImageBindModality ( ) ] , <EOL> ", "gt": "\"<STR_LIT>\" : lambda : [ DocumentGTEModality ( ) ] ,"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> if args . no_wandb : <EOL> mode = \"<STR_LIT>\" <EOL> ", "gt": "wandb . init ( project = args . proj_name , name = args . exptid , entity = \"<STR_LIT>\" , group = args . exptid [ : <NUM_LIT> ] , mode = mode , dir = \"<STR_LIT>\" )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> ", "gt": "pos_r = ("}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> logger . warning ( arg . format ( * args ) ) <EOL> def error ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> ", "gt": "logger . error ( arg )"}
{"input": "import sys <EOL> try : <EOL> archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) <EOL> texto = archivo . readline ( ) <EOL> i = int ( texto . strip ( ) ) <EOL> except OSError as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> ", "gt": "except ValueError :"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] <EOL> print ( matriz3 ) <EOL> lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> print ( lista1 . pop ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> print ( lista1 . remove ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> : ] <EOL> print ( lista1 ) <EOL> del lista1 [ : ] <EOL> ", "gt": "print ( lista1 )"}
{"input": "from . base import db <EOL> class Message ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> content = db . Column ( db . Text , nullable = False ) <EOL> time = db . Column ( db . DateTime , nullable = False ) <EOL> author_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) <EOL> author = db . relationship ( '<STR_LIT>' , lazy = True ) <EOL> target_channel = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' , ondelete = '<STR_LIT>' ) , nullable = False ) <EOL> def __repr__ ( self ) -> str : <EOL> ", "gt": "return f\"<STR_LIT>\""}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> ", "gt": "__all__ = [ '<STR_LIT>' , '<STR_LIT>' ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> images += f\"<STR_LIT>\" <EOL> e_context [ \"<STR_LIT>\" ] = images <EOL> ", "gt": "return e_context"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" ,"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> entry_points = { <EOL> '<STR_LIT>' : [ <EOL> '<STR_LIT>' , <EOL> ] <EOL> } <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> ", "gt": "def push ( self , line ) :"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> logger . warning ( arg . format ( * args ) ) <EOL> def error ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . error ( arg ) <EOL> else : <EOL> ", "gt": "logger . error ( arg . format ( * args ) )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> ", "gt": "\"<STR_LIT>\" : True ,"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> ", "gt": "discnumber = None"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> ", "gt": "tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ]"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> ", "gt": "CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> ", "gt": "if isinstance ( reply , list ) :"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> pass <EOL> class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> ", "gt": "load_run = - <NUM_LIT>"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> ", "gt": "registry . register_factory ( Database , connect_to_db )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> ", "gt": "res . headers [ \"<STR_LIT>\" ]"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , <EOL> ) <EOL> return None <EOL> resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : self . url ,"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> ", "gt": "print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) )"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> token_count += toks <EOL> return token_count , caption_count , full_token_count , full_caption_count <EOL> def get_clip_token_lengths_for_localized_narratives ( ) : <EOL> ", "gt": "toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' )"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , <EOL> self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_FORBIDDEN : \"<STR_LIT>\" ,"}
{"input": "import sys <EOL> try : <EOL> archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) <EOL> texto = archivo . readline ( ) <EOL> i = int ( texto . strip ( ) ) <EOL> except OSError as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> except ValueError : <EOL> \"<STR_LIT>\" <EOL> except Exception as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> ", "gt": "raise"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "depends_on : Union [ str , Sequence [ str ] , None ] = None"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> ", "gt": "lyrics_encode = lyrics_data [ \"<STR_LIT>\" ]"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> ", "gt": "url = \"<STR_LIT>\" ,"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> ", "gt": "return {"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "op . execute ( \"<STR_LIT>\" )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> ", "gt": "def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" :"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> ", "gt": "return self"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> ", "gt": "directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ]"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> ", "gt": "pool . apply_async ( start_process , args = [ type_item , args . config ] )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> for unused_key in unused_keys : <EOL> print ( \"<STR_LIT>\" , unused_key ) <EOL> print ( \"<STR_LIT>\" , len ( missing_keys ) ) <EOL> for missing_key in missing_keys : <EOL> print ( \"<STR_LIT>\" , missing_key ) <EOL> ", "gt": "code_keys_dict = OrderedDict ( ( s , s ) for s in code_keys )"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_not_trace_a_process_if_min_duration_not_reached ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> ", "gt": "assert int ( trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> ) == <NUM_LIT>"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" , \"<STR_LIT>\" ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> ", "gt": "e_context . action = EventAction . BREAK_PASS"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> ", "gt": "assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT>"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> ", "gt": "if ( check_prefix ) :"}
{"input": "from . base import ma <EOL> from . user import UserSchema <EOL> from app . models import Message <EOL> class MessageSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = Message <EOL> ordered = True <EOL> ", "gt": "author = ma . Nested ( UserSchema )"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\""}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> ", "gt": "if tamperers :"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> ", "gt": "elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) :"}
{"input": "from unittest . mock import Mock <EOL> import pytest <EOL> from fastapi . testclient import TestClient <EOL> from simple_fastapi_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> yield client <EOL> def test_db_goes_boom ( client ) : <EOL> db = Mock ( spec_set = Database ) <EOL> db . get_user . side_effect = Exception ( \"<STR_LIT>\" ) <EOL> ", "gt": "lifespan . registry . register_value ( Database , db )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> ", "gt": "if not os . path . exists ( filename ) :"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> ", "gt": "current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> if args . no_wandb : <EOL> mode = \"<STR_LIT>\" <EOL> wandb . init ( project = args . proj_name , name = args . exptid , entity = \"<STR_LIT>\" , group = args . exptid [ : <NUM_LIT> ] , mode = mode , dir = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> env , env_cfg = task_registry . make_env ( name = args . task , args = args ) <EOL> ppo_runner , train_cfg = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args ) <EOL> ", "gt": "ppo_runner . learn ( num_learning_iterations = train_cfg . runner . max_iterations , init_at_random_ep_len = True )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> ", "gt": "'<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text ,"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> token_count += toks <EOL> return token_count , caption_count , full_token_count , full_caption_count <EOL> ", "gt": "def get_clip_token_lengths_for_localized_narratives ( ) :"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> ", "gt": "entry_values . append ( {"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> entry_points = { <EOL> '<STR_LIT>' : [ <EOL> ", "gt": "'<STR_LIT>' ,"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> ", "gt": "if file :"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> ", "gt": "} , {"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> else : <EOL> logger . warning ( arg . format ( * args ) ) <EOL> def error ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . error ( arg ) <EOL> else : <EOL> logger . error ( arg . format ( * args ) ) <EOL> def exception ( e ) : <EOL> logger . exception ( e ) <EOL> ", "gt": "logger = _get_logger ( )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> ", "gt": "/ ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> ", "gt": "def handle_http ( self , e_context : EventContext ) :"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> ", "gt": "data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } )"}
{"input": "from setuptools import setup , find_packages <EOL> setup ( name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> author_email = '<STR_LIT>' , <EOL> license = \"<STR_LIT>\" , <EOL> packages = find_packages ( ) , <EOL> description = '<STR_LIT>' , <EOL> python_requires = '<STR_LIT>' , <EOL> install_requires = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" <EOL> ] , <EOL> ", "gt": ")"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> ", "gt": "standard_data = json . load ( file , object_pairs_hook = OrderedDict )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> ", "gt": "class asset ( LeggedRobotCfg . asset ) :"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> ", "gt": "for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) :"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> def call_reliable_openai ( ) : <EOL> nonlocal error_count , failure_count <EOL> ", "gt": "try :"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> images += f\"<STR_LIT>\" <EOL> e_context [ \"<STR_LIT>\" ] = images <EOL> return e_context <EOL> def send_images ( self , e_context : EventContext ) : <EOL> ", "gt": "channel = e_context [ '<STR_LIT>' ]"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> ", "gt": "load_video ("}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> ", "gt": "async def aget ("}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> ", "gt": "point_in_box ( b2 , l2 , self . get_tlbr ( ) )"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , image_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , group_correct_count / denominator ) <EOL> return { <EOL> '<STR_LIT>' : text_correct_count / denominator , <EOL> '<STR_LIT>' : image_correct_count / denominator , <EOL> '<STR_LIT>' : group_correct_count / denominator , <EOL> } <EOL> if __name__ == '<STR_LIT>' : <EOL> ", "gt": "from transformers import CLIPProcessor , CLIPModel"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> ", "gt": "test_bad_open_ai_call_with_q ( )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> ", "gt": "m = re . search ( r\"<STR_LIT>\" , content_disposition )"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> ", "gt": "result = openai . ChatCompletion . create ( model = model , messages = messages )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> ", "gt": "output_is_path = isinstance ( output , six . string_types )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> ", "gt": "mode = \"<STR_LIT>\""}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> ", "gt": "query , e_context [ '<STR_LIT>' ] )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> ", "gt": "action_scale = <NUM_LIT>"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> nullable = True ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> ", "gt": "existing_type = sa . VARCHAR ( ) ,"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> ", "gt": "afile . set_raw ( norm_key , _tag_name ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : <EOL> continue <EOL> pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> ", "gt": "if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) :"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> ", "gt": "await asyncio . sleep ( <NUM_LIT> )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> ", "gt": "class scales ( LeggedRobotCfg . rewards . scales ) :"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> if not data or not data . p : <EOL> ", "gt": "raise Exception ( \"<STR_LIT>\" )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) ,"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> ", "gt": "class domain_rand ( LeggedRobotCfg . domain_rand ) :"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> ", "gt": "for unused_key in unused_keys :"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> if isinstance ( check_prefix , str ) : <EOL> e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> break <EOL> log . debug ( f\"<STR_LIT>\" ) <EOL> ", "gt": "e_context . action = EventAction . CONTINUE"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> ", "gt": "title = data [ '<STR_LIT>' ]"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> ", "gt": "svc_type8 : type [ T8 ] ,"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . alter_column ( '<STR_LIT>' ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> elapsed_time = time . time ( ) - t_start <EOL> if elapsed_time < elapsed_time_expected : <EOL> time . sleep ( elapsed_time_expected - elapsed_time ) <EOL> ", "gt": "if not quiet :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> return result <EOL> async def aw_all ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> all_list = [ ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> all_list . append ( result ) <EOL> return all_list <EOL> def main ( title , artist , album ) : <EOL> return asyncio . run ( aw_main ( title , artist , album ) ) <EOL> ", "gt": "def allin ( title , artist , album ) :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ", "gt": ") . format ("}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> ", "gt": "return \"<STR_LIT>\""}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> pool . apply_async ( start_process , args = [ type_item , args . config ] ) <EOL> if terminal : <EOL> start_process ( terminal , args . config ) <EOL> pool . close ( ) <EOL> ", "gt": "pool . join ( )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> ", "gt": "self_collisions = <NUM_LIT>"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> ", "gt": "url = tb . launch ( )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> ", "gt": "args . rows = <NUM_LIT>"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> ", "gt": "classifiers = ["}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> ", "gt": "return ("}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text <EOL> directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for article in directory : <EOL> ", "gt": "if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> ", "gt": "overlap = mask_size ( self . _get_overlap_submask ( out_mask ) )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> video_info , <EOL> ) <EOL> for video_info in row [ self . data_key ] <EOL> ] <EOL> videos_enc = self . module . processor ( <EOL> videos = [ list ( video ) for video in video_arrays ] , <EOL> text = [ \"<STR_LIT>\" ] , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> padding = True , <EOL> ) <EOL> row_values . append ( videos_enc ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : <EOL> video_features = [ ] <EOL> for video_batch in encoded_values : <EOL> ", "gt": "video_features . append ( self . module . forward ( video_batch ) )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> ", "gt": "file_path = save_upload_file ( file )"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( \"<STR_LIT>\" ) , nullable = True ) )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> ", "gt": "index_filename_added = ("}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> ", "gt": "if tamperers :"}
{"input": "import sys <EOL> try : <EOL> archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) <EOL> texto = archivo . readline ( ) <EOL> i = int ( texto . strip ( ) ) <EOL> except OSError as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> except ValueError : <EOL> \"<STR_LIT>\" <EOL> except Exception as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> raise <EOL> ", "gt": "finally :"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> ", "gt": "with open ( '<STR_LIT>' , '<STR_LIT>' ) as f :"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] <EOL> print ( matriz3 ) <EOL> lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> print ( lista1 . pop ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> print ( lista1 . remove ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> : ] <EOL> print ( lista1 ) <EOL> ", "gt": "del lista1 [ : ]"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> ", "gt": "continue"}
{"input": "from setuptools import setup , find_packages <EOL> setup ( name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> author_email = '<STR_LIT>' , <EOL> license = \"<STR_LIT>\" , <EOL> packages = find_packages ( ) , <EOL> description = '<STR_LIT>' , <EOL> python_requires = '<STR_LIT>' , <EOL> install_requires = [ <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> ", "gt": "result = getattr ( result , subname )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } <EOL> def _split ( it , i ) : <EOL> return it [ : i ] , it [ i : ] <EOL> def parse_picture_block ( dat ) : <EOL> head , rest = _split ( dat , <NUM_LIT> * <NUM_LIT> ) <EOL> typeid , mime_len = struct . unpack ( '<STR_LIT>' , head ) <EOL> mime , rest = _split ( rest , mime_len ) <EOL> mime = mime . decode ( '<STR_LIT>' ) . lower ( ) <EOL> head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) <EOL> ", "gt": "descr_len , = struct . unpack ( '<STR_LIT>' , head )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> ", "gt": "silence_end = min ( total_frames , silence_start + self . max_sil_kept )"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> ", "gt": "if const . TERMINAL in channel_type :"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' ,"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> ", "gt": "randomize_base_mass = True"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> ", "gt": "sil_tags . append ( ( <NUM_LIT> , pos_r ) )"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , image_correct_count / denominator )"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ XCLIPVideoModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ ImageBindModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ DocumentGTEModality ( ) ] , <EOL> ", "gt": "\"<STR_LIT>\" : lambda : [ DocumentGTEModality ( num_tokens_output = <NUM_LIT> ) ] ,"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> ", "gt": "if isinstance ( check_prefix , str ) :"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' ,"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> ", "gt": "} ,"}
{"input": "from . base import db <EOL> class Message ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> content = db . Column ( db . Text , nullable = False ) <EOL> time = db . Column ( db . DateTime , nullable = False ) <EOL> author_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) <EOL> author = db . relationship ( '<STR_LIT>' , lazy = True ) <EOL> ", "gt": "target_channel = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' , ondelete = '<STR_LIT>' ) , nullable = False )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) <EOL> y = np . pad ( y , padding , mode = pad_mode ) <EOL> axis = - <NUM_LIT> <EOL> out_strides = y . strides + tuple ( [ y . strides [ axis ] ] ) <EOL> x_shape_trimmed = list ( y . shape ) <EOL> x_shape_trimmed [ axis ] -= frame_length - <NUM_LIT> <EOL> out_shape = tuple ( x_shape_trimmed ) + tuple ( [ frame_length ] ) <EOL> xw = np . lib . stride_tricks . as_strided ( y , shape = out_shape , strides = out_strides ) <EOL> if axis < <NUM_LIT> : <EOL> target_axis = axis - <NUM_LIT> <EOL> ", "gt": "else :"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> ", "gt": "elif os . path . exists ( os . path . expanduser ( filename ) ) :"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] <EOL> print ( matriz3 ) <EOL> lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> print ( lista1 . pop ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> print ( lista1 . remove ( <NUM_LIT> ) ) <EOL> ", "gt": "print ( lista1 )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ", "gt": "( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( )"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , image_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , group_correct_count / denominator ) <EOL> return { <EOL> '<STR_LIT>' : text_correct_count / denominator , <EOL> '<STR_LIT>' : image_correct_count / denominator , <EOL> '<STR_LIT>' : group_correct_count / denominator , <EOL> } <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> ", "gt": "clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> ", "gt": "request : Request ,"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( other ) ) <EOL> return overlap / size_1 > thresh or overlap / size_2 > thresh <EOL> def near_equivalent_to ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> if size_1 / size_2 < thresh or size_2 / size_1 < thresh : <EOL> return False <EOL> difference = mask_size ( self . _get_xor_submask ( other ) ) <EOL> if ( difference / size_1 ) > ( <NUM_LIT> - thresh ) or ( difference / size_2 ) > ( <NUM_LIT> - thresh ) : <EOL> return False <EOL> return True <EOL> def union ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> new_mask = self . mask * <NUM_LIT> <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> new_mask [ t2 : b2 , l2 : r2 ] += other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT> <EOL> ", "gt": "return EfficientMask ("}
{"input": "from unittest . mock import Mock <EOL> import pytest <EOL> from fastapi . testclient import TestClient <EOL> from simple_fastapi_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> yield client <EOL> ", "gt": "def test_db_goes_boom ( client ) :"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> nullable = True ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "while True :"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ", "gt": ") -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ..."}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> ", "gt": "silence_start = None"}
{"input": "from . base import db <EOL> class Message ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> content = db . Column ( db . Text , nullable = False ) <EOL> time = db . Column ( db . DateTime , nullable = False ) <EOL> ", "gt": "author_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False )"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> ", "gt": "if len ( submask ) != <NUM_LIT> :"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> ", "gt": "storage = IDStorage ( )"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> ", "gt": "print ( f\"<STR_LIT>\" )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> if __name__ == '<STR_LIT>' : <EOL> ", "gt": "app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = True , use_reloader = True )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , <EOL> ) <EOL> return None <EOL> resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . url , <EOL> \"<STR_LIT>\" : raw_payload , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> ", "gt": "return None"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> ", "gt": "soup = BeautifulSoup ( res . content , '<STR_LIT>' )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> ", "gt": "url_origin ,"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> ", "gt": "index_trained . train ( big_npy )"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> ", "gt": "return ret"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . execute ( \"<STR_LIT>\" ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> ", "gt": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> ", "gt": "'<STR_LIT>' ,"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> ", "gt": "self . req = requester"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ,"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> ", "gt": "raise ImportError ( f\"<STR_LIT>\" )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> ", "gt": "return jsonify ( history )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> ", "gt": "else :"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> ", "gt": "sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> ", "gt": "except ImportError :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> ", "gt": "res = np . full ( self . mask . shape , False )"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> storage = IDStorage ( ) <EOL> id_value = storage . get_id ( ) <EOL> storage . set_id ( \"<STR_LIT>\" ) <EOL> ", "gt": "if id_value is not None :"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) <EOL> parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> ", "gt": "main ( args )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> ", "gt": "pad_mode = \"<STR_LIT>\" ,"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> ", "gt": "trace_repo = InMemoryTraceRepository ( )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> ", "gt": "svc_type5 : type [ T5 ] ,"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , <EOL> ) <EOL> return None <EOL> resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ", "gt": "min_duration = <NUM_LIT>"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } <EOL> def _split ( it , i ) : <EOL> ", "gt": "return it [ : i ] , it [ i : ]"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> ", "gt": "discnumber = str ( afile . mfile . get ( _tag_name , None ) )"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> ", "gt": "headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) }"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_ivf_trained = faiss . extract_index_ivf ( index_trained )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> conn . commit ( ) <EOL> mi_cursor . close ( ) <EOL> conn . close ( ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> ", "gt": "async def aget ("}
{"input": "contador = <NUM_LIT> <EOL> class Vacia : <EOL> pass <EOL> numero = <NUM_LIT> <EOL> while True : <EOL> pregunta = int ( input ( \"<STR_LIT>\" ) ) <EOL> if pregunta == numero : <EOL> break <EOL> print ( \"<STR_LIT>\" ) <EOL> cadena = \"<STR_LIT>\" <EOL> ", "gt": "resultado = eval ( cadena )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> ", "gt": "images = \"<STR_LIT>\""}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> ", "gt": "index_ivf_added . nprobe = <NUM_LIT>"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> ", "gt": "self . callback ("}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> ", "gt": "method = self . method , url = self . url , params = params , data = data"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> ", "gt": "audio_dataset = load_dataset ( ** DATASET_ARGS )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> ", "gt": "svc_type6 : type [ T6 ] ,"}
{"input": "import time <EOL> from tensorboard import program <EOL> log_path = \"<STR_LIT>\" <EOL> def launch_tensorboard_pipeline ( ) : <EOL> tb = program . TensorBoard ( ) <EOL> tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) <EOL> url = tb . launch ( ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> ", "gt": "@ property"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> ", "gt": "state_storage . run_type = RunType . REPL"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> ", "gt": "return await retry_response . text ( )"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> ", "gt": "else :"}
{"input": "contador = <NUM_LIT> <EOL> class Vacia : <EOL> pass <EOL> numero = <NUM_LIT> <EOL> while True : <EOL> pregunta = int ( input ( \"<STR_LIT>\" ) ) <EOL> ", "gt": "if pregunta == numero :"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> ", "gt": "matriz2 . append ( [ fila [ i ] for fila in matriz ] )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> ", "gt": "{"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> ", "gt": "full_caption_count += <NUM_LIT>"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ,"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] <EOL> print ( matriz3 ) <EOL> lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> print ( lista1 . pop ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> print ( lista1 . remove ( <NUM_LIT> ) ) <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> ] <EOL> print ( lista1 ) <EOL> del lista1 [ <NUM_LIT> : ] <EOL> print ( lista1 ) <EOL> del lista1 [ : ] <EOL> print ( lista1 ) <EOL> ", "gt": "del lista1"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> video_info , <EOL> ) <EOL> for video_info in row [ self . data_key ] <EOL> ", "gt": "]"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> import torch <EOL> def test_env ( args ) : <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> ) <EOL> env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) <EOL> for i in range ( int ( <NUM_LIT> * env . max_episode_length ) ) : <EOL> actions = <NUM_LIT> * torch . ones ( env . num_envs , env . num_actions , device = env . device ) <EOL> obs , _ , rew , done , info = env . step ( actions ) <EOL> print ( \"<STR_LIT>\" ) <EOL> ", "gt": "if __name__ == '<STR_LIT>' :"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> ", "gt": "if __name__ == '<STR_LIT>' :"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> ", "gt": "if len ( channel_type ) == <NUM_LIT> :"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> self . HTTP_FOUND : \"<STR_LIT>\" , <EOL> self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , <EOL> self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , <EOL> self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , <EOL> self . HTTP_FORBIDDEN : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_NOT_FOUND : \"<STR_LIT>\" ,"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , <EOL> ) <EOL> return None <EOL> resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . url , <EOL> ", "gt": "\"<STR_LIT>\" : raw_payload ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> ", "gt": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ", "gt": ")"}
{"input": "contador = <NUM_LIT> <EOL> class Vacia : <EOL> pass <EOL> numero = <NUM_LIT> <EOL> while True : <EOL> pregunta = int ( input ( \"<STR_LIT>\" ) ) <EOL> if pregunta == numero : <EOL> break <EOL> print ( \"<STR_LIT>\" ) <EOL> ", "gt": "cadena = \"<STR_LIT>\""}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> ", "gt": "self . id = id"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> ", "gt": "tamperers : Union [ List [ Tamperer ] , None ] = None ,"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> ", "gt": "raise ValueError ( \"<STR_LIT>\" )"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> matriz2 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> matriz2 . append ( [ fila [ i ] for fila in matriz ] ) <EOL> print ( matriz2 ) <EOL> ", "gt": "matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ]"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> ", "gt": "print ( )"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ", "gt": "] ,"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> ", "gt": "conn . commit ( )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "await asyncio . sleep ( <NUM_LIT> )"}
{"input": "class Status : <EOL> HTTP_OK = <NUM_LIT> <EOL> HTTP_CREATED = <NUM_LIT> <EOL> HTTP_NO_CONTENT = <NUM_LIT> <EOL> HTTP_MOVED_PERMANENTLY = <NUM_LIT> <EOL> HTTP_FOUND = <NUM_LIT> <EOL> HTTP_NOT_MODIFIED = <NUM_LIT> <EOL> HTTP_BAD_REQUEST = <NUM_LIT> <EOL> HTTP_UNAUTHORIZED = <NUM_LIT> <EOL> HTTP_FORBIDDEN = <NUM_LIT> <EOL> HTTP_NOT_FOUND = <NUM_LIT> <EOL> HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> <EOL> HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> <EOL> HTTP_NOT_IMPLEMENTED = <NUM_LIT> <EOL> @ classmethod <EOL> def get_message ( self , status_code ) : <EOL> return { <EOL> self . HTTP_OK : \"<STR_LIT>\" , <EOL> self . HTTP_CREATED : \"<STR_LIT>\" , <EOL> self . HTTP_NO_CONTENT : \"<STR_LIT>\" , <EOL> self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , <EOL> ", "gt": "self . HTTP_FOUND : \"<STR_LIT>\" ,"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> images += f\"<STR_LIT>\" <EOL> e_context [ \"<STR_LIT>\" ] = images <EOL> return e_context <EOL> ", "gt": "def send_images ( self , e_context : EventContext ) :"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> ", "gt": "mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) )"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> ", "gt": "\"<STR_LIT>\" : lambda : [ XCLIPVideoModality ( num_tokens_output = <NUM_LIT> ) ] ,"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> ", "gt": "return build_mlp_vector_projector ("}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ", "gt": "]"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> import torch <EOL> def test_env ( args ) : <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> ", "gt": "env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> for unused_key in unused_keys : <EOL> print ( \"<STR_LIT>\" , unused_key ) <EOL> print ( \"<STR_LIT>\" , len ( missing_keys ) ) <EOL> for missing_key in missing_keys : <EOL> print ( \"<STR_LIT>\" , missing_key ) <EOL> code_keys_dict = OrderedDict ( ( s , s ) for s in code_keys ) <EOL> ", "gt": "with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> ", "gt": "return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> ", "gt": "unused_keys = standard_keys - code_keys"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> ", "gt": "else :"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> ", "gt": "svc_type4 : type [ T4 ] ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> ", "gt": "tmp_file = tempfile . mktemp ("}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> ", "gt": "cookie = get_cookie ( )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> ", "gt": "pass"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> ", "gt": "if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> ", "gt": "self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] )"}
{"input": "import sys <EOL> try : <EOL> archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) <EOL> texto = archivo . readline ( ) <EOL> i = int ( texto . strip ( ) ) <EOL> except OSError as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> except ValueError : <EOL> \"<STR_LIT>\" <EOL> except Exception as err : <EOL> print ( \"<STR_LIT>\" , err ) <EOL> raise <EOL> finally : <EOL> archivo . close ( ) <EOL> ", "gt": "print ( texto )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> def run_dense_cap_on_lora ( lora_weight_path : str ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> ", "gt": "base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : ROLE_ASSISTANT ,"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> return result <EOL> async def aw_all ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> all_list = [ ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> all_list . append ( result ) <EOL> return all_list <EOL> def main ( title , artist , album ) : <EOL> return asyncio . run ( aw_main ( title , artist , album ) ) <EOL> def allin ( title , artist , album ) : <EOL> ", "gt": "return asyncio . run ( aw_all ( title , artist , album ) )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> ", "gt": "row_values = [ ]"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> except Exception as error : <EOL> print ( f\"<STR_LIT>\" ) <EOL> if \"<STR_LIT>\" in str ( error ) : <EOL> print ( <EOL> \"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> ", "gt": "banner_strings = ["}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> ", "gt": "filename = file_spec"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> def call_reliable_openai ( ) : <EOL> nonlocal error_count , failure_count <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> future_calls = [ executor . submit ( call_reliable_openai ) for _ in range ( <NUM_LIT> ) ] <EOL> ", "gt": "concurrent . futures . wait ( future_calls )"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> ", "gt": "result = openai . ChatCompletion . create ( model = model , messages = messages )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> ", "gt": "def fetch ( self ) :"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> ", "gt": "pass"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> ", "gt": "padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> ", "gt": "requester : HTTPRequester ,"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> ", "gt": "def upgrade ( ) -> None :"}
{"input": "from . base import db <EOL> class Message ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> content = db . Column ( db . Text , nullable = False ) <EOL> ", "gt": "time = db . Column ( db . DateTime , nullable = False )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> ", "gt": "result = client . reset_all ( )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> ", "gt": "self . data = data if data else { }"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> ", "gt": "\"<STR_LIT>\" : \"<STR_LIT>\" ,"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , task_registry <EOL> from shutil import copyfile <EOL> import torch <EOL> import wandb <EOL> def train ( args ) : <EOL> args . headless = True <EOL> log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid <EOL> try : <EOL> os . makedirs ( log_pth ) <EOL> except : <EOL> pass <EOL> if args . debug : <EOL> mode = \"<STR_LIT>\" <EOL> args . rows = <NUM_LIT> <EOL> args . cols = <NUM_LIT> <EOL> args . num_envs = <NUM_LIT> <EOL> else : <EOL> mode = \"<STR_LIT>\" <EOL> if args . no_wandb : <EOL> mode = \"<STR_LIT>\" <EOL> wandb . init ( project = args . proj_name , name = args . exptid , entity = \"<STR_LIT>\" , group = args . exptid [ : <NUM_LIT> ] , mode = mode , dir = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) <EOL> ", "gt": "env , env_cfg = task_registry . make_env ( name = args . task , args = args )"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> video_info , <EOL> ) <EOL> for video_info in row [ self . data_key ] <EOL> ] <EOL> videos_enc = self . module . processor ( <EOL> videos = [ list ( video ) for video in video_arrays ] , <EOL> text = [ \"<STR_LIT>\" ] , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> padding = True , <EOL> ) <EOL> ", "gt": "row_values . append ( videos_enc )"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> entry_points = { <EOL> '<STR_LIT>' : [ <EOL> '<STR_LIT>' , <EOL> ] <EOL> ", "gt": "}"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> ", "gt": "num_tokens = self . num_tokens_output ,"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> if isinstance ( check_prefix , str ) : <EOL> e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> break <EOL> log . debug ( f\"<STR_LIT>\" ) <EOL> e_context . action = EventAction . CONTINUE <EOL> ", "gt": "e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = model"}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> ", "gt": "existing_type = sa . VARCHAR ( ) ,"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , image_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , group_correct_count / denominator ) <EOL> return { <EOL> '<STR_LIT>' : text_correct_count / denominator , <EOL> '<STR_LIT>' : image_correct_count / denominator , <EOL> '<STR_LIT>' : group_correct_count / denominator , <EOL> } <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> ", "gt": "run_winoground ( clip_model , clip_processor )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> ", "gt": "for url in reply :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> ", "gt": "if not self . _bbox_overlaps ( other ) :"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> ", "gt": "url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date )"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> missing_keys = code_keys - standard_keys <EOL> print ( \"<STR_LIT>\" , len ( unused_keys ) ) <EOL> for unused_key in unused_keys : <EOL> print ( \"<STR_LIT>\" , unused_key ) <EOL> print ( \"<STR_LIT>\" , len ( missing_keys ) ) <EOL> for missing_key in missing_keys : <EOL> ", "gt": "print ( \"<STR_LIT>\" , missing_key )"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> ", "gt": "terminal = channel_type . pop ( index )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> ", "gt": "if lyrics_data :"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> if not data or not data . p : <EOL> raise Exception ( \"<STR_LIT>\" ) <EOL> horoscope = data . p . text <EOL> if lang and lang != NATIVE_LANG : <EOL> ", "gt": "horoscope = text_translate ( data . p . text , lang )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> ", "gt": "index_trained = faiss . index_factory ("}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> ", "gt": "return entry_values"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> standard_keys = set ( standard_data . keys ( ) ) <EOL> unused_keys = standard_keys - code_keys <EOL> ", "gt": "missing_keys = code_keys - standard_keys"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> ", "gt": "for key , args in subtests . items ( ) :"}
{"input": "from unittest . mock import Mock <EOL> import pytest <EOL> from fastapi . testclient import TestClient <EOL> from simple_fastapi_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> ", "gt": "yield client"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> ", "gt": "print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" )"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> ", "gt": "def gen ( ) :"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> ", "gt": "async with aiohttp . ClientSession ( ) as session :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> ", "gt": "t_start = time . time ( )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } <EOL> def _split ( it , i ) : <EOL> return it [ : i ] , it [ i : ] <EOL> def parse_picture_block ( dat ) : <EOL> head , rest = _split ( dat , <NUM_LIT> * <NUM_LIT> ) <EOL> typeid , mime_len = struct . unpack ( '<STR_LIT>' , head ) <EOL> mime , rest = _split ( rest , mime_len ) <EOL> mime = mime . decode ( '<STR_LIT>' ) . lower ( ) <EOL> head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) <EOL> descr_len , = struct . unpack ( '<STR_LIT>' , head ) <EOL> descr , rest = _split ( rest , descr_len ) <EOL> descr = descr . decode ( '<STR_LIT>' ) <EOL> head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) <EOL> ", "gt": "width , height , cdepth , cidx , dat_len = struct . unpack ( '<STR_LIT>' , head )"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> <EOL> group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> <EOL> denominator = len ( winoground_clip_scores ) <EOL> print ( \"<STR_LIT>\" , text_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , image_correct_count / denominator ) <EOL> print ( \"<STR_LIT>\" , group_correct_count / denominator ) <EOL> return { <EOL> '<STR_LIT>' : text_correct_count / denominator , <EOL> '<STR_LIT>' : image_correct_count / denominator , <EOL> '<STR_LIT>' : group_correct_count / denominator , <EOL> } <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> ", "gt": "clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" )"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> ", "gt": "model = selector . get ( '<STR_LIT>' )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <EOL> ", "gt": "<NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' ,"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> ", "gt": "processor : CLIPProcessor ,"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_not_trace_a_process_if_min_duration_not_reached ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> ", "gt": "assert len ( trace_repo . traces ) == <NUM_LIT>"}
{"input": "from setuptools import setup , find_packages <EOL> setup ( name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> author_email = '<STR_LIT>' , <EOL> license = \"<STR_LIT>\" , <EOL> packages = find_packages ( ) , <EOL> ", "gt": "description = '<STR_LIT>' ,"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ", "gt": "ds . save_to_disk ( args . output_folder )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> ", "gt": "'<STR_LIT>' : author ,"}
{"input": "from setuptools import setup , find_packages <EOL> setup ( name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> author_email = '<STR_LIT>' , <EOL> license = \"<STR_LIT>\" , <EOL> packages = find_packages ( ) , <EOL> description = '<STR_LIT>' , <EOL> ", "gt": "python_requires = '<STR_LIT>' ,"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> ", "gt": "return self . num_tokens_output"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> ", "gt": "client = Client ( cookie , isproxy )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> ", "gt": "\"<STR_LIT>\" : '<STR_LIT>' ,"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> ", "gt": "registros = mi_cursor . fetchall ( )"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> except ValueError : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) <EOL> res = requests . get ( url ) <EOL> if res . status_code != Status . HTTP_OK : <EOL> raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> if not data or not data . p : <EOL> raise Exception ( \"<STR_LIT>\" ) <EOL> ", "gt": "horoscope = data . p . text"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ] <EOL> banner = '<STR_LIT>' . join ( banner_strings ) <EOL> REPL ( ) . interact ( banner = banner ) <EOL> else : <EOL> builtins . __import__ = import_wrapper <EOL> ", "gt": "spec = importlib . util . spec_from_file_location ( '<STR_LIT>' , os . path . abspath ( python_file ) )"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> ", "gt": "else :"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> ", "gt": "async def lifespan ("}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> ", "gt": "def select_model ( self , e_context : EventContext ) :"}
{"input": "import ast <EOL> import json <EOL> from pathlib import Path <EOL> from collections import OrderedDict <EOL> def extract_i18n_strings ( node ) : <EOL> i18n_strings = [ ] <EOL> if ( <EOL> isinstance ( node , ast . Call ) <EOL> and isinstance ( node . func , ast . Name ) <EOL> and node . func . id == \"<STR_LIT>\" <EOL> ) : <EOL> for arg in node . args : <EOL> if isinstance ( arg , ast . Str ) : <EOL> i18n_strings . append ( arg . s ) <EOL> for child_node in ast . iter_child_nodes ( node ) : <EOL> i18n_strings . extend ( extract_i18n_strings ( child_node ) ) <EOL> return i18n_strings <EOL> def process_file ( file_path ) : <EOL> with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> code = file . read ( ) <EOL> if \"<STR_LIT>\" in code : <EOL> tree = ast . parse ( code ) <EOL> i18n_strings = extract_i18n_strings ( tree ) <EOL> print ( file_path , len ( i18n_strings ) ) <EOL> return i18n_strings <EOL> return [ ] <EOL> py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) <EOL> code_keys = set ( ) <EOL> for py_file in py_files : <EOL> strings = process_file ( py_file ) <EOL> code_keys . update ( strings ) <EOL> print ( ) <EOL> print ( \"<STR_LIT>\" , len ( code_keys ) ) <EOL> standard_file = \"<STR_LIT>\" <EOL> with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : <EOL> standard_data = json . load ( file , object_pairs_hook = OrderedDict ) <EOL> ", "gt": "standard_keys = set ( standard_data . keys ( ) )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> reply = e_context [ \"<STR_LIT>\" ] <EOL> if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : <EOL> if isinstance ( reply , list ) : <EOL> images = \"<STR_LIT>\" <EOL> for url in reply : <EOL> images += f\"<STR_LIT>\" <EOL> e_context [ \"<STR_LIT>\" ] = images <EOL> return e_context <EOL> def send_images ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> method = self . handles . get ( type ( channel ) , None ) <EOL> ", "gt": "if ( method ) :"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> try : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> discnumber = None <EOL> return discnumber <EOL> def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> ", "gt": "PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> ", "gt": "'<STR_LIT>' : item [ '<STR_LIT>' ] ,"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> video_arrays = [ <EOL> load_video ( <EOL> ", "gt": "video_info ,"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> ", "gt": "@ app . route ( '<STR_LIT>' )"}
{"input": "contador = <NUM_LIT> <EOL> class Vacia : <EOL> pass <EOL> numero = <NUM_LIT> <EOL> while True : <EOL> pregunta = int ( input ( \"<STR_LIT>\" ) ) <EOL> if pregunta == numero : <EOL> break <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> ", "gt": "\"<STR_LIT>\" : {"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ", "gt": "ret = None"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> ", "gt": "async def test_should_not_trace_a_process_if_min_duration_not_reached ( ) :"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> ", "gt": "conversation_id = data [ '<STR_LIT>' ]"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> conn . commit ( ) <EOL> ", "gt": "mi_cursor . close ( )"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> try : <EOL> datetime . strptime ( date , '<STR_LIT>' ) <EOL> ", "gt": "except ValueError :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> ", "gt": "return overlap / size_in > thresh"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> ", "gt": "print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> pass <EOL> class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> ", "gt": "experiment_name = '<STR_LIT>'"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> ", "gt": "lm_hidden_size = lm_hidden_size ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> ", "gt": "output = osp . join ( output , filename_from_url )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> ", "gt": "result = current_context . import_here ( base_name )"}
{"input": "import mysql . connector <EOL> from mysql . connector import errorcode <EOL> config = { <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : \"<STR_LIT>\" , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> try : <EOL> conn = mysql . connector . connect ( ** config ) <EOL> print ( \"<STR_LIT>\" ) <EOL> except mysql . connector . Error as err : <EOL> if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> elif err . errno == errorcode . ER_BAD_DB_ERROR : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( err ) <EOL> else : <EOL> mi_cursor = conn . cursor ( ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> mi_cursor . execute ( \"<STR_LIT>\" ) <EOL> registros = mi_cursor . fetchall ( ) <EOL> print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) <EOL> for row in registros : <EOL> print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> ", "gt": "svc_type2 : type [ T2 ] ,"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> ", "gt": "svc_type3 : type [ T3 ] ,"}
{"input": "contador = <NUM_LIT> <EOL> class Vacia : <EOL> pass <EOL> numero = <NUM_LIT> <EOL> while True : <EOL> pregunta = int ( input ( \"<STR_LIT>\" ) ) <EOL> if pregunta == numero : <EOL> break <EOL> print ( \"<STR_LIT>\" ) <EOL> cadena = \"<STR_LIT>\" <EOL> resultado = eval ( cadena ) <EOL> ", "gt": "print ( resultado )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> ", "gt": "for coro in asyncio . as_completed ( await_list ) :"}
{"input": "from flask import request , jsonify <EOL> from app import app , mongo , photos <EOL> import os <EOL> import uuid <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> ", "gt": "def upload ( ) :"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> ", "gt": "if total is not None :"}
{"input": "from flask import request , jsonify <EOL> from app import app , mongo , photos <EOL> import os <EOL> import uuid <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload ( ) : <EOL> ", "gt": "if '<STR_LIT>' in request . files :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> ", "gt": "else :"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> ", "gt": "else :"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> ", "gt": "queue_requests = True )"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> ", "gt": "assert len ( trace_repo . traces [ <NUM_LIT> ] . data )"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> caption_count += <NUM_LIT> <EOL> token_count += toks <EOL> ", "gt": "return token_count , caption_count , full_token_count , full_caption_count"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> ", "gt": "return entry_values"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> else : <EOL> sil_tags . append ( ( pos_l , pos_r ) ) <EOL> clip_start = pos_r <EOL> silence_start = None <EOL> total_frames = rms_list . shape [ <NUM_LIT> ] <EOL> if ( <EOL> silence_start is not None <EOL> and total_frames - silence_start >= self . min_interval <EOL> ) : <EOL> silence_end = min ( total_frames , silence_start + self . max_sil_kept ) <EOL> pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) <EOL> if not sil_tags : <EOL> return [ waveform ] <EOL> else : <EOL> chunks = [ ] <EOL> if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : <EOL> chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) <EOL> for i in range ( len ( sil_tags ) - <NUM_LIT> ) : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ) <EOL> if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : <EOL> chunks . append ( <EOL> self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) <EOL> ) <EOL> return chunks <EOL> def get_rms ( <EOL> y , <EOL> frame_length = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> pad_mode = \"<STR_LIT>\" , <EOL> ) : <EOL> padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) <EOL> y = np . pad ( y , padding , mode = pad_mode ) <EOL> axis = - <NUM_LIT> <EOL> out_strides = y . strides + tuple ( [ y . strides [ axis ] ] ) <EOL> x_shape_trimmed = list ( y . shape ) <EOL> x_shape_trimmed [ axis ] -= frame_length - <NUM_LIT> <EOL> ", "gt": "out_shape = tuple ( x_shape_trimmed ) + tuple ( [ frame_length ] )"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> ", "gt": "fila_matriz1 = [ ]"}
{"input": "import numpy as np <EOL> import os <EOL> from datetime import datetime <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> import torch <EOL> def test_env ( args ) : <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> ) <EOL> env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) <EOL> ", "gt": "for i in range ( int ( <NUM_LIT> * env . max_episode_length ) ) :"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> ", "gt": "\"<STR_LIT>\" :"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> ", "gt": "index_filepath_added = os . path . join ( exp_dir , index_filename_added )"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False ) <EOL> ", "gt": "def downgrade ( ) -> None :"}
{"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> full_caption_count = <NUM_LIT> <EOL> full_token_count = <NUM_LIT> <EOL> for n in loader . load_annotations ( ln_split ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> full_caption_count += <NUM_LIT> <EOL> full_token_count += toks <EOL> if toks > <NUM_LIT> : <EOL> continue <EOL> ", "gt": "caption_count += <NUM_LIT>"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> ", "gt": "base_height_target = <NUM_LIT>"}
{"input": "from . base import ma <EOL> from . user import UserSchema <EOL> from app . models import Message <EOL> class MessageSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = Message <EOL> ordered = True <EOL> author = ma . Nested ( UserSchema ) <EOL> time = ma . auto_field ( ) <EOL> ", "gt": "content = ma . auto_field ( )"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> except Exception as error : <EOL> print ( f\"<STR_LIT>\" ) <EOL> if \"<STR_LIT>\" in str ( error ) : <EOL> print ( <EOL> ", "gt": "\"<STR_LIT>\""}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> ", "gt": "only_positive_rewards = True"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> logger . debug ( arg . format ( * args ) ) <EOL> def info ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . info ( arg ) <EOL> else : <EOL> logger . info ( arg . format ( * args ) ) <EOL> def warn ( arg , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . warning ( arg ) <EOL> ", "gt": "else :"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( other ) ) <EOL> return overlap / size_1 > thresh or overlap / size_2 > thresh <EOL> def near_equivalent_to ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> if size_1 / size_2 < thresh or size_2 / size_1 < thresh : <EOL> return False <EOL> difference = mask_size ( self . _get_xor_submask ( other ) ) <EOL> if ( difference / size_1 ) > ( <NUM_LIT> - thresh ) or ( difference / size_2 ) > ( <NUM_LIT> - thresh ) : <EOL> return False <EOL> return True <EOL> def union ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> new_mask = self . mask * <NUM_LIT> <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> new_mask [ t2 : b2 , l2 : r2 ] += other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT> <EOL> return EfficientMask ( <EOL> mask = cast ( np . ndarray , new_mask > <NUM_LIT> ) , <EOL> score = ( self . score + other . score ) / <NUM_LIT> , <EOL> ) <EOL> ", "gt": "def subtract ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" :"}
{"input": "from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> ", "gt": "batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False )"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> print ( matriz1 ) <EOL> ", "gt": "matriz2 = [ ]"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ", "gt": "app = Starlette ("}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> ", "gt": "if discnumber in ( None , '<STR_LIT>' ) :"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> ", "gt": "except Exception as e :"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ", "gt": "]"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> ", "gt": "actuator_net_file = \"<STR_LIT>\""}
{"input": "from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . VARCHAR ( ) , <EOL> ", "gt": "nullable = True )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> ", "gt": "discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ]"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> ", "gt": "batch_size_add = <NUM_LIT>"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> ", "gt": "raise CommentFormatError ( '<STR_LIT>' )"}
{"input": "import pytest <EOL> import asyncio <EOL> from profyle . application . profyle import profyle <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_a_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_an_async_process ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_trace_a_process_with_min_duration ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) <EOL> assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> <EOL> @ pytest . mark . asyncio <EOL> async def test_should_not_trace_a_process_if_min_duration_not_reached ( ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> with profyle ( <EOL> name = \"<STR_LIT>\" , <EOL> repo = trace_repo , <EOL> min_duration = <NUM_LIT> <EOL> ) : <EOL> ", "gt": "await asyncio . sleep ( <NUM_LIT> )"}
{"input": "from collections import namedtuple <EOL> import re <EOL> import struct <EOL> string_types = str <EOL> def as_str ( value ) : <EOL> if isinstance ( value , ( list , tuple ) ) : <EOL> value = '<STR_LIT>' . join ( value ) <EOL> return str ( value ) <EOL> def sanitize_year ( year ) : <EOL> try : <EOL> if '<STR_LIT>' in year : <EOL> year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except TypeError : <EOL> pass <EOL> try : <EOL> year = int ( year ) <EOL> except ValueError : <EOL> if re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ : <NUM_LIT> ] ) <EOL> elif re . match ( r'<STR_LIT>' , year ) : <EOL> year = int ( year [ - <NUM_LIT> : ] ) <EOL> else : <EOL> raise ValueError ( \"<STR_LIT>\" . format ( year ) ) <EOL> return year <EOL> def sanitize_int ( val ) : <EOL> try : <EOL> ret = int ( val ) <EOL> except ValueError : <EOL> m = re . match ( r'<STR_LIT>' , val ) <EOL> if m : <EOL> ret = int ( m . group ( <NUM_LIT> ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' . format ( val ) ) <EOL> return ret <EOL> def sanitize_bool ( val ) : <EOL> val = str ( val ) . strip ( ) . lower ( ) <EOL> if val in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return True <EOL> elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> return False <EOL> else : <EOL> return int ( val ) != <NUM_LIT> <EOL> def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return tracknumber <EOL> def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if tracknumber in ( None , '<STR_LIT>' ) : <EOL> tracknumber = None <EOL> else : <EOL> try : <EOL> tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> except IndexError : <EOL> tracknumber = None <EOL> return tracknumber <EOL> def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) <EOL> tracknumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , <EOL> appendable = False ) <EOL> def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = str ( afile . mfile . get ( _tag_name , None ) ) <EOL> if discnumber in ( None , '<STR_LIT>' ) : <EOL> discnumber = None <EOL> else : <EOL> discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return discnumber <EOL> def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : <EOL> discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] <EOL> discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) <EOL> discnumber [ <NUM_LIT> ] = val <EOL> afile . set_raw ( norm_key , _tag_name , <EOL> ", "gt": "'<STR_LIT>' . join ( str ( i ) for i in discnumber ) ,"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> app = Starlette ( <EOL> lifespan = lifespan , <EOL> ", "gt": "middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] ,"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> pass <EOL> class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> ", "gt": "run_name = '<STR_LIT>'"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> ", "gt": "author = author . text"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> ", "gt": "point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> pool . apply_async ( start_process , args = [ type_item , args . config ] ) <EOL> ", "gt": "if terminal :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> faiss . write_index ( index_added , index_filepath_added ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> except Exception as error : <EOL> ", "gt": "print ( f\"<STR_LIT>\" )"}
{"input": "from channel . http . http_channel import HttpChannel <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> import plugins <EOL> from plugins import * <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from common import const <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Createimg ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . handles = { HttpChannel : self . handle_http } <EOL> self . channel_types = { HttpChannel : const . HTTP , <EOL> WechatChannel : const . WECHAT } <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query <EOL> self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def handle_query ( self , e_context : EventContext ) : <EOL> channel = e_context [ '<STR_LIT>' ] <EOL> channel_type = self . channel_types . get ( type ( channel ) , None ) <EOL> if ( channel_type ) : <EOL> query = e_context [ '<STR_LIT>' ] <EOL> if ( query ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : <EOL> e_context [ '<STR_LIT>' ] = channel . handle ( <EOL> { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if ( channel_type == const . WECHAT ) : <EOL> channel . _do_send_img ( <EOL> query , e_context [ '<STR_LIT>' ] ) <EOL> e_context . action = EventAction . BREAK_PASS <EOL> else : <EOL> e_context . action = EventAction . CONTINUE <EOL> return e_context <EOL> def handle_http ( self , e_context : EventContext ) : <EOL> ", "gt": "reply = e_context [ \"<STR_LIT>\" ]"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call_with_q ( ) <EOL> def test_multiple_calls ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> <EOL> } , { <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <EOL> \"<STR_LIT>\" <EOL> } , { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> def call_reliable_openai ( ) : <EOL> nonlocal error_count , failure_count <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> ", "gt": "print ( \"<STR_LIT>\" , e )"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> pass <EOL> ", "gt": "class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) :"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> ", "gt": "@ overload"}
{"input": "import pickle <EOL> class IDStorage : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( \"<STR_LIT>\" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> storage = IDStorage ( ) <EOL> id_value = storage . get_id ( ) <EOL> storage . set_id ( \"<STR_LIT>\" ) <EOL> if id_value is not None : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> ", "gt": "print ( id_value )"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> ", "gt": "history = client . chat_conversation_history ( conversation_id )"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> ", "gt": "else :"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> <EOL> def save_upload_file ( file ) : <EOL> uploads_dir = os . getenv ( '<STR_LIT>' ) <EOL> print ( uploads_dir ) <EOL> file_path = os . path . join ( uploads_dir , file . filename ) <EOL> file . save ( file_path ) <EOL> return file_path <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return jsonify ( conversations ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> if __name__ == '<STR_LIT>' : <EOL> app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = True , use_reloader = True ) <EOL> ", "gt": "app . default_encoding = '<STR_LIT>'"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> ", "gt": "\"<STR_LIT>\" : True ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> ", "gt": "id = gdrive_file_id ,"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ", "gt": ")"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> ", "gt": "return example"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_added = os . path . join ( exp_dir , index_filename_added ) <EOL> batch_size_add = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : <EOL> index_added . add ( big_npy [ i : i + batch_size_add ] ) <EOL> ", "gt": "faiss . write_index ( index_added , index_filepath_added )"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ", "gt": "\"<STR_LIT>\" ,"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> ", "gt": "if retry_response . status < <NUM_LIT> :"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ] <EOL> banner = '<STR_LIT>' . join ( banner_strings ) <EOL> REPL ( ) . interact ( banner = banner ) <EOL> else : <EOL> builtins . __import__ = import_wrapper <EOL> spec = importlib . util . spec_from_file_location ( '<STR_LIT>' , os . path . abspath ( python_file ) ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ '<STR_LIT>' ] = module <EOL> if sys . platform . lower ( ) in ( '<STR_LIT>' , ) : <EOL> cutting_trace_size = <NUM_LIT> <EOL> else : <EOL> cutting_trace_size = <NUM_LIT> <EOL> set_cutting_excepthook ( cutting_trace_size ) <EOL> ", "gt": "spec . loader . exec_module ( module )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> ", "gt": "self . method = method"}
{"input": "import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = \"<STR_LIT>\" <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( \"<STR_LIT>\" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( \"<STR_LIT>\" , response ) <EOL> if response and \"<STR_LIT>\" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == \"<STR_LIT>\" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( \"<STR_LIT>\" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f\"<STR_LIT>\" ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( \"<STR_LIT>\" ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> test_single_call_bad_key ( ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = \"<STR_LIT>\" <EOL> def get_embedding ( text , model = \"<STR_LIT>\" ) : <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> print ( \"<STR_LIT>\" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> result = get_embedding ( \"<STR_LIT>\" ) <EOL> print ( result ) <EOL> test_embedding_bad_key ( ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = \"<STR_LIT>\" , <EOL> fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> queue_requests = True ) <EOL> model = \"<STR_LIT>\" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> ", "gt": "{"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ", "gt": ") -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ..."}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> ", "gt": "response = client . upload_attachment ( file_path )"}
{"input": "import base64 <EOL> import json <EOL> import aiohttp <EOL> import asyncio <EOL> from mod import textcompare <EOL> UA = \"<STR_LIT>\" <EOL> async def kugou ( title , artist , album ) : <EOL> headers = { '<STR_LIT>' : UA , } <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> ratio_max = <NUM_LIT> <EOL> for index in range ( min ( limit , len ( song_info ) ) ) : <EOL> song_item = song_info [ index ] <EOL> song_name = song_item [ \"<STR_LIT>\" ] <EOL> singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> song_hash = song_item [ \"<STR_LIT>\" ] <EOL> album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> <EOL> ratio_max = max ( ratio , ratio_max ) <EOL> if ratio >= ratio_max : <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] <EOL> async with session . get ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" , <EOL> headers = headers , <EOL> timeout = <NUM_LIT> ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> if lyrics_data : <EOL> lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] <EOL> lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) <EOL> return lrc_text <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> async def api_2 ( title , artist , album ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' , <EOL> } <EOL> url = f\"<STR_LIT>\" <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> try : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : <EOL> if response . status < <NUM_LIT> : <EOL> return await response . text ( ) <EOL> else : <EOL> async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : <EOL> if retry_response . status < <NUM_LIT> : <EOL> return await retry_response . text ( ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return None <EOL> api_list = [ kugou , api_2 ] <EOL> async def aw_main ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> return result <EOL> async def aw_all ( title , artist , album ) : <EOL> await_list = [ func ( title , artist , album ) for func in api_list ] <EOL> all_list = [ ] <EOL> for coro in asyncio . as_completed ( await_list ) : <EOL> result = await coro <EOL> all_list . append ( result ) <EOL> return all_list <EOL> ", "gt": "def main ( title , artist , album ) :"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return result <EOL> list_questions = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" <EOL> ] <EOL> for question in list_questions : <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( question ) <EOL> result = simple_openai_call ( question ) <EOL> ", "gt": "print ( \"<STR_LIT>\" )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> ", "gt": "'<STR_LIT>' : item [ '<STR_LIT>' ] ,"}
{"input": "from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoProcessor , AutoModel <EOL> from multi_token . data_tools import load_video <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class XCLIPVideoModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = AutoModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , video_inputs ) -> torch . Tensor : <EOL> with torch . no_grad ( ) : <EOL> outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) <EOL> emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return emb <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class XCLIPVideoModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = \"<STR_LIT>\" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> ", "gt": "def build_projector ( self , lm_hidden_size : int ) -> nn . Module :"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> clip_start = pos_r <EOL> else : <EOL> sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) <EOL> clip_start = max ( pos_r , pos ) <EOL> else : <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> ", "gt": "- self . max_sil_kept"}
{"input": "import numpy as np <EOL> import math <EOL> from typing import List , Optional , Tuple , NewType , cast <EOL> Xdm = NewType ( \"<STR_LIT>\" , int ) <EOL> Ydm = NewType ( \"<STR_LIT>\" , int ) <EOL> def mask_size ( mask : np . ndarray ) -> int : <EOL> return np . sum ( mask * <NUM_LIT> ) <EOL> def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : <EOL> return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) <EOL> def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : <EOL> base = masks [ <NUM_LIT> ] <EOL> for mask in masks [ <NUM_LIT> : ] : <EOL> base = mask_union ( base , mask ) <EOL> return base <EOL> def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : <EOL> return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) <EOL> Point = Tuple [ Ydm , Xdm ] <EOL> def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : <EOL> ( t , l ) , ( b , r ) = tlbr <EOL> return t <= y and y <= b and l <= x and x <= r <EOL> class EfficientMask ( ) : <EOL> def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : <EOL> self . mask = mask <EOL> self . score = score <EOL> self . _size : Optional [ int ] = size <EOL> self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None <EOL> def __repr__ ( self ) -> str : <EOL> return f\"<STR_LIT>\" <EOL> def _reset_cache ( self ) : <EOL> self . _tlbr = None <EOL> self . _size = None <EOL> def set_to ( self , other : \"<STR_LIT>\" ) : <EOL> self . mask = other . mask <EOL> self . score = other . score <EOL> self . _size = other . _size <EOL> self . _tlbr = other . _tlbr <EOL> def get_tlbr ( self ) -> Tuple [ Point , Point ] : <EOL> if self . _tlbr is None : <EOL> try : <EOL> np_where = np . where ( self . mask == True ) <EOL> left = np . min ( np_where [ <NUM_LIT> ] ) <EOL> right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> top = np . min ( np_where [ <NUM_LIT> ] ) <EOL> bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> <EOL> except ValueError : <EOL> top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) <EOL> return self . _tlbr <EOL> def get_size ( self ) -> int : <EOL> if self . _size is None : <EOL> ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) <EOL> self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) <EOL> return self . _size <EOL> def get_density ( self ) -> float : <EOL> size = self . get_size ( ) <EOL> ( t , l ) , ( b , r ) = self . get_tlbr ( ) <EOL> area = ( b - t ) * ( r - l ) + <NUM_LIT> <EOL> return size / area <EOL> def dense_score ( self ) -> float : <EOL> return self . score * math . sqrt ( self . get_density ( ) ) <EOL> def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> return ( <EOL> point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or <EOL> point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or <EOL> point_in_box ( b2 , l2 , self . get_tlbr ( ) ) <EOL> ) <EOL> def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : <EOL> if not self . _bbox_overlaps ( other ) : <EOL> return np . array ( [ ] ) <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) <EOL> maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) <EOL> return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) <EOL> def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> res = np . full ( self . mask . shape , False ) <EOL> submask = self . _get_overlap_submask ( other ) <EOL> if len ( submask ) != <NUM_LIT> : <EOL> ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) <EOL> minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) <EOL> res [ maxt : minb , maxl : minr ] = submask <EOL> return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) <EOL> def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_in = self . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) <EOL> return overlap / size_in > thresh <EOL> def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> overlap = mask_size ( self . _get_overlap_submask ( other ) ) <EOL> return overlap / size_1 > thresh or overlap / size_2 > thresh <EOL> def near_equivalent_to ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : <EOL> size_1 = self . get_size ( ) + <NUM_LIT> <EOL> size_2 = other . get_size ( ) + <NUM_LIT> <EOL> if size_1 / size_2 < thresh or size_2 / size_1 < thresh : <EOL> return False <EOL> difference = mask_size ( self . _get_xor_submask ( other ) ) <EOL> if ( difference / size_1 ) > ( <NUM_LIT> - thresh ) or ( difference / size_2 ) > ( <NUM_LIT> - thresh ) : <EOL> return False <EOL> return True <EOL> def union ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> new_mask = self . mask * <NUM_LIT> <EOL> ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) <EOL> new_mask [ t2 : b2 , l2 : r2 ] += other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT> <EOL> return EfficientMask ( <EOL> mask = cast ( np . ndarray , new_mask > <NUM_LIT> ) , <EOL> score = ( self . score + other . score ) / <NUM_LIT> , <EOL> ) <EOL> def subtract ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : <EOL> new_mask = self . mask * <NUM_LIT> <EOL> ", "gt": "( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( )"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , <EOL> ) <EOL> return None <EOL> resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . url , <EOL> \"<STR_LIT>\" : raw_payload , <EOL> \"<STR_LIT>\" : resp , <EOL> ", "gt": "} ,"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = True <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> pass <EOL> class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> ", "gt": "class runner ( LeggedRobotCfgPPO . runner ) :"}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> ", "gt": "'<STR_LIT>' , '<STR_LIT>' ,"}
{"input": "import os <EOL> from flask import Flask , request , jsonify <EOL> from claude_api import Client <EOL> from common . utils import * <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def create_chat ( ) : <EOL> data = request . get_json ( ) <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> print ( isproxy ) <EOL> client = Client ( cookie , isproxy ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return jsonify ( history ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def send_message_attachment ( ) : <EOL> conversation_id = request . form . get ( \"<STR_LIT>\" ) <EOL> prompt = request . form . get ( \"<STR_LIT>\" ) <EOL> file = request . files [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> file_path = None <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> response = client . send_message ( prompt , conversation_id , file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . reset_all ( ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def rename_conversation ( ) : <EOL> data = request . get_json ( ) <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> title = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return jsonify ( { '<STR_LIT>' : result } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_attachment ( ) : <EOL> file = request . files [ '<STR_LIT>' ] <EOL> if file : <EOL> file_path = save_upload_file ( file ) <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return jsonify ( { '<STR_LIT>' : response } ) <EOL> else : <EOL> ", "gt": "return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT>"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) <EOL> ", "gt": "parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> )"}
{"input": "import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> def fetch ( feed_name , url ) : <EOL> parser = None <EOL> for cls in CustomParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser = cls ( feed_name , url ) <EOL> if not parser : <EOL> raise ValueError ( \"<STR_LIT>\" , url ) <EOL> return parser . fetch ( ) <EOL> class CustomParser ( CachingRequestsMixin ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def __init__ ( self , feed_name , url ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> @ classmethod <EOL> def is_compatible ( cls , feed_url ) : <EOL> return cls . BASE_URL in feed_url <EOL> def fetch ( self ) : <EOL> raise NotImplementedError <EOL> class AgendaBAParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> api_url = f'<STR_LIT>' <EOL> response = requests . get ( api_url ) <EOL> items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry_values = [ ] <EOL> for item in items : <EOL> created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) <EOL> content_url = f'<STR_LIT>' <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : created , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : json . dumps ( item ) <EOL> } ) <EOL> return entry_values <EOL> class RevistaLenguaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for a in soup . select ( '<STR_LIT>' ) : <EOL> article_html = self . request ( a [ '<STR_LIT>' ] ) <EOL> article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) <EOL> script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> item = json . loads ( script . text ) <EOL> del item [ '<STR_LIT>' ] <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : json . dumps ( item ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : None , <EOL> } ) <EOL> return entry_values <EOL> class EternaCadenciaParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> entry_values = [ ] <EOL> for article in soup . find_all ( class_ = '<STR_LIT>' ) : <EOL> content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] <EOL> author = article . find ( class_ = '<STR_LIT>' ) <EOL> if author : <EOL> author = author . text <EOL> date_str = article . find ( class_ = '<STR_LIT>' ) . text <EOL> date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) <EOL> entry_values . append ( { <EOL> '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : author , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : date , <EOL> '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , <EOL> '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : content_url , <EOL> '<STR_LIT>' : content_url , <EOL> } ) <EOL> return entry_values <EOL> class PioneerWorksParser ( CustomParser ) : <EOL> BASE_URL = '<STR_LIT>' <EOL> def fetch ( self ) : <EOL> url = f'<STR_LIT>' <EOL> response = requests . get ( url ) <EOL> ", "gt": "script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> ", "gt": "except ValueError :"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_added = faiss . extract_index_ivf ( index_added ) <EOL> index_ivf_added . nprobe = <NUM_LIT> <EOL> index_added . train ( big_npy ) <EOL> index_filename_added = ( <EOL> ", "gt": "f\"<STR_LIT>\""}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> ", "gt": "filename = os . path . expanduser ( filename )"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> try : <EOL> return JSONResponse ( <EOL> { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } <EOL> ) <EOL> except Exception as e : <EOL> return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ \"<STR_LIT>\" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> app = Starlette ( <EOL> lifespan = lifespan , <EOL> middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , <EOL> ", "gt": "routes = [ Route ( \"<STR_LIT>\" , get_user ) ] ,"}
{"input": "import os <EOL> import plugins <EOL> from plugins import * <EOL> from common import log <EOL> from common import functions <EOL> @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) <EOL> class Selector ( Plugin ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> curdir = os . path . dirname ( __file__ ) <EOL> try : <EOL> self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) <EOL> except Exception as e : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model <EOL> self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model <EOL> log . info ( \"<STR_LIT>\" ) <EOL> def get_events ( self ) : <EOL> return self . handlers <EOL> def select_model ( self , e_context : EventContext ) : <EOL> model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : <EOL> prefix = selector . get ( '<STR_LIT>' , [ ] ) <EOL> check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) <EOL> if ( check_prefix ) : <EOL> model = selector . get ( '<STR_LIT>' ) <EOL> if isinstance ( check_prefix , str ) : <EOL> e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> ", "gt": "break"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> ", "gt": "\"<STR_LIT>\" . format ("}
{"input": "import logging <EOL> import os <EOL> import mutagen <EOL> from . import file <EOL> from . import util <EOL> from . import aac <EOL> from . import aiff <EOL> from . import apev2 <EOL> from . import asf <EOL> from . import dsf <EOL> from . import flac <EOL> from . import id3 <EOL> from . import mp4 <EOL> from . import smf <EOL> from . import vorbis <EOL> from . import wave <EOL> from . file import Artwork , MetadataItem , NotAppendable , AudioFile <EOL> __version__ = <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> log = logger <EOL> def _subclass_spider_dfs ( kls , _lst = None ) : <EOL> if _lst is None : <EOL> _lst = [ ] <EOL> for sub in kls . __subclasses__ ( ) : <EOL> _subclass_spider_dfs ( sub , _lst = _lst ) <EOL> _lst . append ( kls ) <EOL> return _lst <EOL> def load_file ( file_spec , err = '<STR_LIT>' ) : <EOL> if isinstance ( file_spec , mutagen . FileType ) : <EOL> mfile = file_spec <EOL> filename = mfile . filename <EOL> else : <EOL> filename = file_spec <EOL> if not os . path . exists ( filename ) : <EOL> if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : <EOL> filename = os . path . expanduser ( os . path . expandvars ( filename ) ) <EOL> elif os . path . exists ( os . path . expanduser ( filename ) ) : <EOL> filename = os . path . expanduser ( filename ) <EOL> mfile = mutagen . File ( filename , easy = False ) <EOL> ret = None <EOL> for kls in _subclass_spider_dfs ( file . AudioFile ) : <EOL> if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : <EOL> ret = kls ( filename , _mfile = mfile ) <EOL> break <EOL> if ret is None and err == '<STR_LIT>' : <EOL> raise NotImplementedError ( \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( type ( mfile ) ) ) <EOL> return ret <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ", "gt": "]"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> index_ivf_trained . nprobe = <NUM_LIT> <EOL> index_trained . train ( big_npy ) <EOL> index_filename_trained = ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) <EOL> faiss . write_index ( index_trained , index_filepath_trained ) <EOL> index_added = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ", "gt": ")"}
{"input": "import argparse <EOL> import config <EOL> from channel import channel_factory <EOL> from common import log , const <EOL> from multiprocessing import Pool <EOL> from plugins . plugin_manager import PluginManager <EOL> def start_process ( channel_type , config_path ) : <EOL> try : <EOL> config . load_config ( config_path ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> log . info ( \"<STR_LIT>\" , model_type , channel_type ) <EOL> channel = channel_factory . create_channel ( channel_type ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) <EOL> raise e <EOL> def main ( ) : <EOL> try : <EOL> config . load_config ( args . config ) <EOL> model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) <EOL> PluginManager ( ) <EOL> if not isinstance ( channel_type , list ) : <EOL> start_process ( channel_type , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if len ( channel_type ) == <NUM_LIT> : <EOL> start_process ( channel_type [ <NUM_LIT> ] , args . config ) <EOL> exit ( <NUM_LIT> ) <EOL> if const . TERMINAL in channel_type : <EOL> index = channel_type . index ( const . TERMINAL ) <EOL> terminal = channel_type . pop ( index ) <EOL> else : <EOL> terminal = None <EOL> pool = Pool ( len ( channel_type ) ) <EOL> for type_item in channel_type : <EOL> log . info ( \"<STR_LIT>\" , model_type , type_item ) <EOL> pool . apply_async ( start_process , args = [ type_item , args . config ] ) <EOL> if terminal : <EOL> start_process ( terminal , args . config ) <EOL> pool . close ( ) <EOL> pool . join ( ) <EOL> except Exception as e : <EOL> ", "gt": "log . error ( \"<STR_LIT>\" )"}
{"input": "from datetime import datetime <EOL> import requests <EOL> from bs4 import BeautifulSoup <EOL> from app . utils . status_code import Status <EOL> from app . utils . translate import NATIVE_LANG , text_translate <EOL> class HoroscopeRepository : <EOL> MAIN_PATH_URL = \"<STR_LIT>\" <EOL> def _get_horoscope_url_today ( self , sign ) : <EOL> return f\"<STR_LIT>\" <EOL> def _get_horoscope_url_date ( self , sign , date ) : <EOL> return f\"<STR_LIT>\" <EOL> def get_horoscope_info ( self , id , date , lang ) : <EOL> if not id : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if date : <EOL> ", "gt": "try :"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> use_actuator_network = True <EOL> actuator_net_file = \"<STR_LIT>\" <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = \"<STR_LIT>\" <EOL> foot_name = \"<STR_LIT>\" <EOL> penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> terminate_after_contacts_on = [ \"<STR_LIT>\" ] <EOL> self_collisions = <NUM_LIT> <EOL> class domain_rand ( LeggedRobotCfg . domain_rand ) : <EOL> randomize_base_mass = True <EOL> ", "gt": "added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ]"}
{"input": "import torch <EOL> from tqdm import tqdm <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset <EOL> from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag <EOL> from torch . utils . data import DataLoader <EOL> from typing import Optional , List <EOL> def run_dense_cap_test_on_model ( <EOL> model : CLIPModel , <EOL> dataset : DenseCaptionedDataset , <EOL> ) : <EOL> clip_correct_tot = <NUM_LIT> <EOL> neg_correct_tot = <NUM_LIT> <EOL> exs = <NUM_LIT> <EOL> loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> for inputs in tqdm ( loader ) : <EOL> exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . to ( model . device ) , <EOL> '<STR_LIT>' : unstacked_attention . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> clip_logits = outputs . logits_per_image <EOL> similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> maxes = torch . max ( similarity , dim = <NUM_LIT> ) <EOL> clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> neg_logits = neg_outputs . logits_per_image <EOL> pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) <EOL> neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) <EOL> neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) <EOL> return clip_correct_tot / exs , neg_correct_tot / exs <EOL> def run_dense_cap_on_model ( <EOL> model : CLIPModel , <EOL> processor : CLIPProcessor , <EOL> run_subtests : Optional [ List [ str ] ] = None <EOL> ) : <EOL> subtests = { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : False , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : True , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } , <EOL> } <EOL> if run_subtests is None : <EOL> run_subtests = list ( subtests . keys ( ) ) <EOL> for key , args in subtests . items ( ) : <EOL> if key not in run_subtests : <EOL> continue <EOL> dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) <EOL> dataset . processor = processor <EOL> with torch . no_grad ( ) : <EOL> clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> def run_dense_cap_on_lora ( lora_weight_path : str ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) <EOL> base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> ", "gt": "run_dense_cap_on_model ( loaded , processor )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> ", "gt": "return result"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> ", "gt": "image_correct_count = <NUM_LIT>"}
{"input": "import os <EOL> import sys <EOL> import faiss <EOL> import numpy as np <EOL> from sklearn . cluster import MiniBatchKMeans <EOL> from multiprocessing import cpu_count <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> version = sys . argv [ <NUM_LIT> ] <EOL> try : <EOL> if version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> elif version == \"<STR_LIT>\" : <EOL> feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) <EOL> npys = [ ] <EOL> listdir_res = sorted ( os . listdir ( feature_dir ) ) <EOL> for name in listdir_res : <EOL> file_path = os . path . join ( feature_dir , name ) <EOL> phone = np . load ( file_path ) <EOL> npys . append ( phone ) <EOL> big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) <EOL> big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) <EOL> np . random . shuffle ( big_npy_idx ) <EOL> big_npy = big_npy [ big_npy_idx ] <EOL> if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> big_npy = ( <EOL> MiniBatchKMeans ( <EOL> n_clusters = <NUM_LIT> , <EOL> verbose = True , <EOL> batch_size = <NUM_LIT> * cpu_count ( ) , <EOL> compute_labels = False , <EOL> init = \"<STR_LIT>\" , <EOL> ) <EOL> . fit ( big_npy ) <EOL> . cluster_centers_ <EOL> ) <EOL> np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) <EOL> n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) <EOL> index_trained = faiss . index_factory ( <EOL> <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" <EOL> ) <EOL> index_ivf_trained = faiss . extract_index_ivf ( index_trained ) <EOL> ", "gt": "index_ivf_trained . nprobe = <NUM_LIT>"}
{"input": "from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> from typing import Any , AsyncGenerator , Callable , overload <EOL> import attrs <EOL> from starlette . applications import Starlette <EOL> from starlette . requests import Request <EOL> from starlette . types import ASGIApp , Receive , Scope , Send <EOL> import svcs <EOL> from svcs . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request ) -> svcs . Container : <EOL> return getattr ( request . state , _KEY_CONTAINER ) <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : Starlette <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> @ attrs . define <EOL> class SVCSMiddleware : <EOL> app : ASGIApp <EOL> async def __call__ ( <EOL> self , scope : Scope , receive : Receive , send : Send <EOL> ) -> None : <EOL> if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : <EOL> return await self . app ( scope , receive , send ) <EOL> async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : <EOL> scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con <EOL> return await self . app ( scope , receive , send ) <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> ", "gt": "@ overload"}
{"input": "from __future__ import print_function <EOL> import json <EOL> import os <EOL> import os . path as osp <EOL> import re <EOL> import warnings <EOL> from six . moves import urllib_parse <EOL> import shutil <EOL> import sys <EOL> import tempfile <EOL> import textwrap <EOL> import time <EOL> import requests <EOL> import six <EOL> import tqdm <EOL> def indent ( text , prefix ) : <EOL> def prefixed_lines ( ) : <EOL> for line in text . splitlines ( True ) : <EOL> yield ( prefix + line if line . strip ( ) else line ) <EOL> return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) <EOL> class FileURLRetrievalError ( Exception ) : <EOL> pass <EOL> class FolderContentsMaximumLimitError ( Exception ) : <EOL> pass <EOL> def parse_url ( url , warning = True ) : <EOL> parsed = urllib_parse . urlparse ( url ) <EOL> query = urllib_parse . parse_qs ( parsed . query ) <EOL> is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) <EOL> if not is_gdrive : <EOL> return is_gdrive , is_download_link <EOL> file_id = None <EOL> if \"<STR_LIT>\" in query : <EOL> file_ids = query [ \"<STR_LIT>\" ] <EOL> if len ( file_ids ) == <NUM_LIT> : <EOL> file_id = file_ids [ <NUM_LIT> ] <EOL> else : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> match = re . match ( pattern , parsed . path ) <EOL> if match : <EOL> file_id = match . groups ( ) [ <NUM_LIT> ] <EOL> break <EOL> if warning and not is_download_link : <EOL> warnings . warn ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> url = \"<STR_LIT>\" . format ( file_id ) <EOL> ) <EOL> ) <EOL> return file_id , is_download_link <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> home = osp . expanduser ( \"<STR_LIT>\" ) <EOL> def get_url_from_gdrive_confirmation ( contents ) : <EOL> url = \"<STR_LIT>\" <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> uuid = re . search ( <EOL> r'<STR_LIT>' , contents <EOL> ) <EOL> uuid = uuid . groups ( ) [ <NUM_LIT> ] <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> + url <EOL> + \"<STR_LIT>\" <EOL> + uuid <EOL> ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> url = m . groups ( ) [ <NUM_LIT> ] <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> return url <EOL> m = re . search ( r'<STR_LIT>' , contents ) <EOL> if m : <EOL> error = m . groups ( ) [ <NUM_LIT> ] <EOL> raise FileURLRetrievalError ( error ) <EOL> raise FileURLRetrievalError ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> def _get_session ( proxy , use_cookies , return_cookies_file = False ) : <EOL> sess = requests . session ( ) <EOL> sess . headers . update ( <EOL> { \"<STR_LIT>\" : \"<STR_LIT>\" } <EOL> ) <EOL> if proxy is not None : <EOL> sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } <EOL> print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) <EOL> cookies_file = osp . join ( home , \"<STR_LIT>\" ) <EOL> if osp . exists ( cookies_file ) and use_cookies : <EOL> with open ( cookies_file ) as f : <EOL> cookies = json . load ( f ) <EOL> for k , v in cookies : <EOL> sess . cookies [ k ] = v <EOL> if return_cookies_file : <EOL> return sess , cookies_file <EOL> else : <EOL> return sess <EOL> def download ( <EOL> url = None , <EOL> output = None , <EOL> quiet = False , <EOL> proxy = None , <EOL> speed = None , <EOL> use_cookies = True , <EOL> verify = True , <EOL> id = None , <EOL> fuzzy = True , <EOL> resume = False , <EOL> format = None , <EOL> ) : <EOL> if not ( id is None ) ^ ( url is None ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if id is not None : <EOL> url = \"<STR_LIT>\" . format ( id = id ) <EOL> url_origin = url <EOL> sess , cookies_file = _get_session ( <EOL> proxy = proxy , use_cookies = use_cookies , return_cookies_file = True <EOL> ) <EOL> gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) <EOL> if fuzzy and gdrive_file_id : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> url_origin = url <EOL> is_gdrive_download_link = True <EOL> while True : <EOL> res = sess . get ( url , stream = True , verify = verify ) <EOL> if url == url_origin and res . status_code == <NUM_LIT> : <EOL> url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) <EOL> continue <EOL> if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : <EOL> m = re . search ( \"<STR_LIT>\" , res . text ) <EOL> if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> elif ( <EOL> \"<STR_LIT>\" in res . headers <EOL> and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) <EOL> and format not in { None , \"<STR_LIT>\" } <EOL> ) : <EOL> url = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" . format ( <EOL> id = gdrive_file_id , <EOL> format = \"<STR_LIT>\" if format is None else format , <EOL> ) <EOL> ) <EOL> continue <EOL> if use_cookies : <EOL> if not osp . exists ( osp . dirname ( cookies_file ) ) : <EOL> os . makedirs ( osp . dirname ( cookies_file ) ) <EOL> with open ( cookies_file , \"<STR_LIT>\" ) as f : <EOL> cookies = [ <EOL> ( k , v ) <EOL> for k , v in sess . cookies . items ( ) <EOL> if not k . startswith ( \"<STR_LIT>\" ) <EOL> ] <EOL> json . dump ( cookies , f , indent = <NUM_LIT> ) <EOL> if \"<STR_LIT>\" in res . headers : <EOL> break <EOL> if not ( gdrive_file_id and is_gdrive_download_link ) : <EOL> break <EOL> try : <EOL> url = get_url_from_gdrive_confirmation ( res . text ) <EOL> except FileURLRetrievalError as e : <EOL> message = ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) . format ( <EOL> indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , <EOL> url_origin , <EOL> ) <EOL> raise FileURLRetrievalError ( message ) <EOL> if gdrive_file_id and is_gdrive_download_link : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> res . headers [ \"<STR_LIT>\" ] <EOL> ) <EOL> m = re . search ( r\"<STR_LIT>\" , content_disposition ) <EOL> if not m : <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> filename_from_url = m . groups ( ) [ <NUM_LIT> ] <EOL> filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) <EOL> else : <EOL> filename_from_url = osp . basename ( url ) <EOL> if output is None : <EOL> output = filename_from_url <EOL> output_is_path = isinstance ( output , six . string_types ) <EOL> if output_is_path and output . endswith ( osp . sep ) : <EOL> if not osp . exists ( output ) : <EOL> os . makedirs ( output ) <EOL> output = osp . join ( output , filename_from_url ) <EOL> if output_is_path : <EOL> existing_tmp_files = [ ] <EOL> for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : <EOL> if file . startswith ( osp . basename ( output ) ) : <EOL> existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) <EOL> if resume and existing_tmp_files : <EOL> if len ( existing_tmp_files ) != <NUM_LIT> : <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> print ( \"<STR_LIT>\" ) <EOL> for file in existing_tmp_files : <EOL> print ( \"<STR_LIT>\" , file , file = sys . stderr ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> file = sys . stderr , <EOL> ) <EOL> return <EOL> tmp_file = existing_tmp_files [ <NUM_LIT> ] <EOL> else : <EOL> resume = False <EOL> tmp_file = tempfile . mktemp ( <EOL> suffix = tempfile . template , <EOL> prefix = osp . basename ( output ) , <EOL> dir = osp . dirname ( output ) , <EOL> ) <EOL> f = open ( tmp_file , \"<STR_LIT>\" ) <EOL> else : <EOL> tmp_file = None <EOL> f = output <EOL> if tmp_file is not None and f . tell ( ) != <NUM_LIT> : <EOL> headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } <EOL> res = sess . get ( url , headers = headers , stream = True , verify = verify ) <EOL> if not quiet : <EOL> if resume : <EOL> print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) <EOL> print ( <EOL> \"<STR_LIT>\" , <EOL> osp . abspath ( output ) if output_is_path else output , <EOL> file = sys . stderr , <EOL> ) <EOL> try : <EOL> total = res . headers . get ( \"<STR_LIT>\" ) <EOL> if total is not None : <EOL> total = int ( total ) <EOL> if not quiet : <EOL> pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) <EOL> t_start = time . time ( ) <EOL> for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : <EOL> f . write ( chunk ) <EOL> if not quiet : <EOL> pbar . update ( len ( chunk ) ) <EOL> if speed is not None : <EOL> elapsed_time_expected = <NUM_LIT> * pbar . n / speed <EOL> ", "gt": "elapsed_time = time . time ( ) - t_start"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> entry_points = { <EOL> '<STR_LIT>' : [ <EOL> '<STR_LIT>' , <EOL> ", "gt": "]"}
{"input": "cuadrados = [ ] <EOL> for numero in range ( <NUM_LIT> ) : <EOL> cuadrados . append ( numero ** <NUM_LIT> ) <EOL> print ( cuadrados ) <EOL> cuadrados4 = [ ] <EOL> def cuadrados_funcion ( numero ) : <EOL> return numero * numero <EOL> for num in range ( <NUM_LIT> ) : <EOL> cuadrados4 . append ( cuadrados_funcion ( num ) ) <EOL> print ( cuadrados4 ) <EOL> cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) <EOL> print ( cuadrados2 ) <EOL> cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] <EOL> print ( cuadrados3 ) <EOL> from math import pi <EOL> print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> matriz = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ] <EOL> print ( matriz ) <EOL> matriz1 = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> fila_matriz1 = [ ] <EOL> for fila in matriz : <EOL> fila_matriz1 . append ( fila [ i ] ) <EOL> matriz1 . append ( fila_matriz1 ) <EOL> ", "gt": "print ( matriz1 )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> builtins . __import__ = import_wrapper <EOL> class REPL ( code . InteractiveConsole ) : <EOL> def push ( self , line ) : <EOL> state_storage . last_string = line <EOL> return super ( ) . push ( line ) <EOL> banner_strings = [ <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' % ( sys . version , sys . platform ) , <EOL> '<STR_LIT>' , <EOL> ] <EOL> banner = '<STR_LIT>' . join ( banner_strings ) <EOL> REPL ( ) . interact ( banner = banner ) <EOL> else : <EOL> ", "gt": "builtins . __import__ = import_wrapper"}
{"input": "from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> import svcs <EOL> config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> async def get_user ( request : Request ) -> JSONResponse : <EOL> db = await svcs . starlette . aget ( request , Database ) <EOL> ", "gt": "try :"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ", "gt": "]"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> ", "gt": "target_field : str ,"}
{"input": "import logging <EOL> import subprocess <EOL> import html <EOL> from typing import List , Callable , Union , NamedTuple , Dict <EOL> from urllib . parse import quote <EOL> from . form import Form , fill_form <EOL> from . requester import HTTPRequester , TCPRequester <EOL> from . colorize import colored <EOL> from . const import CALLBACK_SUBMIT <EOL> logger = logging . getLogger ( \"<STR_LIT>\" ) <EOL> Tamperer = Callable [ [ str ] , str ] <EOL> def shell_tamperer ( shell_cmd : str ) -> Tamperer : <EOL> def tamperer ( payload : str ) : <EOL> proc = subprocess . Popen ( <EOL> shell_cmd , <EOL> shell = True , <EOL> stdout = subprocess . PIPE , <EOL> stdin = subprocess . PIPE , <EOL> stderr = subprocess . PIPE , <EOL> ) <EOL> assert proc . stdin and proc . stdout <EOL> proc . stdin . write ( payload . encode ( ) ) <EOL> proc . stdin . close ( ) <EOL> ret = proc . wait ( ) <EOL> if ret != <NUM_LIT> : <EOL> raise ValueError ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> out = proc . stdout . read ( ) . decode ( ) <EOL> if out . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" , <EOL> shell_cmd , <EOL> out , <EOL> ) <EOL> return out <EOL> return tamperer <EOL> class HTTPResponse ( NamedTuple ) : <EOL> status_code : int <EOL> text : str <EOL> class BaseSubmitter : <EOL> def __init__ ( self , callback = None ) : <EOL> self . tamperers : List [ Tamperer ] = [ ] <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> def add_tamperer ( self , tamperer : Tamperer ) : <EOL> self . tamperers . append ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> raise NotImplementedError ( ) <EOL> def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : <EOL> if self . tamperers : <EOL> logger . debug ( \"<STR_LIT>\" ) <EOL> for tamperer in self . tamperers : <EOL> payload = tamperer ( payload ) <EOL> logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) <EOL> resp = self . submit_raw ( payload ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) <EOL> class TCPSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> requester : TCPRequester , <EOL> pattern : bytes , <EOL> toreplace = b\"<STR_LIT>\" , <EOL> urlencode_payload = True , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . pattern = pattern <EOL> self . toreplace = toreplace <EOL> self . urlencode_payload = urlencode_payload <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> if self . urlencode_payload : <EOL> raw_payload = quote ( raw_payload ) <EOL> request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) <EOL> result = self . req . request ( request ) <EOL> if result is None : <EOL> return None <EOL> code , text = result <EOL> return HTTPResponse ( code , text ) <EOL> class RequestSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> method : str , <EOL> target_field : str , <EOL> params : Union [ Dict [ str , str ] , None ] , <EOL> data : Union [ Dict [ str , str ] , None ] , <EOL> requester : HTTPRequester , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . method = method <EOL> self . target_field = target_field <EOL> self . params = params if params else { } <EOL> self . data = data if data else { } <EOL> self . req = requester <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload ) : <EOL> params , data = self . params . copy ( ) , self . data . copy ( ) <EOL> if self . method == \"<STR_LIT>\" : <EOL> data . update ( { self . target_field : raw_payload } ) <EOL> else : <EOL> params . update ( { self . target_field : raw_payload } ) <EOL> logger . info ( <EOL> \"<STR_LIT>\" , <EOL> colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , <EOL> ) <EOL> return self . req . request ( <EOL> method = self . method , url = self . url , params = params , data = data <EOL> ) <EOL> class FormSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> form : Form , <EOL> target_field : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> self . url = url <EOL> self . form = form <EOL> self . req = requester <EOL> self . target_field = target_field <EOL> if tamperers : <EOL> for tamperer in tamperers : <EOL> self . add_tamperer ( tamperer ) <EOL> def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : <EOL> inputs = { self . target_field : raw_payload } <EOL> resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) <EOL> self . callback ( <EOL> CALLBACK_SUBMIT , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : self . form , <EOL> \"<STR_LIT>\" : inputs , <EOL> \"<STR_LIT>\" : resp , <EOL> } , <EOL> ) <EOL> if resp is None : <EOL> return None <EOL> return HTTPResponse ( resp . status_code , resp . text ) <EOL> class PathSubmitter ( BaseSubmitter ) : <EOL> def __init__ ( <EOL> self , <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> tamperers : Union [ List [ Tamperer ] , None ] = None , <EOL> ) : <EOL> super ( ) . __init__ ( callback ) <EOL> if not url . endswith ( \"<STR_LIT>\" ) : <EOL> logger . warning ( <EOL> \"<STR_LIT>\" <EOL> ) <EOL> url += \"<STR_LIT>\" <EOL> self . url = url <EOL> ", "gt": "self . req = requester"}
{"input": "from datasets import load_dataset <EOL> from tqdm import tqdm <EOL> import torch <EOL> def run_winoground ( model , processor ) : <EOL> winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] <EOL> winoground_clip_scores = [ ] <EOL> with torch . no_grad ( ) : <EOL> for example in tqdm ( winoground ) : <EOL> input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) <EOL> output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) <EOL> output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) <EOL> output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) <EOL> output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) <EOL> clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) <EOL> clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) <EOL> clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( ) <EOL> clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) <EOL> winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) <EOL> def text_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def image_correct ( result ) : <EOL> return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] <EOL> def group_correct ( result ) : <EOL> return image_correct ( result ) and text_correct ( result ) <EOL> text_correct_count = <NUM_LIT> <EOL> image_correct_count = <NUM_LIT> <EOL> group_correct_count = <NUM_LIT> <EOL> for result in winoground_clip_scores : <EOL> text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> <EOL> ", "gt": "image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT>"}
{"input": "import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = \"<STR_LIT>\" <EOL> openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , <EOL> user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> model = \"<STR_LIT>\" <EOL> messages = [ <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> ", "gt": "return result"}
{"input": "import numpy as np <EOL> class Slicer : <EOL> def __init__ ( <EOL> self , <EOL> sr : int , <EOL> threshold : float = - <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> min_interval : int = <NUM_LIT> , <EOL> hop_size : int = <NUM_LIT> , <EOL> max_sil_kept : int = <NUM_LIT> , <EOL> ) : <EOL> if not min_length >= min_interval >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not max_sil_kept >= hop_size : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> min_interval = sr * min_interval / <NUM_LIT> <EOL> self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) <EOL> self . hop_size = round ( sr * hop_size / <NUM_LIT> ) <EOL> self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) <EOL> self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) <EOL> self . min_interval = round ( min_interval / self . hop_size ) <EOL> self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) <EOL> def _apply_slice ( self , waveform , begin , end ) : <EOL> start_idx = begin * self . hop_size <EOL> if len ( waveform . shape ) > <NUM_LIT> : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ : , start_idx : end_idx ] <EOL> else : <EOL> end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) <EOL> return waveform [ start_idx : end_idx ] <EOL> def slice ( self , waveform ) : <EOL> samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform <EOL> if samples . shape [ <NUM_LIT> ] <= self . min_length : <EOL> return [ waveform ] <EOL> rms_list = get_rms ( <EOL> y = samples , frame_length = self . win_size , hop_length = self . hop_size <EOL> ) . squeeze ( <NUM_LIT> ) <EOL> sil_tags = [ ] <EOL> silence_start , clip_start = None , <NUM_LIT> <EOL> for i , rms in enumerate ( rms_list ) : <EOL> if rms < self . threshold : <EOL> if silence_start is None : <EOL> silence_start = i <EOL> continue <EOL> if silence_start is None : <EOL> continue <EOL> is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept <EOL> need_slice_middle = ( <EOL> i - silence_start >= self . min_interval <EOL> and i - clip_start >= self . min_length <EOL> ) <EOL> if not is_leading_silence and not need_slice_middle : <EOL> silence_start = None <EOL> continue <EOL> if i - silence_start <= self . max_sil_kept : <EOL> pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos ) ) <EOL> else : <EOL> sil_tags . append ( ( pos , pos ) ) <EOL> clip_start = pos <EOL> elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : <EOL> pos = rms_list [ <EOL> i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> pos += i - self . max_sil_kept <EOL> pos_l = ( <EOL> rms_list [ <EOL> silence_start : silence_start + self . max_sil_kept + <NUM_LIT> <EOL> ] . argmin ( ) <EOL> + silence_start <EOL> ) <EOL> pos_r = ( <EOL> rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) <EOL> + i <EOL> - self . max_sil_kept <EOL> ) <EOL> if silence_start == <NUM_LIT> : <EOL> sil_tags . append ( ( <NUM_LIT> , pos_r ) ) <EOL> ", "gt": "clip_start = pos_r"}
{"input": "from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" <EOL> ) <EOL> PRETRAIN_PHRASES = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] <EOL> def _write_convo ( idx , row ) -> List : <EOL> example = { <EOL> \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ \"<STR_LIT>\" ] = [ <EOL> { <EOL> \"<STR_LIT>\" : ROLE_USER , <EOL> \"<STR_LIT>\" : phrase , <EOL> } , <EOL> { <EOL> \"<STR_LIT>\" : ROLE_ASSISTANT , <EOL> \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> audio_dataset = load_dataset ( ** DATASET_ARGS ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> idxes = list ( range ( len ( audio_dataset ) ) ) <EOL> random . shuffle ( idxes ) <EOL> for k in idxes : <EOL> try : <EOL> yield _write_convo ( k , audio_dataset [ k ] ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> ", "gt": "parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str )"}
{"input": "import logging <EOL> import sys <EOL> SWITCH = True <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . setLevel ( logging . INFO ) <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' , <EOL> datefmt = '<STR_LIT>' ) ) <EOL> log . addHandler ( console_handle ) <EOL> return log <EOL> def close_log ( ) : <EOL> global SWITCH <EOL> SWITCH = False <EOL> def debug ( arg , * args ) : <EOL> if SWITCH : <EOL> if len ( args ) == <NUM_LIT> : <EOL> logger . debug ( arg ) <EOL> else : <EOL> ", "gt": "logger . debug ( arg . format ( * args ) )"}
{"input": "import os <EOL> import sys <EOL> import code <EOL> import builtins <EOL> import importlib <EOL> import importlib . util <EOL> import inspect <EOL> from contextlib import contextmanager <EOL> from threading import RLock <EOL> import instld <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> from instld . cli . parsing_arguments . get_python_file import get_python_file <EOL> from instld . cli . traceback_cutting . cutting import set_cutting_excepthook <EOL> from instld . state_management . storage import state_storage , RunType <EOL> from instld . errors import CommentFormatError <EOL> def main ( ) : <EOL> python_file = get_python_file ( ) <EOL> state_storage . run_type = RunType . script <EOL> with instld ( ) as context : <EOL> lock = RLock ( ) <EOL> old_import = builtins . __import__ <EOL> locations = { } <EOL> @ contextmanager <EOL> def set_import ( ) : <EOL> builtins . __import__ = old_import <EOL> yield <EOL> builtins . __import__ = import_wrapper <EOL> def get_current_context ( where ) : <EOL> if where is None : <EOL> return context <EOL> else : <EOL> with lock : <EOL> location_context = locations . get ( where ) <EOL> if location_context is not None : <EOL> return location_context [ <NUM_LIT> ] <EOL> manager = instld ( where = where ) <EOL> local_context = manager . __enter__ ( ) <EOL> locations [ where ] = ( manager , local_context ) <EOL> return local_context <EOL> def import_wrapper ( name , * args , ** kwargs ) : <EOL> splitted_name = name . split ( '<STR_LIT>' ) <EOL> base_name = splitted_name [ <NUM_LIT> ] <EOL> base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) <EOL> last_name = splitted_name [ - <NUM_LIT> ] <EOL> current_frame = inspect . currentframe ( ) <EOL> options = get_options_from_comments_by_frame ( current_frame . f_back ) <EOL> package_name = options . pop ( '<STR_LIT>' , base_name ) <EOL> if '<STR_LIT>' in options : <EOL> package_name = f'<STR_LIT>' <EOL> catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = True <EOL> elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> catch_output = False <EOL> else : <EOL> raise CommentFormatError ( '<STR_LIT>' ) <EOL> current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) <EOL> with lock : <EOL> with set_import ( ) : <EOL> try : <EOL> result = __import__ ( name , * args , ** kwargs ) <EOL> except ( ModuleNotFoundError , ImportError ) : <EOL> current_context . install ( package_name , catch_output = catch_output , ** options ) <EOL> result = current_context . import_here ( base_name ) <EOL> sys . modules [ base_name ] = result <EOL> if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : <EOL> if len ( splitted_name ) > <NUM_LIT> : <EOL> for index , subname in enumerate ( splitted_name ) : <EOL> if index : <EOL> try : <EOL> result = getattr ( result , subname ) <EOL> except AttributeError : <EOL> raise ImportError ( f\"<STR_LIT>\" ) <EOL> return result <EOL> if python_file is None : <EOL> try : <EOL> import readline <EOL> except ImportError : <EOL> pass <EOL> state_storage . run_type = RunType . REPL <EOL> ", "gt": "builtins . __import__ = import_wrapper"}
{"input": "from . base import ma <EOL> from . user import UserSchema <EOL> from app . models import Message <EOL> class MessageSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> ", "gt": "model = Message"}
{"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class AnymalCRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : - <NUM_LIT> , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> ", "gt": "decimation = <NUM_LIT>"}
{"input": "import setuptools <EOL> import os <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> long_description = f . read ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> version = f . read ( ) . strip ( ) <EOL> with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : <EOL> requirements = [ <EOL> line . strip ( ) for line in f . readlines ( ) <EOL> ] <EOL> setuptools . setup ( <EOL> name = \"<STR_LIT>\" , <EOL> version = version , <EOL> author = \"<STR_LIT>\" , <EOL> author_email = \"<STR_LIT>\" , <EOL> description = \"<STR_LIT>\" , <EOL> long_description = long_description , <EOL> long_description_content_type = \"<STR_LIT>\" , <EOL> url = \"<STR_LIT>\" , <EOL> packages = setuptools . find_packages ( ) , <EOL> classifiers = [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> ] , <EOL> install_requires = requirements , <EOL> package_data = { <EOL> \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , <EOL> } , <EOL> ", "gt": "entry_points = {"}
{"input": "from multi_token . modalities . vision_clip import ( <EOL> CLIPVisionModality , <EOL> OUTPUT_LAYER as CLIP_POOL_LAYER , <EOL> ) <EOL> from multi_token . modalities . imagebind import ImageBindModality <EOL> from multi_token . modalities . document_gte import DocumentGTEModality <EOL> from multi_token . modalities . audio_whisper import WhisperAudioModality <EOL> from multi_token . modalities . audio_clap import CLAPAudioModality <EOL> from multi_token . modalities . video_xclip import XCLIPVideoModality <EOL> MODALITY_BUILDERS = { <EOL> \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ <EOL> WhisperAudioModality ( <EOL> num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" <EOL> ) <EOL> ] , <EOL> \"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ XCLIPVideoModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> \"<STR_LIT>\" : lambda : [ ImageBindModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ DocumentGTEModality ( ) ] , <EOL> \"<STR_LIT>\" : lambda : [ DocumentGTEModality ( num_tokens_output = <NUM_LIT> ) ] , <EOL> ", "gt": "}"}
