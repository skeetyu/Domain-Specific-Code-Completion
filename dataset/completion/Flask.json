{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return app . test_client ( ) <EOL> def", "gt": "test_home_horoscope_working ( client ) :", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> return app <EOL> @ pytest . fixture <EOL> def", "gt": "client ( app ) :", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app", "gt": "= Flask ( __name__ )", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return", "gt": "app . test_client ( )", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return app . test_client ( ) <EOL> def test_home_horoscope_working ( client ) : <EOL> response", "gt": "= client . get ( '<STR_LIT>' )", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise", "gt": "ValueError ( \"<STR_LIT>\" )", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not <NUM_LIT> <= len ( target_lang ) <= <NUM_LIT> : <EOL> raise", "gt": "ValueError ( \"<STR_LIT>\" )", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not <NUM_LIT> <= len ( target_lang ) <= <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not is_valid_country_code ( target_lang ) : <EOL> raise", "gt": "ValueError ( \"<STR_LIT>\" )", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if", "gt": "not <NUM_LIT> <= len ( target_lang ) <= <NUM_LIT> :", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if", "gt": "len ( content ) > <NUM_LIT> :", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not <NUM_LIT> <= len ( target_lang ) <= <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if", "gt": "not is_valid_country_code ( target_lang ) :", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app", "gt": ". register_blueprint ( horoscope_blueprint_home )", "repo": "newastro"}
{"input": "import json <EOL> import pytest <EOL> from flask import Flask <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return app . test_client ( ) <EOL> def test_home_horoscope_working ( client ) : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert", "gt": "response . status_code == Status . HTTP_OK", "repo": "newastro"}
{"input": "from googletrans import Translator , LANGUAGES <EOL> NATIVE_LANG = '<STR_LIT>' <EOL> translator = Translator ( service_urls = [ '<STR_LIT>' , ] ) <EOL> def is_valid_country_code ( country_code : str ) -> bool : <EOL> return country_code in LANGUAGES <EOL> def text_translate ( content : str , target_lang = '<STR_LIT>' ) -> str : <EOL> if not content : <EOL> return \"<STR_LIT>\" <EOL> if len ( content ) > <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not <NUM_LIT> <= len ( target_lang ) <= <NUM_LIT> : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> if not is_valid_country_code ( target_lang ) : <EOL> raise ValueError ( \"<STR_LIT>\" ) <EOL> return", "gt": "translator . translate ( content , dest = target_lang , src = NATIVE_LANG ) . text", "repo": "newastro"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text = text . strip ( ) . lower ( ) <EOL> if len ( text ) == <NUM_LIT> : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_LONG ) ) <EOL> else : <EOL> subcommand = text [ <NUM_LIT> : ] <EOL> help_text", "gt": "= HELPS . get ( subcommand , None )", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text", "gt": "= text . strip ( ) . lower ( )", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text = text . strip ( ) . lower ( ) <EOL> if len ( text ) == <NUM_LIT> : <EOL> print_formatted_text", "gt": "( HTML ( INTERACTIVE_MODE_HELP_LONG ) )", "repo": "Fenjing"}
{"input": "from fenjing import exec_cmd_payload , config_payload <EOL> import logging <EOL> logging . basicConfig ( level = logging . INFO ) <EOL> def waf ( s : str ) : <EOL> blacklist = [ <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" <EOL> ] <EOL> return all ( word not in s for word in blacklist ) <EOL> if", "gt": "__name__ == \"<STR_LIT>\" :", "repo": "Fenjing"}
{"input": "from fenjing import exec_cmd_payload , config_payload <EOL> import logging <EOL> logging . basicConfig ( level = logging . INFO ) <EOL> def waf ( s : str ) : <EOL> blacklist = [ <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\"", "gt": ", \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ,", "repo": "Fenjing"}
{"input": "from fenjing import exec_cmd_payload , config_payload <EOL> import logging <EOL> logging . basicConfig ( level = logging . INFO ) <EOL> def waf ( s : str ) : <EOL> blacklist = [ <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" <EOL> ] <EOL> return all ( word not in s for word in blacklist ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> shell_payload", "gt": ", _ = exec_cmd_payload ( waf , \"<STR_LIT>\" )", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if", "gt": "text . strip ( ) == \"<STR_LIT>\" :", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if", "gt": "text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" :", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text = text . strip ( ) . lower ( ) <EOL> if len ( text ) == <NUM_LIT> : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_LONG ) ) <EOL> else : <EOL> subcommand", "gt": "= text [ <NUM_LIT> : ]", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text = text . strip ( ) . lower ( ) <EOL> if len ( text ) == <NUM_LIT> : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_LONG ) ) <EOL> else : <EOL> subcommand = text [ <NUM_LIT> : ] <EOL> help_text = HELPS . get ( subcommand , None ) <EOL> if help_text : <EOL> print ( help_text ) <EOL> else : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> result", "gt": "= cmd_exec_func ( text )", "repo": "Fenjing"}
{"input": "from fenjing import exec_cmd_payload , config_payload <EOL> import logging <EOL> logging . basicConfig ( level = logging . INFO ) <EOL> def waf ( s : str ) : <EOL> blacklist = [ <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\"", "gt": ", \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\"", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text", "gt": "= session . prompt ( \"<STR_LIT>\" )", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer = PygmentsLexer ( BashLexer ) , <EOL> completer = completer , <EOL> style = style , <EOL> include_default_pygments_style = False , <EOL> ) <EOL> while True : <EOL> try : <EOL> text = session . prompt ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> continue <EOL> except EOFError : <EOL> break <EOL> if text . strip ( ) == \"<STR_LIT>\" : <EOL> continue <EOL> if text . strip ( ) . lower ( ) [ : <NUM_LIT> ] == \"<STR_LIT>\" : <EOL> text = text . strip ( ) . lower ( ) <EOL> if", "gt": "len ( text ) == <NUM_LIT> :", "repo": "Fenjing"}
{"input": "from typing import Callable <EOL> from pygments . lexers . shell import BashLexer <EOL> from prompt_toolkit import PromptSession , print_formatted_text , HTML <EOL> from prompt_toolkit . completion import NestedCompleter <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from prompt_toolkit . styles import style_from_pygments_cls <EOL> from prompt_toolkit . lexers import PygmentsLexer <EOL> from pygments . styles import get_style_by_name <EOL> completer = NestedCompleter . from_nested_dict ( <EOL> { <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : None , <EOL> \"<STR_LIT>\" : { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } , <EOL> } <EOL> ) <EOL> style = style_from_pygments_cls ( get_style_by_name ( \"<STR_LIT>\" ) ) <EOL> INTERACTIVE_MODE_HELP_LONG = <EOL> INTERACTIVE_MODE_HELP_SHORT = <EOL> HELPS = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> } <EOL> def interact ( cmd_exec_func : Callable ) : <EOL> print_formatted_text ( HTML ( INTERACTIVE_MODE_HELP_SHORT ) ) <EOL> session = PromptSession ( <EOL> lexer", "gt": "= PygmentsLexer ( BashLexer ) ,", "repo": "Fenjing"}
{"input": "from fenjing import exec_cmd_payload , config_payload <EOL> import logging <EOL> logging . basicConfig ( level = logging . INFO ) <EOL> def waf ( s : str ) : <EOL> blacklist = [ <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" <EOL> ] <EOL> return", "gt": "all ( word not in s for word in blacklist )", "repo": "Fenjing"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" , ( <NUM_LIT> , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> return trace [ \"<STR_LIT>\" ] if trace else None <EOL> def delete_trace_by_id ( self , trace_id : int ) : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> , <EOL> ( trace_id , ) <EOL> ) <EOL> self", "gt": ". db . commit ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces", "gt": "= cursor . fetchall ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor", "gt": "= self . db . cursor ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def", "gt": "get_trace_selected ( self ) -> Optional [ int ] :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self", "gt": ". db . row_factory = Row", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self", "gt": ". db . commit ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace", "gt": "( ** dict ( trace ) )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return", "gt": "Trace ( ** dict ( trace ) )", "repo": "profyle"}
{"input": "from profyle . domain . trace_repository import TraceRepository <EOL> def delete_all_traces ( repo : TraceRepository ) -> int : <EOL> return", "gt": "repo . delete_all_traces ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" , ( <NUM_LIT> , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> return", "gt": "trace [ \"<STR_LIT>\" ] if trace else None", "repo": "profyle"}
{"input": "from profyle . domain . trace_repository import TraceRepository <EOL> def delete_all_traces ( repo : TraceRepository ) -> int : <EOL> return repo . delete_all_traces ( ) <EOL> def delete_trace_by_id ( repo : TraceRepository , trace_id : int ) : <EOL> repo", "gt": ". delete_trace_by_id ( trace_id = trace_id )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor", "gt": "= self . db . cursor ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def", "gt": "get_all_traces ( self ) -> list [ Trace ] :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except", "gt": "Error as error :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def", "gt": "get_trace_by_id ( self , id : int ) -> Optional [ Trace ] :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\"", "gt": ", ( <NUM_LIT> , ) )", "repo": "profyle"}
{"input": "from profyle . domain . trace_repository import TraceRepository <EOL> def delete_all_traces ( repo : TraceRepository ) -> int : <EOL> return repo . delete_all_traces ( ) <EOL> def", "gt": "delete_trace_by_id ( repo : TraceRepository , trace_id : int ) :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json", "gt": ". dumps ( trace . data ) ,", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" , ( <NUM_LIT> , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> return trace [ \"<STR_LIT>\" ] if trace else None <EOL> def delete_trace_by_id ( self , trace_id : int ) : <EOL> cursor", "gt": "= self . db . cursor ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def", "gt": "store_trace ( self , trace : TraceCreate ) -> None :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor", "gt": ". execute ( \"<STR_LIT>\" , ( id , ) )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self", "gt": ". create_trace_table ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" , ( <NUM_LIT> , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> return trace [ \"<STR_LIT>\" ] if trace else None <EOL> def", "gt": "delete_trace_by_id ( self , trace_id : int ) :", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self", "gt": ". db . row_factory = Row", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace", "gt": "= cursor . fetchone ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" ) <EOL> traces = cursor . fetchall ( ) <EOL> return [ <EOL> Trace ( ** dict ( trace ) ) <EOL> for trace in traces <EOL> ] <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( \"<STR_LIT>\" , ( id , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> if trace : <EOL> return Trace ( ** dict ( trace ) ) <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> self . db . row_factory = Row <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> \"<STR_LIT>\" , ( <NUM_LIT> , ) ) <EOL> trace = cursor . fetchone ( ) <EOL> return trace [ \"<STR_LIT>\" ] if trace else None <EOL> def delete_trace_by_id ( self , trace_id : int ) : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> , <EOL> ( trace_id , ) <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor", "gt": ". close ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor", "gt": ". close ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor", "gt": ". execute ( insert_query , data_tuple )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self . db . row_factory = Row <EOL> cursor", "gt": "= self . db . cursor ( )", "repo": "profyle"}
{"input": "from sqlite3 import Connection , Error , Row <EOL> from typing import Optional <EOL> import json <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> class SQLiteTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self , db : Optional [ Connection ] = None ) : <EOL> if not db : <EOL> db = get_connection ( ) <EOL> self . db = db <EOL> def create_trace_selected_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def create_trace_table ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> def delete_all_traces ( self ) -> int : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> return cursor . rowcount <EOL> def vacuum ( self ) -> None : <EOL> cursor = self . db . cursor ( ) <EOL> cursor . execute ( <EOL> ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> try : <EOL> self . create_trace_selected_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> replace_query = <EOL> data_tuple = ( <EOL> <NUM_LIT> , <EOL> trace_id <EOL> ) <EOL> cursor . execute ( replace_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def store_trace ( self , trace : TraceCreate ) -> None : <EOL> try : <EOL> self . create_trace_table ( ) <EOL> cursor = self . db . cursor ( ) <EOL> insert_query = <EOL> data_tuple = ( <EOL> json . dumps ( trace . data ) , <EOL> trace . duration , <EOL> trace . name , <EOL> ) <EOL> cursor . execute ( insert_query , data_tuple ) <EOL> self . db . commit ( ) <EOL> cursor . close ( ) <EOL> except Error as error : <EOL> print ( \"<STR_LIT>\" , error ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> self", "gt": ". db . row_factory = Row", "repo": "profyle"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print ( response . text ) <EOL> api_key", "gt": "= response . json ( ) [ \"<STR_LIT>\" ]", "repo": "reliableGPT"}
{"input": "from pydantic import BaseSettings , Field <EOL> class Settings ( BaseSettings ) : <EOL> ENVIRONMENT : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> APP_NAME : str = \"<STR_LIT>\" <EOL> REDIS_HOST : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> REDIS_PORT : int = Field ( env = \"<STR_LIT>\" , default = <NUM_LIT> ) <EOL> REDIS_DB", "gt": ": int = Field ( env = \"<STR_LIT>\" , default = <NUM_LIT> )", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if", "gt": "llm_provider in cls . hot_cache and cls . valid_api_key :", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print ( response . text ) <EOL> api_key = response . json ( ) [ \"<STR_LIT>\" ] <EOL> cls", "gt": ". hot_cache [ llm_provider ] = api_key", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print ( response . text ) <EOL> api_key = response . json ( ) [ \"<STR_LIT>\" ] <EOL> cls . hot_cache [ llm_provider ] = api_key <EOL> cls", "gt": ". valid_api_key = True", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print ( response . text ) <EOL> api_key = response . json ( ) [ \"<STR_LIT>\" ] <EOL> cls . hot_cache [ llm_provider ] = api_key <EOL> cls . valid_api_key = True <EOL> return api_key <EOL> except", "gt": "Exception as e :", "repo": "reliableGPT"}
{"input": "from pydantic import BaseSettings , Field <EOL> class Settings ( BaseSettings ) : <EOL> ENVIRONMENT : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> APP_NAME : str = \"<STR_LIT>\" <EOL> REDIS_HOST : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> REDIS_PORT : int = Field ( env = \"<STR_LIT>\" , default = <NUM_LIT> ) <EOL> REDIS_DB : int = Field ( env = \"<STR_LIT>\" , default = <NUM_LIT> ) <EOL> class Config : <EOL> env_file = \"<STR_LIT>\" <EOL> settings", "gt": "= Settings ( )", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response", "gt": "= requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring )", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring", "gt": "[ \"<STR_LIT>\" ] = cls . invalid_key_value", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if", "gt": "not cls . valid_api_key :", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls", "gt": ". recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token }", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print ( response . text ) <EOL> api_key = response . json ( ) [ \"<STR_LIT>\" ] <EOL> cls . hot_cache [ llm_provider ] = api_key <EOL> cls . valid_api_key = True <EOL> return api_key <EOL> except Exception as e : <EOL> raise", "gt": "Exception ( \"<STR_LIT>\" )", "repo": "reliableGPT"}
{"input": "from pydantic import BaseSettings , Field <EOL> class Settings ( BaseSettings ) : <EOL> ENVIRONMENT : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> APP_NAME : str = \"<STR_LIT>\" <EOL> REDIS_HOST : str = Field ( env = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) <EOL> REDIS_PORT", "gt": ": int = Field ( env = \"<STR_LIT>\" , default = <NUM_LIT> )", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring", "gt": "[ \"<STR_LIT>\" ] = local_token", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring", "gt": "= { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token }", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key = cls . hot_cache [ llm_provider ] <EOL> else : <EOL> querystring = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : cls . token } <EOL> if local_token : <EOL> querystring [ \"<STR_LIT>\" ] = local_token <EOL> if not cls . valid_api_key : <EOL> print ( f\"<STR_LIT>\" ) <EOL> querystring [ \"<STR_LIT>\" ] = cls . invalid_key_value <EOL> response = requests . get ( cls . api_url + \"<STR_LIT>\" , params = querystring ) <EOL> print", "gt": "( response . text )", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def get_key ( cls , llm_provider , local_token = None ) : <EOL> cls . recent_get_key_params = { \"<STR_LIT>\" : llm_provider , \"<STR_LIT>\" : local_token } <EOL> try : <EOL> print ( f\"<STR_LIT>\" ) <EOL> api_key = None <EOL> if llm_provider in cls . hot_cache and cls . valid_api_key : <EOL> api_key", "gt": "= cls . hot_cache [ llm_provider ]", "repo": "reliableGPT"}
{"input": "import requests <EOL> import openai <EOL> class reliableKey : <EOL> token = \"<STR_LIT>\" <EOL> hot_cache = { } <EOL> api_url = \"<STR_LIT>\" <EOL> valid_api_key = True <EOL> invalid_key_value = None <EOL> recent_get_key_params = None <EOL> class AuthenticationError ( openai . error . AuthenticationError ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> reliableKey . set_invalid_key ( openai . api_key ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> @ classmethod <EOL> def set_invalid_key ( cls , invalid_key ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> cls . valid_api_key = False <EOL> cls . invalid_key_value = invalid_key <EOL> if cls . recent_get_key_params : <EOL> openai . api_key = cls . get_key ( llm_provider = cls . recent_get_key_params [ \"<STR_LIT>\" ] , local_token = cls . recent_get_key_params [ \"<STR_LIT>\" ] ) <EOL> @ classmethod <EOL> def", "gt": "get_key ( cls , llm_provider , local_token = None ) :", "repo": "reliableGPT"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def", "gt": "build_projector ( self , lm_hidden_size : int ) -> nn . Module :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for", "gt": "row in rows :", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp : <EOL> for", "gt": "fn in tqdm . tqdm ( part_paths ) :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self", "gt": ". document_gte_device = \"<STR_LIT>\"", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents", "gt": ". append ( doc )", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with", "gt": "open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self", "gt": ". document_gte_device = device", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp : <EOL> for fn in tqdm . tqdm ( part_paths ) : <EOL> with open ( fn , \"<STR_LIT>\" ) as part_fp : <EOL> merged_fp", "gt": ". write ( part_fp . read ( ) )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents . append ( doc ) <EOL> documents_tokenized = self . tokenizer ( <EOL> documents , <EOL> max_length = GTE_CONTEXT_WINDOW , <EOL> padding = True , <EOL> truncation = True , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> ) <EOL> row_values . append ( documents_tokenized ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ Dict ] ) -> List [ torch . Tensor ] : <EOL> outputs = [ ] <EOL> for", "gt": "val in encoded_values :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents . append ( doc ) <EOL> documents_tokenized = self . tokenizer ( <EOL> documents , <EOL> max_length = GTE_CONTEXT_WINDOW , <EOL> padding = True , <EOL> truncation = True , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> ) <EOL> row_values", "gt": ". append ( documents_tokenized )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for", "gt": "doc in row [ self . data_key ] :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def", "gt": "name ( self ) -> str :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if", "gt": "DOCUMENT_GTE_FORCE_CPU not in os . environ :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents . append ( doc ) <EOL> documents_tokenized = self . tokenizer ( <EOL> documents , <EOL> max_length = GTE_CONTEXT_WINDOW , <EOL> padding = True , <EOL> truncation = True , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> ) <EOL> row_values . append ( documents_tokenized ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def", "gt": "forward ( self , encoded_values : List [ Dict ] ) -> List [ torch . Tensor ] :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def", "gt": "data_key ( self ) -> str :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self", "gt": ". tokenizer = _get_tokenizer ( model_name_or_path )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def", "gt": "token_width ( self ) -> int :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self", "gt": ". device = \"<STR_LIT>\"", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens", "gt": "= self . num_tokens_output ,", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self", "gt": ". dtype = torch . float32", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def", "gt": "to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" :", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp : <EOL> for fn in tqdm . tqdm ( part_paths ) : <EOL> with", "gt": "open ( fn , \"<STR_LIT>\" ) as part_fp :", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp : <EOL> for fn in tqdm . tqdm ( part_paths ) : <EOL> with open ( fn , \"<STR_LIT>\" ) as part_fp : <EOL> merged_fp . write ( part_fp . read ( ) ) <EOL> print ( combined_zip_path ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> parser", "gt": "= argparse . ArgumentParser ( )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self", "gt": ". device = device", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents . append ( doc ) <EOL> documents_tokenized = self . tokenizer ( <EOL> documents , <EOL> max_length = GTE_CONTEXT_WINDOW , <EOL> padding = True , <EOL> truncation = True , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> ) <EOL> row_values . append ( documents_tokenized ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ Dict ] ) -> List [ torch . Tensor ] : <EOL> outputs = [ ] <EOL> for val in encoded_values : <EOL> outputs . append ( <EOL> self . module . forward ( val . to ( device = self . document_gte_device ) ) <EOL> . to ( device = self . device , dtype = self . dtype ) <EOL> .", "gt": "view ( - <NUM_LIT> , <NUM_LIT> , self . module . embedding_size )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def", "gt": "preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] :", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self . module . to ( device = self . document_gte_device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Dict ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> documents = [ ] <EOL> for doc in row [ self . data_key ] : <EOL> documents . append ( doc ) <EOL> documents_tokenized = self . tokenizer ( <EOL> documents , <EOL> max_length = GTE_CONTEXT_WINDOW , <EOL> padding = True , <EOL> truncation = True , <EOL> return_tensors = \"<STR_LIT>\" , <EOL> ) <EOL> row_values . append ( documents_tokenized ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ Dict ] ) -> List [ torch . Tensor ] : <EOL> outputs = [ ] <EOL> for val in encoded_values : <EOL> outputs . append ( <EOL> self", "gt": ". module . forward ( val . to ( device = self . document_gte_device ) )", "repo": "multi_token"}
{"input": "from huggingface_hub import snapshot_download <EOL> import argparse <EOL> import os <EOL> import glob <EOL> import tqdm <EOL> def main ( output_dir ) : <EOL> os . makedirs ( output_dir , exist_ok = True ) <EOL> dl_path = snapshot_download ( repo_id = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" ) <EOL> combined_zip_path = os . path . join ( output_dir , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( combined_zip_path ) : <EOL> part_paths = sorted ( glob . glob ( os . path . join ( dl_path , \"<STR_LIT>\" ) ) ) <EOL> print ( \"<STR_LIT>\" , len ( part_paths ) , \"<STR_LIT>\" ) <EOL> with open ( combined_zip_path , \"<STR_LIT>\" ) as merged_fp : <EOL> for fn in tqdm . tqdm ( part_paths ) : <EOL> with open ( fn , \"<STR_LIT>\" ) as part_fp : <EOL> merged_fp . write ( part_fp . read ( ) ) <EOL> print ( combined_zip_path ) <EOL> if __name__ == \"<STR_LIT>\" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" ) <EOL> args = parser . parse_args ( ) <EOL> main", "gt": "( args . output_dir )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = self . module . embedding_size , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return \"<STR_LIT>\" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : <EOL> self . dtype = dtype <EOL> self . device = device <EOL> if DOCUMENT_GTE_FORCE_CPU not in os . environ : <EOL> self . document_gte_device = device <EOL> self", "gt": ". module . to ( device = self . document_gte_device )", "repo": "multi_token"}
{"input": "from typing import Dict , List <EOL> import torch <EOL> import torch . nn as nn <EOL> import os <EOL> from functools import cache <EOL> from transformers import AutoTokenizer , AutoModel <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> GTE_EMBEDDING_SIZE = <NUM_LIT> <EOL> GTE_CONTEXT_WINDOW = <NUM_LIT> <EOL> GTE_DEFAULT_MODEL = \"<STR_LIT>\" <EOL> DOCUMENT_GTE_FORCE_CPU = \"<STR_LIT>\" <EOL> def average_pool ( <EOL> last_hidden_states : torch . Tensor , attention_mask : torch . Tensor <EOL> ) -> torch . Tensor : <EOL> last_hidden = last_hidden_states . masked_fill ( ~ attention_mask [ ... , None ] . bool ( ) , <NUM_LIT> ) <EOL> return last_hidden . sum ( dim = <NUM_LIT> ) / attention_mask . sum ( dim = <NUM_LIT> ) [ ... , None ] <EOL> @ cache <EOL> def _get_tokenizer ( model_name_or_path : str = GTE_DEFAULT_MODEL ) : <EOL> return AutoTokenizer . from_pretrained ( model_name_or_path ) <EOL> def split_text_into_documents ( text : str ) -> List [ str ] : <EOL> from nltk . tokenize import sent_tokenize <EOL> tokenizer = _get_tokenizer ( GTE_DEFAULT_MODEL ) <EOL> sentences = sent_tokenize ( text ) <EOL> documents = [ [ ] ] <EOL> for sentence in sentences : <EOL> sentence_tokens = tokenizer . encode ( sentence , add_special_tokens = False ) <EOL> if len ( documents [ - <NUM_LIT> ] ) + len ( sentence_tokens ) > GTE_CONTEXT_WINDOW : <EOL> documents . append ( [ ] ) <EOL> documents [ - <NUM_LIT> ] . extend ( sentence_tokens ) <EOL> return [ tokenizer . decode ( doc ) for doc in documents ] <EOL> class DocumentGTEModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = - <NUM_LIT> <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = AutoModel . from_pretrained ( \"<STR_LIT>\" ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , batch_dict ) -> torch . Tensor : <EOL> outputs = self . model ( ** batch_dict ) <EOL> embeddings = average_pool ( <EOL> outputs . last_hidden_state , batch_dict [ \"<STR_LIT>\" ] <EOL> ) <EOL> return embeddings <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return GTE_EMBEDDING_SIZE <EOL> class DocumentGTEModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = GTE_DEFAULT_MODEL , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = DocumentGTEModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . tokenizer = _get_tokenizer ( model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> self . device = \"<STR_LIT>\" <EOL> self . document_gte_device = \"<STR_LIT>\" <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size", "gt": "= self . module . embedding_size ,", "repo": "multi_token"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> conversation_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> ) <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> wss_link : str = \"<STR_LIT>\" , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> ) -> Generator [ str , None , None ] : <EOL> async for item in super ( ) . ask_stream ( prompt = prompt , conversation_style = conversation_style , wss_link = wss_link ) : <EOL> yield item <EOL> class SydneyHubRequest ( ChatHubRequest ) : <EOL> def __init__ ( <EOL> self , <EOL> conversation_signature : str , <EOL> client_id : str , <EOL> conversation_id : str , <EOL> invocation_id : int = <NUM_LIT> , <EOL> ) -> None : <EOL> super ( ) . __init__ ( conversation_signature = conversation_signature , client_id = client_id , <EOL> conversation_id = conversation_id , invocation_id = invocation_id ) <EOL> self . previous_messages = \"<STR_LIT>\" <EOL> def update ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE , <EOL> options", "gt": ": list | None = None ,", "repo": "bot-on-anything"}
{"input": "import json <EOL> import os <EOL> import re <EOL> from common import log <EOL> def singleton ( cls ) : <EOL> instances = { } <EOL> def get_instance ( * args , ** kwargs ) : <EOL> if cls not in instances : <EOL> instances [ cls ] = cls ( * args , ** kwargs ) <EOL> return instances [ cls ] <EOL> return get_instance <EOL> def load_json_file ( curdir : str , file : str = '<STR_LIT>' ) : <EOL> config_path = os . path . join ( curdir , file ) <EOL> try : <EOL> with open ( config_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : <EOL> config = json . load ( f ) <EOL> return config <EOL> except Exception as e : <EOL> if isinstance ( e , FileNotFoundError ) : <EOL> log . warn ( <EOL> f\"<STR_LIT>\" ) <EOL> else : <EOL> log", "gt": ". warn ( \"<STR_LIT>\" )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if", "gt": "self . chat_hub . wss and not self . chat_hub . wss . closed :", "repo": "bot-on-anything"}
{"input": "from channel . channel import Channel <EOL> from common import log <EOL> import sys <EOL> class TerminalChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> log . close_log ( ) <EOL> context = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : True } <EOL> print ( \"<STR_LIT>\" ) <EOL> while True : <EOL> try : <EOL> prompt = self . get_input ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . exit ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . stdout . flush ( ) <EOL> for", "gt": "res in super ( ) . build_reply_content ( prompt , context ) :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature", "gt": "= conversation . struct [ \"<STR_LIT>\" ] ,", "repo": "bot-on-anything"}
{"input": "from channel . channel import Channel <EOL> from common import log <EOL> import sys <EOL> class TerminalChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> log . close_log ( ) <EOL> context = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : True } <EOL> print ( \"<STR_LIT>\" ) <EOL> while True : <EOL> try : <EOL> prompt = self . get_input ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . exit ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . stdout . flush ( ) <EOL> for res in super ( ) . build_reply_content ( prompt , context ) : <EOL> print ( res , end = \"<STR_LIT>\" ) <EOL> sys . stdout . flush ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> def get_input ( self , prompt ) : <EOL> print", "gt": "( prompt , end = \"<STR_LIT>\" )", "repo": "bot-on-anything"}
{"input": "from channel . channel import Channel <EOL> from common import log <EOL> import sys <EOL> class TerminalChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> log . close_log ( ) <EOL> context = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : True } <EOL> print ( \"<STR_LIT>\" ) <EOL> while True : <EOL> try : <EOL> prompt = self . get_input ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . exit ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . stdout . flush ( ) <EOL> for res in super ( ) . build_reply_content ( prompt , context ) : <EOL> print ( res , end = \"<STR_LIT>\" ) <EOL> sys", "gt": ". stdout . flush ( )", "repo": "bot-on-anything"}
{"input": "import json <EOL> import os <EOL> import re <EOL> from common import log <EOL> def singleton ( cls ) : <EOL> instances = { } <EOL> def get_instance ( * args , ** kwargs ) : <EOL> if cls not in instances : <EOL> instances [ cls ] = cls ( * args , ** kwargs ) <EOL> return instances [ cls ] <EOL> return get_instance <EOL> def load_json_file ( curdir : str , file : str = '<STR_LIT>' ) : <EOL> config_path = os . path . join ( curdir , file ) <EOL> try : <EOL> with open ( config_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : <EOL> config = json . load ( f ) <EOL> return config <EOL> except Exception as e : <EOL> if isinstance ( e , FileNotFoundError ) : <EOL> log . warn ( <EOL> f\"<STR_LIT>\" ) <EOL> else : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> def contain_chinese ( str ) : <EOL> pattern = re . compile ( '<STR_LIT>' ) <EOL> match", "gt": "= pattern . search ( str )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self", "gt": ". update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] )", "repo": "bot-on-anything"}
{"input": "from channel . channel import Channel <EOL> from common import log <EOL> import sys <EOL> class TerminalChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> log . close_log ( ) <EOL> context = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : True } <EOL> print ( \"<STR_LIT>\" ) <EOL> while True : <EOL> try : <EOL> prompt = self . get_input ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> sys", "gt": ". exit ( )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id", "gt": "= conversation . struct [ \"<STR_LIT>\" ] ,", "repo": "bot-on-anything"}
{"input": "import json <EOL> import os <EOL> import re <EOL> from common import log <EOL> def singleton ( cls ) : <EOL> instances = { } <EOL> def get_instance ( * args , ** kwargs ) : <EOL> if cls not in instances : <EOL> instances [ cls ] = cls ( * args , ** kwargs ) <EOL> return instances [ cls ] <EOL> return get_instance <EOL> def load_json_file ( curdir : str , file : str = '<STR_LIT>' ) : <EOL> config_path = os . path . join ( curdir , file ) <EOL> try : <EOL> with open ( config_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : <EOL> config = json . load ( f ) <EOL> return config <EOL> except Exception as e : <EOL> if isinstance ( e , FileNotFoundError ) : <EOL> log . warn ( <EOL> f\"<STR_LIT>\" ) <EOL> else : <EOL> log . warn ( \"<STR_LIT>\" ) <EOL> raise e <EOL> def contain_chinese ( str ) : <EOL> pattern = re . compile ( '<STR_LIT>' ) <EOL> match = pattern . search ( str ) <EOL> return match != None <EOL> def check_prefix ( content , prefix_list ) : <EOL> if ( len ( prefix_list ) == <NUM_LIT> ) : <EOL> return True <EOL> for prefix in prefix_list : <EOL> if", "gt": "content . startswith ( prefix ) :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self", "gt": ". loop : bool", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id", "gt": ": str = None", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> conversation_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> ) <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> wss_link : str = \"<STR_LIT>\" , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> ) -> Generator [ str , None , None ] : <EOL> async for item in super ( ) . ask_stream ( prompt = prompt , conversation_style = conversation_style , wss_link = wss_link ) : <EOL> yield item <EOL> class SydneyHubRequest ( ChatHubRequest ) : <EOL> def __init__ ( <EOL> self , <EOL> conversation_signature : str , <EOL> client_id : str , <EOL> conversation_id : str , <EOL> invocation_id : int = <NUM_LIT> , <EOL> ) -> None : <EOL> super ( ) . __init__ ( conversation_signature = conversation_signature , client_id = client_id , <EOL> conversation_id = conversation_id , invocation_id = invocation_id ) <EOL> self . previous_messages = \"<STR_LIT>\" <EOL> def update ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE , <EOL> options : list | None = None , <EOL> ) -> None : <EOL> self . invocation_id = <NUM_LIT> <EOL> super", "gt": "( ) . update ( prompt = prompt , conversation_style = conversation_style , options = options )", "repo": "bot-on-anything"}
{"input": "from enum import Enum <EOL> class Event ( Enum ) : <EOL> ON_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_DECORATE_REPLY = <NUM_LIT> <EOL> ON_SEND_REPLY = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_STREAM_CONTEXT = <NUM_LIT> <EOL> class EventAction ( Enum ) : <EOL> CONTINUE = <NUM_LIT> <EOL> BREAK = <NUM_LIT> <EOL> BREAK_PASS = <NUM_LIT> <EOL> class EventContext : <EOL> def __init__ ( self , event , econtext = dict ( ) ) : <EOL> self . event = event <EOL> self . econtext = econtext <EOL> self . action = EventAction . CONTINUE <EOL> def", "gt": "__getitem__ ( self , key ) :", "repo": "bot-on-anything"}
{"input": "from enum import Enum <EOL> class Event ( Enum ) : <EOL> ON_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_DECORATE_REPLY = <NUM_LIT> <EOL> ON_SEND_REPLY = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_STREAM_CONTEXT = <NUM_LIT> <EOL> class EventAction ( Enum ) : <EOL> CONTINUE = <NUM_LIT> <EOL> BREAK = <NUM_LIT> <EOL> BREAK_PASS = <NUM_LIT> <EOL> class EventContext : <EOL> def __init__ ( self , event , econtext = dict ( ) ) : <EOL> self . event = event <EOL> self . econtext = econtext <EOL> self . action = EventAction . CONTINUE <EOL> def __getitem__ ( self , key ) : <EOL> return self . econtext . get ( key , \"<STR_LIT>\" ) <EOL> def", "gt": "__setitem__ ( self , key , value ) :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except", "gt": "Exception as e :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> conversation_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> ) <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> wss_link : str = \"<STR_LIT>\" , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> ) -> Generator [ str , None , None ] : <EOL> async for item in super ( ) . ask_stream ( prompt = prompt , conversation_style = conversation_style , wss_link = wss_link ) : <EOL> yield item <EOL> class SydneyHubRequest ( ChatHubRequest ) : <EOL> def __init__ ( <EOL> self , <EOL> conversation_signature : str , <EOL> client_id : str , <EOL> conversation_id : str , <EOL> invocation_id", "gt": ": int = <NUM_LIT> ,", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await", "gt": "self . chat_hub . wss . close ( )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self", "gt": ". task : asyncio . Task", "repo": "bot-on-anything"}
{"input": "from channel . channel import Channel <EOL> from common import log <EOL> import sys <EOL> class TerminalChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> log . close_log ( ) <EOL> context = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : True } <EOL> print ( \"<STR_LIT>\" ) <EOL> while True : <EOL> try : <EOL> prompt = self . get_input ( \"<STR_LIT>\" ) <EOL> except KeyboardInterrupt : <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . exit ( ) <EOL> print ( \"<STR_LIT>\" ) <EOL> sys . stdout . flush ( ) <EOL> for res in super ( ) . build_reply_content ( prompt , context ) : <EOL> print", "gt": "( res , end = \"<STR_LIT>\" )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self", "gt": ". request : SydneyHubRequest", "repo": "bot-on-anything"}
{"input": "from enum import Enum <EOL> class Event ( Enum ) : <EOL> ON_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_DECORATE_REPLY = <NUM_LIT> <EOL> ON_SEND_REPLY = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_STREAM_CONTEXT = <NUM_LIT> <EOL> class EventAction ( Enum ) : <EOL> CONTINUE = <NUM_LIT> <EOL> BREAK = <NUM_LIT> <EOL> BREAK_PASS = <NUM_LIT> <EOL> class EventContext : <EOL> def __init__ ( self , event , econtext = dict ( ) ) : <EOL> self . event = event <EOL> self . econtext = econtext <EOL> self . action = EventAction . CONTINUE <EOL> def __getitem__ ( self , key ) : <EOL> return self . econtext . get ( key , \"<STR_LIT>\" ) <EOL> def __setitem__ ( self , key , value ) : <EOL> self . econtext [ key ] = value <EOL> def __delitem__ ( self , key ) : <EOL> del self . econtext [ key ] <EOL> def", "gt": "is_pass ( self ) :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def", "gt": "__init__ ( self , conversation : Conversation ) -> None :", "repo": "bot-on-anything"}
{"input": "from enum import Enum <EOL> class Event ( Enum ) : <EOL> ON_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_DECORATE_REPLY = <NUM_LIT> <EOL> ON_SEND_REPLY = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_CONTEXT = <NUM_LIT> <EOL> ON_BRIDGE_HANDLE_STREAM_CONTEXT = <NUM_LIT> <EOL> class EventAction ( Enum ) : <EOL> CONTINUE = <NUM_LIT> <EOL> BREAK = <NUM_LIT> <EOL> BREAK_PASS = <NUM_LIT> <EOL> class EventContext : <EOL> def", "gt": "__init__ ( self , event , econtext = dict ( ) ) :", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> conversation_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> ) <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> wss_link : str = \"<STR_LIT>\" , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> ) -> Generator [ str , None , None ] : <EOL> async for item in super ( ) . ask_stream ( prompt = prompt , conversation_style = conversation_style , wss_link = wss_link ) : <EOL> yield item <EOL> class SydneyHubRequest ( ChatHubRequest ) : <EOL> def __init__ ( <EOL> self , <EOL> conversation_signature : str , <EOL> client_id : str , <EOL> conversation_id : str , <EOL> invocation_id : int = <NUM_LIT> , <EOL> ) -> None : <EOL> super ( ) . __init__ ( conversation_signature = conversation_signature , client_id = client_id , <EOL> conversation_id", "gt": "= conversation_id , invocation_id = invocation_id )", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async", "gt": "for final , response in self . ask_stream (", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> async for final , response in self . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style , <EOL> message_id = message_id <EOL> ) : <EOL> if final : <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> return response <EOL> def update_reply_cache ( <EOL> self , <EOL> reply , <EOL> ) -> None : <EOL> replyMessage = { <EOL> \"<STR_LIT>\" : uuid . uuid4 ( ) , <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : reply [ \"<STR_LIT>\" ] , <EOL> \"<STR_LIT>\" : reply , <EOL> } <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . append ( <EOL> replyMessage ) <EOL> self . user_message_id = replyMessage [ \"<STR_LIT>\" ] <EOL> class SydneyHub ( ChatHub ) : <EOL> def __init__ ( self , conversation : Conversation ) -> None : <EOL> self . wss : websockets . WebSocketClientProtocol | None = None <EOL> self . request : SydneyHubRequest <EOL> self . loop : bool <EOL> self . task : asyncio . Task <EOL> self . request = SydneyHubRequest ( <EOL> conversation_signature = conversation . struct [ \"<STR_LIT>\" ] , <EOL> client_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> conversation_id = conversation . struct [ \"<STR_LIT>\" ] , <EOL> ) <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> wss_link : str = \"<STR_LIT>\" , <EOL> conversation_style", "gt": ": EdgeGPT . CONVERSATION_STYLE_TYPE = None ,", "repo": "bot-on-anything"}
{"input": "import asyncio <EOL> import time <EOL> import websockets <EOL> import random <EOL> import uuid <EOL> import EdgeGPT <EOL> from EdgeGPT import ChatHubRequest , Chatbot , Conversation , ChatHub <EOL> from typing import Generator <EOL> from config import model_conf_val <EOL> class SydneyBot ( Chatbot ) : <EOL> def __init__ ( <EOL> self , <EOL> cookiePath : str = \"<STR_LIT>\" , <EOL> cookies : dict | None = None , <EOL> proxy : str | None = None , <EOL> options : dict | None = None , <EOL> ) -> None : <EOL> self . conversations_cache = { } <EOL> self . parent_message_id = <NUM_LIT> <EOL> self . user_message_id = <NUM_LIT> <EOL> self . conversation_key = uuid . uuid4 ( ) <EOL> self . cookiePath : str = cookiePath <EOL> self . cookies : dict | None = cookies <EOL> self . proxy : str | None = proxy <EOL> self . chat_hub : SydneyHub <EOL> cache_options = options . get ( '<STR_LIT>' , { } ) <EOL> cache_options [ '<STR_LIT>' ] = cache_options . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . conversations_cache = cache_options <EOL> @ staticmethod <EOL> def get_messages_for_conversation ( messages , parent_message_id ) : <EOL> ordered_messages = [ ] <EOL> current_message_id = parent_message_id <EOL> while current_message_id : <EOL> message = next ( <EOL> ( m for m in messages if m [ '<STR_LIT>' ] == current_message_id ) , None ) <EOL> if not message : <EOL> break <EOL> ordered_messages . insert ( <NUM_LIT> , message ) <EOL> current_message_id = message . get ( '<STR_LIT>' ) <EOL> return ordered_messages <EOL> async def ask_stream ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style : EdgeGPT . CONVERSATION_STYLE_TYPE = None , <EOL> message_id : str = None <EOL> ) -> dict : <EOL> self . chat_hub = SydneyHub ( Conversation ( <EOL> self . cookiePath , self . cookies , self . proxy ) ) <EOL> self . parent_message_id = message_id if message_id != None else uuid . uuid4 ( ) <EOL> conversation = self . conversations_cache . get ( self . conversation_key ) <EOL> if conversation is None : <EOL> conversation = { <EOL> \"<STR_LIT>\" : [ ] , <EOL> \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) <EOL> } <EOL> previous_cached_messages = \"<STR_LIT>\" <EOL> for conversation_message in self . get_messages_for_conversation ( conversation [ \"<STR_LIT>\" ] , self . parent_message_id ) : <EOL> previous_cached_messages += f\"<STR_LIT>\" <EOL> chars = list ( model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) <EOL> chars = [ ( '<STR_LIT>' + c if random . random ( ) < <NUM_LIT> else '<STR_LIT>' + c ) <EOL> if i > <NUM_LIT> else c for i , c in enumerate ( chars ) ] <EOL> previous_messages = '<STR_LIT>' . join ( chars ) <EOL> self . chat_hub . request . previous_messages = previous_messages + \"<STR_LIT>\" + previous_cached_messages <EOL> self . user_message_id = uuid . uuid4 ( ) <EOL> user_message = { <EOL> \"<STR_LIT>\" : self . user_message_id , <EOL> \"<STR_LIT>\" : self . parent_message_id , <EOL> \"<STR_LIT>\" : '<STR_LIT>' , <EOL> \"<STR_LIT>\" : prompt , <EOL> } <EOL> conversation [ \"<STR_LIT>\" ] . append ( user_message ) <EOL> self . conversations_cache [ self . conversation_key ] = conversation <EOL> async for final , response in self . chat_hub . ask_stream ( <EOL> prompt = prompt , <EOL> conversation_style = conversation_style <EOL> ) : <EOL> if final : <EOL> try : <EOL> if self . chat_hub . wss and not self . chat_hub . wss . closed : <EOL> await self . chat_hub . wss . close ( ) <EOL> self . update_reply_cache ( response [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] ) <EOL> except Exception as e : <EOL> self . conversations_cache [ self . conversation_key ] [ \"<STR_LIT>\" ] . pop ( ) <EOL> yield True , f\"<STR_LIT>\" <EOL> yield final , response <EOL> async def ask ( <EOL> self , <EOL> prompt : str , <EOL> conversation_style", "gt": ": EdgeGPT . CONVERSATION_STYLE_TYPE = None ,", "repo": "bot-on-anything"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service", "gt": ", Service , on_registry_close = sync_close", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async def aping ( _ ) : ... <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory , ping = aping ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> assert", "gt": "{ \"<STR_LIT>\" : None } == json . loads ( text )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async", "gt": "def closer ( ) :", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async def aping ( _ ) : ... <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory , ping = aping ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> assert { \"<STR_LIT>\" : None } == json . loads ( text ) <EOL> await server . aclose ( ) <EOL> async def test_client_pool_register_value ( self , app ) : <EOL> registry = svcs . Registry ( ) <EOL> global_session", "gt": "= ClientSession ( )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def", "gt": "test_name ( self ) :", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp", "gt": ", text = await get ( server . base_url )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await", "gt": "registry . aclose ( )", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async", "gt": "def aping ( _ ) : ...", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async def aping ( _ ) : ... <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory , ping = aping ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> assert { \"<STR_LIT>\" : None } == json . loads ( text ) <EOL> await server . aclose ( ) <EOL> async def test_client_pool_register_value ( self , app ) : <EOL> registry = svcs . Registry ( ) <EOL> global_session = ClientSession ( ) <EOL> registry . register_value ( <EOL> ClientSession , <EOL> global_session , <EOL> on_registry_close = global_session . close , <EOL> ) <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> server = await AppServer . start ( app ) <EOL> async with registry , svcs . Container ( registry ) as container : <EOL> sess", "gt": "= await container . aget ( ClientSession )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def test_name ( self ) : <EOL> def factory ( svcs_container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> def test_annotation ( self ) : <EOL> def factory ( foo : svcs . Container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , takes_containers_annotation_string_modules <EOL> ) <EOL> def test_annotation_str ( self , module_source , create_module ) : <EOL> module = create_module ( module_source ) <EOL> assert svcs . _core . _takes_container ( module . factory ) <EOL> def test_catches_invalid_sigs ( self ) : <EOL> def", "gt": "factory ( foo , bar ) : ...", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async", "gt": "def test_aclose_logs_failures ( self , registry , caplog ) :", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert", "gt": "AnotherService not in registry", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def test_name ( self ) : <EOL> def factory ( svcs_container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> def test_annotation ( self ) : <EOL> def factory ( foo : svcs . Container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , takes_containers_annotation_string_modules <EOL> ) <EOL> def test_annotation_str ( self , module_source , create_module ) : <EOL> module = create_module ( module_source ) <EOL> assert svcs . _core . _takes_container ( module . factory ) <EOL> def test_catches_invalid_sigs ( self ) : <EOL> def factory ( foo , bar ) : ... <EOL> with pytest . raises ( <EOL> TypeError", "gt": ", match = \"<STR_LIT>\"", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def test_name ( self ) : <EOL> def factory ( svcs_container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> def test_annotation ( self ) : <EOL> def factory ( foo : svcs . Container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> @", "gt": "pytest . mark . parametrize (", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async def aping ( _ ) : ... <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory , ping = aping ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> assert { \"<STR_LIT>\" : None } == json . loads ( text ) <EOL> await", "gt": "server . aclose ( )", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs", "gt": ". aiohttp . register_value ( app , str , \"<STR_LIT>\" )", "repo": "svcs"}
{"input": "from typing import Generator , Protocol <EOL> from aiohttp . web import Application , Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = Application ( ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg , middleware_pos = <NUM_LIT> ) <EOL> app = svcs . aiohttp . init_app ( app ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . aiohttp . register_factory ( app , str , str ) <EOL> svcs . aiohttp . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . aiohttp . register_value ( app , str , str , ping = lambda : None ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request = Request ( ) <EOL> class P ( Protocol ) : <EOL> def m ( self ) -> None : ... <EOL> async def func ( ) -> None : <EOL> a , b , c , d , e , f , g , h , i , j = await svcs . aiohttp . aget ( <EOL> request", "gt": ", int , str , bool , tuple , object , float , list , dict , set , bytes", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server", "gt": "= await AppServer . start ( app )", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await svcs . aiohttp . aclose_registry ( Application ( ) ) <EOL> async def test_registrations ( self , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , view ) <EOL> svcs . aiohttp . register_value ( app , str , \"<STR_LIT>\" ) <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> await server . aclose ( ) <EOL> assert { <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> } == json . loads ( text ) <EOL> async def test_get_registry ( self , registry , app ) : <EOL> assert registry is svcs . aiohttp . get_registry ( app ) <EOL> async def test_get_pings ( self , registry , container , app ) : <EOL> app . router . add_get ( \"<STR_LIT>\" , health_view ) <EOL> async def aping ( _ ) : ... <EOL> svcs . aiohttp . register_factory ( app , int , async_int_factory , ping = aping ) <EOL> server = await AppServer . start ( app ) <EOL> resp , text = await get ( server . base_url ) <EOL> assert { \"<STR_LIT>\" : None } == json . loads ( text ) <EOL> await server . aclose ( ) <EOL> async def test_client_pool_register_value ( self , app ) : <EOL> registry = svcs . Registry ( ) <EOL> global_session = ClientSession ( ) <EOL> registry . register_value ( <EOL> ClientSession , <EOL> global_session , <EOL> on_registry_close = global_session . close , <EOL> ) <EOL> app", "gt": ". router . add_get ( \"<STR_LIT>\" , health_view )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def test_name ( self ) : <EOL> def factory ( svcs_container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> def test_annotation ( self ) : <EOL> def factory ( foo : svcs . Container ) : ... <EOL> assert svcs . _core . _takes_container ( factory ) <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , takes_containers_annotation_string_modules <EOL> ) <EOL> def test_annotation_str ( self , module_source , create_module ) : <EOL> module = create_module ( module_source ) <EOL> assert svcs . _core . _takes_container ( module . factory ) <EOL> def", "gt": "test_catches_invalid_sigs ( self ) :", "repo": "svcs"}
{"input": "from typing import Generator , Protocol <EOL> from aiohttp . web import Application , Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = Application ( ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg , middleware_pos = <NUM_LIT> ) <EOL> app = svcs . aiohttp . init_app ( app ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . aiohttp . register_factory ( app , str , str ) <EOL> svcs . aiohttp . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . aiohttp . register_value ( app , str , str , ping = lambda : None ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request = Request ( ) <EOL> class", "gt": "P ( Protocol ) :", "repo": "svcs"}
{"input": "from typing import Generator , Protocol <EOL> from aiohttp . web import Application , Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = Application ( ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg , middleware_pos = <NUM_LIT> ) <EOL> app = svcs . aiohttp . init_app ( app ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . aiohttp . register_factory ( app , str , str ) <EOL> svcs . aiohttp . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . aiohttp . register_value ( app , str , str , ping = lambda : None ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request = Request ( ) <EOL> class P ( Protocol ) : <EOL> def m ( self ) -> None : ... <EOL> async", "gt": "def func ( ) -> None :", "repo": "svcs"}
{"input": "from typing import Generator , Protocol <EOL> from aiohttp . web import Application , Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = Application ( ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg , middleware_pos = <NUM_LIT> ) <EOL> app = svcs . aiohttp . init_app ( app ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . aiohttp . register_factory ( app , str , str ) <EOL> svcs . aiohttp . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . aiohttp . register_value ( app , str , str , ping = lambda : None ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request", "gt": "= Request ( )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [ no_args , diff_name , wrong_annotation ] , <EOL> ) <EOL> def test_nope ( self , factory ) : <EOL> assert not svcs . _core . _takes_container ( factory ) <EOL> def test_name ( self ) : <EOL> def factory ( svcs_container ) : ... <EOL> assert", "gt": "svcs . _core . _takes_container ( factory )", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server", "gt": "= await AppServer . start ( app )", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async", "gt": "def test_aclose_registry_robust ( self ) :", "repo": "svcs"}
{"input": "from typing import Generator , Protocol <EOL> from aiohttp . web import Application , Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = Application ( ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg ) <EOL> app = svcs . aiohttp . init_app ( app , registry = reg , middleware_pos = <NUM_LIT> ) <EOL> app = svcs . aiohttp . init_app ( app ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . aiohttp . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . aiohttp . register_factory ( app , str , str ) <EOL> svcs . aiohttp . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . aiohttp . register_value ( app , str , str , ping = lambda : None ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request = Request ( ) <EOL> class P ( Protocol ) : <EOL> def m ( self ) -> None : ... <EOL> async def func ( ) -> None : <EOL> a , b , c , d , e , f , g , h , i , j = await svcs . aiohttp . aget ( <EOL> request , int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> p : P = await svcs . aiohttp . aget_abstract ( request , P ) <EOL> await", "gt": "svcs . aiohttp . aclose_registry ( app )", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @", "gt": "pytest . mark . skipif (", "repo": "svcs"}
{"input": "import contextlib <EOL> import gc <EOL> import importlib . util <EOL> import inspect <EOL> import logging <EOL> import sys <EOL> from unittest . mock import AsyncMock , Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import async_str_gen_factory , str_gen_factory <EOL> from . helpers import nop <EOL> from . ifaces import AnotherService , Interface , Service <EOL> needs_working_async_mock = pytest . mark . skipif ( <EOL> not inspect . iscoroutinefunction ( AsyncMock ( ) ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _create_module ( tmp_path ) : <EOL> def wrapper ( source ) : <EOL> module_name = \"<STR_LIT>\" <EOL> module_path = tmp_path / f\"<STR_LIT>\" <EOL> module_path . write_text ( source ) <EOL> spec = importlib . util . spec_from_file_location ( module_name , module_path ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> sys . modules [ module_name ] = module <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> return wrapper <EOL> class TestRegistry : <EOL> def test_repr_empty ( self , registry ) : <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_repr_counts ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> assert \"<STR_LIT>\" == repr ( registry ) <EOL> def test_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> with cm ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_generators_become_context_managers ( self , registry ) : <EOL> registry . register_factory ( Service , async_str_gen_factory ) <EOL> cm = registry . get_registered_service_for ( Service ) . factory <EOL> async with cm ( ) : <EOL> ... <EOL> def test_empty_close ( self ) : <EOL> svcs . Registry ( ) . close ( ) <EOL> with svcs . Registry ( ) : <EOL> ... <EOL> def test_close_closes ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> AnotherService , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> assert not registry . _services <EOL> assert not registry . _on_close <EOL> def test_overwritten_factories_are_not_forgotten ( self , registry ) : <EOL> close_1 = Mock ( ) <EOL> close_2 = Mock ( ) <EOL> registry . register_factory ( Service , Service , on_registry_close = close_1 ) <EOL> registry . register_value ( <EOL> Service , AnotherService , on_registry_close = close_2 <EOL> ) <EOL> registry . close ( ) <EOL> assert close_1 . called <EOL> assert close_2 . called <EOL> def test_close_warns_about_async ( self , registry ) : <EOL> async def callback ( ) : ... <EOL> registry . register_factory ( Service , Service , on_registry_close = callback ) <EOL> with pytest . warns ( <EOL> UserWarning , <EOL> match = \"<STR_LIT>\" , <EOL> ) : <EOL> registry . close ( ) <EOL> def test_register_factory_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_factory ( Interface , Service ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_factory_name <EOL> def test_register_value_logs ( self , registry , caplog ) : <EOL> caplog . set_level ( logging . DEBUG ) <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert <NUM_LIT> == caplog . records [ <NUM_LIT> ] . svcs_value <EOL> def test_close_logs_failures ( self , registry , caplog ) : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = Mock ( side_effect = ValueError ( ) ) <EOL> ) <EOL> with registry : <EOL> ... <EOL> assert [ <EOL> \"<STR_LIT>\" <EOL> ] == caplog . messages <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_context_manager ( self ) : <EOL> orc = Mock ( ) <EOL> with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( Service , Service , on_registry_close = orc ) <EOL> orc . assert_called_once_with ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , close_me ) : <EOL> async def closer ( ) : <EOL> close_me . close ( ) <EOL> async with svcs . Registry ( ) as registry : <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = closer ( ) <EOL> ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . skipif ( <EOL> not hasattr ( contextlib , \"<STR_LIT>\" ) , <EOL> reason = \"<STR_LIT>\" , <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_empty_close ( self , registry ) : <EOL> await registry . aclose ( ) <EOL> async with svcs . Registry ( ) : <EOL> ... <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_mixed ( self , registry ) : <EOL> sync_close = Mock ( ) <EOL> async_close = AsyncMock ( ) <EOL> registry . register_factory ( <EOL> Service , Service , on_registry_close = sync_close <EOL> ) <EOL> registry . register_factory ( <EOL> AnotherService , AnotherService , on_registry_close = async_close <EOL> ) <EOL> await registry . aclose ( ) <EOL> assert sync_close . called <EOL> async_close . assert_awaited_once ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ needs_working_async_mock <EOL> async def test_aclose_logs_failures ( self , registry , caplog ) : <EOL> close_mock = AsyncMock ( side_effect = ValueError ( ) ) <EOL> registry . register_factory ( <EOL> Service , <EOL> Service , <EOL> on_registry_close = close_mock , <EOL> ) <EOL> await registry . aclose ( ) <EOL> close_mock . assert_awaited_once ( ) <EOL> assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> def test_contains ( self , registry ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service in registry <EOL> assert AnotherService not in registry <EOL> def test_gc_warning ( self , recwarn ) : <EOL> def scope ( ) : <EOL> registry = svcs . Registry ( ) <EOL> registry . register_value ( int , <NUM_LIT> , on_registry_close = nop ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> \"<STR_LIT>\" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestRegisteredService : <EOL> def test_repr ( self , rs ) : <EOL> assert ( <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) == repr ( rs ) <EOL> def test_name ( self , rs ) : <EOL> assert \"<STR_LIT>\" == rs . name <EOL> def wrong_annotation ( foo : svcs . Registry ) -> int : ... <EOL> def no_args ( ) : ... <EOL> def diff_name ( ) : ... <EOL> takes_containers_annotation_string_modules = ( <EOL> , <EOL> , <EOL> , <EOL> , <EOL> , <EOL> ) <EOL> class TestTakesContainer : <EOL> @ pytest . mark . parametrize ( <EOL> \"<STR_LIT>\" , <EOL> [", "gt": "no_args , diff_name , wrong_annotation ] ,", "repo": "svcs"}
{"input": "from __future__ import annotations <EOL> import json <EOL> import attrs <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_int_factory <EOL> from tests . ifaces import Service <EOL> try : <EOL> from aiohttp import ClientSession <EOL> from aiohttp . web import ( <EOL> Application , <EOL> AppRunner , <EOL> Request , <EOL> Response , <EOL> TCPSite , <EOL> json_response , <EOL> ) <EOL> from yarl import URL <EOL> except ImportError : <EOL> pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) <EOL> @ attrs . define <EOL> class AppServer : <EOL> app : Application <EOL> runner : AppRunner <EOL> base_url : URL <EOL> @ classmethod <EOL> async def start ( cls , app : Application ) -> AppServer : <EOL> runner = AppRunner ( app ) <EOL> await runner . setup ( ) <EOL> site = TCPSite ( runner , \"<STR_LIT>\" , <NUM_LIT> ) <EOL> await site . start ( ) <EOL> host , port = runner . addresses [ <NUM_LIT> ] [ : <NUM_LIT> ] <EOL> return cls ( <EOL> app = app , <EOL> runner = runner , <EOL> base_url = URL . build ( scheme = \"<STR_LIT>\" , host = host , port = port ) , <EOL> ) <EOL> async def aclose ( self ) : <EOL> await self . runner . cleanup ( ) <EOL> async def get ( url : URL ) -> tuple [ Response , str ] : <EOL> async with ClientSession ( ) as session , session . get ( url ) as resp : <EOL> return resp , await resp . text ( ) <EOL> async def view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget ( request , str ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . aget_abstract ( request , int ) , <EOL> \"<STR_LIT>\" : await svcs . aiohttp . svcs_from ( request ) . aget ( int ) , <EOL> } <EOL> ) <EOL> async def health_view ( request : Request ) -> Response : <EOL> return json_response ( <EOL> { p . name : ( await p . aping ( ) ) for p in svcs . aiohttp . get_pings ( request ) } <EOL> ) <EOL> @ pytest . fixture ( name = \"<STR_LIT>\" ) <EOL> def _app ( registry ) : <EOL> return svcs . aiohttp . init_app ( Application ( ) , registry = registry ) <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAIOHTTP : <EOL> async def test_aclose_registry_ok ( self , app , close_me ) : <EOL> async def closer ( ) : <EOL> await close_me . aclose ( ) <EOL> svcs . aiohttp . register_factory ( <EOL> app , Service , Service , on_registry_close = closer <EOL> ) <EOL> server = await AppServer . start ( app ) <EOL> await server . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> async def test_aclose_registry_robust ( self ) : <EOL> await", "gt": "svcs . aiohttp . aclose_registry ( Application ( ) )", "repo": "svcs"}
{"input": "import instld <EOL> def test_install_with_options ( ) : <EOL> calls = [ ] <EOL> def runner ( args , logger , catch_output ) : <EOL> calls . append ( args ) <EOL> with", "gt": "instld ( '<STR_LIT>' , runner = runner , platform = '<STR_LIT>' , no_deps = True ) :", "repo": "instld"}
{"input": "import instld <EOL> def test_install_with_options ( ) : <EOL> calls = [ ] <EOL> def runner ( args , logger , catch_output ) : <EOL> calls . append ( args ) <EOL> with instld ( '<STR_LIT>' , runner = runner , platform = '<STR_LIT>' , no_deps = True ) : <EOL> pass <EOL> assert len ( calls ) == <NUM_LIT> <EOL> assert", "gt": "calls [ <NUM_LIT> ] [ <NUM_LIT> : ] == [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ]", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys . modules . pop ( module_name ) <EOL> yield <EOL> sys . path = old_path <EOL> def install ( self , * package_names , catch_output = False , ** options ) : <EOL> if not package_names : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> options", "gt": "= convert_options ( options )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys . modules . pop ( module_name ) <EOL> yield <EOL> sys . path = old_path <EOL> def install ( self , * package_names , catch_output = False , ** options ) : <EOL> if not package_names : <EOL> raise", "gt": "ValueError ( '<STR_LIT>' )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib", "gt": ". reload ( module )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys . modules . pop ( module_name ) <EOL> yield <EOL> sys", "gt": ". path = old_path", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path", "gt": "= sys . path", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys", "gt": ". modules . pop ( module_name )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys . modules . pop ( module_name ) <EOL> yield <EOL> sys . path = old_path <EOL> def install ( self , * package_names , catch_output = False , ** options ) : <EOL> if not package_names : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> options = convert_options ( options ) <EOL> with", "gt": "self . installer ( package_names , catch_output = catch_output , options = options ) :", "repo": "instld"}
{"input": "import instld <EOL> def test_install_with_options ( ) : <EOL> calls = [ ] <EOL> def runner ( args , logger , catch_output ) : <EOL> calls", "gt": ". append ( args )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module", "gt": "= importlib . import_module ( module_name , * args , ** kwargs )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self", "gt": ". logger . info ( f'<STR_LIT>' )", "repo": "instld"}
{"input": "import instld <EOL> def test_install_with_options ( ) : <EOL> calls = [ ] <EOL> def runner ( args , logger , catch_output ) : <EOL> calls . append ( args ) <EOL> with instld ( '<STR_LIT>' , runner = runner , platform = '<STR_LIT>' , no_deps = True ) : <EOL> pass <EOL> assert", "gt": "len ( calls ) == <NUM_LIT>", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def", "gt": "new_path ( self , module_name ) :", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with", "gt": "self . new_path ( module_name ) :", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if", "gt": "module_name in sys . modules :", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys", "gt": ". path = [ self . where ] + copy . copy ( self . original_path )", "repo": "instld"}
{"input": "import sys <EOL> import importlib <EOL> from contextlib import contextmanager <EOL> import copy <EOL> from instld . module . lock import lock <EOL> from instld . common_utils . convert_options import convert_options <EOL> class Context : <EOL> original_path = copy . copy ( sys . path ) <EOL> def __init__ ( self , where , logger , catch_output , options , installer ) : <EOL> self . where = where <EOL> self . logger = logger <EOL> self . catch_output = catch_output <EOL> self . options = options <EOL> self . installer = installer <EOL> def __str__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> def import_here ( self , module_name , * args , ** kwargs ) : <EOL> with lock : <EOL> with self . new_path ( module_name ) : <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> module = importlib . import_module ( module_name , * args , ** kwargs ) <EOL> importlib . reload ( module ) <EOL> return module <EOL> @ contextmanager <EOL> def new_path ( self , module_name ) : <EOL> old_path = sys . path <EOL> sys . path = [ self . where ] + copy . copy ( self . original_path ) <EOL> if module_name in sys . modules : <EOL> sys . modules . pop ( module_name ) <EOL> yield <EOL> sys . path = old_path <EOL> def", "gt": "install ( self , * package_names , catch_output = False , ** options ) :", "repo": "instld"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return", "gt": "admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit", "gt": "= ( ChannelAllowList . query . filter_by ( user_id = current_user . id )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @", "gt": "main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @", "gt": "main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash", "gt": "( Markup ( leave_msg ) , '<STR_LIT>' )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return", "gt": "jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @", "gt": "main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if ( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) : <EOL> flash ( \"<STR_LIT>\" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> channel , user = checked_value <EOL> allow_record = ( ChannelAllowList . query . filter_by ( channel_id = channel . id ) <EOL> .", "gt": "filter_by ( user_id = user . id ) . first ( ) )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return", "gt": "jsonify ( { '<STR_LIT>' : False } )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id", "gt": "= request . form . get ( '<STR_LIT>' )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins", "gt": "= all ( [ is_admin ( channel , user ) for user in users ] )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if ( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) : <EOL> flash", "gt": "( \"<STR_LIT>\" , '<STR_LIT>' )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel", "gt": "= Channel . query . filter_by ( name = channel_name ) . first ( )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name", "gt": "= request . form . get ( '<STR_LIT>' )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return", "gt": "admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if ( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) : <EOL> flash ( \"<STR_LIT>\" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> channel , user = checked_value <EOL> allow_record = ( ChannelAllowList . query . filter_by ( channel_id = channel . id ) <EOL> . filter_by ( user_id = user . id ) . first ( ) ) <EOL> db . session . delete ( allow_record ) <EOL> db", "gt": ". session . commit ( )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> .", "gt": "filter_by ( channel_id = channel . id ) . first ( ) )", "repo": "Wiregate"}
{"input": "from . base import db <EOL> class Channel ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> name = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> password = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> messages = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , cascade = '<STR_LIT>' , lazy = True ) <EOL> allowed_users = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True ) <EOL> def", "gt": "__repr__ ( self ) -> str :", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id", "gt": "= request . form . get ( '<STR_LIT>' )", "repo": "Wiregate"}
{"input": "from os import environ <EOL> from flask import Flask <EOL> import secrets <EOL> import string <EOL> def configure_app ( app : Flask ) -> None : <EOL> characters = string . ascii_letters + string . digits + string . punctuation <EOL> appkey", "gt": "= '<STR_LIT>' . join ( secrets . choice ( characters ) for _ in range ( <NUM_LIT> ) )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if ( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) : <EOL> flash ( \"<STR_LIT>\" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> channel", "gt": ", user = checked_value", "repo": "Wiregate"}
{"input": "from . base import db <EOL> class Channel ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> name = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> password = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> messages = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , cascade = '<STR_LIT>' , lazy = True ) <EOL> allowed_users", "gt": "= db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if ( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) : <EOL> flash ( \"<STR_LIT>\" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> channel , user = checked_value <EOL> allow_record = ( ChannelAllowList . query . filter_by ( channel_id = channel . id ) <EOL> . filter_by ( user_id = user . id ) . first ( ) ) <EOL> db . session . delete ( allow_record ) <EOL> db . session . commit ( ) <EOL> flash ( f\"<STR_LIT>\" , '<STR_LIT>' ) <EOL> return", "gt": "redirect ( f\"<STR_LIT>\" )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if", "gt": "( not checked_value ) or is_admin ( Channel . query . get ( channel_id ) , User . query . get ( user_id ) ) :", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples", "gt": "= [ ( user , is_admin ( channel , user ) ) for user in users ]", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages", "gt": "= get_number_of_channels_messages ( channel )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def channel_settings ( channel_name : str ) -> str : <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return no_channel ( ) <EOL> channel_permit = ( ChannelAllowList . query . filter_by ( user_id = current_user . id ) <EOL> . filter_by ( channel_id = channel . id ) . first ( ) ) <EOL> if channel_permit and channel_permit . user_role == UserRole . ADMIN . value : <EOL> num_users = get_number_of_channels_users ( channel ) <EOL> num_messages = get_number_of_channels_messages ( channel ) <EOL> users = get_channels_users ( channel ) <EOL> only_admins = all ( [ is_admin ( channel , user ) for user in users ] ) <EOL> user_tuples = [ ( user , is_admin ( channel , user ) ) for user in users ] <EOL> return render_template ( <EOL> '<STR_LIT>' , <EOL> channel = channel , num_users = num_users , num_messages = num_messages , user_tuples = user_tuples , <EOL> only_admins = only_admins <EOL> ) <EOL> else : <EOL> return no_channel ( ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def make_admin ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def revoke_admin ( ) : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> return admin_manager ( command = '<STR_LIT>' , channel_id = channel_id , user_id = user_id ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def remove_user ( ) -> str : <EOL> channel_id = request . form . get ( '<STR_LIT>' ) <EOL> user_id = request . form . get ( '<STR_LIT>' ) <EOL> checked_value", "gt": "= check_channel_settings_form ( channel_id , user_id )", "repo": "Wiregate"}
{"input": "from os import environ <EOL> from flask import Flask <EOL> import secrets <EOL> import string <EOL> def configure_app ( app : Flask ) -> None : <EOL> characters = string . ascii_letters + string . digits + string . punctuation <EOL> appkey = '<STR_LIT>' . join ( secrets . choice ( characters ) for _ in range ( <NUM_LIT> ) ) <EOL> app", "gt": ". secret_key = appkey", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db", "gt": ". session . commit ( )", "repo": "Wiregate"}
{"input": "from typing import Any <EOL> from flask import request , render_template , jsonify , flash , redirect , url_for , Markup , abort , Response <EOL> from flask_login import current_user , login_required <EOL> from . base import main <EOL> from . utils import get_messages , process_add_channel_form , process_join_channel_form , get_number_of_channels_users , get_number_of_channels_messages , get_channels_users , is_admin , admin_manager , check_channel_settings_form , no_channel <EOL> from app . models import db , Channel , ChannelAllowList , User <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . forms . channel import AddChannelForm , UpdateChannelForm , JoinChannelForm <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def setup_app ( ) -> str : <EOL> add_channel_form = AddChannelForm ( prefix = '<STR_LIT>' ) <EOL> join_channel_form = JoinChannelForm ( prefix = '<STR_LIT>' ) <EOL> add_channel_form_invalid = False <EOL> join_channel_form_invalid = False <EOL> if add_channel_form . submit_add . data : <EOL> if add_channel_form . validate_on_submit ( ) : <EOL> process_add_channel_form ( add_channel_form ) <EOL> else : <EOL> add_channel_form_invalid = True <EOL> elif join_channel_form . submit_join . data : <EOL> if join_channel_form . validate_on_submit ( ) : <EOL> process_join_channel_form ( join_channel_form ) <EOL> else : <EOL> join_channel_form_invalid = True <EOL> allowed_channels = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . all ( ) <EOL> channels = [ allowed_channel . channel for allowed_channel in allowed_channels ] <EOL> return render_template ( <EOL> '<STR_LIT>' , username = current_user , channels = channels , <EOL> add_channel_form = add_channel_form , <EOL> join_channel_form = join_channel_form , <EOL> add_channel_form_invalid = add_channel_form_invalid , <EOL> join_channel_form_invalid = join_channel_form_invalid , <EOL> ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_messages_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> counter = request . form . get ( '<STR_LIT>' ) <EOL> try : <EOL> counter = int ( counter ) <EOL> except ValueError : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> if counter > len ( Channel . query . filter_by ( name = channel_name ) . first ( ) . messages ) : <EOL> return abort ( Response ( '<STR_LIT>' ) ) <EOL> else : <EOL> return get_messages ( channel_name , counter ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def get_initial_counter_ajax ( ) -> Any : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> return jsonify ( { <EOL> '<STR_LIT>' : len ( channel . messages ) <EOL> } ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def leave_channel ( ) -> str : <EOL> channel_name = request . form . get ( \"<STR_LIT>\" ) <EOL> channel_id = Channel . query . filter_by ( name = channel_name ) . first ( ) . id <EOL> leave_msg = f'<STR_LIT>' <EOL> ( db . session . query ( ChannelAllowList ) . filter ( ChannelAllowList . user_id == current_user . id ) <EOL> . filter ( ChannelAllowList . channel_id == channel_id ) <EOL> . delete ( ) ) <EOL> if not ChannelAllowList . query . filter_by ( channel_id = channel_id ) . first ( ) : <EOL> db . session . delete ( Channel . query . filter_by ( id = channel_id ) . first ( ) ) <EOL> leave_msg += '<STR_LIT>' <EOL> db . session . commit ( ) <EOL> flash ( Markup ( leave_msg ) , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def is_admin_ajax ( ) : <EOL> channel_name = request . form . get ( '<STR_LIT>' ) <EOL> if not channel_name : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> if not channel : <EOL> return jsonify ( { '<STR_LIT>' : False } ) <EOL> return jsonify ( { '<STR_LIT>' : is_admin ( channel , current_user ) } ) <EOL> @", "gt": "main . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] )", "repo": "Wiregate"}
{"input": "import inspect <EOL> class BaseConfig : <EOL> def __init__ ( self ) -> None : <EOL> self . init_member_classes ( self ) <EOL> @ staticmethod <EOL> def init_member_classes ( obj ) : <EOL> for key in dir ( obj ) : <EOL> if key == \"<STR_LIT>\" : <EOL> continue <EOL> var", "gt": "= getattr ( obj , key )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def", "gt": "forward ( self , states ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def", "gt": "forward ( self , states ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super", "gt": "( DiscriminatorLSD , self ) . __init__ ( )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def", "gt": "inference ( self , states ) :", "repo": "extreme-parkour"}
{"input": "import inspect <EOL> class BaseConfig : <EOL> def __init__ ( self ) -> None : <EOL> self . init_member_classes ( self ) <EOL> @ staticmethod <EOL> def init_member_classes ( obj ) : <EOL> for key in dir ( obj ) : <EOL> if key == \"<STR_LIT>\" : <EOL> continue <EOL> var = getattr ( obj , key ) <EOL> if", "gt": "inspect . isclass ( var ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return torch . nn . functional . normalize ( self . discriminator ( states ) ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return", "gt": "self . discriminator ( states )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return torch . nn . functional . normalize ( self . discriminator ( states ) ) <EOL> def", "gt": "inference ( self , states ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for", "gt": "l in range ( len ( hidden_dims ) ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers", "gt": ". append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with", "gt": "torch . no_grad ( ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class", "gt": "DiscriminatorLSD ( nn . Module ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def", "gt": "__init__ ( self , n_states ,", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super", "gt": "( DiscriminatorContDIAYN , self ) . __init__ ( )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return", "gt": "self . discriminator ( states )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers", "gt": ". append ( activation )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if", "gt": "l == len ( hidden_dims ) - <NUM_LIT> :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def", "gt": "forward ( self , states ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation", "gt": "= get_activation ( activation )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def", "gt": "inference ( self , states ) :", "repo": "extreme-parkour"}
{"input": "import inspect <EOL> class BaseConfig : <EOL> def __init__ ( self ) -> None : <EOL> self . init_member_classes ( self ) <EOL> @ staticmethod <EOL> def init_member_classes ( obj ) : <EOL> for key in dir ( obj ) : <EOL> if key == \"<STR_LIT>\" : <EOL> continue <EOL> var = getattr ( obj , key ) <EOL> if inspect . isclass ( var ) : <EOL> i_var = var ( ) <EOL> setattr", "gt": "( obj , key , i_var )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers", "gt": ". append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return", "gt": "self . discriminator ( states )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers", "gt": ". append ( activation )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers", "gt": ". append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation", "gt": "= \"<STR_LIT>\" ) :", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims", "gt": "= [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ,", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorContDIAYN , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return", "gt": "torch . nn . functional . normalize ( self . discriminator ( states ) )", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorContDIAYN ( nn . Module ) : <EOL> def", "gt": "__init__ ( self , n_states ,", "repo": "extreme-parkour"}
{"input": "from turtle import forward <EOL> import numpy as np <EOL> from rsl_rl . modules . actor_critic import get_activation <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . utils . parametrizations import spectral_norm <EOL> class Estimator ( nn . Module ) : <EOL> def __init__ ( self , input_dim , <EOL> output_dim , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" , <EOL> ** kwargs ) : <EOL> super ( Estimator , self ) . __init__ ( ) <EOL> self . input_dim = input_dim <EOL> self . output_dim = output_dim <EOL> activation = get_activation ( activation ) <EOL> estimator_layers = [ ] <EOL> estimator_layers . append ( nn . Linear ( self . input_dim , hidden_dims [ <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , output_dim ) ) <EOL> else : <EOL> estimator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> estimator_layers . append ( activation ) <EOL> self . estimator = nn . Sequential ( * estimator_layers ) <EOL> def forward ( self , input ) : <EOL> return self . estimator ( input ) <EOL> def inference ( self , input ) : <EOL> with torch . no_grad ( ) : <EOL> return self . estimator ( input ) <EOL> class Discriminator ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( Discriminator , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) <EOL> else : <EOL> discriminator_layers . append ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return self . discriminator ( states ) <EOL> class DiscriminatorLSD ( nn . Module ) : <EOL> def __init__ ( self , n_states , <EOL> n_skills , <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = \"<STR_LIT>\" ) : <EOL> super ( DiscriminatorLSD , self ) . __init__ ( ) <EOL> self . n_states = n_states <EOL> self . n_skills = n_skills <EOL> activation = get_activation ( activation ) <EOL> discriminator_layers = [ ] <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( n_states , hidden_dims [ <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> for l in range ( len ( hidden_dims ) ) : <EOL> if l == len ( hidden_dims ) - <NUM_LIT> : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , n_skills ) ) ) <EOL> else : <EOL> discriminator_layers . append ( spectral_norm ( nn . Linear ( hidden_dims [ l ] , hidden_dims [ l + <NUM_LIT> ] ) ) ) <EOL> discriminator_layers . append ( activation ) <EOL> self . discriminator = nn . Sequential ( * discriminator_layers ) <EOL> def forward ( self , states ) : <EOL> return self . discriminator ( states ) <EOL> def inference ( self , states ) : <EOL> with torch . no_grad ( ) : <EOL> return", "gt": "self . discriminator ( states )", "repo": "extreme-parkour"}
{"input": "import flask <EOL> from . import cookie <EOL> from mod . args import GlobalArgs <EOL> args = GlobalArgs ( ) <EOL> def require_auth ( request : flask . request , permission : str = '<STR_LIT>' ) : <EOL> user_cookie : str = request . cookies . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> cookie_key : str = cookie . cookie_key ( user_cookie ) <EOL> auth_header : str = request . headers . get ( '<STR_LIT>' , False ) or request . headers . get ( '<STR_LIT>' , False ) <EOL> cookie_permission : bool = has_permission ( get_permission ( cookie_key ) , permission ) <EOL> header_permission : bool = has_permission ( get_permission ( auth_header ) , permission ) <EOL> if permission == '<STR_LIT>' and not args . auth : <EOL> return <NUM_LIT> <EOL> if cookie_permission or header_permission : <EOL> return <NUM_LIT> <EOL> else : <EOL> return - <NUM_LIT> <EOL> def get_permission ( name : str ) -> str : <EOL> if not name : <EOL> return '<STR_LIT>' <EOL> auth_dict : dict = args . auth <EOL> return", "gt": "auth_dict . get ( name , '<STR_LIT>' )", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> def generate_add_data_options ( root_dir ) : <EOL> options = [ ] <EOL> for root , dirs , files in os . walk ( root_dir ) : <EOL> if files : <EOL> formatted_path = root . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> separator = \"<STR_LIT>\" if PLATFORM == \"<STR_LIT>\" else \"<STR_LIT>\" <EOL> options . append ( f'<STR_LIT>' ) <EOL> return \"<STR_LIT>\" . join ( options ) <EOL> options = generate_add_data_options ( \"<STR_LIT>\" ) <EOL> command = f\"<STR_LIT>\" <EOL> subprocess", "gt": ". run ( command , shell = True )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1", "gt": "= text_1 . lower ( )", "repo": "LrcApi"}
{"input": "import flask <EOL> from . import cookie <EOL> from mod . args import GlobalArgs <EOL> args = GlobalArgs ( ) <EOL> def require_auth ( request : flask . request , permission : str = '<STR_LIT>' ) : <EOL> user_cookie : str = request . cookies . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> cookie_key : str = cookie . cookie_key ( user_cookie ) <EOL> auth_header : str = request . headers . get ( '<STR_LIT>' , False ) or request . headers . get ( '<STR_LIT>' , False ) <EOL> cookie_permission : bool = has_permission ( get_permission ( cookie_key ) , permission ) <EOL> header_permission : bool = has_permission ( get_permission ( auth_header ) , permission ) <EOL> if permission == '<STR_LIT>' and not args . auth : <EOL> return <NUM_LIT> <EOL> if cookie_permission or header_permission : <EOL> return <NUM_LIT> <EOL> else : <EOL> return - <NUM_LIT> <EOL> def get_permission ( name : str ) -> str : <EOL> if not name : <EOL> return '<STR_LIT>' <EOL> auth_dict : dict = args . auth <EOL> return auth_dict . get ( name , '<STR_LIT>' ) <EOL> def has_permission ( supply : str , require : str ) -> bool : <EOL> if not supply : <EOL> return False <EOL> elif", "gt": "supply == \"<STR_LIT>\" :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def", "gt": "association ( text_1 : str , text_2 : str ) -> float :", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> def generate_add_data_options ( root_dir ) : <EOL> options = [ ] <EOL> for", "gt": "root , dirs , files in os . walk ( root_dir ) :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def", "gt": "assoc_artists ( text_1 : str , text_2 : str ) -> float :", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess", "gt": ". run ( \"<STR_LIT>\" , shell = True )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) ) <EOL> text_li_2 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_2 ) ) ) ) <EOL> ar_ratio = calculate_duplicate_rate ( text_li_1 , text_li_2 ) <EOL> return ar_ratio <EOL> def zero_item ( text : str ) -> str : <EOL> punctuation = \"<STR_LIT>\" <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> for", "gt": "text_z in text :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio", "gt": "= common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr", "gt": "= str_duplicate_rate ( text_1 , text_2 )", "repo": "LrcApi"}
{"input": "import re <EOL> def standard_line ( lrc_text : str ) : <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> for match in matches : <EOL> old_time_label = match <EOL> minutes , seconds , millisecond = map ( int , re . split ( '<STR_LIT>' , old_time_label ) ) <EOL> minute_str = ( '<STR_LIT>' + str ( minutes ) ) [ - <NUM_LIT> : ] if minutes < <NUM_LIT> else str ( minutes ) <EOL> second_str = ( '<STR_LIT>' + str ( seconds ) ) [ - <NUM_LIT> : ] <EOL> millisecond_str = ( str ( millisecond ) + '<STR_LIT>' ) [ : <NUM_LIT> ] <EOL> new_time_label = f\"<STR_LIT>\" <EOL> lrc_text = lrc_text . replace ( old_time_label , new_time_label ) <EOL> return lrc_text <EOL> def standard ( lrc_text : str ) : <EOL> if not isinstance ( lrc_text , str ) : <EOL> return '<STR_LIT>' <EOL> parse_string = '<STR_LIT>' <EOL> lines = lrc_text . split ( '<STR_LIT>' ) <EOL> for line_index in range ( len ( lines ) ) : <EOL> if line_index > <NUM_LIT> : <EOL> line = '<STR_LIT>' + lines [ line_index ] <EOL> else : <EOL> line = lines [ line_index ] <EOL> parse_string += standard_line ( line ) <EOL> return parse_string <EOL> def is_valid ( lrc_text : str ) : <EOL> if", "gt": "type ( lrc_text ) is not str :", "repo": "LrcApi"}
{"input": "import flask <EOL> from . import cookie <EOL> from mod . args import GlobalArgs <EOL> args = GlobalArgs ( ) <EOL> def require_auth ( request : flask . request , permission : str = '<STR_LIT>' ) : <EOL> user_cookie : str = request . cookies . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> cookie_key : str = cookie . cookie_key ( user_cookie ) <EOL> auth_header : str = request . headers . get ( '<STR_LIT>' , False ) or request . headers . get ( '<STR_LIT>' , False ) <EOL> cookie_permission : bool = has_permission ( get_permission ( cookie_key ) , permission ) <EOL> header_permission : bool = has_permission ( get_permission ( auth_header ) , permission ) <EOL> if permission == '<STR_LIT>' and not args . auth : <EOL> return <NUM_LIT> <EOL> if cookie_permission or header_permission : <EOL> return <NUM_LIT> <EOL> else : <EOL> return - <NUM_LIT> <EOL> def get_permission ( name : str ) -> str : <EOL> if not name : <EOL> return '<STR_LIT>' <EOL> auth_dict : dict = args . auth <EOL> return auth_dict . get ( name , '<STR_LIT>' ) <EOL> def has_permission ( supply : str , require : str ) -> bool : <EOL> if not supply : <EOL> return False <EOL> elif supply == \"<STR_LIT>\" : <EOL> return True <EOL> else : <EOL> supply_set", "gt": ", require_set = set ( supply ) , set ( require )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) ) <EOL> text_li_2 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_2 ) ) ) ) <EOL> ar_ratio = calculate_duplicate_rate ( text_li_1 , text_li_2 ) <EOL> return ar_ratio <EOL> def zero_item ( text : str ) -> str : <EOL> punctuation = \"<STR_LIT>\" <EOL> text", "gt": "= text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1", "gt": "= list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) ) <EOL> text_li_2 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_2 ) ) ) ) <EOL> ar_ratio = calculate_duplicate_rate ( text_li_1 , text_li_2 ) <EOL> return ar_ratio <EOL> def zero_item ( text : str ) -> str : <EOL> punctuation = \"<STR_LIT>\" <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> for text_z in text : <EOL> if text_z not in punctuation : <EOL> return text_z <EOL> return", "gt": "text [ <NUM_LIT> ] if text else text", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> def generate_add_data_options ( root_dir ) : <EOL> options = [ ] <EOL> for root , dirs , files in os . walk ( root_dir ) : <EOL> if files : <EOL> formatted_path = root . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> separator = \"<STR_LIT>\" if PLATFORM == \"<STR_LIT>\" else \"<STR_LIT>\" <EOL> options", "gt": ". append ( f'<STR_LIT>' )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for", "gt": "char_s in list_2 :", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> def generate_add_data_options ( root_dir ) : <EOL> options = [ ] <EOL> for root , dirs , files in os . walk ( root_dir ) : <EOL> if files : <EOL> formatted_path = root . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> separator = \"<STR_LIT>\" if PLATFORM == \"<STR_LIT>\" else \"<STR_LIT>\" <EOL> options . append ( f'<STR_LIT>' ) <EOL> return", "gt": "\"<STR_LIT>\" . join ( options )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if", "gt": "text_1 == \"<STR_LIT>\" :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) ) <EOL> text_li_2 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_2 ) ) ) ) <EOL> ar_ratio = calculate_duplicate_rate ( text_li_1 , text_li_2 ) <EOL> return ar_ratio <EOL> def zero_item ( text : str ) -> str : <EOL> punctuation = \"<STR_LIT>\" <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> for text_z in text : <EOL> if text_z not in punctuation : <EOL> return text_z <EOL> return text [ <NUM_LIT> ] if text else text <EOL> if __name__ == \"<STR_LIT>\" : <EOL> text_s = \"<STR_LIT>\" <EOL> text_r = \"<STR_LIT>\" <EOL> print", "gt": "( str_duplicate_rate ( text_s , text_r ) )", "repo": "LrcApi"}
{"input": "import re <EOL> def standard_line ( lrc_text : str ) : <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> for match in matches : <EOL> old_time_label = match <EOL> minutes , seconds , millisecond = map ( int , re . split ( '<STR_LIT>' , old_time_label ) ) <EOL> minute_str = ( '<STR_LIT>' + str ( minutes ) ) [ - <NUM_LIT> : ] if minutes < <NUM_LIT> else str ( minutes ) <EOL> second_str = ( '<STR_LIT>' + str ( seconds ) ) [ - <NUM_LIT> : ] <EOL> millisecond_str = ( str ( millisecond ) + '<STR_LIT>' ) [ : <NUM_LIT> ] <EOL> new_time_label = f\"<STR_LIT>\" <EOL> lrc_text = lrc_text . replace ( old_time_label , new_time_label ) <EOL> return lrc_text <EOL> def standard ( lrc_text : str ) : <EOL> if not isinstance ( lrc_text , str ) : <EOL> return '<STR_LIT>' <EOL> parse_string = '<STR_LIT>' <EOL> lines = lrc_text . split ( '<STR_LIT>' ) <EOL> for line_index in range ( len ( lines ) ) : <EOL> if line_index > <NUM_LIT> : <EOL> line = '<STR_LIT>' + lines [ line_index ] <EOL> else : <EOL> line = lines [ line_index ] <EOL> parse_string += standard_line ( line ) <EOL> return parse_string <EOL> def is_valid ( lrc_text : str ) : <EOL> if type ( lrc_text ) is not str : <EOL> return False <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches", "gt": "= pattern . findall ( lrc_text )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2", "gt": "= text_2 . lower ( )", "repo": "LrcApi"}
{"input": "import subprocess <EOL> import platform <EOL> import sys <EOL> import codecs <EOL> import os <EOL> from mod . args import GlobalArgs <EOL> sys . stdout = codecs . getwriter ( \"<STR_LIT>\" ) ( sys . stdout . detach ( ) ) <EOL> PLATFORM = platform . system ( ) <EOL> ARCHITECTURE = platform . machine ( ) <EOL> APP_NAME = \"<STR_LIT>\" <EOL> APP_VERSION = GlobalArgs ( ) . version <EOL> PACK_NAME = f\"<STR_LIT>\" <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> subprocess . run ( \"<STR_LIT>\" , shell = True ) <EOL> def generate_add_data_options ( root_dir ) : <EOL> options = [ ] <EOL> for root , dirs , files in os . walk ( root_dir ) : <EOL> if files : <EOL> formatted_path", "gt": "= root . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if", "gt": "text_1 == '<STR_LIT>' :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters", "gt": "= [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ]", "repo": "LrcApi"}
{"input": "import re <EOL> def standard_line ( lrc_text : str ) : <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> for match in matches : <EOL> old_time_label = match <EOL> minutes , seconds , millisecond = map ( int , re . split ( '<STR_LIT>' , old_time_label ) ) <EOL> minute_str = ( '<STR_LIT>' + str ( minutes ) ) [ - <NUM_LIT> : ] if minutes < <NUM_LIT> else str ( minutes ) <EOL> second_str = ( '<STR_LIT>' + str ( seconds ) ) [ - <NUM_LIT> : ] <EOL> millisecond_str = ( str ( millisecond ) + '<STR_LIT>' ) [ : <NUM_LIT> ] <EOL> new_time_label = f\"<STR_LIT>\" <EOL> lrc_text = lrc_text . replace ( old_time_label , new_time_label ) <EOL> return lrc_text <EOL> def standard ( lrc_text : str ) : <EOL> if not isinstance ( lrc_text , str ) : <EOL> return '<STR_LIT>' <EOL> parse_string = '<STR_LIT>' <EOL> lines = lrc_text . split ( '<STR_LIT>' ) <EOL> for line_index in range ( len ( lines ) ) : <EOL> if line_index > <NUM_LIT> : <EOL> line = '<STR_LIT>' + lines [ line_index ] <EOL> else : <EOL> line = lines [ line_index ] <EOL> parse_string += standard_line ( line ) <EOL> return parse_string <EOL> def", "gt": "is_valid ( lrc_text : str ) :", "repo": "LrcApi"}
{"input": "import re <EOL> def standard_line ( lrc_text : str ) : <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> for match in matches : <EOL> old_time_label = match <EOL> minutes , seconds , millisecond = map ( int , re . split ( '<STR_LIT>' , old_time_label ) ) <EOL> minute_str = ( '<STR_LIT>' + str ( minutes ) ) [ - <NUM_LIT> : ] if minutes < <NUM_LIT> else str ( minutes ) <EOL> second_str = ( '<STR_LIT>' + str ( seconds ) ) [ - <NUM_LIT> : ] <EOL> millisecond_str = ( str ( millisecond ) + '<STR_LIT>' ) [ : <NUM_LIT> ] <EOL> new_time_label = f\"<STR_LIT>\" <EOL> lrc_text = lrc_text . replace ( old_time_label , new_time_label ) <EOL> return lrc_text <EOL> def standard ( lrc_text : str ) : <EOL> if not isinstance ( lrc_text , str ) : <EOL> return '<STR_LIT>' <EOL> parse_string = '<STR_LIT>' <EOL> lines = lrc_text . split ( '<STR_LIT>' ) <EOL> for line_index in range ( len ( lines ) ) : <EOL> if line_index > <NUM_LIT> : <EOL> line = '<STR_LIT>' + lines [ line_index ] <EOL> else : <EOL> line = lines [ line_index ] <EOL> parse_string += standard_line ( line ) <EOL> return parse_string <EOL> def is_valid ( lrc_text : str ) : <EOL> if type ( lrc_text ) is not str : <EOL> return False <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> if matches : <EOL> return True <EOL> else : <EOL> return False <EOL> if __name__ == \"<STR_LIT>\" : <EOL> lrc = <EOL> print ( standard ( lrc ) ) <EOL> from devtools import Benchmark <EOL> b", "gt": "= Benchmark ( threads = <NUM_LIT> , rounds = <NUM_LIT> )", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio = longest_common_substring ( text_1 , text_2 ) / len ( text_1 ) <EOL> string_dr = str_duplicate_rate ( text_1 , text_2 ) <EOL> similar_ratio = common_ratio * ( string_dr ** <NUM_LIT> ) ** ( <NUM_LIT> / <NUM_LIT> ) <EOL> return similar_ratio <EOL> def assoc_artists ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == \"<STR_LIT>\" : <EOL> return <NUM_LIT> <EOL> delimiters = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] <EOL> delimiter_pattern = '<STR_LIT>' . join ( map ( re . escape , delimiters ) ) <EOL> text_li_1 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_1 ) ) ) ) <EOL> text_li_2 = list ( filter ( None , re . split ( delimiter_pattern , t2s ( text_2 ) ) ) ) <EOL> ar_ratio = calculate_duplicate_rate ( text_li_1 , text_li_2 ) <EOL> return ar_ratio <EOL> def zero_item ( text : str ) -> str : <EOL> punctuation = \"<STR_LIT>\" <EOL> text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> for text_z in text : <EOL> if", "gt": "text_z not in punctuation :", "repo": "LrcApi"}
{"input": "import re <EOL> from mod . ttscn import t2s <EOL> def text_convert ( text : str ) : <EOL> patterns = [ <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> r\"<STR_LIT>\" , <EOL> ] <EOL> for pattern in patterns : <EOL> text_re = re . sub ( pattern , '<STR_LIT>' , text ) <EOL> text = text_re if len ( text_re ) else text <EOL> return text <EOL> def longest_common_substring ( str1 , str2 ) : <EOL> m = len ( str1 ) <EOL> n = len ( str2 ) <EOL> dp = [ [ <NUM_LIT> ] * ( n + <NUM_LIT> ) for _ in range ( m + <NUM_LIT> ) ] <EOL> max_length = <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , m + <NUM_LIT> ) : <EOL> for j in range ( <NUM_LIT> , n + <NUM_LIT> ) : <EOL> if str1 [ i - <NUM_LIT> ] == str2 [ j - <NUM_LIT> ] : <EOL> dp [ i ] [ j ] = dp [ i - <NUM_LIT> ] [ j - <NUM_LIT> ] + <NUM_LIT> <EOL> if dp [ i ] [ j ] > max_length : <EOL> max_length = dp [ i ] [ j ] <EOL> else : <EOL> dp [ i ] [ j ] = <NUM_LIT> <EOL> return max_length <EOL> def str_duplicate_rate ( str1 , str2 ) : <EOL> set1 = set ( str1 ) <EOL> set2 = set ( str2 ) <EOL> common_characters = set1 . intersection ( set2 ) <EOL> total_characters = set1 . union ( set2 ) <EOL> similarity_ratio = len ( common_characters ) / len ( total_characters ) <EOL> return similarity_ratio <EOL> def calculate_duplicate_rate ( list_1 , list_2 ) : <EOL> count = <NUM_LIT> <EOL> for char in list_1 : <EOL> char_sim = [ ] <EOL> for char_s in list_2 : <EOL> char_sim . append ( association ( char , char_s ) ) <EOL> count += max ( char_sim ) <EOL> duplicate_rate = count / len ( list_1 ) <EOL> return duplicate_rate <EOL> def association ( text_1 : str , text_2 : str ) -> float : <EOL> if text_1 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> if text_2 == '<STR_LIT>' : <EOL> return <NUM_LIT> <EOL> text_1 = text_1 . lower ( ) <EOL> text_2 = text_2 . lower ( ) <EOL> common_ratio", "gt": "= longest_common_substring ( text_1 , text_2 ) / len ( text_1 )", "repo": "LrcApi"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> session = user_session . get ( user_id , [ ] ) <EOL> if len ( session ) == <NUM_LIT> : <EOL> system_prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } <EOL> session . append ( system_item ) <EOL> user_session", "gt": "[ user_id ] = session", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger", "gt": ". exception ( e )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue . append ( conversation ) <EOL> user_session [ user_id ] = queue <EOL> Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) <EOL> @ staticmethod <EOL> def discard_exceed_conversation ( session , max_tokens ) : <EOL> count = <NUM_LIT> <EOL> count_list = list ( ) <EOL> for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> history_conv = session [ i ] <EOL> count += len ( history_conv [ \"<STR_LIT>\" ] ) + len ( history_conv [ \"<STR_LIT>\" ] ) <EOL> count_list . append ( count ) <EOL> for c in count_list : <EOL> if", "gt": "c > max_tokens :", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger", "gt": ". exception ( e )", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response", "gt": "= client . send_message ( prompt , conversation_id )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger", "gt": ". warn ( e )", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result = create_chat ( msg ) <EOL> Conversation_id = result [ '<STR_LIT>' ] <EOL> return result [ '<STR_LIT>' ] <EOL> def get_group_info ( ) : <EOL> cookie = get_cookie ( ) <EOL> def group_id ( name ) : <EOL> df = itchat . search_chatrooms ( name = name ) <EOL> return df [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> @ itchat . msg_register ( [ TEXT , MAP , CARD , NOTE , SHARING ] ) <EOL> def text_reply ( msg ) : <EOL> itchat . send ( '<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( [ PICTURE , RECORDING , ATTACHMENT , VIDEO ] ) <EOL> def download_files ( msg ) : <EOL> msg [ '<STR_LIT>' ] ( msg [ '<STR_LIT>' ] ) <EOL> return '<STR_LIT>' % ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } . get ( msg [ '<STR_LIT>' ] , '<STR_LIT>' ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def group_text_reply ( msg ) : <EOL> item = group_id ( u'<STR_LIT>' ) <EOL> if msg [ '<STR_LIT>' ] == item : <EOL> itchat . send ( u'<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , item ) <EOL> def qrCallback ( uuid , status , qrcode ) : <EOL> if status == \"<STR_LIT>\" : <EOL> try : <EOL> from PIL import Image <EOL> img = Image . open ( io . BytesIO ( qrcode ) ) <EOL> _thread = threading . Thread ( target = img . show , args = ( \"<STR_LIT>\" , ) ) <EOL> _thread . setDaemon ( True ) <EOL> _thread . start ( ) <EOL> except Exception as e : <EOL> pass <EOL> import qrcode <EOL> url = f\"<STR_LIT>\" <EOL> qr_api1 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api2 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api3 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api4 = \"<STR_LIT>\" . format ( url ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( qr_api3 ) <EOL> print ( qr_api4 ) <EOL> print ( qr_api2 ) <EOL> print ( qr_api1 ) <EOL> qr = qrcode . QRCode ( border = <NUM_LIT> ) <EOL> qr . add_data ( url ) <EOL> qr . make ( fit = True ) <EOL> qr . print_ascii ( invert = True ) <EOL> itchat . instance . receivingRetryCount = <NUM_LIT> <EOL> itchat . auto_login ( enableCmdQR = <NUM_LIT> , hotReload = False , qrCallback = qrCallback ) <EOL> user_id = itchat . instance . storageClass . userName <EOL> name = itchat . instance . storageClass . nickName <EOL> logger", "gt": ". info ( \"<STR_LIT>\" . format ( user_id , name ) )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue . append ( conversation ) <EOL> user_session [ user_id ] = queue <EOL> Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) <EOL> @ staticmethod <EOL> def discard_exceed_conversation ( session , max_tokens ) : <EOL> count = <NUM_LIT> <EOL> count_list = list ( ) <EOL> for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> history_conv = session [ i ] <EOL> count += len ( history_conv [ \"<STR_LIT>\" ] ) + len ( history_conv [ \"<STR_LIT>\" ] ) <EOL> count_list", "gt": ". append ( count )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session", "gt": ". append ( conversation )", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> from bot . bot import Bot <EOL> from config import conf <EOL> from claude_api import Client <EOL> from common . log import logger <EOL> from bot . claude . idStore import idStore <EOL> class ClaudeAiBot ( Bot ) : <EOL> def reply ( self , query , context = None ) : <EOL> storage = idStore ( ) <EOL> Conversation_id = storage . get_id ( ) <EOL> if Conversation_id is not None : <EOL> return self . send_message ( query , Conversation_id ) <EOL> else : <EOL> result = self . create_chat ( query , Conversation_id ) <EOL> storage . set_id ( result [ '<STR_LIT>' ] ) <EOL> return result [ '<STR_LIT>' ] <EOL> def create_chat ( self , msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> self . cookie = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . isproxy = conf ( ) . get ( '<STR_LIT>' ) <EOL> client = Client ( self . cookie , self . isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata", "gt": "= { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer }", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result = create_chat ( msg ) <EOL> Conversation_id = result [ '<STR_LIT>' ] <EOL> return result [ '<STR_LIT>' ] <EOL> def", "gt": "get_group_info ( ) :", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue . append ( conversation ) <EOL> user_session [ user_id ] = queue <EOL> Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) <EOL> @ staticmethod <EOL> def discard_exceed_conversation ( session , max_tokens ) : <EOL> count = <NUM_LIT> <EOL> count_list = list ( ) <EOL> for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> history_conv", "gt": "= session [ i ]", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy", "gt": "= get_proxy ( )", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> from bot . bot import Bot <EOL> from config import conf <EOL> from claude_api import Client <EOL> from common . log import logger <EOL> from bot . claude . idStore import idStore <EOL> class ClaudeAiBot ( Bot ) : <EOL> def reply ( self , query , context = None ) : <EOL> storage = idStore ( ) <EOL> Conversation_id = storage . get_id ( ) <EOL> if Conversation_id is not None : <EOL> return self . send_message ( query , Conversation_id ) <EOL> else : <EOL> result = self . create_chat ( query , Conversation_id ) <EOL> storage . set_id ( result [ '<STR_LIT>' ] ) <EOL> return result [ '<STR_LIT>' ] <EOL> def create_chat ( self , msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> self . cookie = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . isproxy = conf ( ) . get ( '<STR_LIT>' ) <EOL> client = Client ( self . cookie , self . isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger", "gt": ". info ( \"<STR_LIT>\" . format ( answer ) )", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result = create_chat ( msg ) <EOL> Conversation_id = result [ '<STR_LIT>' ] <EOL> return result [ '<STR_LIT>' ] <EOL> def get_group_info ( ) : <EOL> cookie = get_cookie ( ) <EOL> def group_id ( name ) : <EOL> df = itchat . search_chatrooms ( name = name ) <EOL> return df [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> @ itchat . msg_register ( [ TEXT , MAP , CARD , NOTE , SHARING ] ) <EOL> def text_reply ( msg ) : <EOL> itchat . send ( '<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( [ PICTURE , RECORDING , ATTACHMENT , VIDEO ] ) <EOL> def download_files ( msg ) : <EOL> msg [ '<STR_LIT>' ] ( msg [ '<STR_LIT>' ] ) <EOL> return '<STR_LIT>' % ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } . get ( msg [ '<STR_LIT>' ] , '<STR_LIT>' ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def group_text_reply ( msg ) : <EOL> item = group_id ( u'<STR_LIT>' ) <EOL> if msg [ '<STR_LIT>' ] == item : <EOL> itchat . send ( u'<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , item ) <EOL> def qrCallback ( uuid , status , qrcode ) : <EOL> if status == \"<STR_LIT>\" : <EOL> try : <EOL> from PIL import Image <EOL> img = Image . open ( io . BytesIO ( qrcode ) ) <EOL> _thread = threading . Thread ( target = img . show , args = ( \"<STR_LIT>\" , ) ) <EOL> _thread . setDaemon ( True ) <EOL> _thread . start ( ) <EOL> except Exception as e : <EOL> pass <EOL> import qrcode <EOL> url = f\"<STR_LIT>\" <EOL> qr_api1 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api2 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api3 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api4 = \"<STR_LIT>\" . format ( url ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( qr_api3 ) <EOL> print ( qr_api4 ) <EOL> print ( qr_api2 ) <EOL> print ( qr_api1 ) <EOL> qr = qrcode . QRCode ( border = <NUM_LIT> ) <EOL> qr . add_data ( url ) <EOL> qr . make ( fit = True ) <EOL> qr . print_ascii ( invert = True ) <EOL> itchat . instance . receivingRetryCount = <NUM_LIT> <EOL> itchat . auto_login ( enableCmdQR = <NUM_LIT> , hotReload = False , qrCallback = qrCallback ) <EOL> user_id", "gt": "= itchat . instance . storageClass . userName", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> session = user_session . get ( user_id , [ ] ) <EOL> if len ( session ) == <NUM_LIT> : <EOL> system_prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } <EOL> session . append ( system_item ) <EOL> user_session [ user_id ] = session <EOL> user_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : query } <EOL> session . append ( user_item ) <EOL> return session <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> session = user_session . get ( user_id ) <EOL> if session : <EOL> gpt_item", "gt": "= { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : answer }", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result = create_chat ( msg ) <EOL> Conversation_id = result [ '<STR_LIT>' ] <EOL> return result [ '<STR_LIT>' ] <EOL> def get_group_info ( ) : <EOL> cookie = get_cookie ( ) <EOL> def group_id ( name ) : <EOL> df = itchat . search_chatrooms ( name = name ) <EOL> return df [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> @ itchat . msg_register ( [ TEXT , MAP , CARD , NOTE , SHARING ] ) <EOL> def text_reply ( msg ) : <EOL> itchat . send ( '<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( [ PICTURE , RECORDING , ATTACHMENT , VIDEO ] ) <EOL> def download_files ( msg ) : <EOL> msg [ '<STR_LIT>' ] ( msg [ '<STR_LIT>' ] ) <EOL> return '<STR_LIT>' % ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } . get ( msg [ '<STR_LIT>' ] , '<STR_LIT>' ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def group_text_reply ( msg ) : <EOL> item = group_id ( u'<STR_LIT>' ) <EOL> if msg [ '<STR_LIT>' ] == item : <EOL> itchat . send ( u'<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , item ) <EOL> def qrCallback ( uuid , status , qrcode ) : <EOL> if status == \"<STR_LIT>\" : <EOL> try : <EOL> from PIL import Image <EOL> img = Image . open ( io . BytesIO ( qrcode ) ) <EOL> _thread = threading . Thread ( target = img . show , args = ( \"<STR_LIT>\" , ) ) <EOL> _thread . setDaemon ( True ) <EOL> _thread . start ( ) <EOL> except Exception as e : <EOL> pass <EOL> import qrcode <EOL> url = f\"<STR_LIT>\" <EOL> qr_api1 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api2 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api3 = \"<STR_LIT>\" . format ( url ) <EOL> qr_api4 = \"<STR_LIT>\" . format ( url ) <EOL> print ( \"<STR_LIT>\" ) <EOL> print ( qr_api3 ) <EOL> print ( qr_api4 ) <EOL> print ( qr_api2 ) <EOL> print ( qr_api1 ) <EOL> qr = qrcode . QRCode ( border = <NUM_LIT> ) <EOL> qr . add_data ( url ) <EOL> qr", "gt": ". make ( fit = True )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue . append ( conversation ) <EOL> user_session [ user_id ] = queue <EOL> Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) <EOL> @ staticmethod <EOL> def discard_exceed_conversation ( session , max_tokens ) : <EOL> count = <NUM_LIT> <EOL> count_list = list ( ) <EOL> for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> history_conv = session [ i ] <EOL> count += len ( history_conv [ \"<STR_LIT>\" ] ) + len ( history_conv [ \"<STR_LIT>\" ] ) <EOL> count_list . append ( count ) <EOL> for c in count_list : <EOL> if c > max_tokens : <EOL> session . pop ( <NUM_LIT> ) <EOL> @ staticmethod <EOL> def clear_session ( user_id ) : <EOL> user_session", "gt": "[ user_id ] = [ ]", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result = create_chat ( msg ) <EOL> Conversation_id = result [ '<STR_LIT>' ] <EOL> return result [ '<STR_LIT>' ] <EOL> def get_group_info ( ) : <EOL> cookie = get_cookie ( ) <EOL> def group_id ( name ) : <EOL> df = itchat . search_chatrooms ( name = name ) <EOL> return df [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> @ itchat . msg_register ( [ TEXT , MAP , CARD , NOTE , SHARING ] ) <EOL> def text_reply ( msg ) : <EOL> itchat . send ( '<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( [ PICTURE , RECORDING , ATTACHMENT , VIDEO ] ) <EOL> def download_files ( msg ) : <EOL> msg [ '<STR_LIT>' ] ( msg [ '<STR_LIT>' ] ) <EOL> return '<STR_LIT>' % ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } . get ( msg [ '<STR_LIT>' ] , '<STR_LIT>' ) , msg [ '<STR_LIT>' ] ) <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def group_text_reply ( msg ) : <EOL> item = group_id ( u'<STR_LIT>' ) <EOL> if msg [ '<STR_LIT>' ] == item : <EOL> itchat . send ( u'<STR_LIT>' % send_message_judge ( msg [ '<STR_LIT>' ] ) , item ) <EOL> def qrCallback ( uuid , status , qrcode ) : <EOL> if status == \"<STR_LIT>\" : <EOL> try : <EOL> from PIL import Image <EOL> img = Image . open ( io . BytesIO ( qrcode ) ) <EOL> _thread", "gt": "= threading . Thread ( target = img . show , args = ( \"<STR_LIT>\" , ) )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if prompt : <EOL> prompt += \"<STR_LIT>\" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" <EOL> prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> return prompt <EOL> else : <EOL> return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = conf ( ) . get ( \"<STR_LIT>\" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ \"<STR_LIT>\" ] = query <EOL> conversation [ \"<STR_LIT>\" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> logger . debug ( conversation ) <EOL> logger . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue", "gt": ". append ( conversation )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def", "gt": "build_session_query ( query , user_id ) :", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> session = user_session . get ( user_id , [ ] ) <EOL> if len ( session ) == <NUM_LIT> : <EOL> system_prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } <EOL> session", "gt": ". append ( system_item )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except", "gt": "openai . error . RateLimitError as e :", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger", "gt": ". info ( \"<STR_LIT>\" . format ( answer ) )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> elif query == '<STR_LIT>' : <EOL> Session . clear_all_session ( ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> prompt = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> stop = [ \"<STR_LIT>\" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> logger . info ( \"<STR_LIT>\" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger", "gt": ". info ( \"<STR_LIT>\" . format ( image_url ) )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except", "gt": "openai . error . RateLimitError as e :", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> session = user_session . get ( user_id , [ ] ) <EOL> if len ( session ) == <NUM_LIT> : <EOL> system_prompt = conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } <EOL> session . append ( system_item ) <EOL> user_session [ user_id ] = session <EOL> user_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : query } <EOL> session . append ( user_item ) <EOL> return session <EOL> @ staticmethod <EOL> def", "gt": "save_session ( query , answer , user_id ) :", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> import json <EOL> import threading <EOL> import io <EOL> from lib import itchat <EOL> from lib . itchat . content import * <EOL> from common . log import logger <EOL> from common . utils import * <EOL> from claude_api import Client <EOL> apiurl = \"<STR_LIT>\" <EOL> def get_chat ( conversation_id ) : <EOL> url = apiurl + \"<STR_LIT>\" + conversation_id <EOL> payload = \"<STR_LIT>\" <EOL> headers = { } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendMessage ( conversation_id , msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : conversation_id , <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def createMessage ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = json . dumps ( { <EOL> \"<STR_LIT>\" : msg <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def sendattachment ( msg ) : <EOL> url = apiurl + \"<STR_LIT>\" <EOL> payload = { } <EOL> files = [ <EOL> ( '<STR_LIT>' , ( '<STR_LIT>' , open ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) ) <EOL> ] <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = requests . request ( \"<STR_LIT>\" , url , headers = headers , data = payload , files = files ) <EOL> print ( response . text ) <EOL> data = json . loads ( response . text ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> return answer <EOL> def get_chat_history ( msg ) : <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation_id = \"<STR_LIT>\" <EOL> data = client . chat_conversation_history ( conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( data ) ) <EOL> answer = data [ '<STR_LIT>' ] <EOL> texts = [ ] <EOL> for message in answer : <EOL> texts . append ( message [ '<STR_LIT>' ] ) <EOL> logger . info ( \"<STR_LIT>\" . format ( texts ) ) <EOL> return texts <EOL> def create_chat ( msg ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> cookie = get_cookie ( ) <EOL> isproxy = get_proxy ( ) <EOL> client = Client ( cookie , isproxy ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> return answer <EOL> Conversation_id = \"<STR_LIT>\" <EOL> def send_message_judge ( msg ) : <EOL> global Conversation_id <EOL> if Conversation_id != \"<STR_LIT>\" : <EOL> return send_message ( msg , Conversation_id ) <EOL> else : <EOL> result", "gt": "= create_chat ( msg )", "repo": "Claude2-PyAPI"}
{"input": "from bot . bot import Bot <EOL> from config import conf <EOL> from common . log import logger <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTBot ( Bot ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> if query == '<STR_LIT>' : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> logger . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = \"<STR_LIT>\" , <EOL> messages = query , <EOL> temperature = <NUM_LIT> , <EOL> max_tokens = <NUM_LIT> , <EOL> top_p = <NUM_LIT> , <EOL> frequency_penalty = <NUM_LIT> , <EOL> presence_penalty = <NUM_LIT> , <EOL> ) <EOL> logger . info ( response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return \"<STR_LIT>\" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> logger . info ( \"<STR_LIT>\" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = \"<STR_LIT>\" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> logger . info ( \"<STR_LIT>\" . format ( image_url ) ) <EOL> return image_url <EOL> except openai . error . RateLimitError as e : <EOL> logger . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return \"<STR_LIT>\" <EOL> except", "gt": "Exception as e :", "repo": "Claude2-PyAPI"}
{"input": "import requests <EOL> from bot . bot import Bot <EOL> from config import conf <EOL> from claude_api import Client <EOL> from common . log import logger <EOL> from bot . claude . idStore import idStore <EOL> class ClaudeAiBot ( Bot ) : <EOL> def reply ( self , query , context = None ) : <EOL> storage = idStore ( ) <EOL> Conversation_id = storage . get_id ( ) <EOL> if Conversation_id is not None : <EOL> return self . send_message ( query , Conversation_id ) <EOL> else : <EOL> result = self . create_chat ( query , Conversation_id ) <EOL> storage . set_id ( result [ '<STR_LIT>' ] ) <EOL> return result [ '<STR_LIT>' ] <EOL> def create_chat ( self , msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> self . cookie = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . isproxy = conf ( ) . get ( '<STR_LIT>' ) <EOL> client = Client ( self . cookie , self . isproxy ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> logger . info ( \"<STR_LIT>\" + str ( response ) ) <EOL> answer = response <EOL> logger . info ( \"<STR_LIT>\" . format ( answer ) ) <EOL> resultdata = { '<STR_LIT>' : conversation_id , '<STR_LIT>' : answer } <EOL> return resultdata <EOL> def send_message ( self , msg , conversation_id ) : <EOL> data = { '<STR_LIT>' : msg } <EOL> prompt = data [ '<STR_LIT>' ] <EOL> self . cookie = conf ( ) . get ( '<STR_LIT>' ) <EOL> self", "gt": ". isproxy = conf ( ) . get ( '<STR_LIT>' )", "repo": "Claude2-PyAPI"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> def mastodon_request ( server_url , method , access_token , newer_than = None , limit = None ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> if newer_than : <EOL> results = getattr ( client , method ) ( min_id = newer_than ) <EOL> items = results <EOL> while results : <EOL> results = client . fetch_previous ( results ) <EOL> items += results <EOL> elif limit : <EOL> results = getattr ( client , method ) ( limit = limit ) <EOL> items = results <EOL> while", "gt": "results and len ( items ) < limit :", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>'", "gt": ": notification [ '<STR_LIT>' ] ,", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\"", "gt": ": ( '<STR_LIT>' , '<STR_LIT>' ) ,", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def", "gt": "sanitize_content ( html , truncate = True ) :", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> def mastodon_request ( server_url , method , access_token , newer_than = None , limit = None ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> if newer_than : <EOL> results = getattr ( client , method ) ( min_id = newer_than ) <EOL> items = results <EOL> while results : <EOL> results = client . fetch_previous ( results ) <EOL> items += results <EOL> elif limit : <EOL> results = getattr ( client , method ) ( limit = limit ) <EOL> items = results <EOL> while results and len ( items ) < limit : <EOL> results = client . fetch_next ( results ) <EOL> items += results <EOL> else : <EOL> raise", "gt": "ValueError ( \"<STR_LIT>\" )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> def mastodon_request ( server_url , method , access_token , newer_than = None , limit = None ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> if newer_than : <EOL> results = getattr ( client , method ) ( min_id = newer_than ) <EOL> items = results <EOL> while results : <EOL> results", "gt": "= client . fetch_previous ( results )", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def sanitize_content ( html , truncate = True ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if len ( html ) > <NUM_LIT> and truncate : <EOL> html = html [ : <NUM_LIT> ] + '<STR_LIT>' <EOL> soup", "gt": "= BeautifulSoup ( html , '<STR_LIT>' )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>'", "gt": ": notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] ,", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size = round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> ) <EOL> app . config [ '<STR_LIT>' ] = { '<STR_LIT>' : pool_size } <EOL> models . init_db ( app ) <EOL> return app <EOL> def load_config ( app ) : <EOL> app . logger . setLevel ( logging . INFO ) <EOL> env", "gt": "= os . getenv ( '<STR_LIT>' )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>'", "gt": ": display_name ( notification ) ,", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\"", "gt": ": ( '<STR_LIT>' , \"<STR_LIT>\" ) ,", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def sanitize_content ( html , truncate = True ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if len ( html ) > <NUM_LIT> and truncate : <EOL> html = html [ : <NUM_LIT> ] + '<STR_LIT>' <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if soup . html : <EOL> if soup . html . body : <EOL> soup . html . body . unwrap ( ) <EOL> soup . html . unwrap ( ) <EOL> for a in soup . find_all ( '<STR_LIT>' , href = True ) : <EOL> read_url = flask . url_for ( \"<STR_LIT>\" , url = a [ \"<STR_LIT>\" ] , redirect = <NUM_LIT> ) <EOL> a", "gt": "[ '<STR_LIT>' ] =", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def sanitize_content ( html , truncate = True ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if len ( html ) > <NUM_LIT> and truncate : <EOL> html = html [ : <NUM_LIT> ] + '<STR_LIT>' <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if soup . html : <EOL> if soup . html . body : <EOL> soup . html . body . unwrap ( ) <EOL> soup . html . unwrap ( ) <EOL> for a in soup . find_all ( '<STR_LIT>' , href = True ) : <EOL> read_url = flask . url_for ( \"<STR_LIT>\" , url = a [ \"<STR_LIT>\" ] , redirect = <NUM_LIT> ) <EOL> a [ '<STR_LIT>' ] = <EOL> return str ( soup ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def entry_excerpt ( entry ) : <EOL> if not entry . content_short : <EOL> return '<STR_LIT>' <EOL> if entry . content_url and entry . title : <EOL> title = entry . title <EOL> elif entry . has_distinct_user : <EOL> title = entry . display_name or entry . username <EOL> else : <EOL> title = entry . feed . name <EOL> body_text = BeautifulSoup ( entry . content_short , '<STR_LIT>' ) . text <EOL> max_length = <NUM_LIT> <EOL> max_body_length = max ( <NUM_LIT> , max_length - len ( title ) ) <EOL> if", "gt": "len ( body_text ) > max_body_length :", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size = round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> ) <EOL> app . config [ '<STR_LIT>' ] = { '<STR_LIT>' : pool_size } <EOL> models . init_db ( app ) <EOL> return app <EOL> def load_config ( app ) : <EOL> app . logger . setLevel ( logging . INFO ) <EOL> env = os . getenv ( '<STR_LIT>' ) <EOL> if not env : <EOL> app . logger . error ( '<STR_LIT>' ) <EOL> exit ( ) <EOL> app", "gt": ". config . from_object ( '<STR_LIT>' )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if", "gt": "notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES :", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size = round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> ) <EOL> app", "gt": ". config [ '<STR_LIT>' ] = { '<STR_LIT>' : pool_size }", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications", "gt": "= mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit )", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size = round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> ) <EOL> app . config [ '<STR_LIT>' ] = { '<STR_LIT>' : pool_size } <EOL> models", "gt": ". init_db ( app )", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size = round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> ) <EOL> app . config [ '<STR_LIT>' ] = { '<STR_LIT>' : pool_size } <EOL> models . init_db ( app ) <EOL> return app <EOL> def", "gt": "load_config ( app ) :", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def sanitize_content ( html , truncate = True ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if len ( html ) > <NUM_LIT> and truncate : <EOL> html = html [ : <NUM_LIT> ] + '<STR_LIT>' <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if soup . html : <EOL> if soup . html . body : <EOL> soup . html . body . unwrap ( ) <EOL> soup . html . unwrap ( ) <EOL> for a in soup . find_all ( '<STR_LIT>' , href = True ) : <EOL> read_url = flask . url_for ( \"<STR_LIT>\" , url = a [ \"<STR_LIT>\" ] , redirect = <NUM_LIT> ) <EOL> a [ '<STR_LIT>' ] = <EOL> return", "gt": "str ( soup )", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if", "gt": "feed . name == selected_name :", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app", "gt": "= flask . Flask ( '<STR_LIT>' )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries", "gt": ". append ( entry )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return", "gt": "status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ]", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> def mastodon_request ( server_url , method , access_token , newer_than = None , limit = None ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> if newer_than : <EOL> results", "gt": "= getattr ( client , method ) ( min_id = newer_than )", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def sanitize_content ( html , truncate = True ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if len ( html ) > <NUM_LIT> and truncate : <EOL> html = html [ : <NUM_LIT> ] + '<STR_LIT>' <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> if soup . html : <EOL> if soup . html . body : <EOL> soup", "gt": ". html . body . unwrap ( )", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> } <EOL> if notification [ '<STR_LIT>' ] not in NOTIFICATION_PHRASES : <EOL> continue <EOL> ( icon , phrase ) = NOTIFICATION_PHRASES [ notification [ \"<STR_LIT>\" ] ] <EOL> body = f'<STR_LIT>' <EOL> entry = { <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : json . dumps ( notification , default = str ) , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : notification [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : display_name ( notification ) , <EOL> '<STR_LIT>' : body } <EOL> if notification [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = user_url ( server_url , notification ) <EOL> else : <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , notification [ '<STR_LIT>' ] ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> entries . append ( entry ) <EOL> return entries <EOL> def user_url ( server_url , status_dict ) : <EOL> return f'<STR_LIT>' <EOL> def status_url ( server_url , status_dict ) : <EOL> \"<STR_LIT>\" <EOL> return f'<STR_LIT>' <EOL> def display_name ( status_dict ) : <EOL> return status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] or status_dict [ '<STR_LIT>' ] [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> def mastodon_request ( server_url , method , access_token , newer_than = None , limit = None ) : <EOL> client", "gt": "= mastodon . Mastodon ( access_token = access_token ,", "repo": "feedi"}
{"input": "import json <EOL> import logging <EOL> import mastodon <EOL> logger = logging . getLogger ( __name__ ) <EOL> CLIENT_NAME = '<STR_LIT>' <EOL> SCOPES = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def register_app ( server_url , callback_url ) : <EOL> return mastodon . Mastodon . create_app ( CLIENT_NAME , api_base_url = server_url , <EOL> redirect_uris = [ callback_url ] , <EOL> scopes = SCOPES ) <EOL> def auth_redirect_url ( server_url , client_id , client_secret , callback_url ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . auth_request_url ( client_id = client_id , scopes = SCOPES , <EOL> redirect_uris = callback_url ) <EOL> def oauth_login ( server_url , client_id , client_secret , callback_url , code ) : <EOL> client = mastodon . Mastodon ( client_id = client_id , <EOL> client_secret = client_secret , <EOL> api_base_url = server_url ) <EOL> return client . log_in ( code = code , redirect_uri = callback_url , scopes = SCOPES ) <EOL> def fetch_account_data ( server_url , access_token ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> return client . me ( ) <EOL> def boost ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_reblog ( id = toot_id ) <EOL> def favorite ( server_url , access_token , toot_id ) : <EOL> client = mastodon . Mastodon ( access_token = access_token , <EOL> api_base_url = server_url ) <EOL> client . status_favourite ( id = toot_id ) <EOL> def fetch_toots ( server_url , access_token , newer_than = None , limit = None ) : <EOL> toots = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for toot in toots : <EOL> entry = { <EOL> '<STR_LIT>' : json . dumps ( toot , default = str ) <EOL> } <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] or toot [ '<STR_LIT>' ] <EOL> if toot . get ( '<STR_LIT>' ) and not toot . get ( '<STR_LIT>' ) : <EOL> continue <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> reblogged_by = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = f'<STR_LIT>' <EOL> toot = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = display_name ( toot ) <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] <EOL> entry [ '<STR_LIT>' ] = status_url ( server_url , toot ) <EOL> entry [ '<STR_LIT>' ] = entry [ '<STR_LIT>' ] <EOL> media = [ m [ '<STR_LIT>' ] for m in toot [ '<STR_LIT>' ] if m [ '<STR_LIT>' ] == '<STR_LIT>' ] <EOL> if media : <EOL> entry [ '<STR_LIT>' ] = media [ <NUM_LIT> ] <EOL> elif toot [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] = toot [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) <EOL> if toot . get ( '<STR_LIT>' ) : <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> for option in toot [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> entry [ '<STR_LIT>' ] += f'<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] += '<STR_LIT>' <EOL> entries . append ( entry ) <EOL> return entries <EOL> def fetch_notifications ( server_url , access_token , newer_than = None , limit = None ) : <EOL> notifications = mastodon_request ( server_url , '<STR_LIT>' , access_token , newer_than , limit ) <EOL> entries = [ ] <EOL> for notification in notifications : <EOL> NOTIFICATION_PHRASES = { <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" : ( '<STR_LIT>' , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\"", "gt": ": ( '<STR_LIT>' , \"<STR_LIT>\" ) ,", "repo": "feedi"}
{"input": "import datetime <EOL> import urllib <EOL> import flask <EOL> from bs4 import BeautifulSoup <EOL> from flask import current_app as app <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def humanize_date ( dt ) : <EOL> delta = datetime . datetime . utcnow ( ) - dt <EOL> if delta < datetime . timedelta ( seconds = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( hours = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return f\"<STR_LIT>\" <EOL> elif delta < datetime . timedelta ( days = <NUM_LIT> ) : <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> return dt . strftime ( \"<STR_LIT>\" ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def feed_domain ( url ) : <EOL> parts = urllib . parse . urlparse ( url ) <EOL> return parts . netloc . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def should_unfold_folder ( filters , folder_name , folder_feeds ) : <EOL> if filters . get ( '<STR_LIT>' ) == folder_name : <EOL> return True <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> if filters [ '<STR_LIT>' ] in [ f . name for f in folder_feeds ] : <EOL> return True <EOL> return False <EOL> @ app . template_filter ( '<STR_LIT>' ) <EOL> def contains_feed_name ( feed_list , selected_name ) : <EOL> for feed in feed_list : <EOL> if feed . name == selected_name : <EOL> return True <EOL> return False <EOL> @", "gt": "app . template_filter ( '<STR_LIT>' )", "repo": "feedi"}
{"input": "from gevent import monkey <EOL> monkey . patch_all ( ) <EOL> import logging <EOL> import os <EOL> import flask <EOL> from werkzeug . serving import is_running_from_reloader <EOL> import feedi . models as models <EOL> def create_app ( ) : <EOL> app = flask . Flask ( __package__ ) <EOL> load_config ( app ) <EOL> app . logger . info ( '<STR_LIT>' , os . getenv ( '<STR_LIT>' ) ) <EOL> with app . app_context ( ) : <EOL> from . import auth , filters , routes , tasks <EOL> models . init_db ( app ) <EOL> auth . init ( ) <EOL> if not is_running_from_reloader ( ) and not os . environ . get ( '<STR_LIT>' ) : <EOL> app . logger . info ( \"<STR_LIT>\" ) <EOL> tasks . huey . start ( ) <EOL> @ app . teardown_appcontext <EOL> def shutdown_session ( exception = None ) : <EOL> models . db . session . remove ( ) <EOL> return app <EOL> def create_huey_app ( ) : <EOL> app = flask . Flask ( '<STR_LIT>' ) <EOL> load_config ( app ) <EOL> with app . app_context ( ) : <EOL> pool_size", "gt": "= round ( app . config [ '<STR_LIT>' ] * <NUM_LIT> )", "repo": "feedi"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map", "gt": "[ w_id ] . append ( stats )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if", "gt": "self . is_test :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>'", "gt": ": inputs [ '<STR_LIT>' ] ,", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self", "gt": ". processor = get_clip_processor ( ) if processor is None else processor", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> while len ( image_to_data_dict_list ) > <NUM_LIT> and len ( curr_data_list ) > <NUM_LIT> : <EOL> if len ( curr_batch ) == self . bs : <EOL> batches . append ( curr_batch ) <EOL> curr_batch = [ ] <EOL> if len ( curr_batch ) < self . bs : <EOL> curr_batch . append ( curr_data_list . pop ( <NUM_LIT> ) ) <EOL> if len ( curr_data_list ) == <NUM_LIT> and len ( image_to_data_dict_list ) > <NUM_LIT> : <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> return batches <EOL> def __iter__ ( self ) : <EOL> batch_list = self . _generate_batch ( self . dataset ) <EOL> while len ( batch_list ) > <NUM_LIT> : <EOL> yield", "gt": "batch_list . pop ( <NUM_LIT> )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def", "gt": "__init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours", "gt": "= total_duration / <NUM_LIT> / <NUM_LIT>", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def", "gt": "_generate_batch ( self , dataset : DenseCaptionedDataset ) :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay", "gt": "= target_cpuw * total_unique_words", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives", "gt": "= item [ '<STR_LIT>' ] [ : self . caption_bag_size ]", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> while len ( image_to_data_dict_list ) > <NUM_LIT> and len ( curr_data_list ) > <NUM_LIT> : <EOL> if len ( curr_batch ) == self . bs : <EOL> batches . append ( curr_batch ) <EOL> curr_batch = [ ] <EOL> if len ( curr_batch ) < self . bs : <EOL> curr_batch . append ( curr_data_list . pop ( <NUM_LIT> ) ) <EOL> if len ( curr_data_list ) == <NUM_LIT> and len ( image_to_data_dict_list ) > <NUM_LIT> : <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> return batches <EOL> def __iter__ ( self ) : <EOL> batch_list = self . _generate_batch ( self . dataset ) <EOL> while len ( batch_list ) > <NUM_LIT> : <EOL> yield batch_list . pop ( <NUM_LIT> ) <EOL> def", "gt": "__len__ ( self ) :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if", "gt": "self . is_test :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _", "gt": ", curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if", "gt": "item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay = target_cpuw * total_unique_words <EOL> cpuw_bonus = max ( cpuw_target_pay - total_pay , <NUM_LIT> ) <EOL> bonus_pay = min ( deficit_pay , cpuw_bonus ) <EOL> expected_pay_per_worker [ w_id ] = bonus_pay <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> total_deficit += max ( target_pay - total_pay , <NUM_LIT> ) <EOL> expected_pay += bonus_pay <EOL> print ( f\"<STR_LIT>\" ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( worker_to_stats_map , jsonf , indent = <NUM_LIT> ) <EOL> if len ( worker_to_units_map ) == <NUM_LIT> : <EOL> return <EOL> do_bonus = input ( \"<STR_LIT>\" ) <EOL> if not do_bonus . startswith ( '<STR_LIT>' ) : <EOL> return <EOL> for w_id , pay_amount in expected_pay_per_worker . items ( ) : <EOL> if pay_amount == <NUM_LIT> : <EOL> continue <EOL> pay_per_unit = int ( pay_amount / len ( worker_to_stats_map [ w_id ] ) * <NUM_LIT> ) / <NUM_LIT> <EOL> doit = input ( f\"<STR_LIT>\" ) <EOL> if doit . startswith ( '<STR_LIT>' ) : <EOL> continue <EOL> for", "gt": "u in worker_to_units_map [ w_id ] :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> while len ( image_to_data_dict_list ) > <NUM_LIT> and len ( curr_data_list ) > <NUM_LIT> : <EOL> if len ( curr_batch ) == self . bs : <EOL> batches . append ( curr_batch ) <EOL> curr_batch = [ ] <EOL> if len ( curr_batch ) < self . bs : <EOL> curr_batch . append ( curr_data_list . pop ( <NUM_LIT> ) ) <EOL> if len ( curr_data_list ) == <NUM_LIT> and len ( image_to_data_dict_list ) > <NUM_LIT> : <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> return batches <EOL> def __iter__ ( self ) : <EOL> batch_list = self . _generate_batch ( self . dataset ) <EOL> while len ( batch_list ) > <NUM_LIT> : <EOL> yield batch_list . pop ( <NUM_LIT> ) <EOL> def __len__ ( self ) : <EOL> return self . _max_len <EOL> def get_clip_ready_ds ( <EOL> split", "gt": ": str = '<STR_LIT>' ,", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def", "gt": "get_data_iterator ( source , cap = <NUM_LIT> ) :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> while len ( image_to_data_dict_list ) > <NUM_LIT> and len ( curr_data_list ) > <NUM_LIT> : <EOL> if len ( curr_batch ) == self . bs : <EOL> batches . append ( curr_batch ) <EOL> curr_batch = [ ] <EOL> if len ( curr_batch ) < self . bs : <EOL> curr_batch", "gt": ". append ( curr_data_list . pop ( <NUM_LIT> ) )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def", "gt": "gen_dci ( ) :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay = target_cpuw * total_unique_words <EOL> cpuw_bonus = max ( cpuw_target_pay - total_pay , <NUM_LIT> ) <EOL> bonus_pay = min ( deficit_pay , cpuw_bonus ) <EOL> expected_pay_per_worker [ w_id ] = bonus_pay <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> total_deficit += max ( target_pay - total_pay , <NUM_LIT> ) <EOL> expected_pay += bonus_pay <EOL> print ( f\"<STR_LIT>\" ) <EOL> with", "gt": "open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf :", "repo": "DCI"}
{"input": "import os <EOL> import json <EOL> def print_all_elevater_scores ( ) : <EOL> LOG_DIR = input ( \"<STR_LIT>\" ) <EOL> elevater_dir = LOG_DIR + '<STR_LIT>' <EOL> MODELS = os . listdir ( os . path . join ( LOG_DIR , '<STR_LIT>' ) ) <EOL> for model_dir in MODELS : <EOL> print ( model_dir ) <EOL> full_model = elevater_dir . format ( model_dir ) <EOL> for shot_dir in os . listdir ( full_model ) : <EOL> results = [ ] <EOL> full_shot_dir", "gt": "= os . path . join ( full_model , shot_dir )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random . shuffle ( data_id_array ) <EOL> image_to_data_dict_list = list ( image_to_data_dict . items ( ) ) <EOL> random . shuffle ( image_to_data_dict_list ) <EOL> batches = [ ] <EOL> curr_batch = [ ] <EOL> _ , curr_data_list = image_to_data_dict_list . pop ( <NUM_LIT> ) <EOL> while", "gt": "len ( image_to_data_dict_list ) > <NUM_LIT> and len ( curr_data_list ) > <NUM_LIT> :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay = target_cpuw * total_unique_words <EOL> cpuw_bonus = max ( cpuw_target_pay - total_pay , <NUM_LIT> ) <EOL> bonus_pay = min ( deficit_pay , cpuw_bonus ) <EOL> expected_pay_per_worker [ w_id ] = bonus_pay <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> total_deficit += max ( target_pay - total_pay , <NUM_LIT> ) <EOL> expected_pay += bonus_pay <EOL> print ( f\"<STR_LIT>\" ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( worker_to_stats_map , jsonf , indent = <NUM_LIT> ) <EOL> if len ( worker_to_units_map ) == <NUM_LIT> : <EOL> return <EOL> do_bonus = input ( \"<STR_LIT>\" ) <EOL> if not do_bonus . startswith ( '<STR_LIT>' ) : <EOL> return <EOL> for w_id , pay_amount in expected_pay_per_worker . items ( ) : <EOL> if pay_amount == <NUM_LIT> : <EOL> continue <EOL> pay_per_unit = int ( pay_amount / len ( worker_to_stats_map [ w_id ] ) * <NUM_LIT> ) / <NUM_LIT> <EOL> doit = input ( f\"<STR_LIT>\" ) <EOL> if doit . startswith ( '<STR_LIT>' ) : <EOL> continue <EOL> for u in worker_to_units_map [ w_id ] : <EOL> try : <EOL> u . get_assigned_agent ( ) . get_worker ( ) . bonus_worker ( <EOL> pay_per_unit , <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> \"<STR_LIT>\" , <EOL> unit = u , <EOL> ) <EOL> except Exception as e : <EOL> print ( f\"<STR_LIT>\" ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as cutoff_f : <EOL> cutoff_f . write ( str ( last_time ) ) <EOL> if", "gt": "__name__ == '<STR_LIT>' :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay", "gt": "= TARGET_PAY * total_hours", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if", "gt": "w_id not in worker_to_stats_map :", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict [ image_idx ] . append ( data_id ) <EOL> for data_id_array in image_to_data_dict . values ( ) : <EOL> random", "gt": ". shuffle ( data_id_array )", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay = target_cpuw * total_unique_words <EOL> cpuw_bonus = max ( cpuw_target_pay - total_pay , <NUM_LIT> ) <EOL> bonus_pay = min ( deficit_pay , cpuw_bonus ) <EOL> expected_pay_per_worker [ w_id ] = bonus_pay <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> total_deficit += max ( target_pay - total_pay , <NUM_LIT> ) <EOL> expected_pay += bonus_pay <EOL> print ( f\"<STR_LIT>\" ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( worker_to_stats_map , jsonf , indent = <NUM_LIT> ) <EOL> if len ( worker_to_units_map ) == <NUM_LIT> : <EOL> return <EOL> do_bonus = input ( \"<STR_LIT>\" ) <EOL> if not do_bonus . startswith ( '<STR_LIT>' ) : <EOL> return <EOL> for w_id , pay_amount in expected_pay_per_worker . items ( ) : <EOL> if pay_amount == <NUM_LIT> : <EOL> continue <EOL> pay_per_unit = int ( pay_amount / len ( worker_to_stats_map [ w_id ] ) * <NUM_LIT> ) / <NUM_LIT> <EOL> doit = input ( f\"<STR_LIT>\" ) <EOL> if", "gt": "doit . startswith ( '<STR_LIT>' ) :", "repo": "DCI"}
{"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import json <EOL> from mephisto . data_model . unit import Unit <EOL> from typing import Dict , List <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> CUTOFF_TIME = <NUM_LIT> <EOL> PAY_AMOUNT_PER_TASK = - <NUM_LIT> <EOL> TARGET_PAY = - <NUM_LIT> <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' ) as time_f : <EOL> CUTOFF_TIME = float ( time_f . read ( ) ) <EOL> def get_all_words ( short_caption , extra_caption , mask_data ) : <EOL> caption = short_caption . strip ( ) <EOL> caption += \"<STR_LIT>\" + extra_caption . strip ( ) <EOL> for entry in mask_data . values ( ) : <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> if len ( entry [ '<STR_LIT>' ] . strip ( ) ) > <NUM_LIT> : <EOL> caption += \"<STR_LIT>\" + entry [ '<STR_LIT>' ] . strip ( ) <EOL> caption = caption . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return caption <EOL> def extract_stats_data ( m_data , duration ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> words = get_all_words ( outputs [ '<STR_LIT>' ] , outputs [ '<STR_LIT>' ] , reconstructed_masks ) <EOL> words = \"<STR_LIT>\" . join ( [ w for w in words . split ( ) if len ( w ) > <NUM_LIT> ] ) <EOL> unique_words = set ( words . lower ( ) . split ( ) ) <EOL> return { <EOL> '<STR_LIT>' : len ( [ r for r in reconstructed_masks . values ( ) if len ( r [ '<STR_LIT>' ] ) ] ) , <EOL> '<STR_LIT>' : len ( words . split ( ) ) , <EOL> '<STR_LIT>' : len ( unique_words ) , <EOL> '<STR_LIT>' : duration , <EOL> } <EOL> def main ( ) : <EOL> last_time = <NUM_LIT> <EOL> worker_to_units_map : Dict [ str , List [ Unit ] ] = { } <EOL> if os . path . exists ( '<STR_LIT>' ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> worker_to_stats_map = json . load ( jsonf ) <EOL> else : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( \"<STR_LIT>\" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f\"<STR_LIT>\" , flush = True ) <EOL> approved_units += units <EOL> print ( f\"<STR_LIT>\" ) <EOL> worker_to_stats_map = { } <EOL> worker_to_units_map = { } <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> agent = u . get_assigned_agent ( ) <EOL> w_id = agent . get_worker ( ) . worker_name <EOL> data = agent . state . get_data ( ) <EOL> task_start = agent . state . get_task_start ( ) <EOL> if task_start is not None and task_start <= CUTOFF_TIME : <EOL> continue <EOL> last_time = max ( task_start , last_time ) <EOL> duration = <NUM_LIT> <EOL> if agent . state . get_task_end ( ) is not None and agent . state . get_task_start ( ) is not None : <EOL> duration = agent . state . get_task_end ( ) - agent . state . get_task_start ( ) <EOL> stats = extract_stats_data ( data , duration ) <EOL> if w_id not in worker_to_stats_map : <EOL> worker_to_stats_map [ w_id ] = [ ] <EOL> worker_to_units_map [ w_id ] = [ ] <EOL> worker_to_stats_map [ w_id ] . append ( stats ) <EOL> worker_to_units_map [ w_id ] . append ( u ) <EOL> except OSError : <EOL> print ( f\"<STR_LIT>\" ) <EOL> continue <EOL> total_deficit = <NUM_LIT> <EOL> expected_pay = <NUM_LIT> <EOL> expected_pay_per_worker = { w_id : <NUM_LIT> for w_id in worker_to_stats_map . keys ( ) } <EOL> print ( f\"<STR_LIT>\" ) <EOL> for w_id , stats in sorted ( list ( worker_to_stats_map . items ( ) ) , key = lambda x : len ( x [ <NUM_LIT> ] ) , reverse = True ) : <EOL> total_work = len ( stats ) <EOL> total_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_unique_words = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_duration = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_masks = sum ( [ s [ '<STR_LIT>' ] for s in stats ] ) <EOL> total_pay = total_work * PAY_AMOUNT_PER_TASK <EOL> total_hours = total_duration / <NUM_LIT> / <NUM_LIT> <EOL> target_pay = TARGET_PAY * total_hours <EOL> deficit_pay = max ( target_pay - total_pay , <NUM_LIT> ) <EOL> target_cpuw = <NUM_LIT> <EOL> cpuw_target_pay = target_cpuw * total_unique_words <EOL> cpuw_bonus = max ( cpuw_target_pay - total_pay , <NUM_LIT> ) <EOL> bonus_pay = min ( deficit_pay , cpuw_bonus ) <EOL> expected_pay_per_worker [ w_id ] = bonus_pay <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> f\"<STR_LIT>\" <EOL> \"<STR_LIT>\" <EOL> ) <EOL> total_deficit += max ( target_pay - total_pay , <NUM_LIT> ) <EOL> expected_pay += bonus_pay <EOL> print ( f\"<STR_LIT>\" ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( worker_to_stats_map , jsonf , indent = <NUM_LIT> ) <EOL> if len ( worker_to_units_map ) == <NUM_LIT> : <EOL> return <EOL> do_bonus = input ( \"<STR_LIT>\" ) <EOL> if not do_bonus . startswith ( '<STR_LIT>' ) : <EOL> return <EOL> for w_id , pay_amount in expected_pay_per_worker . items ( ) : <EOL> if pay_amount == <NUM_LIT> : <EOL> continue <EOL> pay_per_unit = int ( pay_amount / len ( worker_to_stats_map [ w_id ] ) * <NUM_LIT> ) / <NUM_LIT> <EOL> doit = input ( f\"<STR_LIT>\" ) <EOL> if doit . startswith ( '<STR_LIT>' ) : <EOL> continue <EOL> for u in worker_to_units_map [ w_id ] : <EOL> try : <EOL> u", "gt": ". get_assigned_agent ( ) . get_worker ( ) . bonus_worker (", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> class DenseCaptionBatchSampler ( torch . utils . data . sampler . BatchSampler ) : <EOL> def __init__ ( self , dataset : DenseCaptionedDataset , batch_size : int ) : <EOL> self . dataset = dataset <EOL> self . bs = batch_size <EOL> self . _max_len = len ( dataset ) // batch_size <EOL> def _generate_batch ( self , dataset : DenseCaptionedDataset ) : <EOL> all_data = dataset . data_list <EOL> image_to_data_dict = defaultdict ( lambda : [ ] ) <EOL> for data_id , ( image_idx , _ ) in enumerate ( all_data ) : <EOL> image_to_data_dict", "gt": "[ image_idx ] . append ( data_id )", "repo": "DCI"}
{"input": "from tqdm import tqdm <EOL> import random <EOL> from collections import defaultdict <EOL> import torch <EOL> import os <EOL> import json <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . dataset . dense_image import ( <EOL> DenseCaptionedImage , get_dci_count , DCIEntry , NegativeEntry , DCINegatives , DCISummaries <EOL> ) <EOL> from densely_captioned_images . dataset . utils import get_clip_processor <EOL> from densely_captioned_images . dataset . config import DATASET_BASE <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_negative_by_strategy ( <EOL> negatives : NegativeEntry , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> clip_scores : Optional [ Dict [ str , float ] ] = None , <EOL> ) -> str : <EOL> negative_sources = [ ] <EOL> if negative_source != '<STR_LIT>' : <EOL> negative_cands = negatives [ negative_source ] <EOL> negative_sources = [ f\"<STR_LIT>\" for i in range ( len ( negative_cands ) ) ] <EOL> else : <EOL> negative_cands = [ ] <EOL> for neg_type , val in negatives . items ( ) : <EOL> negative_cands += val <EOL> negative_sources += [ f\"<STR_LIT>\" for i in range ( len ( val ) ) ] <EOL> if negative_strategy == '<STR_LIT>' : <EOL> return negative_cands [ <NUM_LIT> ] <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> return random . choice ( negative_cands ) <EOL> elif negative_strategy == '<STR_LIT>' : <EOL> zipped = list ( zip ( negative_cands , negative_sources ) ) <EOL> zipped . sort ( key = lambda x : clip_scores [ x [ <NUM_LIT> ] ] ) <EOL> return zipped [ - <NUM_LIT> ] [ <NUM_LIT> ] <EOL> def get_complete_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> entries_per_image . append ( entries ) <EOL> return entries_per_image <EOL> def get_summarized_dataset_with_settings ( <EOL> split : str = '<STR_LIT>' , <EOL> load_base_image : bool = True , <EOL> load_subcaptions : bool = True , <EOL> negative_source : str = '<STR_LIT>' , <EOL> negative_strategy : str = '<STR_LIT>' , <EOL> count : Optional [ int ] = None , <EOL> ) -> List [ List [ DCIEntry ] ] : <EOL> assert split in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_source in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> assert negative_strategy in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , f\"<STR_LIT>\" <EOL> with open ( os . path . join ( DATASET_BASE , '<STR_LIT>' ) ) as jsonf : <EOL> split_metadata = json . load ( jsonf ) <EOL> sources = split_metadata [ split ] <EOL> entries_per_image = [ ] <EOL> for source_path in tqdm ( sources , desc = \"<STR_LIT>\" ) : <EOL> if count is not None and len ( entries_per_image ) > count : <EOL> break <EOL> try : <EOL> dci = DenseCaptionedImage ( source_path ) <EOL> summaries = dci . get_summaries ( ) <EOL> if summaries is None : <EOL> continue <EOL> negatives = dci . get_negatives ( ) <EOL> if negatives is None or len ( negatives ) == <NUM_LIT> and load_subcaptions : <EOL> continue <EOL> entries = [ ] <EOL> if load_base_image : <EOL> entries += dci . get_formatted_complete_description ( ) <EOL> if load_subcaptions : <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> entries += [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in all_masks ] <EOL> for entry in entries : <EOL> if isinstance ( summaries [ entry [ '<STR_LIT>' ] ] , str ) : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> else : <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] [ <NUM_LIT> ] <EOL> entry [ '<STR_LIT>' ] = summaries [ entry [ '<STR_LIT>' ] ] <EOL> for entry in entries : <EOL> if negative_source == '<STR_LIT>' or negative_source == '<STR_LIT>' : <EOL> use_antonyms = negative_source == '<STR_LIT>' <EOL> entry [ '<STR_LIT>' ] = get_spacy_negative ( entry [ '<STR_LIT>' ] , use_antonyms = use_antonyms ) <EOL> else : <EOL> negs = negatives [ entry [ '<STR_LIT>' ] ] <EOL> entry [ '<STR_LIT>' ] = get_negative_by_strategy ( <EOL> negs , <EOL> negative_source , <EOL> negative_strategy , <EOL> dci . _data [ '<STR_LIT>' ] [ entry [ '<STR_LIT>' ] ] , <EOL> ) <EOL> entries_per_image . append ( entries ) <EOL> except Exception : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> return entries_per_image <EOL> def get_data_iterator ( source , cap = <NUM_LIT> ) : <EOL> def gen_dci ( ) : <EOL> yielded_count = <NUM_LIT> <EOL> for i , dci_entry_list in enumerate ( source ) : <EOL> for entry in dci_entry_list : <EOL> yield ( i , entry ) <EOL> yielded_count += <NUM_LIT> <EOL> if yielded_count == cap : <EOL> return <EOL> return gen_dci <EOL> class DenseCaptionedDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , data_list , caption_bag_size = <NUM_LIT> , processor = None , is_test = False ) : <EOL> self . data_list = data_list <EOL> if caption_bag_size > <NUM_LIT> : <EOL> self . data_list = [ <EOL> ( i , item ) for ( i , item ) in data_list if <EOL> len ( item . get ( '<STR_LIT>' , [ ] ) ) >= caption_bag_size <EOL> ] <EOL> if len ( self . data_list ) < len ( data_list ) : <EOL> print ( f\"<STR_LIT>\" ) <EOL> self . processor = get_clip_processor ( ) if processor is None else processor <EOL> self . caption_bag_size = caption_bag_size <EOL> self . is_test = is_test <EOL> def __getitem__ ( self , idx ) : <EOL> _ , item = self . data_list [ idx ] <EOL> bag_inputs = None <EOL> bag_negatives = None <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_captions = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> if self . is_test : <EOL> use_negatives = item [ '<STR_LIT>' ] [ : self . caption_bag_size ] <EOL> else : <EOL> use_negatives = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_negatives = self . processor ( text = use_negatives , images = [ item [ '<STR_LIT>' ] ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_negatives [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return", "gt": "len ( self . data_list )", "repo": "DCI"}
{"input": "def factorial ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> print ( numero , \"<STR_LIT>\" , ( numero - <NUM_LIT> ) , \"<STR_LIT>\" , numero * ( numero - <NUM_LIT> ) ) <EOL> return numero * factorial ( numero - <NUM_LIT> ) <EOL> print ( factorial ( <NUM_LIT> ) ) <EOL> from functools import reduce <EOL> def factorial_masoquista ( numero ) : <EOL> if", "gt": "numero == <NUM_LIT> or numero == <NUM_LIT> :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def factorial ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> print ( numero , \"<STR_LIT>\" , ( numero - <NUM_LIT> ) , \"<STR_LIT>\" , numero * ( numero - <NUM_LIT> ) ) <EOL> return numero * factorial ( numero - <NUM_LIT> ) <EOL> print ( factorial ( <NUM_LIT> ) ) <EOL> from functools import reduce <EOL> def factorial_masoquista ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> return reduce ( lambda x , y : x * y , range ( <NUM_LIT> , numero + <NUM_LIT> ) ) <EOL> numeros = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> factoriales", "gt": "= list ( map ( factorial_masoquista , numeros ) )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "from pymongo import MongoClient <EOL> client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) <EOL> db = client [ '<STR_LIT>' ] <EOL> collection = db [ '<STR_LIT>' ] <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } <EOL> } <EOL> } ) <EOL> collection . update_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , { \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } ) <EOL> for", "gt": "document in collection . find ( ) :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def fibo3 ( n , temp = { } ) : <EOL> if n <= <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif n == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif n not in temp : <EOL> temp", "gt": "[ n ] = fibo3 ( n - <NUM_LIT> , temp ) + fibo3 ( n - <NUM_LIT> , temp )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "from pymongo import MongoClient <EOL> client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) <EOL> db = client [ '<STR_LIT>' ] <EOL> collection = db [ '<STR_LIT>' ] <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } <EOL> } <EOL> } ) <EOL> collection", "gt": ". update_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , { \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def buscar ( lista , objetivo ) : <EOL> visto = { } <EOL> for num in lista : <EOL> if objetivo - num in visto : <EOL> return", "gt": "[ objetivo - num , num ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "from pymongo import MongoClient <EOL> client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) <EOL> db = client [ '<STR_LIT>' ] <EOL> collection = db [ '<STR_LIT>' ] <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } <EOL> } <EOL> } ) <EOL> collection . update_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , { \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection", "gt": ". delete_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a", "gt": ", b = b , a + b", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def factorial ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> print ( numero , \"<STR_LIT>\" , ( numero - <NUM_LIT> ) , \"<STR_LIT>\" , numero * ( numero - <NUM_LIT> ) ) <EOL> return numero * factorial ( numero - <NUM_LIT> ) <EOL> print ( factorial ( <NUM_LIT> ) ) <EOL> from functools import reduce <EOL> def factorial_masoquista ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> return reduce ( lambda x , y : x * y , range ( <NUM_LIT> , numero + <NUM_LIT> ) ) <EOL> numeros", "gt": "= [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def buscar ( lista , objetivo ) : <EOL> visto = { } <EOL> for num in lista : <EOL> if objetivo - num in visto : <EOL> return [ objetivo - num , num ] <EOL> visto [ num ] = True <EOL> return [ ] <EOL> lista_numeros = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print", "gt": "( buscar ( lista_numeros , objetivo = <NUM_LIT> ) )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def bubble_sort ( palabras ) : <EOL> for _ in range ( len ( palabras ) ) : <EOL> palabras = list ( palabras ) <EOL> for j in range ( len ( palabras ) - <NUM_LIT> ) : <EOL> if palabras [ j ] > palabras [ j + <NUM_LIT> ] : <EOL> palabras", "gt": "[ j ] , palabras [ j + <NUM_LIT> ] = palabras [ j + <NUM_LIT> ] , palabras [ j ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def bubble_sort ( palabras ) : <EOL> for _ in range ( len ( palabras ) ) : <EOL> palabras = list ( palabras ) <EOL> for j in range ( len ( palabras ) - <NUM_LIT> ) : <EOL> if palabras [ j ] > palabras [ j + <NUM_LIT> ] : <EOL> palabras [ j ] , palabras [ j + <NUM_LIT> ] = palabras [ j + <NUM_LIT> ] , palabras [ j ] <EOL> return palabras <EOL> frase = \"<STR_LIT>\" <EOL> palabras = frase . split ( ) <EOL> palabras_ordenadas", "gt": "= [ bubble_sort ( palabra ) for palabra in palabras ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def factorial ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> print ( numero , \"<STR_LIT>\" , ( numero - <NUM_LIT> ) , \"<STR_LIT>\" , numero * ( numero - <NUM_LIT> ) ) <EOL> return numero * factorial ( numero - <NUM_LIT> ) <EOL> print ( factorial ( <NUM_LIT> ) ) <EOL> from functools import reduce <EOL> def", "gt": "factorial_masoquista ( numero ) :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def buscar ( lista , objetivo ) : <EOL> visto = { } <EOL> for num in lista : <EOL> if objetivo - num in visto : <EOL> return [ objetivo - num , num ] <EOL> visto [ num ] = True <EOL> return [ ] <EOL> lista_numeros", "gt": "= [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def fibo3 ( n , temp = { } ) : <EOL> if n <= <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif", "gt": "n == <NUM_LIT> :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado", "gt": ". append ( a )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def fibo3 ( n , temp = { } ) : <EOL> if", "gt": "n <= <NUM_LIT> :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def factorial ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> print ( numero , \"<STR_LIT>\" , ( numero - <NUM_LIT> ) , \"<STR_LIT>\" , numero * ( numero - <NUM_LIT> ) ) <EOL> return numero * factorial ( numero - <NUM_LIT> ) <EOL> print ( factorial ( <NUM_LIT> ) ) <EOL> from functools import reduce <EOL> def factorial_masoquista ( numero ) : <EOL> if numero == <NUM_LIT> or numero == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> else : <EOL> return", "gt": "reduce ( lambda x , y : x * y , range ( <NUM_LIT> , numero + <NUM_LIT> ) )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def bubble_sort ( palabras ) : <EOL> for _ in range ( len ( palabras ) ) : <EOL> palabras = list ( palabras ) <EOL> for j in range ( len ( palabras ) - <NUM_LIT> ) : <EOL> if palabras [ j ] > palabras [ j + <NUM_LIT> ] : <EOL> palabras [ j ] , palabras [ j + <NUM_LIT> ] = palabras [ j + <NUM_LIT> ] , palabras [ j ] <EOL> return palabras <EOL> frase = \"<STR_LIT>\" <EOL> palabras", "gt": "= frase . split ( )", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "from pymongo import MongoClient <EOL> client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) <EOL> db = client [ '<STR_LIT>' ] <EOL> collection = db [ '<STR_LIT>' ] <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection . insert_one ( { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } , <EOL> \"<STR_LIT>\" : { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" : <NUM_LIT> <EOL> } <EOL> } <EOL> } ) <EOL> collection . update_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , { \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } ) <EOL> for document in collection . find ( ) : <EOL> print ( document ) <EOL> print ( \"<STR_LIT>\" ) <EOL> collection . delete_one ( { \"<STR_LIT>\" : \"<STR_LIT>\" } ) <EOL> for", "gt": "document in collection . find ( ) :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def fibo3 ( n , temp = { } ) : <EOL> if n <= <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif n == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif n not in temp : <EOL> temp [ n ] = fibo3 ( n - <NUM_LIT> , temp ) + fibo3 ( n - <NUM_LIT> , temp ) <EOL> return", "gt": "temp [ n ]", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def", "gt": "fibo3 ( n , temp = { } ) :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def buscar ( lista , objetivo ) : <EOL> visto = { } <EOL> for num in lista : <EOL> if objetivo - num in visto : <EOL> return [ objetivo - num , num ] <EOL> visto", "gt": "[ num ] = True", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def funcion3 ( ) : <EOL> print ( \"<STR_LIT>\" ) <EOL> class Clase2 : <EOL> def", "gt": "metodo3 ( ) :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "def fibo1 ( n ) : <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> print ( a , end = \"<STR_LIT>\" ) <EOL> a , b = b , a + b <EOL> print ( ) <EOL> def fibo2 ( n ) : <EOL> resultado = [ ] <EOL> a , b = <NUM_LIT> , <NUM_LIT> <EOL> while a < n : <EOL> resultado . append ( a ) <EOL> a , b = b , a + b <EOL> return resultado <EOL> def fibo3 ( n , temp = { } ) : <EOL> if n <= <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif n == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> elif", "gt": "n not in temp :", "repo": "bootcamp-programacion-brujeriatech"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_audio_analyzer_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def analyzer ( ) : <EOL> with gr . Column ( ) : <EOL> audio_input = gr . Audio ( type = \"<STR_LIT>\" ) <EOL> output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> get_info_button = gr . Button ( <EOL> value", "gt": "= i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\"", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with", "gt": "gr . Column ( ) :", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> save_existing_folders ( get_existing_folders ( ) + [ folder_name ] ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> gr . Info ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> restart_applio ( ) <EOL> return None <EOL> def check_new_folders ( ) : <EOL> existing_folders = get_existing_folders ( ) <EOL> new_folders = set ( current_folders ) - set ( existing_folders ) <EOL> save_existing_folders ( current_folders ) <EOL> if new_folders : <EOL> for", "gt": "new_folder in new_folders :", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> import importlib . util <EOL> import tabs . plugins . plugins_core as plugins_core <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> plugins_core . check_new_folders ( ) <EOL> def plugins_tab ( ) : <EOL> with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) : <EOL> dropbox = gr . File ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = plugins_core . save_plugin_dropbox , <EOL> inputs = [ dropbox ] , <EOL> outputs = [ dropbox ] , <EOL> ) <EOL> for plugin in os . listdir ( os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) : <EOL> plugin_main = f\"<STR_LIT>\" <EOL> plugin_import = importlib . import_module ( plugin_main ) <EOL> with", "gt": "gr . TabItem ( plugin ) :", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ \"<STR_LIT>\" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> : <EOL> f0 = np . pad ( f0 , [ [ pad_size , p_len - len ( f0 ) - pad_size ] ] , mode = \"<STR_LIT>\" ) <EOL> f0 , uv = self . interpolate_f0 ( f0 ) <EOL> return f0 <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling", "gt": "= self . f0_max ,", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ \"<STR_LIT>\" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> : <EOL> f0 = np . pad ( f0 , [ [ pad_size , p_len - len ( f0 ) - pad_size ] ] , mode = \"<STR_LIT>\" ) <EOL> f0 , uv = self . interpolate_f0 ( f0 ) <EOL> return f0 <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> .", "gt": "selected_array [ \"<STR_LIT>\" ]", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_download_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) <EOL> model_download_button . click ( <EOL> run_download_script , <EOL> [ model_link ] , <EOL> model_download_output_info , <EOL> api_name = \"<STR_LIT>\" , <EOL> ) <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> dropbox = gr . File ( <EOL> label = i18n ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = save_drop_model , <EOL> inputs = [ dropbox ] , <EOL> outputs = [ dropbox ] , <EOL> ) <EOL> gr", "gt": ". Markdown ( value = i18n ( \"<STR_LIT>\" ) )", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os", "gt": ". path . join ( folder_path , \"<STR_LIT>\" ) ,", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info", "gt": "= i18n ( \"<STR_LIT>\" ) ,", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_download_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) <EOL> model_download_button . click ( <EOL> run_download_script , <EOL> [ model_link ] , <EOL> model_download_output_info , <EOL> api_name = \"<STR_LIT>\" , <EOL> ) <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> dropbox", "gt": "= gr . File (", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len", "gt": "= x . shape [ <NUM_LIT> ] // self . hop_length", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> save_existing_folders ( get_existing_folders ( ) + [ folder_name ] ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> gr . Info ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> restart_applio ( ) <EOL> return None <EOL> def check_new_folders ( ) : <EOL> existing_folders = get_existing_folders ( ) <EOL> new_folders = set ( current_folders ) - set ( existing_folders ) <EOL> save_existing_folders ( current_folders ) <EOL> if new_folders : <EOL> for new_folder in new_folders : <EOL> complete_path = os . path . join ( plugins_path , new_folder ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if os . path . exists ( os . path . join ( complete_path , \"<STR_LIT>\" ) ) : <EOL> subprocess . run ( <EOL> [ <EOL> os", "gt": ". path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ,", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_audio_analyzer_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def analyzer ( ) : <EOL> with gr . Column ( ) : <EOL> audio_input = gr . Audio ( type = \"<STR_LIT>\" ) <EOL> output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> get_info_button", "gt": "= gr . Button (", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ \"<STR_LIT>\" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> : <EOL> f0 = np . pad ( f0 , [ [ pad_size , p_len - len ( f0 ) - pad_size ] ] , mode = \"<STR_LIT>\" ) <EOL> f0 , uv = self . interpolate_f0 ( f0 ) <EOL> return f0 <EOL> def", "gt": "compute_f0_uv ( self , wav , p_len = None ) :", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> save_existing_folders ( get_existing_folders ( ) + [ folder_name ] ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> gr . Info ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> restart_applio ( ) <EOL> return None <EOL> def check_new_folders ( ) : <EOL> existing_folders = get_existing_folders ( ) <EOL> new_folders = set ( current_folders ) - set ( existing_folders ) <EOL> save_existing_folders ( current_folders ) <EOL> if new_folders : <EOL> for new_folder in new_folders : <EOL> complete_path = os . path . join ( plugins_path , new_folder ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> if", "gt": "os . path . exists ( os . path . join ( complete_path , \"<STR_LIT>\" ) ) :", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_download_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) <EOL> model_download_button . click ( <EOL> run_download_script , <EOL> [ model_link ] , <EOL> model_download_output_info , <EOL> api_name = \"<STR_LIT>\" , <EOL> ) <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> dropbox = gr . File ( <EOL> label = i18n ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = save_drop_model , <EOL> inputs = [ dropbox ] , <EOL> outputs", "gt": "= [ dropbox ] ,", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info", "gt": "= gr . Textbox (", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> save_existing_folders ( get_existing_folders ( ) + [ folder_name ] ) <EOL> print ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> gr . Info ( <EOL> f\"<STR_LIT>\" <EOL> ) <EOL> restart_applio ( ) <EOL> return None <EOL> def check_new_folders ( ) : <EOL> existing_folders", "gt": "= get_existing_folders ( )", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_download_button", "gt": "= gr . Button ( i18n ( \"<STR_LIT>\" ) )", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> print ( \"<STR_LIT>\" ) <EOL> save_existing_folders", "gt": "( get_existing_folders ( ) + [ folder_name ] )", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> import importlib . util <EOL> import tabs . plugins . plugins_core as plugins_core <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> plugins_core . check_new_folders ( ) <EOL> def plugins_tab ( ) : <EOL> with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) : <EOL> dropbox = gr . File ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = plugins_core . save_plugin_dropbox , <EOL> inputs = [ dropbox ] , <EOL> outputs = [ dropbox ] , <EOL> ) <EOL> for", "gt": "plugin in os . listdir ( os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) :", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_audio_analyzer_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def analyzer ( ) : <EOL> with gr . Column ( ) : <EOL> audio_input = gr . Audio ( type = \"<STR_LIT>\" ) <EOL> output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> get_info_button = gr . Button ( <EOL> value = i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\" <EOL> ) <EOL> image_output = gr . Image ( type = \"<STR_LIT>\" , interactive = False ) <EOL> get_info_button . click ( <EOL> fn = run_audio_analyzer_script , <EOL> inputs = [ audio_input ] , <EOL> outputs", "gt": "= [ output_info , image_output ] ,", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label", "gt": "= i18n ( \"<STR_LIT>\" ) ,", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> import importlib . util <EOL> import tabs . plugins . plugins_core as plugins_core <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> plugins_core . check_new_folders ( ) <EOL> def plugins_tab ( ) : <EOL> with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) : <EOL> dropbox = gr . File ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = plugins_core . save_plugin_dropbox , <EOL> inputs = [ dropbox ] , <EOL> outputs = [ dropbox ] , <EOL> ) <EOL> for plugin in os . listdir ( os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) : <EOL> plugin_main = f\"<STR_LIT>\" <EOL> plugin_import", "gt": "= importlib . import_module ( plugin_main )", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ \"<STR_LIT>\" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if", "gt": "pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> :", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import json <EOL> import gradio as gr <EOL> import zipfile <EOL> import subprocess <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . settings . restart import restart_applio <EOL> plugins_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> if not os . path . exists ( plugins_path ) : <EOL> os . makedirs ( plugins_path ) <EOL> json_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) <EOL> current_folders = os . listdir ( plugins_path ) <EOL> def get_existing_folders ( ) : <EOL> if os . path . exists ( json_file_path ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ \"<STR_LIT>\" ] <EOL> else : <EOL> return [ ] <EOL> def save_existing_folders ( existing_folders ) : <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> config = json . load ( file ) <EOL> config [ \"<STR_LIT>\" ] = existing_folders <EOL> with open ( json_file_path , \"<STR_LIT>\" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def save_plugin_dropbox ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> folder_name = file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> folder_path = os . path . join ( plugins_path , folder_name ) <EOL> zip_file_path = os . path . join ( plugins_path , file_name ) <EOL> if os . path . exists ( folder_name ) : <EOL> os . remove ( folder_name ) <EOL> shutil . move ( dropbox , os . path . join ( plugins_path , file_name ) ) <EOL> print ( \"<STR_LIT>\" ) <EOL> with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : <EOL> zip_ref . extractall ( plugins_path ) <EOL> os . remove ( zip_file_path ) <EOL> if os . path . exists ( os . path . join ( folder_path , \"<STR_LIT>\" ) ) : <EOL> if os . name == \"<STR_LIT>\" : <EOL> subprocess . run ( <EOL> [ <EOL> os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os . path . join ( folder_path , \"<STR_LIT>\" ) , <EOL> ] <EOL> ) <EOL> else : <EOL> subprocess . run ( <EOL> [ <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> \"<STR_LIT>\" , <EOL> os", "gt": ". path . join ( folder_path , \"<STR_LIT>\" ) ,", "repo": "Applio"}
{"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import parselmouth <EOL> import numpy as np <EOL> class PMF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> else : <EOL> assert abs ( p_len - x . shape [ <NUM_LIT> ] // self . hop_length ) < <NUM_LIT> , \"<STR_LIT>\" <EOL> time_step = self . hop_length / self . sampling_rate * <NUM_LIT> <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sampling_rate ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ \"<STR_LIT>\" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> : <EOL> f0 = np . pad ( f0 , [ [ pad_size , p_len - len ( f0 ) - pad_size ] ] , mode = \"<STR_LIT>\" ) <EOL> f0 , uv = self . interpolate_f0 ( f0 ) <EOL> return f0 <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> x = wav <EOL> if", "gt": "p_len is None :", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> import importlib . util <EOL> import tabs . plugins . plugins_core as plugins_core <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> plugins_core . check_new_folders ( ) <EOL> def plugins_tab ( ) : <EOL> with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) : <EOL> dropbox = gr . File ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = plugins_core . save_plugin_dropbox , <EOL> inputs = [ dropbox ] , <EOL> outputs = [ dropbox ] , <EOL> ) <EOL> for plugin in os . listdir ( os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) ) : <EOL> plugin_main = f\"<STR_LIT>\" <EOL> plugin_import = importlib . import_module ( plugin_main ) <EOL> with gr . TabItem ( plugin ) : <EOL> plugin_import", "gt": ". applio_plugin ( )", "repo": "Applio"}
{"input": "import os , sys <EOL> import gradio as gr <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_audio_analyzer_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def analyzer ( ) : <EOL> with gr . Column ( ) : <EOL> audio_input = gr . Audio ( type = \"<STR_LIT>\" ) <EOL> output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> get_info_button = gr . Button ( <EOL> value = i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\" <EOL> ) <EOL> image_output = gr . Image ( type = \"<STR_LIT>\" , interactive = False ) <EOL> get_info_button . click ( <EOL> fn = run_audio_analyzer_script , <EOL> inputs", "gt": "= [ audio_input ] ,", "repo": "Applio"}
{"input": "import os , sys , shutil <EOL> import tempfile <EOL> import gradio as gr <EOL> import pandas as pd <EOL> import requests <EOL> from core import run_download_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> gradio_temp_dir = os . path . join ( tempfile . gettempdir ( ) , \"<STR_LIT>\" ) <EOL> if os . path . exists ( gradio_temp_dir ) : <EOL> shutil . rmtree ( gradio_temp_dir ) <EOL> def save_drop_model ( dropbox ) : <EOL> if \"<STR_LIT>\" not in dropbox and \"<STR_LIT>\" not in dropbox : <EOL> raise gr . Error ( <EOL> message = \"<STR_LIT>\" <EOL> ) <EOL> else : <EOL> file_name = format_title ( os . path . basename ( dropbox ) ) <EOL> if \"<STR_LIT>\" in dropbox : <EOL> model_name = format_title ( file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) <EOL> else : <EOL> if \"<STR_LIT>\" not in dropbox : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> else : <EOL> model_name = format_title ( <EOL> file_name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] <EOL> ) <EOL> model_path = os . path . join ( now_dir , \"<STR_LIT>\" , model_name ) <EOL> if not os . path . exists ( model_path ) : <EOL> os . makedirs ( model_path ) <EOL> if os . path . exists ( os . path . join ( model_path , file_name ) ) : <EOL> os . remove ( os . path . join ( model_path , file_name ) ) <EOL> shutil . move ( dropbox , os . path . join ( model_path , file_name ) ) <EOL> print ( f\"<STR_LIT>\" ) <EOL> gr . Info ( f\"<STR_LIT>\" ) <EOL> return None <EOL> def search_models ( name ) : <EOL> url = f\"<STR_LIT>\" <EOL> headers = { <EOL> \"<STR_LIT>\" : \"<STR_LIT>\" <EOL> } <EOL> response = requests . get ( url , headers = headers ) <EOL> data = response . json ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> gr . Info ( i18n ( \"<STR_LIT>\" ) ) <EOL> return None <EOL> else : <EOL> df = pd . DataFrame ( data ) [ [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ] <EOL> df [ \"<STR_LIT>\" ] = df [ \"<STR_LIT>\" ] . apply ( <EOL> lambda x : f'<STR_LIT>' <EOL> ) <EOL> return df <EOL> def download_tab ( ) : <EOL> with gr . Column ( ) : <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> model_link = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> placeholder = i18n ( \"<STR_LIT>\" ) , <EOL> interactive = True , <EOL> ) <EOL> model_download_output_info = gr . Textbox ( <EOL> label = i18n ( \"<STR_LIT>\" ) , <EOL> info = i18n ( \"<STR_LIT>\" ) , <EOL> value = \"<STR_LIT>\" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_download_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) <EOL> model_download_button . click ( <EOL> run_download_script , <EOL> [ model_link ] , <EOL> model_download_output_info , <EOL> api_name = \"<STR_LIT>\" , <EOL> ) <EOL> gr . Markdown ( value = i18n ( \"<STR_LIT>\" ) ) <EOL> dropbox = gr . File ( <EOL> label = i18n ( <EOL> \"<STR_LIT>\" <EOL> ) , <EOL> type = \"<STR_LIT>\" , <EOL> ) <EOL> dropbox . upload ( <EOL> fn = save_drop_model , <EOL> inputs", "gt": "= [ dropbox ] ,", "repo": "Applio"}
