[{"input": "from __future__ import print_function \n import json \n import os \n import os . path as osp \n import re \n import warnings \n from six . moves import urllib_parse \n import shutil \n import sys \n import tempfile \n import textwrap \n import time \n import requests \n import six \n import tqdm \n def indent ( text , prefix ) : \n def prefixed_lines ( ) : \n for line in text . splitlines ( True ) : \n yield ( prefix + line if line . strip ( ) else line ) \n return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) \n class FileURLRetrievalError ( Exception ) : \n pass \n class FolderContentsMaximumLimitError ( Exception ) : \n pass \n def parse_url ( url , warning = True ) : \n parsed = urllib_parse . urlparse ( url ) \n query = urllib_parse . parse_qs ( parsed . query ) \n is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) \n if not is_gdrive : \n return is_gdrive , is_download_link \n file_id = None \n if \"<STR_LIT>\" in query : \n file_ids = query [ \"<STR_LIT>\" ] \n if len ( file_ids ) == <NUM_LIT> : \n file_id = file_ids [ <NUM_LIT> ] \n else : \n patterns = [ \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" ,", "output": "r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n ] \n for pattern in patterns : \n match = re . match ( pattern , parsed . path ) \n if match : \n file_id = match . groups ( ) [ <NUM_LIT> ] \n break \n if warning and not is_download_link : \n warnings . warn ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n url = \"<STR_LIT>\" . format ( file_id ) \n ) \n ) \n return file_id , is_download_link \n CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> \n home = osp . expanduser ( \"<STR_LIT>\" ) \n def get_url_from_gdrive_confirmation ( contents ) : \n url = \"<STR_LIT>\" \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = m . groups ( ) [ <NUM_LIT> ] \n uuid = re . search ( \n r'<STR_LIT>' , contents \n ) \n uuid = uuid . groups ( ) [ <NUM_LIT> ] \n url = ( \n \"<STR_LIT>\" \n + url \n + \"<STR_LIT>\" \n + uuid \n ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = m . groups ( ) [ <NUM_LIT> ] \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n error = m . groups ( ) [ <NUM_LIT> ] \n raise FileURLRetrievalError ( error ) \n raise FileURLRetrievalError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n def _get_session ( proxy , use_cookies , return_cookies_file = False ) : \n sess = requests . session ( ) \n sess . headers . update ( \n { \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n if proxy is not None : \n sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } \n print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) \n cookies_file = osp . join ( home , \"<STR_LIT>\" ) \n if osp . exists ( cookies_file ) and use_cookies : \n with open ( cookies_file ) as f : \n cookies = json . load ( f ) \n for k , v in cookies : \n sess . cookies [ k ] = v \n if return_cookies_file : \n return sess , cookies_file \n else : \n return sess \n def download ( \n url = None , \n output = None , \n quiet = False , \n proxy = None , \n speed = None , \n use_cookies = True , \n verify = True , \n id = None , \n fuzzy = True , \n resume = False , \n format = None , \n ) : \n if not ( id is None ) ^ ( url is None ) : \n raise ValueError ( \"<STR_LIT>\" ) \n if id is not None : \n url = \"<STR_LIT>\" . format ( id = id ) \n url_origin = url \n sess , cookies_file = _get_session ( \n proxy = proxy , use_cookies = use_cookies , return_cookies_file = True \n ) \n gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) \n if fuzzy and gdrive_file_id : \n url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) \n url_origin = url \n is_gdrive_download_link = True \n while True : \n res = sess . get ( url , stream = True , verify = verify ) \n if url == url_origin and res . status_code == <NUM_LIT> : \n url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) \n continue \n if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : \n m = re . search ( \"<STR_LIT>\" , res . text ) \n if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif ( \n \"<STR_LIT>\" in res . headers \n and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) \n and format not in { None , \"<STR_LIT>\" } \n ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n if use_cookies : \n if not osp . exists ( osp . dirname ( cookies_file ) ) : \n os . makedirs ( osp . dirname ( cookies_file ) ) \n with open ( cookies_file , \"<STR_LIT>\" ) as f : \n cookies = [ \n ( k , v ) \n for k , v in sess . cookies . items ( ) \n if not k . startswith ( \"<STR_LIT>\" ) \n ] \n json . dump ( cookies , f , indent = <NUM_LIT> ) \n if \"<STR_LIT>\" in res . headers : \n break \n if not ( gdrive_file_id and is_gdrive_download_link ) : \n break \n try : \n url = get_url_from_gdrive_confirmation ( res . text ) \n except FileURLRetrievalError as e : \n message = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) . format ( \n indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , \n url_origin , \n ) \n raise FileURLRetrievalError ( message ) \n if gdrive_file_id and is_gdrive_download_link : \n content_disposition = six . moves . urllib_parse . unquote ( \n res . headers [ \"<STR_LIT>\" ] \n ) \n m = re . search ( r\"<STR_LIT>\" , content_disposition ) \n if not m : \n m = re . search ( r'<STR_LIT>' , content_disposition ) \n filename_from_url = m . groups ( ) [ <NUM_LIT> ] \n filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) \n else : \n filename_from_url = osp . basename ( url ) \n if output is None : \n output = filename_from_url \n output_is_path = isinstance ( output , six . string_types ) \n if output_is_path and output . endswith ( osp . sep ) : \n if not osp . exists ( output ) : \n os . makedirs ( output ) \n output = osp . join ( output , filename_from_url ) \n if output_is_path : \n existing_tmp_files = [ ] \n for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : \n if file . startswith ( osp . basename ( output ) ) : \n existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) \n if resume and existing_tmp_files : \n if len ( existing_tmp_files ) != <NUM_LIT> : \n print ( \n \"<STR_LIT>\" , \n file = sys . stderr , \n ) \n print ( \"<STR_LIT>\" ) \n for file in existing_tmp_files : \n print ( \"<STR_LIT>\" , file , file = sys . stderr ) \n print ( \"<STR_LIT>\" ) \n print ( \n \"<STR_LIT>\" , \n file = sys . stderr , \n ) \n return \n tmp_file = existing_tmp_files [ <NUM_LIT> ] \n else : \n resume = False \n tmp_file = tempfile . mktemp ( \n suffix = tempfile . template , \n prefix = osp . basename ( output ) , \n dir = osp . dirname ( output ) , \n ) \n f = open ( tmp_file , \"<STR_LIT>\" ) \n else : \n tmp_file = None \n f = output \n if tmp_file is not None and f . tell ( ) != <NUM_LIT> : \n headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } \n res = sess . get ( url , headers = headers , stream = True , verify = verify ) \n if not quiet : \n if resume : \n print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) \n print ( \n \"<STR_LIT>\" , \n osp . abspath ( output ) if output_is_path else output , \n file = sys . stderr , \n ) \n try : \n total = res . headers . get ( \"<STR_LIT>\" ) \n if total is not None : \n total = int ( total ) \n if not quiet : \n pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) \n t_start = time . time ( ) \n for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : \n f . write ( chunk ) \n if not quiet : \n pbar . update ( len ( chunk ) ) \n if speed is not None : \n elapsed_time_expected = <NUM_LIT> * pbar . n / speed \n elapsed_time = time . time ( ) - t_start \n if elapsed_time < elapsed_time_expected : \n time . sleep ( elapsed_time_expected - elapsed_time ) \n if not quiet : \n pbar . close ( ) \n if tmp_file : \n f . close ( ) \n shutil . move ( tmp_file , output ) \n finally : \n sess . close ( ) \n return output"}, {"input": "import time \n from tensorboard import program \n log_path = \"<STR_LIT>\" \n def launch_tensorboard_pipeline ( ) :", "output": "tb = program . TensorBoard ( ) \n tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) \n url = tb . launch ( ) \n print ( \n f\"<STR_LIT>\" \n ) \n while True : \n time . sleep ( <NUM_LIT> )"}, {"input": "import numpy as np \n class Slicer : \n def __init__ ( \n self , \n sr : int , \n threshold : float = - <NUM_LIT> , \n min_length : int = <NUM_LIT> , \n min_interval : int = <NUM_LIT> , \n hop_size : int = <NUM_LIT> , \n max_sil_kept : int = <NUM_LIT> , \n ) : \n if not min_length >= min_interval >= hop_size : \n raise ValueError ( \"<STR_LIT>\" ) \n if not max_sil_kept >= hop_size : \n raise ValueError ( \"<STR_LIT>\" ) \n min_interval = sr * min_interval / <NUM_LIT> \n self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) \n self . hop_size = round ( sr * hop_size / <NUM_LIT> ) \n self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) \n self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) \n self . min_interval = round ( min_interval / self . hop_size ) \n self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) \n def _apply_slice ( self , waveform , begin , end ) : \n start_idx = begin * self . hop_size \n if len ( waveform . shape ) > <NUM_LIT> : \n end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) \n return waveform [ : , start_idx : end_idx ] \n else : \n end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) \n return waveform [ start_idx : end_idx ] \n def slice ( self , waveform ) : \n samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform \n if samples . shape [ <NUM_LIT> ] <= self . min_length : \n return [ waveform ] \n rms_list = get_rms ( \n y = samples , frame_length = self . win_size , hop_length = self . hop_size \n ) . squeeze ( <NUM_LIT> ) \n sil_tags = [ ] \n silence_start , clip_start = None , <NUM_LIT> \n for i , rms in enumerate ( rms_list ) : \n if rms < self . threshold : \n if silence_start is None : \n silence_start = i \n continue \n if silence_start is None : \n continue \n is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept \n need_slice_middle = ( \n i - silence_start >= self . min_interval", "output": "and i - clip_start >= self . min_length \n ) \n if not is_leading_silence and not need_slice_middle : \n silence_start = None \n continue \n if i - silence_start <= self . max_sil_kept : \n pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos ) ) \n else : \n sil_tags . append ( ( pos , pos ) ) \n clip_start = pos \n elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : \n pos = rms_list [ \n i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n pos += i - self . max_sil_kept \n pos_l = ( \n rms_list [ \n silence_start : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n + silence_start \n ) \n pos_r = ( \n rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) \n + i \n - self . max_sil_kept \n ) \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos_r ) ) \n clip_start = pos_r \n else : \n sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) \n clip_start = max ( pos_r , pos ) \n else : \n pos_l = ( \n rms_list [ \n silence_start : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n + silence_start \n ) \n pos_r = ( \n rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) \n + i \n - self . max_sil_kept \n ) \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos_r ) ) \n else : \n sil_tags . append ( ( pos_l , pos_r ) ) \n clip_start = pos_r \n silence_start = None \n total_frames = rms_list . shape [ <NUM_LIT> ] \n if ( \n silence_start is not None \n and total_frames - silence_start >= self . min_interval \n ) : \n silence_end = min ( total_frames , silence_start + self . max_sil_kept ) \n pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start \n sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) \n if not sil_tags : \n return [ waveform ] \n else : \n chunks = [ ] \n if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : \n chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) \n for i in range ( len ( sil_tags ) - <NUM_LIT> ) : \n chunks . append ( \n self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) \n ) \n if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : \n chunks . append ( \n self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) \n ) \n return chunks \n def get_rms ( \n y , \n frame_length = <NUM_LIT> , \n hop_length = <NUM_LIT> , \n pad_mode = \"<STR_LIT>\" , \n ) : \n padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) \n y = np . pad ( y , padding , mode = pad_mode ) \n axis = - <NUM_LIT> \n out_strides = y . strides + tuple ( [ y . strides [ axis ] ] ) \n x_shape_trimmed = list ( y . shape ) \n x_shape_trimmed [ axis ] -= frame_length - <NUM_LIT> \n out_shape = tuple ( x_shape_trimmed ) + tuple ( [ frame_length ] ) \n xw = np . lib . stride_tricks . as_strided ( y , shape = out_shape , strides = out_strides ) \n if axis < <NUM_LIT> : \n target_axis = axis - <NUM_LIT> \n else : \n target_axis = axis + <NUM_LIT> \n xw = np . moveaxis ( xw , - <NUM_LIT> , target_axis ) \n slices = [ slice ( None ) ] * xw . ndim \n slices [ axis ] = slice ( <NUM_LIT> , None , hop_length ) \n x = xw [ tuple ( slices ) ] \n power = np . mean ( np . abs ( x ) ** <NUM_LIT> , axis = - <NUM_LIT> , keepdims = True ) \n return np . sqrt ( power )"}, {"input": "import os \n import sys \n import faiss \n import numpy as np \n from sklearn . cluster import MiniBatchKMeans \n from multiprocessing import cpu_count \n exp_dir = sys . argv [ <NUM_LIT> ] \n version = sys . argv [ <NUM_LIT> ] \n try : \n if version == \"<STR_LIT>\" : \n feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) \n elif version == \"<STR_LIT>\" :", "output": "feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) \n npys = [ ] \n listdir_res = sorted ( os . listdir ( feature_dir ) ) \n for name in listdir_res : \n file_path = os . path . join ( feature_dir , name ) \n phone = np . load ( file_path ) \n npys . append ( phone ) \n big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) \n big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) \n np . random . shuffle ( big_npy_idx ) \n big_npy = big_npy [ big_npy_idx ] \n if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : \n big_npy = ( \n MiniBatchKMeans ( \n n_clusters = <NUM_LIT> , \n verbose = True , \n batch_size = <NUM_LIT> * cpu_count ( ) , \n compute_labels = False , \n init = \"<STR_LIT>\" , \n ) \n . fit ( big_npy ) \n . cluster_centers_ \n ) \n np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) \n n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) \n index_trained = faiss . index_factory ( \n <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" \n ) \n index_ivf_trained = faiss . extract_index_ivf ( index_trained ) \n index_ivf_trained . nprobe = <NUM_LIT> \n index_trained . train ( big_npy ) \n index_filename_trained = ( \n f\"<STR_LIT>\" \n ) \n index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) \n faiss . write_index ( index_trained , index_filepath_trained ) \n index_added = faiss . index_factory ( \n <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" \n ) \n index_ivf_added = faiss . extract_index_ivf ( index_added ) \n index_ivf_added . nprobe = <NUM_LIT> \n index_added . train ( big_npy ) \n index_filename_added = ( \n f\"<STR_LIT>\" \n ) \n index_filepath_added = os . path . join ( exp_dir , index_filename_added ) \n batch_size_add = <NUM_LIT> \n for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : \n index_added . add ( big_npy [ i : i + batch_size_add ] ) \n faiss . write_index ( index_added , index_filepath_added ) \n print ( f\"<STR_LIT>\" ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n if \"<STR_LIT>\" in str ( error ) : \n print ( \n \"<STR_LIT>\" \n )"}, {"input": "import ast \n import json \n from pathlib import Path \n from collections import OrderedDict \n def extract_i18n_strings ( node ) : \n i18n_strings = [ ] \n if ( \n isinstance ( node , ast . Call ) \n and isinstance ( node . func , ast . Name ) \n and node . func . id == \"<STR_LIT>\" \n ) : \n for arg in node . args : \n if isinstance ( arg , ast . Str ) : \n i18n_strings . append ( arg . s ) \n for child_node in ast . iter_child_nodes ( node ) : \n i18n_strings . extend ( extract_i18n_strings ( child_node ) ) \n return i18n_strings \n def process_file ( file_path ) : \n with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n code = file . read ( ) \n if \"<STR_LIT>\" in code : \n tree = ast . parse ( code ) \n i18n_strings = extract_i18n_strings ( tree ) \n print ( file_path , len ( i18n_strings ) ) \n return i18n_strings \n return [ ] \n py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) \n code_keys = set ( ) \n for py_file in py_files : \n strings = process_file ( py_file ) \n code_keys . update ( strings ) \n print ( ) \n print ( \"<STR_LIT>\" , len ( code_keys ) ) \n standard_file = \"<STR_LIT>\" \n with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n standard_data = json . load ( file , object_pairs_hook = OrderedDict ) \n standard_keys = set ( standard_data . keys ( ) ) \n unused_keys = standard_keys - code_keys \n missing_keys = code_keys - standard_keys \n print ( \"<STR_LIT>\" , len ( unused_keys ) ) \n for unused_key in unused_keys : \n print ( \"<STR_LIT>\" , unused_key ) \n print ( \"<STR_LIT>\" , len ( missing_keys ) ) \n for missing_key in missing_keys :", "output": "print ( \"<STR_LIT>\" , missing_key ) \n code_keys_dict = OrderedDict ( ( s , s ) for s in code_keys ) \n with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( code_keys_dict , file , ensure_ascii = False , indent = <NUM_LIT> , sort_keys = True ) \n file . write ( \"<STR_LIT>\" )"}]