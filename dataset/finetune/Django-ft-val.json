[{"input": "import os \n from channels . auth import AuthMiddlewareStack \n from channels . routing import ProtocolTypeRouter , URLRouter \n from channels . security . websocket import AllowedHostsOriginValidator \n from django . core . asgi import get_asgi_application \n from channels . routing import get_default_application \n import django \n os . environ . setdefault ( '<STR_LIT>' , '<STR_LIT>' ) \n django . setup ( ) \n from summarizer . routing import websocket_urlpatterns \n django_asgi_app = get_asgi_application ( ) \n import summarizer . routing \n print ( '<STR_LIT>' ) \n application = ProtocolTypeRouter (", "output": "{ \n \"<STR_LIT>\" : django_asgi_app , \n \"<STR_LIT>\" : AllowedHostsOriginValidator ( \n AuthMiddlewareStack ( URLRouter ( websocket_urlpatterns ) ) \n ) , \n } \n )"}, {"input": "import requests , datetime , os , re , json \n from utils . general import headers \n from bs4 import BeautifulSoup as bs \n def get_epgs_tvb ( channel , channel_id , dt , func_arg ) : \n epgs = [ ] \n msg = '<STR_LIT>'", "output": "success = <NUM_LIT> \n dt_str = dt . strftime ( '<STR_LIT>' ) \n url = '<STR_LIT>' % ( dt_str , channel_id ) \n try : \n res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) \n res . encoding = '<STR_LIT>' \n res_j = res . json ( ) \n epg_list = res_j [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n for li in epg_list : \n starttime = datetime . datetime . fromtimestamp ( int ( li [ '<STR_LIT>' ] ) ) \n title = li [ '<STR_LIT>' ] \n title_en = li [ '<STR_LIT>' ] \n desc = li [ '<STR_LIT>' ] \n desc_en = li [ '<STR_LIT>' ] \n url = li [ '<STR_LIT>' ] \n epg = { '<STR_LIT>' : channel . id , \n '<STR_LIT>' : starttime , \n '<STR_LIT>' : None , \n '<STR_LIT>' : '<STR_LIT>' % ( title , title_en ) , \n '<STR_LIT>' : title_en , \n '<STR_LIT>' : desc , \n '<STR_LIT>' : desc_en , \n '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , \n } \n epgs . append ( epg ) \n except Exception as e : \n success = <NUM_LIT> \n spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n msg = '<STR_LIT>' % ( spidername , e ) \n ret = { \n '<STR_LIT>' : success , \n '<STR_LIT>' : epgs , \n '<STR_LIT>' : msg , \n '<STR_LIT>' : dt , \n '<STR_LIT>' : <NUM_LIT> , \n } \n return ret \n def get_epgs_tvb_old ( channel , channel_id , dt , func_arg ) : \n epgs = [ ] \n msg = '<STR_LIT>' \n success = <NUM_LIT> \n url = '<STR_LIT>' % channel_id \n try : \n res = requests . get ( url , timeout = <NUM_LIT> , headers = headers ) \n res . encoding = '<STR_LIT>' \n soup = bs ( res . text . replace ( '<STR_LIT>' , '<STR_LIT>' ) , '<STR_LIT>' ) \n lis = soup . select ( '<STR_LIT>' ) \n for li in lis : \n if '<STR_LIT>' not in li . attrs : \n continue \n starttime = datetime . datetime . fromtimestamp ( int ( li . attrs [ '<STR_LIT>' ] ) ) \n em_time = li . select ( '<STR_LIT>' ) [ <NUM_LIT> ] . text \n title = li . text . strip ( ) . replace ( em_time , '<STR_LIT>' ) \n if starttime . date ( ) < dt : \n continue \n epg = { '<STR_LIT>' : channel . id , \n '<STR_LIT>' : starttime , \n '<STR_LIT>' : None , \n '<STR_LIT>' : title , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , \n } \n epgs . append ( epg ) \n except Exception as e : \n success = <NUM_LIT> \n spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n msg = '<STR_LIT>' % ( spidername , e ) \n ret = { \n '<STR_LIT>' : success , \n '<STR_LIT>' : epgs , \n '<STR_LIT>' : msg , \n '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in locals ( ) else dt , \n '<STR_LIT>' : <NUM_LIT> , \n } \n return ret \n def get_channels_tvb ( ) : \n url = '<STR_LIT>' \n res = requests . get ( url ) \n res_re = re . search ( '<STR_LIT>' , res . text ) \n channels_str = res_re . group ( <NUM_LIT> ) \n channels_str = channels_str . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' ) . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n channels_str = re . sub ( '<STR_LIT>' , lambda m : m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) + '<STR_LIT>' + m . group ( <NUM_LIT> ) , \n channels_str ) \n channels_j = json . loads ( channels_str ) \n channels = [ ] \n for li in channels_j : \n name = li [ '<STR_LIT>' ] \n name_en = li [ '<STR_LIT>' ] \n href = li [ '<STR_LIT>' ] if '<STR_LIT>' in li else '<STR_LIT>' \n id = li [ '<STR_LIT>' ] \n desc = li [ '<STR_LIT>' ] \n channel = { \n '<STR_LIT>' : name , \n '<STR_LIT>' : name_en , \n '<STR_LIT>' : [ id ] , \n '<STR_LIT>' : href , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : desc , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n channels . append ( channel ) \n return channels"}, {"input": "from django . db import migrations , models \n class Migration ( migrations . Migration ) :", "output": "dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = models . CharField ( blank = True , max_length = <NUM_LIT> ) , \n ) , \n migrations . AddField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = models . CharField ( blank = True , max_length = <NUM_LIT> ) , \n ) , \n ]"}, {"input": "from typing import List , Optional , Union \n from ninja_crud . testing . core . components import utils \n class Headers : \n def __init__ ( \n self , \n ok : Union [ dict , List [ dict ] ] , \n forbidden : Union [ dict , List [ dict ] , None ] = None , \n unauthorized : Union [ dict , List [ dict ] , None ] = None , \n ) -> None : \n self . ok : List [ dict ] = utils . ensure_list_of_dicts ( ok )", "output": "self . forbidden : Optional [ List [ dict ] ] = ( \n utils . ensure_list_of_dicts ( forbidden ) if forbidden is not None else None \n ) \n self . unauthorized : Optional [ List [ dict ] ] = ( \n utils . ensure_list_of_dicts ( unauthorized ) \n if unauthorized is not None \n else None \n )"}, {"input": "from urllib . parse import quote \n from django . db import transaction \n from django . http import HttpResponse \n from openpyxl import Workbook \n from openpyxl . worksheet . datavalidation import DataValidation \n from openpyxl . utils import get_column_letter , quote_sheetname \n from openpyxl . worksheet . table import Table , TableStyleInfo \n from rest_framework . decorators import action \n from rest_framework . request import Request \n from dvadmin . utils . import_export import import_to_data \n from dvadmin . utils . json_response import DetailResponse \n from dvadmin . utils . request_util import get_verbose_name \n class ImportSerializerMixin : \n import_field_dict = { } \n import_serializer_class = None \n export_column_width = <NUM_LIT> \n def is_number ( self , num ) : \n try : \n float ( num ) \n return True \n except ValueError : \n pass \n try : \n import unicodedata \n unicodedata . numeric ( num ) \n return True \n except ( TypeError , ValueError ) : \n pass \n return False \n def get_string_len ( self , string ) : \n length = <NUM_LIT> \n if string is None : \n return length \n if self . is_number ( string ) : \n return length \n for char in string : \n length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> \n return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width \n @ action ( methods = [ '<STR_LIT>' , '<STR_LIT>' ] , detail = False ) \n @ transaction . atomic \n def import_data ( self , request : Request , * args , ** kwargs ) : \n assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ \n if request . method == \"<STR_LIT>\" : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n response = HttpResponse ( content_type = \"<STR_LIT>\" ) \n response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" \n response [ \n \"<STR_LIT>\" \n ] = f'<STR_LIT>' \n wb = Workbook ( ) \n ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) \n ws1 . sheet_state = \"<STR_LIT>\" \n ws = wb . active \n row = get_column_letter ( len ( self . import_field_dict ) + <NUM_LIT> ) \n column = <NUM_LIT> \n header_data = [ \n \"<STR_LIT>\" , \n ] \n validation_data_dict = { } \n for index , ele in enumerate ( self . import_field_dict . values ( ) ) : \n if isinstance ( ele , dict ) : \n header_data . append ( ele . get ( \"<STR_LIT>\" ) ) \n choices = ele . get ( \"<STR_LIT>\" , { } ) \n if choices . get ( \"<STR_LIT>\" ) : \n data_list = [ ] \n data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) \n validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = data_list \n elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : \n data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) \n validation_data_dict [ ele . get ( \"<STR_LIT>\" ) ] = list ( data_list ) \n else : \n continue \n column_letter = get_column_letter ( len ( validation_data_dict ) ) \n dv = DataValidation ( \n type = \"<STR_LIT>\" , \n formula1 = f\"<STR_LIT>\" , \n allow_blank = True , \n ) \n ws . add_data_validation ( dv ) \n dv . add ( f\"<STR_LIT>\" ) \n else : \n header_data . append ( ele ) \n ws1 . append ( list ( validation_data_dict . keys ( ) ) ) \n for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : \n for inx , ele in enumerate ( validation_data ) : \n ws1 [ f\"<STR_LIT>\" ] = ele \n df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] \n ws . append ( header_data ) \n for index , width in enumerate ( df_len_max ) : \n ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width \n tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) \n style = TableStyleInfo ( \n name = \"<STR_LIT>\" , \n showFirstColumn = True , \n showLastColumn = True , \n showRowStripes = True , \n showColumnStripes = True , \n ) \n tab . tableStyleInfo = style \n ws . add_table ( tab ) \n wb . save ( response ) \n return response \n else : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n m2m_fields = [ \n ele . name \n for ele in queryset . model . _meta . get_fields ( ) \n if hasattr ( ele , \"<STR_LIT>\" ) and ele . many_to_many == True \n ] \n import_field_dict = { '<STR_LIT>' : '<STR_LIT>' , ** self . import_field_dict } \n data = import_to_data ( request . data . get ( \"<STR_LIT>\" ) , import_field_dict , m2m_fields ) \n for ele in data : \n filter_dic = { '<STR_LIT>' : ele . get ( '<STR_LIT>' ) } \n instance = filter_dic and queryset . filter ( ** filter_dic ) . first ( ) \n serializer = self . import_serializer_class ( instance , data = ele , request = request ) \n serializer . is_valid ( raise_exception = True ) \n serializer . save ( ) \n return DetailResponse ( msg = f\"<STR_LIT>\" ) \n @ action ( methods = [ '<STR_LIT>' ] , detail = False ) \n def update_template ( self , request ) : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n assert self . import_field_dict , \"<STR_LIT>\" % self . __class__ . __name__ \n assert self . import_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ \n data = self . import_serializer_class ( queryset , many = True , request = request ) . data \n response = HttpResponse ( content_type = \"<STR_LIT>\" ) \n response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" \n response [ \"<STR_LIT>\" ] = f'<STR_LIT>' \n wb = Workbook ( ) \n ws1 = wb . create_sheet ( \"<STR_LIT>\" , <NUM_LIT> ) \n ws1 . sheet_state = \"<STR_LIT>\" \n ws = wb . active \n import_field_dict = { } \n header_data = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n hidden_header = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n validation_data_dict = { } \n for index , item in enumerate ( self . import_field_dict . items ( ) ) : \n items = list ( item ) \n key = items [ <NUM_LIT> ] \n value = items [ <NUM_LIT> ] \n if isinstance ( value , dict ) : \n header_data . append ( value . get ( \"<STR_LIT>\" ) ) \n hidden_header . append ( value . get ( '<STR_LIT>' ) ) \n choices = value . get ( \"<STR_LIT>\" , { } ) \n if choices . get ( \"<STR_LIT>\" ) : \n data_list = [ ] \n data_list . extend ( choices . get ( \"<STR_LIT>\" ) . keys ( ) ) \n validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = data_list \n elif choices . get ( \"<STR_LIT>\" ) and choices . get ( \"<STR_LIT>\" ) : \n data_list = choices . get ( \"<STR_LIT>\" ) . values_list ( choices . get ( \"<STR_LIT>\" ) , flat = True ) \n validation_data_dict [ value . get ( \"<STR_LIT>\" ) ] = list ( data_list ) \n else : \n continue \n column_letter = get_column_letter ( len ( validation_data_dict ) ) \n dv = DataValidation ( \n type = \"<STR_LIT>\" , \n formula1 = f\"<STR_LIT>\" , \n allow_blank = True , \n ) \n ws . add_data_validation ( dv ) \n dv . add ( f\"<STR_LIT>\" ) \n else : \n header_data . append ( value ) \n hidden_header . append ( key ) \n ws1 . append ( list ( validation_data_dict . keys ( ) ) ) \n for index , validation_data in enumerate ( validation_data_dict . values ( ) ) : \n for inx , ele in enumerate ( validation_data ) : \n ws1 [ f\"<STR_LIT>\" ] = ele \n df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] \n row = get_column_letter ( len ( hidden_header ) + <NUM_LIT> ) \n column = <NUM_LIT> \n ws . append ( header_data ) \n for index , results in enumerate ( data ) : \n results_list = [ ] \n for h_index , h_item in enumerate ( hidden_header ) : \n for key , val in results . items ( ) : \n if key == h_item : \n if val is None or val == \"<STR_LIT>\" : \n results_list . append ( \"<STR_LIT>\" ) \n elif isinstance ( val , list ) : \n results_list . append ( str ( val ) ) \n else : \n results_list . append ( val ) \n if isinstance ( val , str ) : \n result_column_width = self . get_string_len ( val ) \n if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : \n df_len_max [ h_index ] = result_column_width \n ws . append ( [ index + <NUM_LIT> , * results_list ] ) \n column += <NUM_LIT> \n for index , width in enumerate ( df_len_max ) : \n ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width \n tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) \n style = TableStyleInfo ( \n name = \"<STR_LIT>\" , \n showFirstColumn = True , \n showLastColumn = True , \n showRowStripes = True , \n showColumnStripes = True , \n ) \n tab . tableStyleInfo = style \n ws . add_table ( tab ) \n wb . save ( response ) \n return response \n class ExportSerializerMixin : \n export_field_label = [ ] \n export_serializer_class = None \n export_column_width = <NUM_LIT> \n def is_number ( self , num ) : \n try :", "output": "float ( num ) \n return True \n except ValueError : \n pass \n try : \n import unicodedata \n unicodedata . numeric ( num ) \n return True \n except ( TypeError , ValueError ) : \n pass \n return False \n def get_string_len ( self , string ) : \n length = <NUM_LIT> \n if string is None : \n return length \n if self . is_number ( string ) : \n return length \n for char in string : \n length += <NUM_LIT> if ord ( char ) > <NUM_LIT> else <NUM_LIT> \n return round ( length , <NUM_LIT> ) if length <= self . export_column_width else self . export_column_width \n @ action ( methods = [ '<STR_LIT>' ] , detail = False ) \n def export_data ( self , request : Request , * args , ** kwargs ) : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n assert self . export_field_label , \"<STR_LIT>\" % self . __class__ . __name__ \n assert self . export_serializer_class , \"<STR_LIT>\" % self . __class__ . __name__ \n data = self . export_serializer_class ( queryset , many = True , request = request ) . data \n response = HttpResponse ( content_type = \"<STR_LIT>\" ) \n response [ \"<STR_LIT>\" ] = f\"<STR_LIT>\" \n response [ \"<STR_LIT>\" ] = f'<STR_LIT>' \n wb = Workbook ( ) \n ws = wb . active \n header_data = [ \"<STR_LIT>\" , * self . export_field_label . values ( ) ] \n hidden_header = [ \"<STR_LIT>\" , * self . export_field_label . keys ( ) ] \n df_len_max = [ self . get_string_len ( ele ) for ele in header_data ] \n row = get_column_letter ( len ( self . export_field_label ) + <NUM_LIT> ) \n column = <NUM_LIT> \n ws . append ( header_data ) \n for index , results in enumerate ( data ) : \n results_list = [ ] \n for h_index , h_item in enumerate ( hidden_header ) : \n for key , val in results . items ( ) : \n if key == h_item : \n if val is None or val == \"<STR_LIT>\" : \n results_list . append ( \"<STR_LIT>\" ) \n else : \n results_list . append ( val ) \n result_column_width = self . get_string_len ( val ) \n if h_index != <NUM_LIT> and result_column_width > df_len_max [ h_index ] : \n df_len_max [ h_index ] = result_column_width \n ws . append ( [ index + <NUM_LIT> , * results_list ] ) \n column += <NUM_LIT> \n for index , width in enumerate ( df_len_max ) : \n ws . column_dimensions [ get_column_letter ( index + <NUM_LIT> ) ] . width = width \n tab = Table ( displayName = \"<STR_LIT>\" , ref = f\"<STR_LIT>\" ) \n style = TableStyleInfo ( \n name = \"<STR_LIT>\" , \n showFirstColumn = True , \n showLastColumn = True , \n showRowStripes = True , \n showColumnStripes = True , \n ) \n tab . tableStyleInfo = style \n ws . add_table ( tab ) \n wb . save ( response ) \n return response"}, {"input": "import torch , math \n from transformers import BertTokenizer , XLNetTokenizer , RobertaTokenizer , LongformerTokenizer \n import logging \n log = logging . getLogger ( ) \n def encode_documents ( documents : list , tokenizer : BertTokenizer , max_input_length ) : \n tokenized_documents = [ tokenizer . tokenize ( document ) for document in documents ] \n max_sequences_per_document = math . ceil ( max ( len ( x ) / ( max_input_length - <NUM_LIT> ) for x in tokenized_documents ) ) \n output = torch . zeros ( size = ( len ( documents ) , max_sequences_per_document , <NUM_LIT> , max_input_length ) , dtype = torch . long ) \n document_seq_lengths = [ ] \n for doc_index , tokenized_document in enumerate ( tokenized_documents ) : \n max_seq_index = <NUM_LIT> \n for seq_index , i in enumerate ( range ( <NUM_LIT> , len ( tokenized_document ) , ( max_input_length - <NUM_LIT> ) ) ) : \n raw_tokens = tokenized_document [ i : i + ( max_input_length - <NUM_LIT> ) ] \n tokens = [ ] \n input_type_ids = [ ] \n tokens . append ( \"<STR_LIT>\" ) \n input_type_ids . append ( <NUM_LIT> ) \n for token in raw_tokens : \n tokens . append ( token ) \n input_type_ids . append ( <NUM_LIT> ) \n tokens . append ( \"<STR_LIT>\" ) \n input_type_ids . append ( <NUM_LIT> ) \n input_ids = tokenizer . convert_tokens_to_ids ( tokens ) \n attention_masks = [ <NUM_LIT> ] * len ( input_ids ) \n while len ( input_ids ) < max_input_length : \n input_ids . append ( <NUM_LIT> ) \n input_type_ids . append ( <NUM_LIT> )", "output": "attention_masks . append ( <NUM_LIT> ) \n output [ doc_index ] [ seq_index ] = torch . cat ( ( torch . LongTensor ( input_ids ) . unsqueeze ( <NUM_LIT> ) , \n torch . LongTensor ( input_type_ids ) . unsqueeze ( <NUM_LIT> ) , \n torch . LongTensor ( attention_masks ) . unsqueeze ( <NUM_LIT> ) ) , \n dim = <NUM_LIT> ) \n max_seq_index = seq_index \n document_seq_lengths . append ( max_seq_index + <NUM_LIT> ) \n return output , torch . LongTensor ( document_seq_lengths )"}, {"input": "from django . contrib import admin \n from django . urls import include \n from django . urls import path \n from django . urls import reverse_lazy \n from django . views . generic . base import RedirectView \n urlpatterns = [", "output": "path ( \"<STR_LIT>\" , admin . site . urls ) , \n path ( \"<STR_LIT>\" , RedirectView . as_view ( url = reverse_lazy ( \"<STR_LIT>\" ) ) ) , \n path ( \"<STR_LIT>\" , include ( \"<STR_LIT>\" , namespace = \"<STR_LIT>\" ) ) , \n ]"}, {"input": "from django . db import migrations , models \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ]", "output": "operations = [ \n migrations . AlterField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n field = models . BinaryField ( editable = True ) , \n ) , \n ]"}, {"input": "from django . db import models , transaction \n from django . db . models import Q \n from dateutil import tz \n from utils . general import cht_to_chs , add_info_desc , add_info_title \n import datetime \n from django . utils import timezone \n tz_sh = tz . gettz ( '<STR_LIT>' ) \n class Channel ( models . Model ) : \n source_choices = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n need_get = [ \n ( <NUM_LIT> , '<STR_LIT>' ) , \n ( <NUM_LIT> , '<STR_LIT>' ) \n ] \n channel_id = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n tvg_name = models . CharField ( '<STR_LIT>' , null = False , max_length = <NUM_LIT> , db_index = True ) \n name = models . CharField ( '<STR_LIT>' , null = False , max_length = <NUM_LIT> , db_index = True ) \n sort = models . CharField ( '<STR_LIT>' , null = False , max_length = <NUM_LIT> , db_index = True ) \n logo = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> , null = True ) \n last_program_date = models . DateField ( '<STR_LIT>' , db_index = True , null = True ) \n last_crawl_dt = models . DateTimeField ( '<STR_LIT>' , auto_now = True , null = True ) \n create_dt = models . DateTimeField ( '<STR_LIT>' , auto_now_add = True , null = True ) \n descr = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> , null = True , blank = True ) \n has_epg = models . IntegerField ( '<STR_LIT>' , choices = need_get , default = <NUM_LIT> , db_index = True ) \n ineed = models . IntegerField ( '<STR_LIT>' , choices = need_get , default = <NUM_LIT> , db_index = True ) \n source = models . CharField ( '<STR_LIT>' , choices = source_choices , max_length = <NUM_LIT> , db_index = True , null = True ) \n recrawl = models . IntegerField ( '<STR_LIT>' , choices = need_get , db_index = True , default = <NUM_LIT> ) \n patten = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> , null = True , blank = True ) \n remark = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> , null = True , blank = True ) \n def __str__ ( self ) : \n return '<STR_LIT>' . join ( [ str ( self . id ) , self . name , self . tvg_name , self . source if self . source else '<STR_LIT>' ] ) \n class Meta : \n verbose_name = '<STR_LIT>' \n verbose_name_plural = '<STR_LIT>' \n def get_crawl_channels ( self , need_date , recrawl = <NUM_LIT> ) : \n if recrawl == <NUM_LIT> : \n return self . objects . filter ( ineed = <NUM_LIT> , has_epg = <NUM_LIT> ) . filter ( Q ( recrawl = <NUM_LIT> ) | Q ( last_program_date__lt = need_date ) ) \n else : \n return self . objects . filter ( has_epg = <NUM_LIT> , ineed = <NUM_LIT> , last_program_date__lt = need_date ) \n def get_spec_channel ( self , name = <NUM_LIT> , id = <NUM_LIT> ) : \n if name : \n ret = self . objects . filter ( tvg_name__icontains = name ) [ : <NUM_LIT> ] \n if ret . count ( ) == <NUM_LIT> : \n ret = self . objects . filter ( name__icontains = name ) [ : <NUM_LIT> ] \n elif id : \n ret = self . objects . filter ( id = id ) \n else : \n ret = self . objects . filter ( tvg_name = '<STR_LIT>' ) \n return ret \n def get_spec_channel_strict ( self , name = <NUM_LIT> , id = <NUM_LIT> ) : \n if name : \n ret = self . objects . filter ( tvg_name = name ) \n else : \n ret = self . objects . filter ( id = id ) \n return ret \n def get_need_channels ( self , sorts ) : \n if sorts == '<STR_LIT>' : \n channels = self . objects . filter ( ineed = <NUM_LIT> , has_epg = <NUM_LIT> ) \n else : \n channels = self . objects . filter ( sort__in = sorts , ineed = <NUM_LIT> , has_epg = <NUM_LIT> ) \n return [ channels , channels . values_list ( '<STR_LIT>' ) ] \n def get_match_channels ( self ) : \n channels = self . objects . filter ( ineed = <NUM_LIT> , has_epg = <NUM_LIT> ) \n return channels \n def save ( self , * args , ** kwargs ) : \n if self . source in self . channel_id : \n super ( ) . save ( * args , ** kwargs ) \n else : \n pass \n class Epg ( models . Model ) : \n channel_id = models . CharField ( '<STR_LIT>' , db_index = True , max_length = <NUM_LIT> ) \n starttime = models . DateTimeField ( '<STR_LIT>' , db_index = True ) \n endtime = models . DateTimeField ( '<STR_LIT>' , null = True ) \n title = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n descr = models . TextField ( '<STR_LIT>' , null = True ) \n program_date = models . DateField ( '<STR_LIT>' , db_index = True ) \n crawl_dt = models . DateTimeField ( '<STR_LIT>' , auto_now_add = True , null = True ) \n source = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> , null = True ) \n def __str__ ( self ) : \n return '<STR_LIT>' % ( self . channel_id , self . starttime . astimezone ( tz = tz_sh ) , self . title ) \n def to_dict ( self ) : \n ret = { \n '<STR_LIT>' : self . channel_id , \n '<STR_LIT>' : self . starttime , \n '<STR_LIT>' : self . endtime , \n '<STR_LIT>' : self . title , \n '<STR_LIT>' : self . descr , \n '<STR_LIT>' : self . program_date , \n '<STR_LIT>' : self . source , \n } \n return ret \n class Meta : \n verbose_name = '<STR_LIT>' \n verbose_name_plural = '<STR_LIT>' \n def save ( self , * args , ** kwargs ) : \n self . crawl_dt = timezone . now ( ) \n super ( ) . save ( * args , ** kwargs ) \n def get_epgs ( self , channel_ids , need_program_date ) : \n if need_program_date > datetime . datetime . now ( ) . date ( ) : \n epgs = self . objects . filter ( \n channel_id__in = channel_ids , program_date__gte = datetime . datetime . now ( ) , program_date__lte = need_program_date ) \n if epgs . count ( ) == <NUM_LIT> : \n epgs = self . objects . filter ( channel_id__in = channel_ids , program_date = datetime . datetime . now ( ) . date ( ) ) \n else : \n epgs = self . objects . filter ( channel_id__in = channel_ids , program_date = need_program_date ) \n for epg in epgs : \n if epg . endtime is None : \n epg . endtime = datetime . datetime . combine ( epg . starttime . date ( ) , \n datetime . time ( hour = <NUM_LIT> , minute = <NUM_LIT> , second = <NUM_LIT> ) ) \n return epgs \n def test ( self ) : \n return self . objects . filter ( source = '<STR_LIT>' ) \n def get_single_epg ( self , channel , need_date ) : \n epgs_list = [ ] \n no_endtime_endtime = datetime . datetime . combine ( need_date , \n datetime . time ( hour = <NUM_LIT> , minute = <NUM_LIT> , second = <NUM_LIT> ) ) . astimezone ( \n tz = tz_sh ) \n epgs = self . objects . filter ( channel_id = channel . id , program_date = need_date ) \n for epg in epgs : \n if epg . endtime is None : \n epg . endtime = no_endtime_endtime \n epg . starttime = epg . starttime . astimezone ( tz = tz_sh ) . strftime ( '<STR_LIT>' ) \n epg . endtime = epg . endtime . astimezone ( tz = tz_sh ) . strftime ( '<STR_LIT>' ) \n epg . descr = '<STR_LIT>' % ( epg . descr , add_info_desc ) \n epg . title = '<STR_LIT>' % ( epg . title , add_info_title ) \n epg1 = epg . to_dict ( ) \n epgs_list . append ( epg1 ) \n return epgs_list \n def save_to_dbs ( self , ret ) : \n success = <NUM_LIT> \n msg = '<STR_LIT>' \n querylist = [ ] \n n = <NUM_LIT> \n if ret [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n epglen = len ( ret [ '<STR_LIT>' ] ) \n for x in range ( epglen ) : \n if x < epglen - <NUM_LIT> : \n ret [ '<STR_LIT>' ] [ x ] [ '<STR_LIT>' ] = ret [ '<STR_LIT>' ] [ x + <NUM_LIT> ] [ '<STR_LIT>' ] \n cs = self . objects . filter ( channel_id = ret [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] , \n starttime__lt = ret [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] . astimezone ( tz = tz_sh ) ) \n if cs : \n cs = cs . latest ( '<STR_LIT>' ) \n if ( ret [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] . astimezone ( tz = tz_sh ) - cs . starttime . astimezone ( \n tz = tz_sh ) ) . seconds < <NUM_LIT> : \n cs . endtime = ret [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] . astimezone ( tz = tz_sh ) \n cs . save ( ) \n n = <NUM_LIT> \n for epg in ret [ '<STR_LIT>' ] : \n try : \n n += <NUM_LIT> \n if ret [ '<STR_LIT>' ] in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n epg [ '<STR_LIT>' ] = cht_to_chs ( epg [ '<STR_LIT>' ] ) \n descr = cht_to_chs ( epg [ '<STR_LIT>' ] ) if '<STR_LIT>' in epg else '<STR_LIT>' \n else : \n descr = epg [ '<STR_LIT>' ] if '<STR_LIT>' in epg else '<STR_LIT>' \n querye = Epg ( channel_id = epg [ '<STR_LIT>' ] , starttime = epg [ '<STR_LIT>' ] . astimezone ( tz = tz_sh ) , \n endtime = epg [ '<STR_LIT>' ] . astimezone ( tz = tz_sh ) if epg [ '<STR_LIT>' ] else None , \n title = epg [ '<STR_LIT>' ] , descr = descr , \n program_date = epg [ '<STR_LIT>' ] , source = ret [ '<STR_LIT>' ] ) \n querylist . append ( querye ) \n except Exception as e : \n success = <NUM_LIT> \n msg = '<STR_LIT>' % ( e ) \n continue \n try : \n ret = self . objects . bulk_create ( querylist ) \n except Exception as e : \n success = <NUM_LIT> \n msg = '<STR_LIT>' % ( e ) \n return { \n '<STR_LIT>' : success , \n '<STR_LIT>' : ret , \n '<STR_LIT>' : msg \n } \n def del_channel_epgs ( self , channel_id , program_date , last_program_date ) : \n if program_date == last_program_date : \n ret = self . objects . filter ( channel_id = channel_id , program_date = program_date ) . delete ( ) \n else : \n ret = self . objects . filter ( channel_id = channel_id , program_date__gte = program_date , \n program_date_lte = last_program_date ) . delete ( ) \n return ret \n class Crawl_log ( models . Model ) : \n LOG_LEVELS = ( \n ( <NUM_LIT> , '<STR_LIT>' ) , \n ( <NUM_LIT> , '<STR_LIT>' ) \n ) \n dt = models . DateTimeField ( '<STR_LIT>' , auto_now_add = True , db_index = True ) \n msg = models . TextField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n level = models . IntegerField ( '<STR_LIT>' , choices = LOG_LEVELS , default = <NUM_LIT> ) \n def __str__ ( self ) : \n return '<STR_LIT>' % ( self . dt . strftime ( '<STR_LIT>' ) , self . msg , '<STR_LIT>' if self . level == <NUM_LIT> else '<STR_LIT>' ) \n def save ( self , * args , ** kwargs ) : \n self . msg = self . msg [ : <NUM_LIT> ] + '<STR_LIT>' \n super ( ) . save ( self , * args , ** kwargs ) \n class Meta : \n verbose_name = '<STR_LIT>' \n verbose_name_plural = '<STR_LIT>' \n class Channel_list ( models . Model ) : \n is_alive_choice = ( \n ( <NUM_LIT> , '<STR_LIT>' ) , \n ( <NUM_LIT> , '<STR_LIT>' ) \n ) \n inner_channel_id = models . IntegerField ( '<STR_LIT>' , default = <NUM_LIT> , db_index = True ) \n out_channel_id = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n inner_name = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n out_name = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n source = models . CharField ( '<STR_LIT>' , max_length = <NUM_LIT> ) \n is_alive = models . IntegerField ( '<STR_LIT>' , choices = is_alive_choice , default = <NUM_LIT> ) \n create_date = models . DateField ( '<STR_LIT>' , auto_now_add = True ) \n update_date = models . DateField ( '<STR_LIT>' , auto_now_add = True ) \n def __str__ ( self ) : \n return '<STR_LIT>' % ( self . inner_channel_id , self . inner_name , self . out_name ) \n class Meta : \n verbose_name = \"<STR_LIT>\" \n verbose_name_plural = \"<STR_LIT>\"", "output": "def save_to_db ( self , channels ) : \n msg = '<STR_LIT>' \n success = <NUM_LIT> \n old_channels_same_source = self . objects . filter ( source = channels [ <NUM_LIT> ] [ '<STR_LIT>' ] ) \n querylist = [ ] \n new_no = <NUM_LIT> \n update_no = <NUM_LIT> \n not_alive_no = <NUM_LIT> \n dt_now = datetime . datetime . now ( ) . date ( ) \n is_alive_list = [ ] \n for channel in channels : \n try : \n for channel_old in old_channels_same_source : \n if channel_old . out_name == channel [ '<STR_LIT>' ] : \n channel_old . out_channel_id = '<STR_LIT>' . join ( channel [ '<STR_LIT>' ] ) \n channel_old . update_date = dt_now \n channel_old . save ( ) \n is_alive_list . append ( channel_old ) \n update_no += <NUM_LIT> \n break \n else : \n querye = Channel_list ( inner_channel_id = <NUM_LIT> , \n out_channel_id = '<STR_LIT>' . join ( channel [ '<STR_LIT>' ] ) , \n inner_name = '<STR_LIT>' , \n out_name = channel [ '<STR_LIT>' ] , \n source = channel [ '<STR_LIT>' ] , \n is_alive = True , \n create_date = dt_now , \n update_date = dt_now ) \n new_no += <NUM_LIT> \n querylist . append ( querye ) \n except Exception as e : \n success = <NUM_LIT> \n msg = '<STR_LIT>' % ( e ) \n continue \n for old_channel in old_channels_same_source : \n if old_channel not in is_alive_list : \n old_channel . is_alive = False \n old_channel . save ( ) \n not_alive_no += <NUM_LIT> \n try : \n ret = self . objects . bulk_create ( querylist ) \n except Exception as e : \n success = <NUM_LIT> \n msg = '<STR_LIT>' % ( e ) \n msg = '<STR_LIT>' % ( new_no , update_no , not_alive_no , msg ) \n ret = { \n '<STR_LIT>' : success , \n '<STR_LIT>' : msg \n } \n return ret"}, {"input": "from django . db import models , migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] \n operations = [ \n migrations . AlterField ( \n model_name = \"<STR_LIT>\" , \n name = \"<STR_LIT>\" , \n field = models . CharField ( \n choices = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] ,", "output": "max_length = <NUM_LIT> , \n ) , \n ) , \n ]"}, {"input": "import os \n import sys \n import argparse \n import shutil \n import math \n from collections import OrderedDict \n import json \n import cv2 \n from sklearn . model_selection import train_test_split \n import base64 \n import numpy as np \n class Labelme2YOLO ( object ) : \n def __init__ ( self , json_dir , output_path ) : \n self . _json_dir = json_dir \n self . _label_id_map = self . _get_label_id_map ( self . _json_dir ) \n self . output_path = output_path \n def _make_train_val_dir ( self ) : \n self . _label_dir_path = os . path . join ( self . output_path , \n '<STR_LIT>' ) \n self . _image_dir_path = os . path . join ( self . output_path , \n '<STR_LIT>' ) \n for yolo_path in ( os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , \n os . path . join ( self . _label_dir_path + '<STR_LIT>' ) , \n os . path . join ( self . _image_dir_path + '<STR_LIT>' ) , \n os . path . join ( self . _image_dir_path + '<STR_LIT>' ) ) : \n if os . path . exists ( yolo_path ) : \n shutil . rmtree ( yolo_path ) \n os . makedirs ( yolo_path ) \n def _get_label_id_map ( self , json_dir ) : \n label_set = set ( ) \n for file_name in os . listdir ( json_dir ) : \n if file_name . endswith ( '<STR_LIT>' ) :", "output": "json_path = os . path . join ( json_dir , file_name ) \n data = json . load ( open ( json_path ) ) \n for shape in data [ '<STR_LIT>' ] : \n label_set . add ( shape [ '<STR_LIT>' ] ) \n return OrderedDict ( [ ( label , label_id ) for label_id , label in enumerate ( label_set ) ] ) \n def _train_test_split ( self , folders , json_names , val_size ) : \n if len ( folders ) > <NUM_LIT> and '<STR_LIT>' in folders and '<STR_LIT>' in folders : \n train_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) \n train_json_names = [ train_sample_name + '<STR_LIT>' for train_sample_name in os . listdir ( train_folder ) if os . path . isdir ( os . path . join ( train_folder , train_sample_name ) ) ] \n val_folder = os . path . join ( self . _json_dir , '<STR_LIT>' ) \n val_json_names = [ val_sample_name + '<STR_LIT>' for val_sample_name in os . listdir ( val_folder ) if os . path . isdir ( os . path . join ( val_folder , val_sample_name ) ) ] \n return train_json_names , val_json_names \n train_idxs , val_idxs = train_test_split ( range ( len ( json_names ) ) , \n test_size = val_size ) \n train_json_names = [ json_names [ train_idx ] for train_idx in train_idxs ] \n val_json_names = [ json_names [ val_idx ] for val_idx in val_idxs ] \n return train_json_names , val_json_names \n def convert ( self , val_size ) : \n json_names = [ file_name for file_name in os . listdir ( self . _json_dir ) if os . path . isfile ( os . path . join ( self . _json_dir , file_name ) ) and file_name . endswith ( '<STR_LIT>' ) ] \n folders = [ file_name for file_name in os . listdir ( self . output_path ) if os . path . isdir ( os . path . join ( self . output_path , file_name ) ) ] \n train_json_names , val_json_names = self . _train_test_split ( folders , json_names , val_size ) \n self . _make_train_val_dir ( ) \n for target_dir , json_names in zip ( ( '<STR_LIT>' , '<STR_LIT>' ) , \n ( train_json_names , val_json_names ) ) : \n for json_name in json_names : \n json_path = os . path . join ( self . _json_dir , json_name ) \n json_data = json . load ( open ( json_path ) ) \n print ( '<STR_LIT>' % ( json_name , target_dir . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) ) \n img_path = self . _save_yolo_image ( json_data , \n json_name , \n self . output_path , \n \"<STR_LIT>\" + target_dir ) \n yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) \n self . _save_yolo_label ( json_name , \n self . _label_dir_path , \n target_dir , \n yolo_obj_list ) \n print ( '<STR_LIT>' ) \n self . _save_dataset_yaml ( ) \n def convert_one ( self , json_name ) : \n json_path = os . path . join ( self . _json_dir , json_name ) \n json_data = json . load ( open ( json_path ) ) \n print ( '<STR_LIT>' % json_name ) \n img_path = self . _save_yolo_image ( json_data , json_name , \n self . output_path , '<STR_LIT>' ) \n yolo_obj_list = self . _get_yolo_object_list ( json_data , img_path ) \n self . _save_yolo_label ( json_name , self . output_path , \n '<STR_LIT>' , yolo_obj_list ) \n def _get_yolo_object_list ( self , json_data , img_path ) : \n yolo_obj_list = [ ] \n img_h , img_w , _ = cv2 . imread ( img_path ) . shape \n for shape in json_data [ '<STR_LIT>' ] : \n if shape [ '<STR_LIT>' ] == '<STR_LIT>' : \n yolo_obj = self . _get_circle_shape_yolo_object ( shape , img_h , img_w ) \n else : \n yolo_obj = self . _get_other_shape_yolo_object ( shape , img_h , img_w ) \n yolo_obj_list . append ( yolo_obj ) \n return yolo_obj_list \n def _get_circle_shape_yolo_object ( self , shape , img_h , img_w ) : \n obj_center_x , obj_center_y = shape [ '<STR_LIT>' ] [ <NUM_LIT> ] \n radius = math . sqrt ( ( obj_center_x - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> + \n ( obj_center_y - shape [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] ) ** <NUM_LIT> ) \n obj_w = <NUM_LIT> * radius \n obj_h = <NUM_LIT> * radius \n yolo_center_x = round ( float ( obj_center_x / img_w ) , <NUM_LIT> ) \n yolo_center_y = round ( float ( obj_center_y / img_h ) , <NUM_LIT> ) \n yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) \n yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) \n label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] \n return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h \n def _get_other_shape_yolo_object ( self , shape , img_h , img_w ) : \n def __get_object_desc ( obj_port_list ) : \n __get_dist = lambda int_list : max ( int_list ) - min ( int_list ) \n x_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] \n y_lists = [ port [ <NUM_LIT> ] for port in obj_port_list ] \n return min ( x_lists ) , __get_dist ( x_lists ) , min ( y_lists ) , __get_dist ( y_lists ) \n obj_x_min , obj_w , obj_y_min , obj_h = __get_object_desc ( shape [ '<STR_LIT>' ] ) \n yolo_center_x = round ( float ( ( obj_x_min + obj_w / <NUM_LIT> ) / img_w ) , <NUM_LIT> ) \n yolo_center_y = round ( float ( ( obj_y_min + obj_h / <NUM_LIT> ) / img_h ) , <NUM_LIT> ) \n yolo_w = round ( float ( obj_w / img_w ) , <NUM_LIT> ) \n yolo_h = round ( float ( obj_h / img_h ) , <NUM_LIT> ) \n label_id = self . _label_id_map [ shape [ '<STR_LIT>' ] ] \n return label_id , yolo_center_x , yolo_center_y , yolo_w , yolo_h \n def _save_yolo_label ( self , json_name , label_dir_path , target_dir , yolo_obj_list ) : \n txt_path = os . path . join ( label_dir_path , \n target_dir , \n json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) ) \n with open ( txt_path , '<STR_LIT>' ) as f : \n for yolo_obj_idx , yolo_obj in enumerate ( yolo_obj_list ) : \n yolo_obj_line = '<STR_LIT>' % yolo_obj if yolo_obj_idx + <NUM_LIT> != len ( yolo_obj_list ) else '<STR_LIT>' % yolo_obj \n f . write ( yolo_obj_line ) \n def _save_yolo_image ( self , json_data , json_name , image_dir_path , target_dir ) : \n img_name = json_name . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n img_path = os . path . join ( image_dir_path , target_dir , img_name ) \n if not os . path . exists ( img_path ) : \n with open ( img_path , \"<STR_LIT>\" ) as fh : \n fh . write ( base64 . b64decode ( ( json_data [ '<STR_LIT>' ] ) ) ) \n return img_path \n def _save_dataset_yaml ( self ) : \n yaml_path = os . path . join ( self . output_path , '<STR_LIT>' ) \n with open ( yaml_path , '<STR_LIT>' ) as yaml_file : \n yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) \n yaml_file . write ( '<STR_LIT>' % os . path . join ( self . _image_dir_path , '<STR_LIT>' ) ) \n yaml_file . write ( '<STR_LIT>' % len ( self . _label_id_map ) ) \n names_str = '<STR_LIT>' \n for label , _ in self . _label_id_map . items ( ) : \n names_str += \"<STR_LIT>\" % label \n names_str = names_str . rstrip ( '<STR_LIT>' ) \n yaml_file . write ( '<STR_LIT>' % names_str ) \n if __name__ == '<STR_LIT>' : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( '<STR_LIT>' , type = str , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = float , nargs = '<STR_LIT>' , default = None , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , nargs = '<STR_LIT>' , default = None , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , \n default = '<STR_LIT>' , help = '<STR_LIT>' ) \n args = parser . parse_args ( sys . argv [ <NUM_LIT> : ] ) \n convertor = Labelme2YOLO ( args . json_dir ) \n if args . json_name is None : \n convertor . convert ( val_size = args . val_size ) \n else : \n convertor . convert_one ( args . json_name )"}, {"input": "from collections import OrderedDict \n from django . core import paginator \n from django . core . paginator import Paginator as DjangoPaginator , InvalidPage \n from rest_framework . pagination import PageNumberPagination \n from rest_framework . response import Response \n class CustomPagination ( PageNumberPagination ) : \n page_size = <NUM_LIT> \n page_size_query_param = \"<STR_LIT>\" \n max_page_size = <NUM_LIT> \n django_paginator_class = DjangoPaginator \n def paginate_queryset ( self , queryset , request , view = None ) : \n empty = True \n page_size = self . get_page_size ( request ) \n if not page_size : \n return None \n paginator = self . django_paginator_class ( queryset , page_size ) \n page_number = request . query_params . get ( self . page_query_param , <NUM_LIT> ) \n if page_number in self . last_page_strings : \n page_number = paginator . num_pages \n try : \n self . page = paginator . page ( page_number ) \n except InvalidPage as exc : \n empty = False \n if paginator . num_pages > <NUM_LIT> and self . template is not None : \n self . display_page_controls = True \n self . request = request \n if not empty : \n self . page = [ ] \n return list ( self . page ) \n def get_paginated_response ( self , data ) : \n code = <NUM_LIT> \n msg = '<STR_LIT>' \n page = int ( self . get_page_number ( self . request , paginator ) ) or <NUM_LIT> \n total = self . page . paginator . count if self . page else <NUM_LIT> \n limit = int ( self . get_page_size ( self . request ) ) or <NUM_LIT> \n is_next = self . page . has_next ( ) if self . page else False \n is_previous = self . page . has_previous ( ) if self . page else False \n if not data : \n code = <NUM_LIT> \n msg = \"<STR_LIT>\" \n data = [ ] \n return Response ( OrderedDict ( [ \n ( '<STR_LIT>' , code ) , \n ( '<STR_LIT>' , msg ) , \n ( '<STR_LIT>' , page ) ,", "output": "( '<STR_LIT>' , limit ) , \n ( '<STR_LIT>' , total ) , \n ( '<STR_LIT>' , is_next ) , \n ( '<STR_LIT>' , is_previous ) , \n ( '<STR_LIT>' , data ) \n ] ) )"}, {"input": "import random \n asap_ranges = { \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n <NUM_LIT> : ( <NUM_LIT> , <NUM_LIT> ) , \n } \n asap_essay_lengths = { \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> ,", "output": "<NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> , \n <NUM_LIT> : <NUM_LIT> \n } \n def fix_score ( score , prompt ) : \n if prompt == <NUM_LIT> : \n int_part = float ( int ( score ) ) \n float_part = score - int_part \n result = int_part \n if float_part < <NUM_LIT> : \n result = int_part \n elif float_part < <NUM_LIT> : \n result = int_part + <NUM_LIT> \n else : \n result = int_part + <NUM_LIT> \n min_score , max_score = asap_ranges [ prompt ] \n if result < min_score : \n return min_score \n elif result > max_score : \n return max_score \n else : \n return result \n elif prompt <= <NUM_LIT> : \n min_score , max_score = asap_ranges [ prompt ] \n if score < min_score : \n return min_score \n elif score > max_score : \n return max_score \n else : \n return round ( score ) \n else : \n return score \n def is_zh ( s ) : \n for c in s : \n if c >= '<STR_LIT>' and c <= '<STR_LIT>' : \n return True \n return False \n def load_asap_data ( data_file , max_len = <NUM_LIT> , data_sample_rate = <NUM_LIT> ) : \n ids = [ ] \n texts = [ ] \n labels = [ ] \n sample_index = <NUM_LIT> \n with open ( data_file ) as fin : \n for line in fin : \n rand_value = random . random ( ) \n if rand_value > data_sample_rate : \n continue \n line = line . strip ( ) \n line_vec = line . split ( \"<STR_LIT>\" ) \n if len ( line_vec ) == <NUM_LIT> : \n ids . append ( line_vec [ <NUM_LIT> ] ) \n if len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : \n line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) \n texts . append ( line_vec [ <NUM_LIT> ] ) \n labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) \n else : \n ids . append ( str ( sample_index ) ) \n sample_index += <NUM_LIT> \n if is_zh ( line_vec [ <NUM_LIT> ] ) and len ( line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) >= max_len : \n line_vec [ <NUM_LIT> ] = line_vec [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] \n elif len ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) ) >= max_len : \n line_vec [ <NUM_LIT> ] = \"<STR_LIT>\" . join ( line_vec [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> : max_len ] ) \n texts . append ( line_vec [ <NUM_LIT> ] ) \n labels . append ( float ( line_vec [ <NUM_LIT> ] ) ) \n for id , text , label in zip ( ids , texts , labels ) : \n yield ( id , text , label )"}, {"input": "import re \n import pytest \n from playwright . sync_api import Locator , Page , expect \n from dev . football . core . management . commands . testdata import PASSWORD , USERNAME \n @ pytest . fixture \n def page_admin ( page ) -> Page : \n page . goto ( f\"<STR_LIT>\" ) \n re_username = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) \n page . get_by_role ( \"<STR_LIT>\" , name = re_username ) . type ( USERNAME ) \n re_password = re . compile ( \"<STR_LIT>\" , re . IGNORECASE )", "output": "page . get_by_role ( \"<STR_LIT>\" , name = re_password ) . type ( PASSWORD ) \n page . get_by_role ( \"<STR_LIT>\" , name = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) ) . click ( ) \n re_logout = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) \n expect ( page . get_by_role ( \"<STR_LIT>\" , name = re_logout ) ) . to_be_visible ( ) \n return page \n def search_button ( page : Page ) -> Locator : \n re_search_site = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) \n return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site ) \n def search_box ( page : Page ) -> Locator : \n re_search_site_input = re . compile ( \"<STR_LIT>\" , re . IGNORECASE ) \n return page . get_by_role ( \"<STR_LIT>\" , name = re_search_site_input ) \n def expect_modal_open ( page : Page ) : \n expect ( search_box ( page ) ) . to_be_visible ( ) \n def expect_modal_closed ( page : Page ) : \n expect ( search_box ( page ) ) . not_to_be_visible ( ) \n def open_modal ( page : Page , with_keyboard : bool = False ) : \n if with_keyboard : \n page . keyboard . press ( \"<STR_LIT>\" ) \n else : \n search_button ( page ) . click ( ) \n expect_modal_open ( page )"}, {"input": "from django . db import migrations , models \n class Migration ( migrations . Migration ) : \n initial = True \n dependencies = [ \n ] \n operations = [ \n migrations . CreateModel ( \n name = '<STR_LIT>' , \n fields = [ \n ( '<STR_LIT>' , models . BigAutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name = '<STR_LIT>' ) ) , \n ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> , unique = True ) ) , \n ( '<STR_LIT>' , models . IntegerField ( default = <NUM_LIT> ) ) , \n ( '<STR_LIT>' , models . CharField ( max_length = <NUM_LIT> ) ) , \n ( '<STR_LIT>' , models . BooleanField ( default = True ) ) , \n ( '<STR_LIT>' , models . DateTimeField ( auto_now_add = True ) ) , \n ] ,", "output": ") , \n ]"}, {"input": "import os \n import sys \n def main ( ) : \n os . environ . setdefault ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n try : \n from django . core . management import execute_from_command_line \n except ImportError as exc : \n raise ImportError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) from exc \n execute_from_command_line ( sys . argv ) \n if __name__ == \"<STR_LIT>\" :", "output": "main ( )"}, {"input": "from django . db import models , migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ]", "output": "operations = [ \n migrations . AddField ( \n model_name = \"<STR_LIT>\" , \n name = \"<STR_LIT>\" , \n field = models . CharField ( max_length = <NUM_LIT> , null = True ) , \n ) , \n migrations . AddField ( \n model_name = \"<STR_LIT>\" , \n name = \"<STR_LIT>\" , \n field = models . DateTimeField ( blank = True , null = True ) , \n ) , \n ]"}, {"input": "import hashlib \n from django . contrib . auth . hashers import make_password , check_password \n from django_restql . fields import DynamicSerializerMethodField \n from rest_framework import serializers \n from rest_framework . decorators import action \n from rest_framework . permissions import IsAuthenticated \n from django . db import connection \n from django . db . models import Q \n from application import dispatch \n from dvadmin . system . models import Users , Role , Dept \n from dvadmin . system . views . role import RoleSerializer \n from dvadmin . utils . json_response import ErrorResponse , DetailResponse , SuccessResponse \n from dvadmin . utils . serializers import CustomModelSerializer \n from dvadmin . utils . validator import CustomUniqueValidator \n from dvadmin . utils . viewset import CustomModelViewSet \n def recursion ( instance , parent , result ) : \n new_instance = getattr ( instance , parent , None ) \n res = [ ] \n data = getattr ( instance , result , None ) \n if data : \n res . append ( data ) \n if new_instance : \n array = recursion ( new_instance , parent , result ) \n res += array \n return res \n class UserSerializer ( CustomModelSerializer ) : \n dept_name = serializers . CharField ( source = '<STR_LIT>' , read_only = True ) \n role_info = DynamicSerializerMethodField ( ) \n dept_name_all = serializers . SerializerMethodField ( ) \n class Meta : \n model = Users \n read_only_fields = [ \"<STR_LIT>\" ] \n exclude = [ \"<STR_LIT>\" ] \n extra_kwargs = { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n } \n def get_dept_name_all ( self , instance ) : \n dept_name_all = recursion ( instance . dept , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n dept_name_all . reverse ( ) \n return \"<STR_LIT>\" . join ( dept_name_all ) \n def get_role_info ( self , instance , parsed_query ) : \n roles = instance . role . all ( ) \n serializer = RoleSerializer ( \n roles , \n many = True , \n parsed_query = parsed_query \n ) \n return serializer . data \n class UserCreateSerializer ( CustomModelSerializer ) : \n username = serializers . CharField ( \n max_length = <NUM_LIT> , \n validators = [ \n CustomUniqueValidator ( queryset = Users . objects . all ( ) , message = \"<STR_LIT>\" ) \n ] , \n ) \n password = serializers . CharField ( \n required = False , \n ) \n def validate_password ( self , value ) : \n md5 = hashlib . md5 ( ) \n md5 . update ( value . encode ( '<STR_LIT>' ) ) \n md5_password = md5 . hexdigest ( ) \n return make_password ( md5_password ) \n def save ( self , ** kwargs ) : \n data = super ( ) . save ( ** kwargs ) \n data . dept_belong_id = data . dept_id \n data . save ( ) \n data . post . set ( self . initial_data . get ( \"<STR_LIT>\" , [ ] ) ) \n return data \n class Meta : \n model = Users \n fields = \"<STR_LIT>\" \n read_only_fields = [ \"<STR_LIT>\" ] \n extra_kwargs = { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n } \n class UserUpdateSerializer ( CustomModelSerializer ) : \n username = serializers . CharField ( \n max_length = <NUM_LIT> , \n validators = [ \n CustomUniqueValidator ( queryset = Users . objects . all ( ) , message = \"<STR_LIT>\" ) \n ] , \n ) \n def save ( self , ** kwargs ) : \n data = super ( ) . save ( ** kwargs ) \n data . dept_belong_id = data . dept_id \n data . save ( ) \n data . post . set ( self . initial_data . get ( \"<STR_LIT>\" , [ ] ) ) \n return data \n class Meta : \n model = Users \n read_only_fields = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n fields = \"<STR_LIT>\" \n extra_kwargs = { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False , \"<STR_LIT>\" : True } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n } \n class UserInfoUpdateSerializer ( CustomModelSerializer ) : \n mobile = serializers . CharField ( \n max_length = <NUM_LIT> , \n validators = [ \n CustomUniqueValidator ( queryset = Users . objects . all ( ) , message = \"<STR_LIT>\" ) \n ] , \n allow_blank = True \n )", "output": "def update ( self , instance , validated_data ) : \n return super ( ) . update ( instance , validated_data ) \n class Meta : \n model = Users \n fields = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n extra_kwargs = { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False , \"<STR_LIT>\" : True } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : False } , \n } \n class ExportUserProfileSerializer ( CustomModelSerializer ) : \n last_login = serializers . DateTimeField ( \n format = \"<STR_LIT>\" , required = False , read_only = True \n ) \n is_active = serializers . SerializerMethodField ( read_only = True ) \n dept_name = serializers . CharField ( source = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) \n dept_owner = serializers . CharField ( source = \"<STR_LIT>\" , default = \"<STR_LIT>\" ) \n gender = serializers . CharField ( source = \"<STR_LIT>\" , read_only = True ) \n def get_is_active ( self , instance ) : \n return \"<STR_LIT>\" if instance . is_active else \"<STR_LIT>\" \n class Meta : \n model = Users \n fields = ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) \n class UserProfileImportSerializer ( CustomModelSerializer ) : \n password = serializers . CharField ( read_only = True , required = False ) \n def save ( self , ** kwargs ) : \n data = super ( ) . save ( ** kwargs ) \n password = hashlib . new ( \n \"<STR_LIT>\" , str ( self . initial_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) . encode ( encoding = \"<STR_LIT>\" ) \n ) . hexdigest ( ) \n data . set_password ( password ) \n data . save ( ) \n return data \n class Meta : \n model = Users \n exclude = ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) \n class UserViewSet ( CustomModelViewSet ) : \n queryset = Users . objects . exclude ( is_superuser = <NUM_LIT> ) . all ( ) \n serializer_class = UserSerializer \n create_serializer_class = UserCreateSerializer \n update_serializer_class = UserUpdateSerializer \n filter_fields = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n search_fields = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n export_field_label = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n export_serializer_class = ExportUserProfileSerializer \n import_serializer_class = UserProfileImportSerializer \n import_field_dict = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : <NUM_LIT> } , \n } \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : True , \"<STR_LIT>\" : False } , \n } \n } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : Dept . objects . filter ( status = True ) , \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : Role . objects . filter ( status = True ) , \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n } \n @ action ( methods = [ \"<STR_LIT>\" ] , detail = False , permission_classes = [ IsAuthenticated ] ) \n def user_info ( self , request ) : \n user = request . user \n result = { \n \"<STR_LIT>\" : user . id , \n \"<STR_LIT>\" : user . username , \n \"<STR_LIT>\" : user . name , \n \"<STR_LIT>\" : user . mobile , \n \"<STR_LIT>\" : user . user_type , \n \"<STR_LIT>\" : user . gender , \n \"<STR_LIT>\" : user . email , \n \"<STR_LIT>\" : user . avatar , \n \"<STR_LIT>\" : user . dept_id , \n \"<STR_LIT>\" : user . is_superuser , \n \"<STR_LIT>\" : user . role . values_list ( '<STR_LIT>' , flat = True ) , \n } \n if hasattr ( connection , '<STR_LIT>' ) : \n result [ '<STR_LIT>' ] = connection . tenant and connection . tenant . id \n result [ '<STR_LIT>' ] = connection . tenant and connection . tenant . name \n dept = getattr ( user , '<STR_LIT>' , None ) \n if dept : \n result [ '<STR_LIT>' ] = { \n '<STR_LIT>' : dept . id , \n '<STR_LIT>' : dept . name \n } \n else : \n result [ '<STR_LIT>' ] = { \n '<STR_LIT>' : None , \n '<STR_LIT>' : \"<STR_LIT>\" \n } \n role = getattr ( user , '<STR_LIT>' , None ) \n if role : \n result [ '<STR_LIT>' ] = role . values ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n return DetailResponse ( data = result , msg = \"<STR_LIT>\" ) \n @ action ( methods = [ \"<STR_LIT>\" ] , detail = False , permission_classes = [ IsAuthenticated ] ) \n def update_user_info ( self , request ) : \n serializer = UserInfoUpdateSerializer ( request . user , data = request . data , request = request ) \n serializer . is_valid ( raise_exception = True ) \n serializer . save ( ) \n return DetailResponse ( data = None , msg = \"<STR_LIT>\" ) \n @ action ( methods = [ \"<STR_LIT>\" ] , detail = False , permission_classes = [ IsAuthenticated ] ) \n def change_password ( self , request , * args , ** kwargs ) : \n data = request . data \n old_pwd = data . get ( \"<STR_LIT>\" ) \n new_pwd = data . get ( \"<STR_LIT>\" ) \n new_pwd2 = data . get ( \"<STR_LIT>\" ) \n if old_pwd is None or new_pwd is None or new_pwd2 is None : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n if new_pwd != new_pwd2 : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n verify_password = check_password ( old_pwd , self . request . user . password ) \n if not verify_password : \n verify_password = check_password ( hashlib . md5 ( old_pwd . encode ( encoding = '<STR_LIT>' ) ) . hexdigest ( ) , self . request . user . password ) \n if verify_password : \n request . user . password = make_password ( hashlib . md5 ( new_pwd . encode ( encoding = '<STR_LIT>' ) ) . hexdigest ( ) ) \n request . user . save ( ) \n return DetailResponse ( data = None , msg = \"<STR_LIT>\" ) \n else : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n @ action ( methods = [ \"<STR_LIT>\" ] , detail = True , permission_classes = [ IsAuthenticated ] ) \n def reset_to_default_password ( self , request , * args , ** kwargs ) : \n instance = Users . objects . filter ( id = kwargs . get ( \"<STR_LIT>\" ) ) . first ( ) \n if instance : \n instance . set_password ( dispatch . get_system_config_values ( \"<STR_LIT>\" ) ) \n instance . save ( ) \n return DetailResponse ( data = None , msg = \"<STR_LIT>\" ) \n else : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n @ action ( methods = [ \"<STR_LIT>\" ] , detail = True ) \n def reset_password ( self , request , pk ) : \n if not self . request . user . is_superuser : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n instance = Users . objects . filter ( id = pk ) . first ( ) \n data = request . data \n new_pwd = data . get ( \"<STR_LIT>\" ) \n new_pwd2 = data . get ( \"<STR_LIT>\" ) \n if instance : \n if new_pwd != new_pwd2 : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n else : \n instance . password = make_password ( new_pwd ) \n instance . save ( ) \n return DetailResponse ( data = None , msg = \"<STR_LIT>\" ) \n else : \n return ErrorResponse ( msg = \"<STR_LIT>\" ) \n def list ( self , request , * args , ** kwargs ) : \n dept_id = request . query_params . get ( '<STR_LIT>' ) \n show_all = request . query_params . get ( '<STR_LIT>' ) \n if not dept_id : \n dept_id = '<STR_LIT>' \n if not show_all : \n show_all = <NUM_LIT> \n if int ( show_all ) : \n all_did = [ dept_id ] \n def inner ( did ) : \n sub = Dept . objects . filter ( parent_id = did ) \n if not sub . exists ( ) : \n return \n for i in sub : \n all_did . append ( i . pk ) \n inner ( i ) \n if dept_id != '<STR_LIT>' : \n inner ( dept_id ) \n searchs = [ \n Q ( ** { f + '<STR_LIT>' : i } ) \n for f in self . search_fields \n ] if ( i := request . query_params . get ( '<STR_LIT>' ) ) else [ ] \n q_obj = [ ] \n if searchs : \n q = searchs [ <NUM_LIT> ] \n for i in searchs [ <NUM_LIT> : ] : \n q |= i \n q_obj . append ( Q ( q ) ) \n queryset = Users . objects . filter ( * q_obj , dept_id__in = all_did ) \n else : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n else : \n queryset = self . filter_queryset ( self . get_queryset ( ) ) \n page = self . paginate_queryset ( queryset ) \n if page is not None : \n serializer = self . get_serializer ( page , many = True , request = request ) \n return self . get_paginated_response ( serializer . data ) \n serializer = self . get_serializer ( queryset , many = True , request = request ) \n return SuccessResponse ( data = serializer . data , msg = \"<STR_LIT>\" )"}, {"input": "from django . db import migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ] \n operations = [ \n migrations . RemoveField (", "output": "model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n ) , \n ]"}, {"input": "import configparser \n import torch \n import torch . nn as nn \n from transformers import AutoModel \n import configargparse \n from transformers import AutoTokenizer \n from scoreblocks . MSPLM . encoder import encode_documents \n from scoreblocks . MSPLM . plms import mainplm \n from torch . cuda . amp import autocast \n def _initialize_arguments ( p ) : \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' )", "output": "p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n p . add_section ( '<STR_LIT>' ) \n config_dict = { } \n for section in p . sections ( ) : \n section_dict = { } \n for option in p . options ( section ) : \n section_dict [ option ] = p . get ( section , option ) \n config_dict [ section ] = section_dict \n config_dict = config_dict [ '<STR_LIT>' ] \n print ( config_dict ) \n if torch . cuda . is_available ( ) and config_dict [ '<STR_LIT>' ] : \n config_dict [ '<STR_LIT>' ] = '<STR_LIT>' \n else : \n config_dict [ '<STR_LIT>' ] = '<STR_LIT>' \n return config_dict \n def init_weights ( m ) : \n if isinstance ( m , nn . Linear ) : \n torch . nn . init . xavier_uniform ( m . weight ) \n m . bias . data . fill_ ( <NUM_LIT> ) \n class model : \n def __init__ ( self ) : \n p = configparser . ConfigParser ( ) \n p . read ( \"<STR_LIT>\" , encoding = '<STR_LIT>' ) \n self . args = _initialize_arguments ( p ) \n print ( f\"<STR_LIT>\" ) \n self . tokenizer = AutoTokenizer . from_pretrained ( self . args [ '<STR_LIT>' ] ) \n self . prompt = int ( self . args [ \"<STR_LIT>\" ] [ <NUM_LIT> ] ) \n self . chunk_sizes = [ ] \n self . bert_batch_sizes = [ ] \n self . bert_regression_by_word_document = mainplm ( self . args ) \n self . bert_regression_by_word_document . load_state_dict ( torch . load ( '<STR_LIT>' ) ) \n self . bert_regression_by_word_document . to ( self . args [ '<STR_LIT>' ] ) \n self . bert_regression_by_word_document . eval ( ) \n self . plt_x = [ ] \n self . plt_train_qwk = [ ] \n self . plt_val_qwk = [ ] \n self . plt_test_qwk = [ ] \n self . best_val_qwk = <NUM_LIT> \n def getscore ( self , valdata ) : \n with torch . no_grad ( ) : \n target_scores = None \n doctok_token_indexes , doctok_token_indexes_slicenum = encode_documents ( \n valdata , self . tokenizer , max_input_length = <NUM_LIT> ) \n predictions = torch . empty ( ( doctok_token_indexes . shape [ <NUM_LIT> ] ) ) \n acculation_loss = <NUM_LIT> \n for i in range ( <NUM_LIT> , doctok_token_indexes . shape [ <NUM_LIT> ] , self . args [ '<STR_LIT>' ] ) : \n batch_doctok_token_indexes = doctok_token_indexes [ i : i + self . args [ '<STR_LIT>' ] ] . to ( \n device = self . args [ '<STR_LIT>' ] ) \n with autocast ( ) : \n batch_doctok_predictions = self . bert_regression_by_word_document ( batch_doctok_token_indexes , \n device = self . args [ '<STR_LIT>' ] ) \n batch_doctok_predictions = torch . squeeze ( batch_doctok_predictions ) \n batch_predictions = batch_doctok_predictions \n if len ( batch_predictions . shape ) == <NUM_LIT> : \n batch_predictions = torch . tensor ( [ batch_predictions ] , device = self . args [ '<STR_LIT>' ] ) \n predictions [ i : i + self . args [ '<STR_LIT>' ] ] = batch_predictions \n predictions = predictions . detach ( ) . numpy ( ) \n for i in range ( len ( predictions ) ) : \n if predictions [ i ] < <NUM_LIT> : \n predictions [ i ] = <NUM_LIT> \n elif predictions [ i ] > <NUM_LIT> : \n predictions [ i ] = <NUM_LIT> \n return predictions \n if __name__ == '<STR_LIT>' : \n model = model ( ) \n text = \n valdata = [ '<STR_LIT>' , '<STR_LIT>' , text ] \n print ( model . getscore ( valdata ) )"}, {"input": "import torch \n import time \n import pickle as pkl \n from torch . utils . data import DataLoader , Dataset , RandomSampler \n class HMERDataset ( Dataset ) :", "output": "def __init__ ( self , params , image_path , label_path , words , is_train = True ) : \n super ( HMERDataset , self ) . __init__ ( ) \n if image_path . endswith ( '<STR_LIT>' ) : \n with open ( image_path , '<STR_LIT>' ) as f : \n self . images = pkl . load ( f ) \n elif image_path . endswith ( '<STR_LIT>' ) : \n with open ( image_path , '<STR_LIT>' ) as f : \n lines = f . readlines ( ) \n self . images = { } \n print ( f'<STR_LIT>' ) \n for line in lines : \n name = line . strip ( ) \n print ( f'<STR_LIT>' ) \n start = time . time ( ) \n with open ( name , '<STR_LIT>' ) as f : \n images = pkl . load ( f ) \n self . images . update ( images ) \n print ( f'<STR_LIT>' ) \n with open ( label_path , '<STR_LIT>' ) as f : \n self . labels = f . readlines ( ) \n self . words = words \n self . is_train = is_train \n self . params = params \n def __len__ ( self ) : \n assert len ( self . images ) == len ( self . labels ) \n return len ( self . labels ) \n def __getitem__ ( self , idx ) : \n name , * labels = self . labels [ idx ] . strip ( ) . split ( ) \n name = name . split ( '<STR_LIT>' ) [ <NUM_LIT> ] if name . endswith ( '<STR_LIT>' ) else name \n image = self . images [ name ] \n image = torch . Tensor ( <NUM_LIT> - image ) / <NUM_LIT> \n image = image . unsqueeze ( <NUM_LIT> ) \n labels . append ( '<STR_LIT>' ) \n words = self . words . encode ( labels ) \n words = torch . LongTensor ( words ) \n return image , words \n def get_crohme_dataset ( params ) : \n words = Words ( params [ '<STR_LIT>' ] ) \n params [ '<STR_LIT>' ] = len ( words ) \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n train_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = True ) \n eval_dataset = HMERDataset ( params , params [ '<STR_LIT>' ] , params [ '<STR_LIT>' ] , words , is_train = False ) \n train_sampler = RandomSampler ( train_dataset ) \n eval_sampler = RandomSampler ( eval_dataset ) \n train_loader = DataLoader ( train_dataset , batch_size = params [ '<STR_LIT>' ] , sampler = train_sampler , \n num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) \n eval_loader = DataLoader ( eval_dataset , batch_size = <NUM_LIT> , sampler = eval_sampler , \n num_workers = params [ '<STR_LIT>' ] , collate_fn = collate_fn_dict [ params [ '<STR_LIT>' ] ] , pin_memory = True ) \n print ( f'<STR_LIT>' \n f'<STR_LIT>' ) \n return train_loader , eval_loader \n def collate_fn ( batch_images ) : \n max_width , max_height , max_length = <NUM_LIT> , <NUM_LIT> , <NUM_LIT> \n batch , channel = len ( batch_images ) , batch_images [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] \n proper_items = [ ] \n for item in batch_images : \n if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_width > <NUM_LIT> * <NUM_LIT> or item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] * max_height > <NUM_LIT> * <NUM_LIT> : \n continue \n max_height = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_height else max_height \n max_width = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_width else max_width \n max_length = item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] if item [ <NUM_LIT> ] . shape [ <NUM_LIT> ] > max_length else max_length \n proper_items . append ( item ) \n images , image_masks = torch . zeros ( ( len ( proper_items ) , channel , max_height , max_width ) ) , torch . zeros ( ( len ( proper_items ) , <NUM_LIT> , max_height , max_width ) ) \n labels , labels_masks = torch . zeros ( ( len ( proper_items ) , max_length ) ) . long ( ) , torch . zeros ( ( len ( proper_items ) , max_length ) ) \n for i in range ( len ( proper_items ) ) : \n _ , h , w = proper_items [ i ] [ <NUM_LIT> ] . shape \n images [ i ] [ : , : h , : w ] = proper_items [ i ] [ <NUM_LIT> ] \n image_masks [ i ] [ : , : h , : w ] = <NUM_LIT> \n l = proper_items [ i ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] \n labels [ i ] [ : l ] = proper_items [ i ] [ <NUM_LIT> ] \n labels_masks [ i ] [ : l ] = <NUM_LIT> \n return images , image_masks , labels , labels_masks \n class Words : \n def __init__ ( self , words_path ) : \n with open ( words_path ) as f : \n words = f . readlines ( ) \n print ( f'<STR_LIT>' ) \n self . words_dict = { words [ i ] . strip ( ) : i for i in range ( len ( words ) ) } \n self . words_index_dict = { i : words [ i ] . strip ( ) for i in range ( len ( words ) ) } \n def __len__ ( self ) : \n return len ( self . words_dict ) \n def encode ( self , labels ) : \n label_index = [ self . words_dict [ item ] for item in labels ] \n return label_index \n def decode ( self , label_index ) : \n label = '<STR_LIT>' . join ( [ self . words_index_dict [ int ( item ) ] for item in label_index ] ) \n return label \n collate_fn_dict = { \n '<STR_LIT>' : collate_fn \n }"}, {"input": "from django . db import models \n from django . utils import timezone \n class Product ( models . Model ) : \n name = models . CharField ( max_length = <NUM_LIT> , unique = True ) \n description = models . TextField ( )", "output": "price = models . DecimalField ( max_digits = <NUM_LIT> , decimal_places = <NUM_LIT> ) \n sku = models . CharField ( max_length = <NUM_LIT> , unique = True ) \n created_at = models . DateTimeField ( default = timezone . now ) \n @ property \n def slug ( self ) : \n return self . name . lower ( ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" )"}, {"input": "from functools import wraps \n from typing import List \n from django . core . exceptions import PermissionDenied \n from ninja import Router \n from ninja_crud . views import ( \n DeleteModelView , \n ListModelView , \n ReadModelView , \n UpdateModelView , \n ) \n from ninja_crud . viewsets import ModelViewSet \n from tests . test_app . models import Item , Tag \n from tests . test_app . schemas import ItemIn , ItemOut , OrderByFilterSchema , TagOut \n router = Router ( ) \n def user_is_collection_creator ( func ) : \n @ wraps ( func ) \n def wrapper ( request , * args , ** kwargs ) : \n item_id = getattr ( kwargs . get ( \"<STR_LIT>\" ) , \"<STR_LIT>\" , None ) \n item = Item . objects . get ( id = item_id ) \n if item . collection . created_by != request . auth : \n raise PermissionDenied ( ) \n return func ( request , * args , ** kwargs ) \n return wrapper", "output": "class ItemViewSet ( ModelViewSet ) : \n model = Item \n default_request_body = ItemIn \n default_response_body = ItemOut \n list_items = ListModelView ( \n query_parameters = OrderByFilterSchema , \n get_queryset = lambda request , path_parameters : Item . objects . get_queryset ( ) , \n ) \n read_item = ReadModelView ( \n read_model = lambda request , path_parameters , _ : Item . objects . get ( \n id = getattr ( path_parameters , \"<STR_LIT>\" , None ) \n ) , \n decorators = [ user_is_collection_creator ] , \n ) \n update_item = UpdateModelView ( \n pre_save = lambda request , instance : None , \n post_save = lambda request , instance : None , \n decorators = [ user_is_collection_creator ] , \n ) \n delete_item = DeleteModelView ( decorators = [ user_is_collection_creator ] ) \n list_tags = ListModelView ( \n path = \"<STR_LIT>\" , \n get_queryset = lambda request , path_parameters : Tag . objects . filter ( \n items__id = getattr ( path_parameters , \"<STR_LIT>\" , None ) \n ) , \n response_body = List [ TagOut ] , \n ) \n ItemViewSet . register_routes ( router )"}, {"input": "import subprocess \n from cappa . testing import CommandRunner \n from falco . utils import run_in_shell \n def makemigrations ( ) : \n subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) \n def migrate ( ) : \n subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , check = False ) \n def change_model_attribute ( django_project_dir ) : \n models_file = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" \n models_file_content = models_file . read_text ( ) \n models_file . write_text ( \n models_file_content + \"<STR_LIT>\" + \"<STR_LIT>\"", "output": ") \n def insert_a_post ( ) : \n from blog . models import Post \n Post . objects . create ( title = \"<STR_LIT>\" , content = \"<STR_LIT>\" ) \n def count_nbr_of_posts ( ) -> int : \n from blog . models import Post \n return Post . objects . all ( ) . count ( ) \n def count_migrations ( django_project_dir ) : \n migrations_folder = django_project_dir / \"<STR_LIT>\" / \"<STR_LIT>\" \n return len ( [ file for file in migrations_folder . iterdir ( ) if file . name . startswith ( \"<STR_LIT>\" ) ] ) \n def test_reset_migrations ( django_project , runner : CommandRunner , set_git_repo_to_clean ) : \n makemigrations ( ) \n migrate ( ) \n run_in_shell ( insert_a_post , eval_result = False ) \n count = run_in_shell ( count_nbr_of_posts , eval_result = True ) \n assert count == <NUM_LIT> \n assert count == <NUM_LIT> \n change_model_attribute ( django_project ) \n makemigrations ( ) \n migrate ( ) \n assert count_migrations ( django_project ) == <NUM_LIT> \n runner . invoke ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n assert count_migrations ( django_project ) == <NUM_LIT> \n count = run_in_shell ( count_nbr_of_posts , eval_result = True ) \n assert count == <NUM_LIT>"}, {"input": "from pathlib import Path \n DEBUG = True \n BASE_DIR = Path ( __file__ ) . resolve ( ) . parent . parent \n SECRET_KEY = \"<STR_LIT>\" \n ALLOWED_HOSTS = [ ] \n ROOT_URLCONF = \"<STR_LIT>\" \n STATIC_URL = \"<STR_LIT>\" \n WSGI_APPLICATION = \"<STR_LIT>\" \n DEFAULT_AUTO_FIELD = \"<STR_LIT>\" \n LANGUAGE_CODE = \"<STR_LIT>\" \n TIME_ZONE = \"<STR_LIT>\" \n USE_I18N = True \n USE_TZ = True \n INSTALLED_APPS = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n MIDDLEWARE = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n TEMPLATES = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n } , \n } , \n ] \n DATABASES = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : BASE_DIR / \"<STR_LIT>\" , \n } \n } \n PASSWORD_HASHERS = [ \"<STR_LIT>\" ]"}, {"input": "import json \n from channels . generic . websocket import AsyncWebsocketConsumer \n from delphic . utils . collections import load_collection_model \n from delphic . utils . paths import extract_connection_id \n class CollectionQueryConsumer ( AsyncWebsocketConsumer ) : \n async def connect ( self ) : \n print ( \"<STR_LIT>\" ) \n try : \n self . collection_id = extract_connection_id ( self . scope [ \"<STR_LIT>\" ] ) \n print ( f\"<STR_LIT>\" ) \n self . index = await load_collection_model ( self . collection_id ) \n print ( f\"<STR_LIT>\" ) \n await self . accept ( ) \n print ( \"<STR_LIT>\" ) \n except ValueError as e : \n print ( f\"<STR_LIT>\" ) \n await self . accept ( ) \n await self . close ( code = <NUM_LIT> ) \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n async def disconnect ( self , close_code ) : \n pass \n async def receive ( self , text_data ) : \n text_data_json = json . loads ( text_data ) \n print ( f\"<STR_LIT>\" ) \n if self . index is not None : \n query_str = text_data_json [ \"<STR_LIT>\" ] \n modified_query_str = \n response = self . index . query ( modified_query_str )", "output": "markdown_response = f\"<STR_LIT>\" \n if response . source_nodes : \n markdown_sources = f\"<STR_LIT>\" \n else : \n markdown_sources = \"<STR_LIT>\" \n formatted_response = f\"<STR_LIT>\" \n await self . send ( json . dumps ( { \"<STR_LIT>\" : formatted_response } , indent = <NUM_LIT> ) ) \n else : \n await self . send ( \n json . dumps ( { \"<STR_LIT>\" : \"<STR_LIT>\" } , indent = <NUM_LIT> ) \n )"}, {"input": "from http import HTTPStatus \n from typing import Callable , List , Optional , TypeVar , Union , cast \n from django . http import HttpResponse \n from django . test import TestCase \n from ninja_crud . testing . core . components import ( \n Headers , \n PathParameters , \n Payloads , \n QueryParameters , \n ) \n T = TypeVar ( \"<STR_LIT>\" ) \n TestCaseType = TypeVar ( \"<STR_LIT>\" , bound = TestCase ) \n ArgOrCallable = Union [ T , property , Callable [ [ TestCaseType ] , T ] ] \n CompletionCallback = Callable [ [ HttpResponse , dict , dict , dict , dict ] , None ] \n class ViewTestManager : \n def __init__ ( \n self , \n simulate_request : Callable [ [ dict , dict , dict , dict ] , HttpResponse ] , \n path_parameters : Optional [ ArgOrCallable [ PathParameters , TestCaseType ] ] = None , \n query_parameters : Optional [ ArgOrCallable [ QueryParameters , TestCaseType ] ] = None , \n headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , \n payloads : Optional [ ArgOrCallable [ Payloads , TestCaseType ] ] = None , \n ) -> None : \n self . simulate_request = simulate_request \n self . path_parameters = path_parameters or PathParameters ( ok = { } ) \n self . query_parameters = query_parameters or QueryParameters ( ok = { } ) \n self . headers = headers or Headers ( ok = { } ) \n self . payloads = payloads or Payloads ( ok = { } ) \n @ staticmethod \n def _get_arg_or_callable ( \n test_case : TestCase , params : ArgOrCallable [ T , TestCaseType ] \n ) -> T : \n if callable ( params ) : \n return params ( test_case ) \n elif isinstance ( params , property ) : \n return cast ( Callable , params . fget ) ( test_case ) \n else : \n return params \n def get_path_parameters ( self , test_case : TestCase ) -> PathParameters : \n path_parameters = self . _get_arg_or_callable ( test_case , self . path_parameters ) \n if not isinstance ( path_parameters , PathParameters ) : \n raise TypeError ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n return path_parameters \n def get_query_parameters ( self , test_case : TestCase ) -> QueryParameters : \n query_parameters = self . _get_arg_or_callable ( test_case , self . query_parameters ) \n if not isinstance ( query_parameters , QueryParameters ) : \n raise TypeError ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n return query_parameters \n def get_headers ( self , test_case : TestCase ) -> Headers : \n headers = self . _get_arg_or_callable ( test_case , self . headers ) \n if not isinstance ( headers , Headers ) : \n raise TypeError ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n return headers \n def get_payloads ( self , test_case : TestCase ) -> Payloads : \n payloads = self . _get_arg_or_callable ( test_case , self . payloads ) \n if not isinstance ( payloads , Payloads ) : \n raise TypeError ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n return payloads \n @ staticmethod \n def wrap_completion_with_status_check ( \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus , \n ) -> CompletionCallback : \n def on_completion_with_status_check ( \n response : HttpResponse , \n path_parameters : dict , \n query_parameters : dict , \n headers : dict , \n payload : dict , \n ) : \n test_case . assertEqual ( response . status_code , status ) \n on_completion ( response , path_parameters , query_parameters , headers , payload ) \n return on_completion_with_status_check \n def run_combinatorial_tests ( \n self , \n test_case : TestCase , \n path_parameters_list : List [ dict ] , \n query_parameters_list : List [ dict ] , \n headers_list : List [ dict ] , \n payload_list : List [ dict ] , \n on_completion : CompletionCallback , \n ) : \n for path_parameters in path_parameters_list : \n for query_parameters in query_parameters_list : \n for headers in headers_list : \n for payload in payload_list : \n with test_case . subTest ( \n path_parameters = path_parameters , \n query_parameters = query_parameters , \n headers = headers , \n payload = payload , \n ) : \n response = self . simulate_request ( \n path_parameters , query_parameters , headers , payload \n ) \n on_completion ( \n response , \n path_parameters , \n query_parameters , \n headers , \n payload , \n ) \n def test_view_ok ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . OK , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . ok , \n payload_list = payloads . ok , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_payloads_bad_request ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . BAD_REQUEST , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if payloads . bad_request is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . ok , \n payload_list = payloads . bad_request , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_payloads_conflict ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . CONFLICT , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if payloads . conflict is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . ok , \n payload_list = payloads . conflict , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_query_parameters_bad_request ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback ,", "output": "status : HTTPStatus = HTTPStatus . BAD_REQUEST , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if query_parameters . bad_request is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . bad_request , \n headers_list = headers . ok , \n payload_list = payloads . ok , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_headers_unauthorized ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . UNAUTHORIZED , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if headers . unauthorized is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . unauthorized , \n payload_list = payloads . ok , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_headers_forbidden ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . FORBIDDEN , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if headers . forbidden is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . ok , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . forbidden , \n payload_list = payloads . ok , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n ) \n def test_view_path_parameters_not_found ( \n self , \n test_case : TestCase , \n on_completion : CompletionCallback , \n status : HTTPStatus = HTTPStatus . NOT_FOUND , \n ) : \n path_parameters = self . get_path_parameters ( test_case ) \n query_parameters = self . get_query_parameters ( test_case ) \n headers = self . get_headers ( test_case ) \n payloads = self . get_payloads ( test_case ) \n if path_parameters . not_found is None : \n test_case . skipTest ( reason = \"<STR_LIT>\" ) \n else : \n self . run_combinatorial_tests ( \n test_case = test_case , \n path_parameters_list = path_parameters . not_found , \n query_parameters_list = query_parameters . ok , \n headers_list = headers . ok , \n payload_list = payloads . ok , \n on_completion = self . wrap_completion_with_status_check ( \n test_case , on_completion = on_completion , status = status \n ) , \n )"}, {"input": "import django . contrib . postgres . fields \n from django . db import models , migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] \n operations = [ \n migrations . AlterField (", "output": "model_name = \"<STR_LIT>\" , \n name = \"<STR_LIT>\" , \n field = django . contrib . postgres . fields . ArrayField ( \n base_field = models . CharField ( max_length = <NUM_LIT> ) , null = True , size = None \n ) , \n ) , \n ]"}, {"input": "import warnings \n from collections . abc import Mapping \n from dataclasses import dataclass \n from typing import NotRequired , Required , TypedDict , cast \n from django . conf import settings \n from django . core . exceptions import ImproperlyConfigured \n from django . utils . module_loading import import_string \n from . . text_splitters . langchain import LangchainRecursiveCharacterTextSplitter \n from . . text_splitters . length import NaiveTextSplitterCalculator \n from . . types import TextSplitterLengthCalculatorProtocol , TextSplitterProtocol \n from . . utils import deprecation \n from . base import AIBackend , BackendFeature , BaseAIBackendConfigSettings \n class TextSplittingSettingsDict ( TypedDict ) : \n SPLITTER_CLASS : NotRequired [ str ] \n SPLITTER_LENGTH_CALCULATOR_CLASS : NotRequired [ str ] \n class InvalidAIBackendError ( ImproperlyConfigured ) : \n def __init__ ( self , alias ) : \n super ( ) . __init__ ( f\"<STR_LIT>\" ) \n class AIBackendSettingsDict ( TypedDict ) : \n CONFIG : Required [ BaseAIBackendConfigSettings ] \n CLASS : Required [ str ] \n TEXT_SPLITTING : NotRequired [ TextSplittingSettingsDict ] \n def get_ai_backends_settings ( ) -> Mapping [ str , AIBackendSettingsDict ] : \n try : \n return settings . WAGTAIL_AI [ \"<STR_LIT>\" ] \n except AttributeError : \n pass \n try : \n legacy_settings = settings . WAGTAIL_AI_BACKENDS \n except AttributeError : \n pass \n else : \n warnings . warn ( \n '<STR_LIT>' , \n deprecation . WagtailAISettingsDeprecationWarning , \n stacklevel = <NUM_LIT> , \n ) \n return legacy_settings \n return { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } \n } \n def _validate_backend_settings ( * , settings : AIBackendSettingsDict , alias : str ) -> None : \n if \"<STR_LIT>\" not in settings : \n raise ImproperlyConfigured ( \n '<STR_LIT>' \n ) \n if not isinstance ( settings [ \"<STR_LIT>\" ] , Mapping ) : \n raise ImproperlyConfigured ( \n '<STR_LIT>' \n ) \n if \"<STR_LIT>\" not in settings [ \"<STR_LIT>\" ] : \n raise ImproperlyConfigured ( \n '<STR_LIT>' \n ) \n def get_ai_backend_settings ( alias : str ) -> AIBackendSettingsDict : \n backends_settings = get_ai_backends_settings ( ) \n try : \n return backends_settings [ alias ] \n except ( KeyError , ImportError ) as e : \n raise InvalidAIBackendError ( alias ) from e \n _validate_backend_settings ( settings = backends_settings , alias = alias ) \n def _get_default_text_splitter_class ( ) -> type [ LangchainRecursiveCharacterTextSplitter ] : \n return LangchainRecursiveCharacterTextSplitter \n def _get_default_text_splitter_length_class ( ) -> type [ NaiveTextSplitterCalculator ] :", "output": "return NaiveTextSplitterCalculator \n @ dataclass ( kw_only = True ) \n class _TextSplitterConfig : \n splitter_class : type [ TextSplitterProtocol ] \n splitter_length_calculator_class : type [ TextSplitterLengthCalculatorProtocol ] \n def _get_text_splitter_config ( \n * , backend_alias : str , config : TextSplittingSettingsDict \n ) -> _TextSplitterConfig : \n splitter_class_path = config . get ( \"<STR_LIT>\" ) \n length_calculator_class_path = config . get ( \"<STR_LIT>\" ) \n if splitter_class_path is not None : \n try : \n splitter_class = cast ( \n type [ TextSplitterProtocol ] , import_string ( splitter_class_path ) \n ) \n except ImportError as e : \n raise ImproperlyConfigured ( \n f'<STR_LIT>' \n ) from e \n else : \n splitter_class = _get_default_text_splitter_class ( ) \n if length_calculator_class_path is not None : \n try : \n length_calculator_class = cast ( \n type [ TextSplitterLengthCalculatorProtocol ] , \n import_string ( length_calculator_class_path ) , \n ) \n except ImportError as e : \n raise ImproperlyConfigured ( \n f'<STR_LIT>' \n ) from e \n else : \n length_calculator_class = _get_default_text_splitter_length_class ( ) \n return _TextSplitterConfig ( \n splitter_class = splitter_class , \n splitter_length_calculator_class = length_calculator_class , \n ) \n def get_ai_backend ( alias : str ) -> AIBackend : \n backend_dict = get_ai_backend_settings ( alias ) \n if \"<STR_LIT>\" not in backend_dict : \n raise ImproperlyConfigured ( \n f'<STR_LIT>' \n ) \n try : \n ai_backend_cls = cast ( type [ AIBackend ] , import_string ( backend_dict [ \"<STR_LIT>\" ] ) ) \n except ImportError as e : \n raise InvalidAIBackendError ( \n f'<STR_LIT>' \n ) from e \n backend_settings = backend_dict [ \"<STR_LIT>\" ] \n text_splitting = _get_text_splitter_config ( \n backend_alias = alias , \n config = backend_dict . get ( \"<STR_LIT>\" , { } ) , \n ) \n config = ai_backend_cls . config_cls . from_settings ( \n backend_settings , \n text_splitter_class = text_splitting . splitter_class , \n text_splitter_length_calculator_class = text_splitting . splitter_length_calculator_class , \n ) \n return ai_backend_cls ( config = config ) \n class BackendNotFound ( Exception ) : \n pass \n def get_backend ( feature : BackendFeature = BackendFeature . TEXT_COMPLETION ) -> AIBackend : \n match feature : \n case BackendFeature . TEXT_COMPLETION : \n alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n case BackendFeature . IMAGE_DESCRIPTION : \n alias = settings . WAGTAIL_AI . get ( \"<STR_LIT>\" ) \n case _ : \n alias = None \n if alias is None : \n raise BackendNotFound ( f\"<STR_LIT>\" ) \n return get_ai_backend ( alias )"}, {"input": "from django . http import JsonResponse \n import json \n import time \n import datetime \n import hashlib \n from django . contrib import auth \n from django . forms . models import model_to_dict \n from api . models import RustDeskToken , UserProfile , RustDeskTag , RustDeskPeer , RustDesDevice \n from django . db . models import Q \n import copy \n from . views_front import * \n from django . utils . translation import gettext as _ \n def login ( request ) : \n result = { } \n if request . method == '<STR_LIT>' : \n result [ '<STR_LIT>' ] = _ ( '<STR_LIT>' )", "output": "return JsonResponse ( result ) \n data = json . loads ( request . body . decode ( ) ) \n username = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n password = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n rid = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n uuid = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n autoLogin = data . get ( '<STR_LIT>' , True ) \n rtype = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n deviceInfo = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n user = auth . authenticate ( username = username , password = password ) \n if not user : \n result [ '<STR_LIT>' ] = _ ( '<STR_LIT>' ) \n return JsonResponse ( result ) \n user . rid = rid \n user . uuid = uuid \n user . autoLogin = autoLogin \n user . rtype = rtype \n user . deviceInfo = json . dumps ( deviceInfo ) \n user . save ( ) \n token = RustDeskToken . objects . filter ( Q ( uid = user . id ) & Q ( username = user . username ) & Q ( rid = user . rid ) ) . first ( ) \n if token : \n now_t = datetime . datetime . now ( ) \n nums = ( now_t - token . create_time ) . seconds if now_t > token . create_time else <NUM_LIT> \n if nums >= EFFECTIVE_SECONDS : \n token . delete ( ) \n token = None \n if not token : \n token = RustDeskToken ( \n username = user . username , \n uid = user . id , \n uuid = user . uuid , \n rid = user . rid , \n access_token = getStrMd5 ( str ( time . time ( ) ) + salt ) \n ) \n token . save ( ) \n result [ '<STR_LIT>' ] = token . access_token \n result [ '<STR_LIT>' ] = '<STR_LIT>' \n result [ '<STR_LIT>' ] = { '<STR_LIT>' : user . username } \n return JsonResponse ( result ) \n def logout ( request ) : \n if request . method == '<STR_LIT>' : \n result = { '<STR_LIT>' : _ ( '<STR_LIT>' ) } \n return JsonResponse ( result ) \n data = json . loads ( request . body . decode ( ) ) \n rid = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n uuid = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n user = UserProfile . objects . filter ( Q ( rid = rid ) & Q ( uuid = uuid ) ) . first ( ) \n if not user : \n result = { '<STR_LIT>' : _ ( '<STR_LIT>' ) } \n return JsonResponse ( result ) \n token = RustDeskToken . objects . filter ( Q ( uid = user . id ) & Q ( rid = user . rid ) ) . first ( ) \n if token : \n token . delete ( ) \n result = { '<STR_LIT>' : <NUM_LIT> } \n return JsonResponse ( result ) \n def currentUser ( request ) : \n result = { } \n if request . method == '<STR_LIT>' : \n result [ '<STR_LIT>' ] = _ ( '<STR_LIT>' ) \n return JsonResponse ( result ) \n postdata = json . loads ( request . body ) \n rid = postdata . get ( '<STR_LIT>' , '<STR_LIT>' ) \n uuid = postdata . get ( '<STR_LIT>' , '<STR_LIT>' ) \n access_token = request . META . get ( '<STR_LIT>' , '<STR_LIT>' ) \n access_token = access_token . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n token = RustDeskToken . objects . filter ( Q ( access_token = access_token ) ) . first ( ) \n user = None \n if token : \n user = UserProfile . objects . filter ( Q ( id = token . uid ) ) . first ( ) \n if user : \n if token : \n result [ '<STR_LIT>' ] = token . access_token \n result [ '<STR_LIT>' ] = '<STR_LIT>' \n result [ '<STR_LIT>' ] = user . username \n return JsonResponse ( result ) \n def ab ( request ) : \n access_token = request . META . get ( '<STR_LIT>' , '<STR_LIT>' ) \n access_token = access_token . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n token = RustDeskToken . objects . filter ( Q ( access_token = access_token ) ) . first ( ) \n if not token : \n result = { '<STR_LIT>' : _ ( '<STR_LIT>' ) } \n return JsonResponse ( result ) \n if request . method == '<STR_LIT>' : \n result = { } \n uid = token . uid \n tags = RustDeskTag . objects . filter ( Q ( uid = uid ) ) \n tag_names = [ ] \n tag_colors = { } \n if tags : \n tag_names = [ str ( x . tag_name ) for x in tags ] \n tag_colors = { str ( x . tag_name ) : int ( x . tag_color ) for x in tags if x . tag_color != '<STR_LIT>' } \n peers_result = [ ] \n peers = RustDeskPeer . objects . filter ( Q ( uid = uid ) ) \n if peers : \n for peer in peers : \n tmp = { \n '<STR_LIT>' : peer . rid , \n '<STR_LIT>' : peer . username , \n '<STR_LIT>' : peer . hostname , \n '<STR_LIT>' : peer . alias , \n '<STR_LIT>' : peer . platform , \n '<STR_LIT>' : peer . tags . split ( '<STR_LIT>' ) , \n '<STR_LIT>' : peer . rhash , \n } \n peers_result . append ( tmp ) \n result [ '<STR_LIT>' ] = datetime . datetime . now ( ) \n result [ '<STR_LIT>' ] = { \n '<STR_LIT>' : tag_names , \n '<STR_LIT>' : peers_result , \n '<STR_LIT>' : json . dumps ( tag_colors ) \n } \n result [ '<STR_LIT>' ] = json . dumps ( result [ '<STR_LIT>' ] ) \n return JsonResponse ( result ) \n else : \n postdata = json . loads ( request . body . decode ( ) ) \n data = postdata . get ( '<STR_LIT>' , '<STR_LIT>' ) \n data = { } if data == '<STR_LIT>' else json . loads ( data ) \n tagnames = data . get ( '<STR_LIT>' , [ ] ) \n tag_colors = data . get ( '<STR_LIT>' , '<STR_LIT>' ) \n tag_colors = { } if tag_colors == '<STR_LIT>' else json . loads ( tag_colors ) \n peers = data . get ( '<STR_LIT>' , [ ] ) \n if tagnames : \n RustDeskTag . objects . filter ( uid = token . uid ) . delete ( ) \n newlist = [ ] \n for name in tagnames : \n tag = RustDeskTag ( \n uid = token . uid , \n tag_name = name , \n tag_color = tag_colors . get ( name , '<STR_LIT>' ) \n ) \n newlist . append ( tag ) \n RustDeskTag . objects . bulk_create ( newlist ) \n if peers : \n RustDeskPeer . objects . filter ( uid = token . uid ) . delete ( ) \n newlist = [ ] \n for one in peers : \n peer = RustDeskPeer ( \n uid = token . uid , \n rid = one [ '<STR_LIT>' ] , \n username = one [ '<STR_LIT>' ] , \n hostname = one [ '<STR_LIT>' ] , \n alias = one [ '<STR_LIT>' ] , \n platform = one [ '<STR_LIT>' ] , \n tags = '<STR_LIT>' . join ( one [ '<STR_LIT>' ] ) , \n rhash = one [ '<STR_LIT>' ] , \n ) \n newlist . append ( peer ) \n RustDeskPeer . objects . bulk_create ( newlist ) \n result = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : _ ( '<STR_LIT>' ) \n } \n return JsonResponse ( result ) \n def ab_get ( request ) : \n request . method = '<STR_LIT>' \n return ab ( request ) \n def sysinfo ( request ) : \n result = { } \n if request . method == '<STR_LIT>' : \n result [ '<STR_LIT>' ] = _ ( '<STR_LIT>' ) \n return JsonResponse ( result ) \n postdata = json . loads ( request . body ) \n device = RustDesDevice . objects . filter ( Q ( rid = postdata [ '<STR_LIT>' ] ) & Q ( uuid = postdata [ '<STR_LIT>' ] ) ) . first ( ) \n if not device : \n device = RustDesDevice ( \n rid = postdata [ '<STR_LIT>' ] , \n cpu = postdata [ '<STR_LIT>' ] , \n hostname = postdata [ '<STR_LIT>' ] , \n memory = postdata [ '<STR_LIT>' ] , \n os = postdata [ '<STR_LIT>' ] , \n username = postdata . get ( '<STR_LIT>' , '<STR_LIT>' ) , \n uuid = postdata [ '<STR_LIT>' ] , \n version = postdata [ '<STR_LIT>' ] , \n ) \n device . save ( ) \n else : \n postdata2 = copy . copy ( postdata ) \n postdata2 [ '<STR_LIT>' ] = postdata2 [ '<STR_LIT>' ] \n postdata2 . pop ( '<STR_LIT>' ) \n RustDesDevice . objects . filter ( Q ( rid = postdata [ '<STR_LIT>' ] ) & Q ( uuid = postdata [ '<STR_LIT>' ] ) ) . update ( ** postdata2 ) \n result [ '<STR_LIT>' ] = '<STR_LIT>' \n return JsonResponse ( result ) \n def heartbeat ( request ) : \n postdata = json . loads ( request . body ) \n device = RustDesDevice . objects . filter ( Q ( rid = postdata [ '<STR_LIT>' ] ) & Q ( uuid = postdata [ '<STR_LIT>' ] ) ) . first ( ) \n if device : \n device . save ( ) \n create_time = datetime . datetime . now ( ) + datetime . timedelta ( seconds = EFFECTIVE_SECONDS ) \n RustDeskToken . objects . filter ( Q ( rid = postdata [ '<STR_LIT>' ] ) & Q ( uuid = postdata [ '<STR_LIT>' ] ) ) . update ( create_time = create_time ) \n result = { } \n result [ '<STR_LIT>' ] = _ ( '<STR_LIT>' ) \n return JsonResponse ( result ) \n def users ( request ) : \n result = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : _ ( '<STR_LIT>' ) \n } \n return JsonResponse ( result ) \n def peers ( request ) : \n result = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' \n } \n return JsonResponse ( result )"}, {"input": "import http \n import json \n from typing import Optional , Type , cast \n import django . http \n import django . test \n import django . utils . http \n from ninja import Schema \n from ninja_crud . testing . core import ArgOrCallable , TestCaseType , ViewTestManager \n from ninja_crud . testing . core . components import Headers , PathParameters \n from ninja_crud . testing . views import AbstractModelViewTest \n from ninja_crud . views import ReadModelView \n class ReadModelViewTest ( AbstractModelViewTest ) : \n model_view : ReadModelView \n def __init__ ( \n self , \n path_parameters : ArgOrCallable [ PathParameters , TestCaseType ] , \n headers : Optional [ ArgOrCallable [ Headers , TestCaseType ] ] = None , \n ) -> None : \n super ( ) . __init__ ( model_view_class = ReadModelView ) \n self . view_test_manager = ViewTestManager ( \n simulate_request = self . simulate_request , \n path_parameters = path_parameters , \n headers = headers , \n ) \n def on_successful_request ( \n self , \n response : django . http . HttpResponse , \n path_parameters : dict , \n query_parameters : dict , \n headers : dict , \n payload : dict , \n ) : \n actual_output = json . loads ( response . content ) \n expected_output = self . _get_expected_output ( \n response = response , \n path_parameters = path_parameters , \n query_parameters = query_parameters , \n ) \n self . model_viewset_test_case . assertDictEqual ( actual_output , expected_output ) \n def _get_expected_output ( \n self , \n response : django . http . HttpResponse , \n path_parameters : dict , \n query_parameters : dict , \n ) -> dict : \n path_parameters_schema : Optional [ Schema ] = ( \n self . model_view . path_parameters ( ** path_parameters )", "output": "if self . model_view . path_parameters \n else None \n ) \n query_parameters_schema = ( \n self . model_view . query_parameters ( ** query_parameters ) \n if self . model_view . query_parameters \n else None \n ) \n instance = self . model_view . read_model ( \n getattr ( response , \"<STR_LIT>\" , None ) , \n path_parameters_schema , \n query_parameters_schema , \n ) \n schema = cast ( Type [ Schema ] , self . model_view . response_body ) . from_orm ( instance ) \n return json . loads ( schema . json ( ) ) \n def on_failed_request ( \n self , \n response : django . http . HttpResponse , \n path_parameters : dict , \n query_parameters : dict , \n headers : dict , \n payload : dict , \n ) : \n pass \n @ django . test . tag ( \"<STR_LIT>\" ) \n def test_read_model_ok ( self ) : \n self . view_test_manager . test_view_ok ( \n test_case = self . model_viewset_test_case , \n on_completion = self . on_successful_request , \n status = http . HTTPStatus . OK , \n ) \n @ django . test . tag ( \"<STR_LIT>\" ) \n def test_read_model_headers_unauthorized ( self ) : \n self . view_test_manager . test_view_headers_unauthorized ( \n test_case = self . model_viewset_test_case , \n on_completion = self . on_failed_request , \n ) \n @ django . test . tag ( \"<STR_LIT>\" ) \n def test_read_model_headers_forbidden ( self ) : \n self . view_test_manager . test_view_headers_forbidden ( \n test_case = self . model_viewset_test_case , \n on_completion = self . on_failed_request , \n ) \n @ django . test . tag ( \"<STR_LIT>\" ) \n def test_read_model_path_parameters_not_found ( self ) : \n self . view_test_manager . test_view_path_parameters_not_found ( \n test_case = self . model_viewset_test_case , \n on_completion = self . on_failed_request , \n )"}, {"input": "from dvadmin . system . models import LoginLog \n from dvadmin . utils . serializers import CustomModelSerializer \n from dvadmin . utils . viewset import CustomModelViewSet \n class LoginLogSerializer ( CustomModelSerializer ) : \n class Meta : \n model = LoginLog \n fields = \"<STR_LIT>\" \n read_only_fields = [ \"<STR_LIT>\" ] \n class LoginLogViewSet ( CustomModelViewSet ) : \n queryset = LoginLog . objects . all ( ) \n serializer_class = LoginLogSerializer", "output": "extra_filter_class = [ ]"}, {"input": "import requests , datetime , os \n from utils . general import headers , cht_to_chs \n def get_epgs_4gtv ( channel , channel_id , dt , func_arg ) : \n epgs = [ ] \n msg = '<STR_LIT>' \n success = <NUM_LIT> \n url = '<STR_LIT>' % ( channel_id ) \n try : \n res = requests . get ( url , headers = headers , timeout = <NUM_LIT> ) \n res . encoding = '<STR_LIT>' \n res_json = res . json ( ) \n for j in res_json : \n title = j [ '<STR_LIT>' ] \n start_date = j [ '<STR_LIT>' ] \n start_time = j [ '<STR_LIT>' ] \n end_date = j [ '<STR_LIT>' ] \n end_time = j [ '<STR_LIT>' ] \n starttime = datetime . datetime . strptime ( '<STR_LIT>' % ( start_date , start_time ) , '<STR_LIT>' ) \n endtime = datetime . datetime . strptime ( '<STR_LIT>' % ( end_date , end_time ) , '<STR_LIT>' ) \n if starttime . date ( ) < dt : \n continue \n epg = { '<STR_LIT>' : channel . id , \n '<STR_LIT>' : starttime , \n '<STR_LIT>' : endtime , \n '<STR_LIT>' : title , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : starttime . date ( ) , \n } \n epgs . append ( epg ) \n epglen = len ( epgs ) \n except Exception as e : \n success = <NUM_LIT> \n spidername = os . path . basename ( __file__ ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n msg = '<STR_LIT>' % ( spidername , e ) \n ret = { \n '<STR_LIT>' : success , \n '<STR_LIT>' : epgs , \n '<STR_LIT>' : msg , \n '<STR_LIT>' : starttime . date ( ) if '<STR_LIT>' in dir ( ) else dt , \n '<STR_LIT>' : <NUM_LIT> , \n } \n return ret \n def get_channels_4gtv ( ) : \n url = '<STR_LIT>' \n headers . update ( { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : \"<STR_LIT>\" , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } ) \n res = requests . get ( url , headers = headers ) \n res . encoding = '<STR_LIT>' \n js = res . json ( ) [ '<STR_LIT>' ] \n channels = [ ] \n for j in js : \n id = str ( j [ '<STR_LIT>' ] ) \n type = j [ '<STR_LIT>' ] \n name = cht_to_chs ( j [ '<STR_LIT>' ] ) \n gtvid = j [ '<STR_LIT>' ] \n logo = j [ '<STR_LIT>' ] \n desc = j [ '<STR_LIT>' ] if '<STR_LIT>' in j else '<STR_LIT>' \n all = [ name , gtvid , logo ] \n channel = { \n '<STR_LIT>' : name , \n '<STR_LIT>' : [ gtvid ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : logo , \n '<STR_LIT>' : desc , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n channels . append ( channel )", "output": "return channels"}, {"input": "import pytest \n from freezegun import freeze_time \n from django_webhook . tasks import fire_webhook \n from django_webhook . test_factories import ( \n WebhookFactory , \n WebhookSecretFactory , \n WebhookTopicFactory , \n ) \n pytestmark = pytest . mark . django_db \n @ freeze_time ( \"<STR_LIT>\" ) \n def test_fire ( responses ) : \n webhook = WebhookFactory ( \n secrets__token = \"<STR_LIT>\" , \n topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , \n ) \n responses . post ( webhook . url ) \n fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) \n assert len ( responses . calls ) == <NUM_LIT> \n req = responses . calls [ <NUM_LIT> ] . request \n assert req . body == b'<STR_LIT>' \n h = req . headers \n assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" \n assert ( \n h [ \"<STR_LIT>\" ] \n == \"<STR_LIT>\" \n )", "output": "assert h [ \"<STR_LIT>\" ] == \"<STR_LIT>\" \n def test_does_not_fire_inactive ( responses ) : \n webhook = WebhookFactory ( active = False ) \n fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) \n assert len ( responses . calls ) == <NUM_LIT> \n @ freeze_time ( \"<STR_LIT>\" ) \n def test_multiple_signatures ( responses ) : \n webhook = WebhookFactory ( \n topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] , secrets = [ ] \n ) \n WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) \n WebhookSecretFactory ( webhook = webhook , token = \"<STR_LIT>\" ) \n responses . post ( webhook . url ) \n fire_webhook . delay ( webhook . id , payload = dict ( hello = \"<STR_LIT>\" ) ) \n assert len ( responses . calls ) == <NUM_LIT> \n h = responses . calls [ <NUM_LIT> ] . request . headers \n assert ( \n h [ \"<STR_LIT>\" ] \n == \"<STR_LIT>\" \n )"}, {"input": "from django . db import migrations \n class Migration ( migrations . Migration ) : \n dependencies = [ \n ( '<STR_LIT>' , '<STR_LIT>' ) , \n ]", "output": "operations = [ \n migrations . RemoveField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n ) , \n migrations . RemoveField ( \n model_name = '<STR_LIT>' , \n name = '<STR_LIT>' , \n ) , \n ]"}, {"input": "from django . contrib import admin \n from django . forms import BooleanField \n from django . forms . widgets import CheckboxInput \n from . models import ApiKey \n @ admin . register ( ApiKey ) \n class ApiKeyAdmin ( admin . ModelAdmin ) : \n list_display = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' )", "output": "formfield_overrides = { \n BooleanField : { '<STR_LIT>' : CheckboxInput } , \n }"}, {"input": "from django . contrib import admin \n from django . contrib . auth import admin as auth_admin \n from django . contrib . auth import get_user_model \n from django . utils . translation import gettext_lazy as _ \n from delphic . users . forms import UserAdminChangeForm , UserAdminCreationForm \n User = get_user_model ( ) \n @ admin . register ( User ) \n class UserAdmin ( auth_admin . UserAdmin ) : \n form = UserAdminChangeForm", "output": "add_form = UserAdminCreationForm \n fieldsets = ( \n ( None , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , \n ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , \n ( \n _ ( \"<STR_LIT>\" ) , \n { \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n } , \n ) , \n ( _ ( \"<STR_LIT>\" ) , { \"<STR_LIT>\" : ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } ) , \n ) \n list_display = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n search_fields = [ \"<STR_LIT>\" ]"}, {"input": "import django . contrib . sites . models \n from django . contrib . sites . models import _simple_domain_name_validator \n from django . db import migrations , models \n class Migration ( migrations . Migration ) : \n dependencies = [ ]", "output": "operations = [ \n migrations . CreateModel ( \n name = \"<STR_LIT>\" , \n fields = [ \n ( \n \"<STR_LIT>\" , \n models . AutoField ( \n verbose_name = \"<STR_LIT>\" , \n serialize = False , \n auto_created = True , \n primary_key = True , \n ) , \n ) , \n ( \n \"<STR_LIT>\" , \n models . CharField ( \n max_length = <NUM_LIT> , \n verbose_name = \"<STR_LIT>\" , \n validators = [ _simple_domain_name_validator ] , \n ) , \n ) , \n ( \"<STR_LIT>\" , models . CharField ( max_length = <NUM_LIT> , verbose_name = \"<STR_LIT>\" ) ) , \n ] , \n options = { \n \"<STR_LIT>\" : ( \"<STR_LIT>\" , ) , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n bases = ( models . Model , ) , \n managers = [ ( \"<STR_LIT>\" , django . contrib . sites . models . SiteManager ( ) ) ] , \n ) \n ]"}, {"input": "from django import forms \n from django . core . exceptions import ValidationError \n from django . utils . translation import gettext_lazy as _ \n from wagtail . admin . staticfiles import versioned_static \n from wagtail . images . forms import BaseImageForm \n class PromptTextField ( forms . CharField ) : \n default_error_messages = { \n \"<STR_LIT>\" : _ ( \n \"<STR_LIT>\" \n ) , \n } \n class PromptUUIDField ( forms . UUIDField ) : \n default_error_messages = {", "output": "\"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , \n \"<STR_LIT>\" : _ ( \"<STR_LIT>\" ) , \n } \n class ApiForm ( forms . Form ) : \n def errors_for_json_response ( self ) -> str : \n errors_for_response = [ ] \n for _field , errors in self . errors . get_json_data ( ) . items ( ) : \n for error in errors : \n errors_for_response . append ( error [ \"<STR_LIT>\" ] ) \n return \"<STR_LIT>\" . join ( errors_for_response ) \n class PromptForm ( ApiForm ) : \n text = PromptTextField ( ) \n prompt = PromptUUIDField ( ) \n def clean_prompt ( self ) : \n prompt_uuid = self . cleaned_data [ \"<STR_LIT>\" ] \n if prompt_uuid . version != <NUM_LIT> : \n raise ValidationError ( \n self . fields [ \"<STR_LIT>\" ] . error_messages [ \"<STR_LIT>\" ] , code = \"<STR_LIT>\" \n ) \n return prompt_uuid \n class DescribeImageApiForm ( ApiForm ) : \n image_id = forms . CharField ( ) \n maxlength = forms . IntegerField ( required = False , min_value = <NUM_LIT> , max_value = <NUM_LIT> ) \n class DescribeImageForm ( BaseImageForm ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( ) . __init__ ( * args , ** kwargs ) \n if self . instance and self . instance . pk : \n widget = self . fields [ \"<STR_LIT>\" ] . widget \n widget . attrs [ \"<STR_LIT>\" ] = str ( self . instance . pk ) \n widget . attrs [ \"<STR_LIT>\" ] = _ ( \"<STR_LIT>\" ) \n widget . template_name = \"<STR_LIT>\" \n class Media : \n js = [ versioned_static ( \"<STR_LIT>\" ) ] \n css = { \"<STR_LIT>\" : [ versioned_static ( \"<STR_LIT>\" ) ] }"}, {"input": "from datetime import date , datetime , timedelta \n import pytest \n from django . core . serializers . json import DjangoJSONEncoder \n from django . test import override_settings \n from django . utils import timezone \n from django_webhook . models import WebhookEvent \n from django_webhook . tasks import clear_webhook_events \n from django_webhook . test_factories import ( \n WebhookEventFactory , \n WebhookFactory , \n WebhookTopicFactory , \n ) \n from tests . model_data import TEST_USER \n from tests . models import User \n pytestmark = pytest . mark . django_db \n @ override_settings ( \n DJANGO_WEBHOOK = dict ( \n STORE_EVENTS = True , \n PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , \n MODELS = [ \"<STR_LIT>\" ] , \n USE_CACHE = False , \n ) \n ) \n def test_creates_events_when_enabled ( responses ) : \n webhook = WebhookFactory ( \n active = True , topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] \n ) \n responses . post ( webhook . url ) \n User . objects . create ( \n name = \"<STR_LIT>\" , \n email = \"<STR_LIT>\" , \n join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , \n last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , \n ) \n assert WebhookEvent . objects . count ( ) == <NUM_LIT> \n event = WebhookEvent . objects . get ( ) \n assert event . webhook == webhook \n assert event . object == { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : TEST_USER , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : str ( webhook . uuid ) , \n } \n assert event . object_type == \"<STR_LIT>\" \n assert event . topic == \"<STR_LIT>\" \n assert event . status == \"<STR_LIT>\" \n assert event . url == webhook . url \n @ override_settings ( \n DJANGO_WEBHOOK = dict (", "output": "STORE_EVENTS = False , \n PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , \n MODELS = [ \"<STR_LIT>\" ] , \n USE_CACHE = False , \n ) \n ) \n def test_does_not_create_events_when_disabled ( responses ) : \n webhook = WebhookFactory ( topics = [ WebhookTopicFactory ( name = \"<STR_LIT>\" ) ] ) \n responses . post ( webhook . url ) \n User . objects . create ( \n name = \"<STR_LIT>\" , \n email = \"<STR_LIT>\" , \n join_date = date ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , \n last_active = datetime ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , \n ) \n assert WebhookEvent . objects . count ( ) == <NUM_LIT> \n @ override_settings ( \n DJANGO_WEBHOOK = dict ( \n STORE_EVENTS = False , \n PAYLOAD_ENCODER_CLASS = DjangoJSONEncoder , \n MODELS = [ \"<STR_LIT>\" ] , \n USE_CACHE = False , \n EVENTS_RETENTION_DAYS = <NUM_LIT> , \n ) \n ) \n def test_clear_webhook_events ( ) : \n now = timezone . now ( ) \n retained_event = WebhookEventFactory ( ) \n older_event = WebhookEventFactory ( ) \n WebhookEvent . objects . filter ( id = older_event . id ) . update ( \n created = now - timedelta ( days = <NUM_LIT> ) \n ) \n clear_webhook_events . delay ( ) \n assert WebhookEvent . objects . count ( ) == <NUM_LIT> \n assert WebhookEvent . objects . get ( ) == retained_event"}, {"input": "import unittest \n from typing import List \n from ninja_crud . testing . core . components import utils \n class TestUtils ( unittest . TestCase ) : \n def test_ensure_list_of_dicts_single_dict ( self ) : \n data = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n result = utils . ensure_list_of_dicts ( data = data ) \n self . assertEqual ( result , [ data ] ) \n def test_ensure_list_of_dicts_list_of_dicts ( self ) : \n data = [ { \"<STR_LIT>\" : \"<STR_LIT>\" } ] \n result = utils . ensure_list_of_dicts ( data = data )", "output": "self . assertEqual ( result , data ) \n def test_ensure_list_of_dicts_empty_list ( self ) : \n data : List [ dict ] = [ ] \n with self . assertRaises ( ValueError ) : \n utils . ensure_list_of_dicts ( data = data ) \n def test_ensure_list_of_dicts_not_list_or_dict ( self ) : \n data = <NUM_LIT> \n with self . assertRaises ( TypeError ) : \n utils . ensure_list_of_dicts ( data = data )"}]