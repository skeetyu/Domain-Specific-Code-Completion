[{"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>'", "output": "branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . execute ( \"<STR_LIT>\" ) \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n def downgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None", "output": "def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_index ( '<STR_LIT>' )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . VARCHAR ( ) , \n nullable = True ) \n def downgrade ( ) -> None :", "output": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . VARCHAR ( ) , \n nullable = False )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None :", "output": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n def downgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( \"<STR_LIT>\" ) , nullable = True ) )"}, {"input": "import datetime \n import json \n import dateparser \n from bs4 import BeautifulSoup \n from feedi . requests import requests \n from feedi . scraping import CachingRequestsMixin \n def fetch ( feed_name , url ) : \n parser = None \n for cls in CustomParser . __subclasses__ ( ) : \n if cls . is_compatible ( url ) : \n parser = cls ( feed_name , url ) \n if not parser : \n raise ValueError ( \"<STR_LIT>\" , url ) \n return parser . fetch ( ) \n class CustomParser ( CachingRequestsMixin ) : \n BASE_URL = '<STR_LIT>' \n def __init__ ( self , feed_name , url ) : \n super ( ) . __init__ ( ) \n self . feed_name = feed_name \n self . url = url \n @ classmethod \n def is_compatible ( cls , feed_url ) : \n return cls . BASE_URL in feed_url \n def fetch ( self ) : \n raise NotImplementedError \n class AgendaBAParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n api_url = f'<STR_LIT>' \n response = requests . get ( api_url ) \n items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n entry_values = [ ] \n for item in items : \n created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) \n content_url = f'<STR_LIT>' \n entry_values . append ( { \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n '<STR_LIT>' : created , \n '<STR_LIT>' : created , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , \n '<STR_LIT>' : content_url , \n '<STR_LIT>' : json . dumps ( item ) \n } ) \n return entry_values \n class RevistaLenguaParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n soup = BeautifulSoup ( response . content , '<STR_LIT>' ) \n entry_values = [ ] \n for a in soup . select ( '<STR_LIT>' ) : \n article_html = self . request ( a [ '<STR_LIT>' ] ) \n article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) \n script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] \n item = json . loads ( script . text ) \n del item [ '<STR_LIT>' ] \n entry_values . append ( { \n '<STR_LIT>' : json . dumps ( item ) , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , \n '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : None , \n } ) \n return entry_values \n class EternaCadenciaParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n soup = BeautifulSoup ( response . content , '<STR_LIT>' ) \n entry_values = [ ] \n for article in soup . find_all ( class_ = '<STR_LIT>' ) : \n content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] \n author = article . find ( class_ = '<STR_LIT>' ) \n if author : \n author = author . text \n date_str = article . find ( class_ = '<STR_LIT>' ) . text \n date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) \n entry_values . append ( { \n '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , \n '<STR_LIT>' : author , \n '<STR_LIT>' : date , \n '<STR_LIT>' : date , \n '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , \n '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , \n '<STR_LIT>' : content_url , \n '<STR_LIT>' : content_url , \n } ) \n return entry_values \n class PioneerWorksParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text \n directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n entry_values = [ ] \n for article in directory : \n if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : \n continue \n pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) \n if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : \n continue \n article_url = f'<STR_LIT>' \n entry_values . append ( { \n '<STR_LIT>' : json . dumps ( article ) , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : pub_date , \n '<STR_LIT>' : pub_date , \n '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) ,", "output": "'<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : article_url , \n } ) \n return entry_values"}]