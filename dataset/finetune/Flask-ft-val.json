[{"input": "import base64 \n import json \n import aiohttp \n import asyncio \n from mod import textcompare \n UA = \"<STR_LIT>\" \n async def kugou ( title , artist , album ) : \n headers = { '<STR_LIT>' : UA , } \n limit = <NUM_LIT> \n async with aiohttp . ClientSession ( ) as session : \n async with session . get ( \n f'<STR_LIT>' \n f'<STR_LIT>' , \n headers = headers , \n timeout = <NUM_LIT> ) as response : \n if response . status == <NUM_LIT> : \n song_info_t = await response . text ( ) \n song_info = json . loads ( song_info_t ) \n song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n if len ( song_info ) >= <NUM_LIT> : \n ratio_max = <NUM_LIT> \n for index in range ( min ( limit , len ( song_info ) ) ) : \n song_item = song_info [ index ] \n song_name = song_item [ \"<STR_LIT>\" ] \n singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n song_hash = song_item [ \"<STR_LIT>\" ] \n album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n title_conform_ratio = textcompare . association ( title , song_name ) \n artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name )", "output": "ratio = ( title_conform_ratio * artist_conform_ratio ) ** <NUM_LIT> \n ratio_max = max ( ratio , ratio_max ) \n if ratio >= ratio_max : \n async with session . get ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" , \n headers = headers , \n timeout = <NUM_LIT> ) as response2 : \n lyrics_info = await response2 . json ( ) \n lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n async with session . get ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" , \n headers = headers , \n timeout = <NUM_LIT> ) as response3 : \n lyrics_data = await response3 . json ( ) \n if lyrics_data : \n lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] \n lrc_text = base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) \n return lrc_text \n await asyncio . sleep ( <NUM_LIT> ) \n return None \n async def api_2 ( title , artist , album ) : \n headers = { \n '<STR_LIT>' : '<STR_LIT>' \n '<STR_LIT>' , \n } \n url = f\"<STR_LIT>\" \n async with aiohttp . ClientSession ( ) as session : \n try : \n async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as response : \n if response . status < <NUM_LIT> : \n return await response . text ( ) \n else : \n async with session . get ( url , headers = headers , timeout = <NUM_LIT> ) as retry_response : \n if retry_response . status < <NUM_LIT> : \n return await retry_response . text ( ) \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n await asyncio . sleep ( <NUM_LIT> ) \n return None \n api_list = [ kugou , api_2 ] \n async def aw_main ( title , artist , album ) : \n await_list = [ func ( title , artist , album ) for func in api_list ] \n for coro in asyncio . as_completed ( await_list ) : \n result = await coro \n return result \n async def aw_all ( title , artist , album ) : \n await_list = [ func ( title , artist , album ) for func in api_list ] \n all_list = [ ] \n for coro in asyncio . as_completed ( await_list ) : \n result = await coro \n all_list . append ( result ) \n return all_list \n def main ( title , artist , album ) : \n return asyncio . run ( aw_main ( title , artist , album ) ) \n def allin ( title , artist , album ) : \n return asyncio . run ( aw_all ( title , artist , album ) ) \n if __name__ == \"<STR_LIT>\" : \n print ( allin ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) )"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class AnymalCRoughCfg ( LeggedRobotCfg ) :", "output": "class env ( LeggedRobotCfg . env ) : \n num_envs = <NUM_LIT> \n num_actions = <NUM_LIT> \n class terrain ( LeggedRobotCfg . terrain ) : \n mesh_type = '<STR_LIT>' \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : - <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } \n class control ( LeggedRobotCfg . control ) : \n stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n use_actuator_network = True \n actuator_net_file = \"<STR_LIT>\" \n class asset ( LeggedRobotCfg . asset ) : \n file = \"<STR_LIT>\" \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n terminate_after_contacts_on = [ \"<STR_LIT>\" ] \n self_collisions = <NUM_LIT> \n class domain_rand ( LeggedRobotCfg . domain_rand ) : \n randomize_base_mass = True \n added_mass_range = [ - <NUM_LIT> , <NUM_LIT> ] \n class rewards ( LeggedRobotCfg . rewards ) : \n base_height_target = <NUM_LIT> \n max_contact_force = <NUM_LIT> \n only_positive_rewards = True \n class scales ( LeggedRobotCfg . rewards . scales ) : \n pass \n class AnymalCRoughCfgPPO ( LeggedRobotCfgPPO ) : \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>' \n load_run = - <NUM_LIT>"}, {"input": "from . base import db \n class Message ( db . Model ) :", "output": "__tablename__ = '<STR_LIT>' \n id = db . Column ( db . Integer , primary_key = True ) \n content = db . Column ( db . Text , nullable = False ) \n time = db . Column ( db . DateTime , nullable = False ) \n author_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) \n author = db . relationship ( '<STR_LIT>' , lazy = True ) \n target_channel = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' , ondelete = '<STR_LIT>' ) , nullable = False ) \n def __repr__ ( self ) -> str : \n return f\"<STR_LIT>\""}, {"input": "import logging \n import sys \n SWITCH = True \n def _get_logger ( ) : \n log = logging . getLogger ( '<STR_LIT>' ) \n log . setLevel ( logging . INFO ) \n console_handle = logging . StreamHandler ( sys . stdout ) \n console_handle . setFormatter ( logging . Formatter ( '<STR_LIT>' ,", "output": "datefmt = '<STR_LIT>' ) ) \n log . addHandler ( console_handle ) \n return log \n def close_log ( ) : \n global SWITCH \n SWITCH = False \n def debug ( arg , * args ) : \n if SWITCH : \n if len ( args ) == <NUM_LIT> : \n logger . debug ( arg ) \n else : \n logger . debug ( arg . format ( * args ) ) \n def info ( arg , * args ) : \n if SWITCH : \n if len ( args ) == <NUM_LIT> : \n logger . info ( arg ) \n else : \n logger . info ( arg . format ( * args ) ) \n def warn ( arg , * args ) : \n if len ( args ) == <NUM_LIT> : \n logger . warning ( arg ) \n else : \n logger . warning ( arg . format ( * args ) ) \n def error ( arg , * args ) : \n if len ( args ) == <NUM_LIT> : \n logger . error ( arg ) \n else : \n logger . error ( arg . format ( * args ) ) \n def exception ( e ) : \n logger . exception ( e ) \n logger = _get_logger ( )"}, {"input": "import logging \n import subprocess \n import html \n from typing import List , Callable , Union , NamedTuple , Dict \n from urllib . parse import quote \n from . form import Form , fill_form \n from . requester import HTTPRequester , TCPRequester \n from . colorize import colored \n from . const import CALLBACK_SUBMIT \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Tamperer = Callable [ [ str ] , str ] \n def shell_tamperer ( shell_cmd : str ) -> Tamperer : \n def tamperer ( payload : str ) : \n proc = subprocess . Popen ( \n shell_cmd , \n shell = True , \n stdout = subprocess . PIPE , \n stdin = subprocess . PIPE , \n stderr = subprocess . PIPE , \n ) \n assert proc . stdin and proc . stdout \n proc . stdin . write ( payload . encode ( ) ) \n proc . stdin . close ( ) \n ret = proc . wait ( ) \n if ret != <NUM_LIT> : \n raise ValueError ( \n f\"<STR_LIT>\" \n ) \n out = proc . stdout . read ( ) . decode ( ) \n if out . endswith ( \"<STR_LIT>\" ) : \n logger . warning ( \n \"<STR_LIT>\" , \n shell_cmd , \n out , \n ) \n return out \n return tamperer \n class HTTPResponse ( NamedTuple ) : \n status_code : int \n text : str \n class BaseSubmitter : \n def __init__ ( self , callback = None ) : \n self . tamperers : List [ Tamperer ] = [ ] \n self . callback : Callable [ [ str , Dict ] , None ] = ( \n callback if callback else ( lambda x , y : None ) \n ) \n def add_tamperer ( self , tamperer : Tamperer ) : \n self . tamperers . append ( tamperer ) \n def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : \n raise NotImplementedError ( ) \n def submit ( self , payload : str ) -> Union [ HTTPResponse , None ] : \n if self . tamperers : \n logger . debug ( \"<STR_LIT>\" ) \n for tamperer in self . tamperers : \n payload = tamperer ( payload ) \n logger . debug ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) \n resp = self . submit_raw ( payload ) \n if resp is None : \n return None \n return HTTPResponse ( resp . status_code , html . unescape ( resp . text ) ) \n class TCPSubmitter ( BaseSubmitter ) : \n def __init__ ( \n self , \n requester : TCPRequester , \n pattern : bytes , \n toreplace = b\"<STR_LIT>\" , \n urlencode_payload = True , \n tamperers : Union [ List [ Tamperer ] , None ] = None , \n ) : \n super ( ) . __init__ ( ) \n self . pattern = pattern \n self . toreplace = toreplace \n self . urlencode_payload = urlencode_payload \n self . req = requester \n if tamperers : \n for tamperer in tamperers : \n self . add_tamperer ( tamperer ) \n def submit_raw ( self , raw_payload ) : \n if self . urlencode_payload : \n raw_payload = quote ( raw_payload ) \n request = self . pattern . replace ( self . toreplace , raw_payload . encode ( ) ) \n result = self . req . request ( request ) \n if result is None : \n return None \n code , text = result \n return HTTPResponse ( code , text ) \n class RequestSubmitter ( BaseSubmitter ) : \n def __init__ ( \n self , \n url : str , \n method : str , \n target_field : str , \n params : Union [ Dict [ str , str ] , None ] , \n data : Union [ Dict [ str , str ] , None ] , \n requester : HTTPRequester , \n tamperers : Union [ List [ Tamperer ] , None ] = None , \n ) : \n super ( ) . __init__ ( ) \n self . url = url \n self . method = method \n self . target_field = target_field \n self . params = params if params else { } \n self . data = data if data else { } \n self . req = requester \n if tamperers : \n for tamperer in tamperers : \n self . add_tamperer ( tamperer ) \n def submit_raw ( self , raw_payload ) :", "output": "params , data = self . params . copy ( ) , self . data . copy ( ) \n if self . method == \"<STR_LIT>\" : \n data . update ( { self . target_field : raw_payload } ) \n else : \n params . update ( { self . target_field : raw_payload } ) \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) , \n ) \n return self . req . request ( \n method = self . method , url = self . url , params = params , data = data \n ) \n class FormSubmitter ( BaseSubmitter ) : \n def __init__ ( \n self , \n url : str , \n form : Form , \n target_field : str , \n requester : HTTPRequester , \n callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , \n tamperers : Union [ List [ Tamperer ] , None ] = None , \n ) : \n super ( ) . __init__ ( callback ) \n self . url = url \n self . form = form \n self . req = requester \n self . target_field = target_field \n if tamperers : \n for tamperer in tamperers : \n self . add_tamperer ( tamperer ) \n def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : \n inputs = { self . target_field : raw_payload } \n resp = self . req . request ( ** fill_form ( self . url , self . form , inputs ) ) \n self . callback ( \n CALLBACK_SUBMIT , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : self . form , \n \"<STR_LIT>\" : inputs , \n \"<STR_LIT>\" : resp , \n } , \n ) \n if resp is None : \n return None \n return HTTPResponse ( resp . status_code , resp . text ) \n class PathSubmitter ( BaseSubmitter ) : \n def __init__ ( \n self , \n url : str , \n requester : HTTPRequester , \n callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , \n tamperers : Union [ List [ Tamperer ] , None ] = None , \n ) : \n super ( ) . __init__ ( callback ) \n if not url . endswith ( \"<STR_LIT>\" ) : \n logger . warning ( \n \"<STR_LIT>\" \n ) \n url += \"<STR_LIT>\" \n self . url = url \n self . req = requester \n if tamperers : \n for tamperer in tamperers : \n self . add_tamperer ( tamperer ) \n def submit_raw ( self , raw_payload : str ) -> Union [ HTTPResponse , None ] : \n if any ( w in raw_payload for w in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( raw_payload ) ) , \n ) \n return None \n resp = self . req . request ( method = \"<STR_LIT>\" , url = self . url + quote ( raw_payload ) ) \n self . callback ( \n CALLBACK_SUBMIT , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : self . url , \n \"<STR_LIT>\" : raw_payload , \n \"<STR_LIT>\" : resp , \n } , \n ) \n if resp is None : \n return None \n return HTTPResponse ( resp . status_code , resp . text ) \n Submitter = BaseSubmitter"}, {"input": "", "output": "contador = <NUM_LIT> \n class Vacia : \n pass \n numero = <NUM_LIT> \n while True : \n pregunta = int ( input ( \"<STR_LIT>\" ) ) \n if pregunta == numero : \n break \n print ( \"<STR_LIT>\" ) \n cadena = \"<STR_LIT>\" \n resultado = eval ( cadena ) \n print ( resultado )"}, {"input": "from collections import namedtuple \n import re \n import struct \n string_types = str \n def as_str ( value ) : \n if isinstance ( value , ( list , tuple ) ) : \n value = '<STR_LIT>' . join ( value ) \n return str ( value ) \n def sanitize_year ( year ) : \n try : \n if '<STR_LIT>' in year : \n year = year . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n except TypeError : \n pass \n try : \n year = int ( year ) \n except ValueError : \n if re . match ( r'<STR_LIT>' , year ) : \n year = int ( year [ : <NUM_LIT> ] ) \n elif re . match ( r'<STR_LIT>' , year ) : \n year = int ( year [ - <NUM_LIT> : ] ) \n else : \n raise ValueError ( \"<STR_LIT>\" . format ( year ) )", "output": "return year \n def sanitize_int ( val ) : \n try : \n ret = int ( val ) \n except ValueError : \n m = re . match ( r'<STR_LIT>' , val ) \n if m : \n ret = int ( m . group ( <NUM_LIT> ) ) \n else : \n raise ValueError ( '<STR_LIT>' . format ( val ) ) \n return ret \n def sanitize_bool ( val ) : \n val = str ( val ) . strip ( ) . lower ( ) \n if val in ( '<STR_LIT>' , '<STR_LIT>' ) : \n return True \n elif val in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n return False \n else : \n return int ( val ) != <NUM_LIT> \n def get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : \n tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) \n if tracknumber in ( None , '<STR_LIT>' ) : \n tracknumber = None \n else : \n tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n return tracknumber \n def set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : \n tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] \n tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) \n tracknumber [ <NUM_LIT> ] = val \n afile . set_raw ( norm_key , _tag_name , \n '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , \n appendable = False ) \n def get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) : \n tracknumber = str ( afile . mfile . get ( _tag_name , None ) ) \n if tracknumber in ( None , '<STR_LIT>' ) : \n tracknumber = None \n else : \n try : \n tracknumber = tracknumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n except IndexError : \n tracknumber = None \n return tracknumber \n def set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : \n tracknumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] \n tracknumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( tracknumber ) ) \n tracknumber [ <NUM_LIT> ] = val \n afile . set_raw ( norm_key , _tag_name , \n '<STR_LIT>' . join ( str ( i ) for i in tracknumber ) , \n appendable = False ) \n def get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) : \n discnumber = str ( afile . mfile . get ( _tag_name , None ) ) \n if discnumber in ( None , '<STR_LIT>' ) : \n discnumber = None \n else : \n discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n return discnumber \n def set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : \n discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] \n discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) \n discnumber [ <NUM_LIT> ] = val \n afile . set_raw ( norm_key , _tag_name , \n '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , \n appendable = False ) \n def get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) : \n discnumber = str ( afile . mfile . get ( _tag_name , None ) ) \n if discnumber in ( None , '<STR_LIT>' ) : \n discnumber = None \n else : \n try : \n discnumber = discnumber . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n except IndexError : \n discnumber = None \n return discnumber \n def set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) : \n discnumber = [ i for i in str ( afile . mfile . get ( _tag_name , '<STR_LIT>' ) ) . split ( '<STR_LIT>' ) ] \n discnumber += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( discnumber ) ) \n discnumber [ <NUM_LIT> ] = val \n afile . set_raw ( norm_key , _tag_name , \n '<STR_LIT>' . join ( str ( i ) for i in discnumber ) , \n appendable = False ) \n PicBlock = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ) ) \n PICTURE_TYPE_LUT = { <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' , \n <NUM_LIT> : '<STR_LIT>' , <NUM_LIT> : '<STR_LIT>' } \n def _split ( it , i ) : \n return it [ : i ] , it [ i : ] \n def parse_picture_block ( dat ) : \n head , rest = _split ( dat , <NUM_LIT> * <NUM_LIT> ) \n typeid , mime_len = struct . unpack ( '<STR_LIT>' , head ) \n mime , rest = _split ( rest , mime_len ) \n mime = mime . decode ( '<STR_LIT>' ) . lower ( ) \n head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) \n descr_len , = struct . unpack ( '<STR_LIT>' , head ) \n descr , rest = _split ( rest , descr_len ) \n descr = descr . decode ( '<STR_LIT>' ) \n head , rest = _split ( rest , <NUM_LIT> * <NUM_LIT> ) \n width , height , cdepth , cidx , dat_len = struct . unpack ( '<STR_LIT>' , head ) \n dat = rest \n pic = PicBlock ( typeid = typeid , picturetype = PICTURE_TYPE_LUT [ typeid ] , \n mime = mime , format = mime . split ( '<STR_LIT>' ) [ <NUM_LIT> ] , \n descr = descr , width = width , height = height , color_depth = cdepth , \n colors_indexed = cidx , data = dat ) \n assert len ( dat ) == dat_len \n return pic"}, {"input": "from multi_token . modalities . vision_clip import ( \n CLIPVisionModality , \n OUTPUT_LAYER as CLIP_POOL_LAYER , \n ) \n from multi_token . modalities . imagebind import ImageBindModality \n from multi_token . modalities . document_gte import DocumentGTEModality \n from multi_token . modalities . audio_whisper import WhisperAudioModality \n from multi_token . modalities . audio_clap import CLAPAudioModality \n from multi_token . modalities . video_xclip import XCLIPVideoModality", "output": "MODALITY_BUILDERS = { \n \"<STR_LIT>\" : lambda : [ CLIPVisionModality ( ) ] , \n \"<STR_LIT>\" : lambda : [ \n CLIPVisionModality ( feature_layer = CLIP_POOL_LAYER , num_tokens_output = <NUM_LIT> ) \n ] , \n \"<STR_LIT>\" : lambda : [ \n WhisperAudioModality ( \n num_tokens_output = <NUM_LIT> , model_name_or_path = \"<STR_LIT>\" \n ) \n ] , \n \"<STR_LIT>\" : lambda : [ CLAPAudioModality ( num_tokens_output = <NUM_LIT> ) ] , \n \"<STR_LIT>\" : lambda : [ XCLIPVideoModality ( num_tokens_output = <NUM_LIT> ) ] , \n \"<STR_LIT>\" : lambda : [ ImageBindModality ( ) ] , \n \"<STR_LIT>\" : lambda : [ DocumentGTEModality ( ) ] , \n \"<STR_LIT>\" : lambda : [ DocumentGTEModality ( num_tokens_output = <NUM_LIT> ) ] , \n }"}, {"input": "import pickle \n class IDStorage : \n def __init__ ( self ) : \n self . id = None \n def get_id ( self ) : \n if self . id is None : \n try : \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : \n self . id = pickle . load ( f ) \n except FileNotFoundError : \n print ( \"<STR_LIT>\" ) \n return self . id \n def set_id ( self , id ) : \n self . id = id \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : \n pickle . dump ( self . id , f )", "output": "storage = IDStorage ( ) \n id_value = storage . get_id ( ) \n storage . set_id ( \"<STR_LIT>\" ) \n if id_value is not None : \n print ( \"<STR_LIT>\" ) \n else : \n print ( id_value )"}, {"input": "from __future__ import annotations \n import contextlib \n import inspect \n from typing import Any , AsyncGenerator , Callable , overload \n import attrs \n from starlette . applications import Starlette \n from starlette . requests import Request \n from starlette . types import ASGIApp , Receive , Scope , Send \n import svcs \n from svcs . _core import ( \n _KEY_CONTAINER , \n _KEY_REGISTRY , \n T1 , \n T2 , \n T3 , \n T4 , \n T5 , \n T6 , \n T7 , \n T8 , \n T9 , \n T10 , \n ) \n def svcs_from ( request : Request ) -> svcs . Container : \n return getattr ( request . state , _KEY_CONTAINER ) \n @ attrs . define \n class lifespan : \n _lifespan : ( \n Callable [ \n [ Starlette , svcs . Registry ] , \n contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , \n ] \n | Callable [ \n [ Starlette , svcs . Registry ] , \n contextlib . AbstractAsyncContextManager [ None ] , \n ] \n | Callable [ \n [ Starlette , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] \n ] \n | Callable [ [ Starlette , svcs . Registry ] , AsyncGenerator [ None , None ] ] \n ) \n _state : dict [ str , object ] = attrs . field ( factory = dict ) \n registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) \n @ contextlib . asynccontextmanager \n async def __call__ ( \n self , app : Starlette \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n cm : Callable [ \n [ Starlette , svcs . Registry ] , contextlib . AbstractAsyncContextManager \n ] \n if inspect . isasyncgenfunction ( self . _lifespan ) : \n cm = contextlib . asynccontextmanager ( self . _lifespan ) \n else : \n cm = self . _lifespan \n async with self . registry , cm ( app , self . registry ) as state : \n self . _state = state or { } \n self . _state [ _KEY_REGISTRY ] = self . registry \n yield self . _state \n @ attrs . define \n class SVCSMiddleware : \n app : ASGIApp \n async def __call__ ( \n self , scope : Scope , receive : Receive , send : Send \n ) -> None : \n if scope [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n return await self . app ( scope , receive , send ) \n async with svcs . Container ( scope [ \"<STR_LIT>\" ] [ _KEY_REGISTRY ] ) as con : \n scope [ \"<STR_LIT>\" ] [ _KEY_CONTAINER ] = con \n return await self . app ( scope , receive , send ) \n def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : \n return svcs_from ( request ) . get_pings ( ) \n async def aget_abstract ( request : Request , * svc_types : type ) -> Any : \n return await svcs_from ( request ) . aget_abstract ( * svc_types ) \n @ overload \n async def aget ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... \n @ overload \n async def aget ( \n request : Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / \n ) -> tuple [ T1 , T2 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... \n @ overload \n async def aget ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] ,", "output": "svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n svc_type10 : type [ T10 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... \n async def aget ( request : Request , * svc_types : type ) -> object : \n return await svcs_from ( request ) . aget ( * svc_types )"}, {"input": "import re , os , sys , subprocess , copy , traceback , logging \n try : \n from HTMLParser import HTMLParser \n except ImportError : \n from html . parser import HTMLParser \n try : \n from urllib import quote as _quote \n quote = lambda n : _quote ( n . encode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n except ImportError : \n from urllib . parse import quote \n import requests \n from . import config \n logger = logging . getLogger ( '<STR_LIT>' ) \n emojiRegex = re . compile ( r'<STR_LIT>' ) \n htmlParser = HTMLParser ( ) \n if not hasattr ( htmlParser , '<STR_LIT>' ) : \n import html \n htmlParser . unescape = html . unescape \n try : \n b = u'<STR_LIT>' \n sys . stdout . write ( b + '<STR_LIT>' ) \n sys . stdout . flush ( ) \n except UnicodeEncodeError : \n BLOCK = '<STR_LIT>' \n else : \n BLOCK = b \n friendInfoTemplate = { } \n for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n friendInfoTemplate [ k ] = '<STR_LIT>' \n for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ) : \n friendInfoTemplate [ k ] = <NUM_LIT> \n friendInfoTemplate [ '<STR_LIT>' ] = [ ] \n def clear_screen ( ) : \n os . system ( '<STR_LIT>' if config . OS == '<STR_LIT>' else '<STR_LIT>' ) \n def emoji_formatter ( d , k ) : \n def _emoji_debugger ( d , k ) : \n s = d [ k ] . replace ( '<STR_LIT>' , \n '<STR_LIT>' ) \n def __fix_miss_match ( m ) : \n return '<STR_LIT>' % ( { \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n } . get ( m . group ( <NUM_LIT> ) , m . group ( <NUM_LIT> ) ) ) \n return emojiRegex . sub ( __fix_miss_match , s ) \n def _emoji_formatter ( m ) : \n s = m . group ( <NUM_LIT> ) \n if len ( s ) == <NUM_LIT> : \n return ( '<STR_LIT>' % ( s [ : <NUM_LIT> ] . rjust ( <NUM_LIT> , '<STR_LIT>' ) , s [ <NUM_LIT> : ] . rjust ( <NUM_LIT> , '<STR_LIT>' ) ) \n ) . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' , '<STR_LIT>' ) \n elif len ( s ) == <NUM_LIT> : \n return ( '<STR_LIT>' % ( s [ : <NUM_LIT> ] . rjust ( <NUM_LIT> , '<STR_LIT>' ) , s [ <NUM_LIT> : ] . rjust ( <NUM_LIT> , '<STR_LIT>' ) ) \n ) . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' , '<STR_LIT>' ) \n else : \n return ( '<STR_LIT>' % m . group ( <NUM_LIT> ) . rjust ( <NUM_LIT> , '<STR_LIT>' ) \n ) . encode ( '<STR_LIT>' ) . decode ( '<STR_LIT>' , '<STR_LIT>' ) \n d [ k ] = _emoji_debugger ( d , k ) \n d [ k ] = emojiRegex . sub ( _emoji_formatter , d [ k ] ) \n def msg_formatter ( d , k ) : \n emoji_formatter ( d , k ) \n d [ k ] = d [ k ] . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n d [ k ] = htmlParser . unescape ( d [ k ] ) \n def check_file ( fileDir ) : \n try : \n with open ( fileDir ) : \n pass \n return True \n except : \n return False \n def print_qr ( fileDir ) : \n if config . OS == '<STR_LIT>' :", "output": "subprocess . call ( [ '<STR_LIT>' , fileDir ] ) \n elif config . OS == '<STR_LIT>' : \n subprocess . call ( [ '<STR_LIT>' , fileDir ] ) \n else : \n os . startfile ( fileDir ) \n def print_cmd_qr ( qrText , white = BLOCK , black = '<STR_LIT>' , enableCmdQR = True ) : \n blockCount = int ( enableCmdQR ) \n if abs ( blockCount ) == <NUM_LIT> : \n blockCount = <NUM_LIT> \n white *= abs ( blockCount ) \n if blockCount < <NUM_LIT> : \n white , black = black , white \n sys . stdout . write ( '<STR_LIT>' * <NUM_LIT> + '<STR_LIT>' ) \n sys . stdout . flush ( ) \n qr = qrText . replace ( '<STR_LIT>' , white ) . replace ( '<STR_LIT>' , black ) \n sys . stdout . write ( qr ) \n sys . stdout . flush ( ) \n def struct_friend_info ( knownInfo ) : \n member = copy . deepcopy ( friendInfoTemplate ) \n for k , v in copy . deepcopy ( knownInfo ) . items ( ) : member [ k ] = v \n return member \n def search_dict_list ( l , key , value ) : \n for i in l : \n if i . get ( key ) == value : \n return i \n def print_line ( msg , oneLine = False ) : \n if oneLine : \n sys . stdout . write ( '<STR_LIT>' * <NUM_LIT> + '<STR_LIT>' ) \n sys . stdout . flush ( ) \n else : \n sys . stdout . write ( '<STR_LIT>' ) \n sys . stdout . write ( msg . encode ( sys . stdin . encoding or '<STR_LIT>' , '<STR_LIT>' \n ) . decode ( sys . stdin . encoding or '<STR_LIT>' , '<STR_LIT>' ) ) \n sys . stdout . flush ( ) \n def test_connect ( retryTime = <NUM_LIT> ) : \n for i in range ( retryTime ) : \n try : \n r = requests . get ( config . BASE_URL ) \n return True \n except : \n if i == retryTime - <NUM_LIT> : \n logger . error ( traceback . format_exc ( ) ) \n return False \n def contact_deep_copy ( core , contact ) : \n with core . storageClass . updateLock : \n return copy . deepcopy ( contact ) \n def get_image_postfix ( data ) : \n data = data [ : <NUM_LIT> ] \n if b'<STR_LIT>' in data : \n return '<STR_LIT>' \n elif b'<STR_LIT>' in data : \n return '<STR_LIT>' \n elif b'<STR_LIT>' in data : \n return '<STR_LIT>' \n return '<STR_LIT>' \n def update_info_dict ( oldInfoDict , newInfoDict ) : \n for k , v in newInfoDict . items ( ) : \n if any ( ( isinstance ( v , t ) for t in ( tuple , list , dict ) ) ) : \n pass \n elif oldInfoDict . get ( k ) is None or v not in ( None , '<STR_LIT>' , '<STR_LIT>' , <NUM_LIT> ) : \n oldInfoDict [ k ] = v"}, {"input": "from setuptools import setup , find_packages \n setup ( name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n author = '<STR_LIT>' , \n author_email = '<STR_LIT>' , \n license = \"<STR_LIT>\" , \n packages = find_packages ( ) , \n description = '<STR_LIT>' , \n python_requires = '<STR_LIT>' , \n install_requires = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\"", "output": "] , \n )"}, {"input": "import os \n import plugins \n from plugins import * \n from common import log \n from common import functions \n @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) \n class Selector ( Plugin ) : \n def __init__ ( self ) : \n super ( ) . __init__ ( ) \n curdir = os . path . dirname ( __file__ ) \n try : \n self . config = functions . load_json_file ( curdir , \"<STR_LIT>\" ) \n except Exception as e : \n log . warn ( \"<STR_LIT>\" ) \n raise e \n self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . select_model \n self . handlers [ Event . ON_BRIDGE_HANDLE_STREAM_CONTEXT ] = self . select_model \n log . info ( \"<STR_LIT>\" ) \n def get_events ( self ) : \n return self . handlers \n def select_model ( self , e_context : EventContext ) :", "output": "model = e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) \n for selector in self . config . get ( \"<STR_LIT>\" , [ ] ) : \n prefix = selector . get ( '<STR_LIT>' , [ ] ) \n check_prefix = functions . check_prefix ( e_context [ \"<STR_LIT>\" ] , prefix ) \n if ( check_prefix ) : \n model = selector . get ( '<STR_LIT>' ) \n if isinstance ( check_prefix , str ) : \n e_context [ \"<STR_LIT>\" ] = e_context [ \"<STR_LIT>\" ] . split ( check_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n break \n log . debug ( f\"<STR_LIT>\" ) \n e_context . action = EventAction . CONTINUE \n e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = model \n return e_context"}, {"input": "import setuptools \n import os \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n long_description = f . read ( ) \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n version = f . read ( ) . strip ( ) \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n requirements = [ \n line . strip ( ) for line in f . readlines ( ) \n ] \n setuptools . setup ( \n name = \"<STR_LIT>\" , \n version = version , \n author = \"<STR_LIT>\" , \n author_email = \"<STR_LIT>\" , \n description = \"<STR_LIT>\" , \n long_description = long_description , \n long_description_content_type = \"<STR_LIT>\" , \n url = \"<STR_LIT>\" ,", "output": "packages = setuptools . find_packages ( ) , \n classifiers = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n install_requires = requirements , \n package_data = { \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } , \n entry_points = { \n '<STR_LIT>' : [ \n '<STR_LIT>' , \n ] \n } \n )"}, {"input": "import torch \n from tqdm import tqdm \n from transformers import CLIPProcessor , CLIPModel \n from densely_captioned_images . dataset . impl import get_clip_ready_ds , DenseCaptionedDataset \n from densely_captioned_images . dataset . loss import get_pooled_groups , get_pooled_diag \n from torch . utils . data import DataLoader \n from typing import Optional , List \n def run_dense_cap_test_on_model ( \n model : CLIPModel , \n dataset : DenseCaptionedDataset , \n ) : \n clip_correct_tot = <NUM_LIT> \n neg_correct_tot = <NUM_LIT> \n exs = <NUM_LIT> \n loader = DataLoader ( dataset , batch_size = <NUM_LIT> , shuffle = False ) \n for inputs in tqdm ( loader ) : \n exs += inputs [ '<STR_LIT>' ] . shape [ <NUM_LIT> ] \n if '<STR_LIT>' in inputs : \n bs , n , t = inputs [ '<STR_LIT>' ] . shape \n unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n pos_inputs = { \n '<STR_LIT>' : unstacked_inputs . to ( model . device ) , \n '<STR_LIT>' : unstacked_attention . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n } \n else : \n pos_inputs = { \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n } \n if '<STR_LIT>' in inputs : \n bs , n , t = inputs [ '<STR_LIT>' ] . shape \n unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n neg_inputs = { \n '<STR_LIT>' : unstacked_inputs . to ( model . device ) , \n '<STR_LIT>' : unstacked_attention . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n } \n else : \n neg_inputs = { \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . to ( model . device ) , \n } \n outputs = model ( ** pos_inputs ) \n clip_logits = outputs . logits_per_image \n similarity = get_pooled_groups ( clip_logits , pool_type = '<STR_LIT>' ) \n min_sim_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) \n similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag \n maxes = torch . max ( similarity , dim = <NUM_LIT> ) \n clip_correct_tot += sum ( [ int ( i == m ) for i , m in enumerate ( maxes . indices ) ] ) \n neg_outputs = model ( ** neg_inputs ) \n neg_logits = neg_outputs . logits_per_image \n pos_diag = get_pooled_diag ( clip_logits , pool_type = '<STR_LIT>' ) \n neg_diag = get_pooled_diag ( neg_logits , pool_type = '<STR_LIT>' ) \n neg_correct_tot += sum ( pos_diag > neg_diag ) . item ( ) \n return clip_correct_tot / exs , neg_correct_tot / exs \n def run_dense_cap_on_model ( \n model : CLIPModel , \n processor : CLIPProcessor , \n run_subtests : Optional [ List [ str ] ] = None \n ) : \n subtests = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n \"<STR_LIT>\" : {", "output": "\"<STR_LIT>\" : True , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : False , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : '<STR_LIT>' , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n } \n if run_subtests is None : \n run_subtests = list ( subtests . keys ( ) ) \n for key , args in subtests . items ( ) : \n if key not in run_subtests : \n continue \n dataset = get_clip_ready_ds ( split = '<STR_LIT>' , ** args ) \n dataset . processor = processor \n with torch . no_grad ( ) : \n clip_correct_prop , neg_correct_prop = run_dense_cap_test_on_model ( model , dataset ) \n print ( f\"<STR_LIT>\" ) \n def run_dense_cap_on_lora ( lora_weight_path : str ) : \n from peft import PeftModel \n processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) \n loaded = loaded . merge_and_unload ( ) \n run_dense_cap_on_model ( loaded , processor ) \n if __name__ == '<STR_LIT>' : \n from transformers import CLIPProcessor , CLIPModel \n clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n run_dense_cap_on_model ( clip_model , clip_processor )"}, {"input": "from densely_captioned_images . repro . config import LOCALIZED_NARRATIVES_DATAPATH \n from densely_captioned_images . dataset . utils import get_clip_token_length \n from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader \n def get_clip_token_lengths_for_localized_narratives_split ( split = '<STR_LIT>' ) : \n if split == '<STR_LIT>' : \n ln_split = '<STR_LIT>' \n elif split == '<STR_LIT>' : \n ln_split = '<STR_LIT>' \n else : \n raise NotImplementedError ( '<STR_LIT>' ) \n loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) \n caption_count = <NUM_LIT> \n token_count = <NUM_LIT> \n full_caption_count = <NUM_LIT> \n full_token_count = <NUM_LIT> \n for n in loader . load_annotations ( ln_split ) : \n toks = get_clip_token_length ( n . caption ) \n full_caption_count += <NUM_LIT> \n full_token_count += toks \n if toks > <NUM_LIT> : \n continue \n caption_count += <NUM_LIT> \n token_count += toks", "output": "return token_count , caption_count , full_token_count , full_caption_count \n def get_clip_token_lengths_for_localized_narratives ( ) : \n toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' ) \n print ( f\"<STR_LIT>\" ) \n toks , caps , full_toks , full_caps = get_clip_token_lengths_for_localized_narratives_split ( '<STR_LIT>' ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n get_clip_token_lengths_for_localized_narratives ( )"}, {"input": "from typing import Dict , List , Optional \n import torch \n import torch . nn as nn \n from transformers import AutoProcessor , AutoModel \n from multi_token . data_tools import load_video \n from multi_token . modalities . base_modality import Modality \n from multi_token . modalities . projectors import ( \n build_mlp_vector_projector ,", "output": ") \n OUTPUT_EMB_SIZE = <NUM_LIT> \n class XCLIPVideoModule ( nn . Module ) : \n def __init__ ( self , model_name_or_path : str ) : \n super ( ) . __init__ ( ) \n self . model_name_or_path = model_name_or_path \n self . model = None \n self . processor = None \n self . load_model ( ) \n def load_model ( self ) : \n self . model = AutoModel . from_pretrained ( self . model_name_or_path ) \n self . processor = AutoProcessor . from_pretrained ( self . model_name_or_path ) \n self . model . requires_grad_ ( False ) \n @ torch . no_grad ( ) \n def forward ( self , video_inputs ) -> torch . Tensor : \n with torch . no_grad ( ) : \n outputs = self . model ( ** ( video_inputs . to ( device = self . device ) ) ) \n emb = outputs . video_embeds . to ( device = self . device , dtype = self . dtype ) . view ( \n - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE \n ) \n return emb \n @ property \n def dtype ( self ) : \n return self . model . dtype \n @ property \n def device ( self ) : \n return self . model . device \n class XCLIPVideoModality ( Modality ) : \n def __init__ ( \n self , \n model_name_or_path : str = \"<STR_LIT>\" , \n num_projector_layers : int = <NUM_LIT> , \n num_tokens_output : int = <NUM_LIT> , \n ) : \n self . model_name_or_path = model_name_or_path \n self . module = XCLIPVideoModule ( model_name_or_path = self . model_name_or_path ) \n self . num_projector_layers = num_projector_layers \n self . num_tokens_output = num_tokens_output \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n return build_mlp_vector_projector ( \n input_hidden_size = OUTPUT_EMB_SIZE , \n lm_hidden_size = lm_hidden_size , \n num_layers = self . num_projector_layers , \n num_tokens = self . num_tokens_output , \n ) \n @ property \n def name ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def data_key ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token_width ( self ) -> int : \n return self . num_tokens_output \n def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n self . module . to ( dtype = dtype , device = device ) \n return self \n def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : \n row_values = [ ] \n for row in rows : \n video_arrays = [ \n load_video ( \n video_info , \n ) \n for video_info in row [ self . data_key ] \n ] \n videos_enc = self . module . processor ( \n videos = [ list ( video ) for video in video_arrays ] , \n text = [ \"<STR_LIT>\" ] , \n return_tensors = \"<STR_LIT>\" , \n padding = True , \n ) \n row_values . append ( videos_enc ) \n return row_values \n @ torch . no_grad ( ) \n def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : \n video_features = [ ] \n for video_batch in encoded_values : \n video_features . append ( self . module . forward ( video_batch ) ) \n return video_features"}, {"input": "import numpy as np \n import os \n from datetime import datetime \n import isaacgym \n from legged_gym . envs import * \n from legged_gym . utils import get_args , task_registry \n from shutil import copyfile \n import torch \n import wandb \n def train ( args ) : \n args . headless = True \n log_pth = LEGGED_GYM_ROOT_DIR + \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid \n try : \n os . makedirs ( log_pth ) \n except : \n pass \n if args . debug : \n mode = \"<STR_LIT>\" \n args . rows = <NUM_LIT> \n args . cols = <NUM_LIT> \n args . num_envs = <NUM_LIT> \n else : \n mode = \"<STR_LIT>\" \n if args . no_wandb : \n mode = \"<STR_LIT>\" \n wandb . init ( project = args . proj_name , name = args . exptid , entity = \"<STR_LIT>\" , group = args . exptid [ : <NUM_LIT> ] , mode = mode , dir = \"<STR_LIT>\" ) \n wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) \n wandb . save ( LEGGED_GYM_ENVS_DIR + \"<STR_LIT>\" , policy = \"<STR_LIT>\" ) \n env , env_cfg = task_registry . make_env ( name = args . task , args = args ) \n ppo_runner , train_cfg = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args ) \n ppo_runner . learn ( num_learning_iterations = train_cfg . runner . max_iterations , init_at_random_ep_len = True ) \n if __name__ == '<STR_LIT>' : \n args = get_args ( )", "output": "train ( args )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op", "output": "revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . execute ( \"<STR_LIT>\" ) \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n def downgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) )"}, {"input": "from datetime import datetime \n import requests \n from bs4 import BeautifulSoup \n from app . utils . status_code import Status \n from app . utils . translate import NATIVE_LANG , text_translate \n class HoroscopeRepository : \n MAIN_PATH_URL = \"<STR_LIT>\" \n def _get_horoscope_url_today ( self , sign ) :", "output": "return f\"<STR_LIT>\" \n def _get_horoscope_url_date ( self , sign , date ) : \n return f\"<STR_LIT>\" \n def get_horoscope_info ( self , id , date , lang ) : \n if not id : \n raise ValueError ( \"<STR_LIT>\" ) \n if date : \n try : \n datetime . strptime ( date , '<STR_LIT>' ) \n except ValueError : \n raise ValueError ( \"<STR_LIT>\" ) \n url = self . _get_horoscope_url_today ( id ) if not date else self . _get_horoscope_url_date ( id , date ) \n res = requests . get ( url ) \n if res . status_code != Status . HTTP_OK : \n raise Exception ( \"<STR_LIT>\" . format ( res . status_code ) ) \n soup = BeautifulSoup ( res . content , '<STR_LIT>' ) \n data = soup . find ( '<STR_LIT>' , attrs = { '<STR_LIT>' : '<STR_LIT>' } ) \n if not data or not data . p : \n raise Exception ( \"<STR_LIT>\" ) \n horoscope = data . p . text \n if lang and lang != NATIVE_LANG : \n horoscope = text_translate ( data . p . text , lang ) \n return horoscope"}, {"input": "cuadrados = [ ] \n for numero in range ( <NUM_LIT> ) : \n cuadrados . append ( numero ** <NUM_LIT> ) \n print ( cuadrados ) \n cuadrados4 = [ ] \n def cuadrados_funcion ( numero ) : \n return numero * numero \n for num in range ( <NUM_LIT> ) : \n cuadrados4 . append ( cuadrados_funcion ( num ) ) \n print ( cuadrados4 ) \n cuadrados2 = list ( map ( lambda numero : numero ** <NUM_LIT> , range ( <NUM_LIT> ) ) ) \n print ( cuadrados2 ) \n cuadrados3 = [ numero ** <NUM_LIT> for numero in range ( <NUM_LIT> ) ] \n print ( cuadrados3 ) \n from math import pi \n print ( [ round ( pi , i ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] ) \n matriz = [ \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n ] \n print ( matriz ) \n matriz1 = [ ] \n for i in range ( <NUM_LIT> ) : \n fila_matriz1 = [ ] \n for fila in matriz : \n fila_matriz1 . append ( fila [ i ] ) \n matriz1 . append ( fila_matriz1 ) \n print ( matriz1 ) \n matriz2 = [ ] \n for i in range ( <NUM_LIT> ) : \n matriz2 . append ( [ fila [ i ] for fila in matriz ] ) \n print ( matriz2 ) \n matriz3 = [ [ fila [ i ] for fila in matriz ] for i in range ( <NUM_LIT> ) ] \n print ( matriz3 ) \n lista1 = [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n print ( lista1 ) \n print ( lista1 . pop ( <NUM_LIT> ) ) \n print ( lista1 ) \n print ( lista1 . remove ( <NUM_LIT> ) ) \n print ( lista1 ) \n del lista1 [ <NUM_LIT> ] \n print ( lista1 ) \n del lista1 [ <NUM_LIT> : ] \n print ( lista1 ) \n del lista1 [ : ] \n print ( lista1 )", "output": "del lista1 \n print ( lista1 )"}, {"input": "import os \n import sys \n import code \n import builtins \n import importlib \n import importlib . util \n import inspect \n from contextlib import contextmanager \n from threading import RLock \n import instld \n from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame \n from instld . cli . parsing_arguments . get_python_file import get_python_file \n from instld . cli . traceback_cutting . cutting import set_cutting_excepthook \n from instld . state_management . storage import state_storage , RunType \n from instld . errors import CommentFormatError \n def main ( ) : \n python_file = get_python_file ( ) \n state_storage . run_type = RunType . script \n with instld ( ) as context : \n lock = RLock ( ) \n old_import = builtins . __import__ \n locations = { } \n @ contextmanager \n def set_import ( ) : \n builtins . __import__ = old_import \n yield \n builtins . __import__ = import_wrapper \n def get_current_context ( where ) : \n if where is None : \n return context \n else : \n with lock : \n location_context = locations . get ( where ) \n if location_context is not None : \n return location_context [ <NUM_LIT> ] \n manager = instld ( where = where ) \n local_context = manager . __enter__ ( ) \n locations [ where ] = ( manager , local_context ) \n return local_context \n def import_wrapper ( name , * args , ** kwargs ) : \n splitted_name = name . split ( '<STR_LIT>' ) \n base_name = splitted_name [ <NUM_LIT> ] \n base_sequence = '<STR_LIT>' . join ( splitted_name [ : - <NUM_LIT> ] ) \n last_name = splitted_name [ - <NUM_LIT> ] \n current_frame = inspect . currentframe ( ) \n options = get_options_from_comments_by_frame ( current_frame . f_back ) \n package_name = options . pop ( '<STR_LIT>' , base_name ) \n if '<STR_LIT>' in options : \n package_name = f'<STR_LIT>' \n catch_output = options . pop ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) \n if catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n catch_output = True \n elif catch_output in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n catch_output = False \n else : \n raise CommentFormatError ( '<STR_LIT>' ) \n current_context = get_current_context ( options . pop ( '<STR_LIT>' , None ) ) \n with lock : \n with set_import ( ) : \n try : \n result = __import__ ( name , * args , ** kwargs ) \n except ( ModuleNotFoundError , ImportError ) : \n current_context . install ( package_name , catch_output = catch_output , ** options ) \n result = current_context . import_here ( base_name ) \n sys . modules [ base_name ] = result \n if '<STR_LIT>' in kwargs and kwargs [ '<STR_LIT>' ] : \n if len ( splitted_name ) > <NUM_LIT> : \n for index , subname in enumerate ( splitted_name ) : \n if index : \n try : \n result = getattr ( result , subname ) \n except AttributeError : \n raise ImportError ( f\"<STR_LIT>\" ) \n return result \n if python_file is None : \n try : \n import readline \n except ImportError : \n pass \n state_storage . run_type = RunType . REPL \n builtins . __import__ = import_wrapper \n class REPL ( code . InteractiveConsole ) : \n def push ( self , line ) : \n state_storage . last_string = line \n return super ( ) . push ( line ) \n banner_strings = [ \n '<STR_LIT>' \n '<STR_LIT>' % ( sys . version , sys . platform ) , \n '<STR_LIT>' , \n ] \n banner = '<STR_LIT>' . join ( banner_strings ) \n REPL ( ) . interact ( banner = banner ) \n else : \n builtins . __import__ = import_wrapper \n spec = importlib . util . spec_from_file_location ( '<STR_LIT>' , os . path . abspath ( python_file ) ) \n module = importlib . util . module_from_spec ( spec ) \n sys . modules [ '<STR_LIT>' ] = module \n if sys . platform . lower ( ) in ( '<STR_LIT>' , ) : \n cutting_trace_size = <NUM_LIT>", "output": "else : \n cutting_trace_size = <NUM_LIT> \n set_cutting_excepthook ( cutting_trace_size ) \n spec . loader . exec_module ( module ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . create_index ( '<STR_LIT>' , [ sa . text ( '<STR_LIT>' ) ] , unique = False )", "output": "def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_index ( '<STR_LIT>' )"}, {"input": "from __future__ import print_function \n import json \n import os \n import os . path as osp \n import re \n import warnings \n from six . moves import urllib_parse \n import shutil \n import sys \n import tempfile \n import textwrap \n import time \n import requests \n import six \n import tqdm \n def indent ( text , prefix ) : \n def prefixed_lines ( ) : \n for line in text . splitlines ( True ) : \n yield ( prefix + line if line . strip ( ) else line ) \n return \"<STR_LIT>\" . join ( prefixed_lines ( ) ) \n class FileURLRetrievalError ( Exception ) : \n pass \n class FolderContentsMaximumLimitError ( Exception ) : \n pass \n def parse_url ( url , warning = True ) : \n parsed = urllib_parse . urlparse ( url ) \n query = urllib_parse . parse_qs ( parsed . query ) \n is_gdrive = parsed . hostname in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n is_download_link = parsed . path . endswith ( \"<STR_LIT>\" ) \n if not is_gdrive : \n return is_gdrive , is_download_link \n file_id = None \n if \"<STR_LIT>\" in query : \n file_ids = query [ \"<STR_LIT>\" ] \n if len ( file_ids ) == <NUM_LIT> : \n file_id = file_ids [ <NUM_LIT> ] \n else : \n patterns = [ \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n ] \n for pattern in patterns : \n match = re . match ( pattern , parsed . path ) \n if match : \n file_id = match . groups ( ) [ <NUM_LIT> ] \n break \n if warning and not is_download_link : \n warnings . warn ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n url = \"<STR_LIT>\" . format ( file_id ) \n ) \n ) \n return file_id , is_download_link \n CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> \n home = osp . expanduser ( \"<STR_LIT>\" ) \n def get_url_from_gdrive_confirmation ( contents ) : \n url = \"<STR_LIT>\" \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = \"<STR_LIT>\" + m . groups ( ) [ <NUM_LIT> ] \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = m . groups ( ) [ <NUM_LIT> ] \n uuid = re . search ( \n r'<STR_LIT>' , contents \n ) \n uuid = uuid . groups ( ) [ <NUM_LIT> ] \n url = ( \n \"<STR_LIT>\" \n + url \n + \"<STR_LIT>\" \n + uuid \n ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n url = m . groups ( ) [ <NUM_LIT> ] \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n return url \n m = re . search ( r'<STR_LIT>' , contents ) \n if m : \n error = m . groups ( ) [ <NUM_LIT> ] \n raise FileURLRetrievalError ( error ) \n raise FileURLRetrievalError ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n def _get_session ( proxy , use_cookies , return_cookies_file = False ) : \n sess = requests . session ( ) \n sess . headers . update ( \n { \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n if proxy is not None : \n sess . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } \n print ( \"<STR_LIT>\" , proxy , file = sys . stderr ) \n cookies_file = osp . join ( home , \"<STR_LIT>\" ) \n if osp . exists ( cookies_file ) and use_cookies : \n with open ( cookies_file ) as f : \n cookies = json . load ( f ) \n for k , v in cookies : \n sess . cookies [ k ] = v \n if return_cookies_file : \n return sess , cookies_file \n else : \n return sess \n def download ( \n url = None , \n output = None , \n quiet = False , \n proxy = None , \n speed = None , \n use_cookies = True , \n verify = True , \n id = None , \n fuzzy = True , \n resume = False , \n format = None , \n ) : \n if not ( id is None ) ^ ( url is None ) : \n raise ValueError ( \"<STR_LIT>\" ) \n if id is not None : \n url = \"<STR_LIT>\" . format ( id = id ) \n url_origin = url \n sess , cookies_file = _get_session ( \n proxy = proxy , use_cookies = use_cookies , return_cookies_file = True \n ) \n gdrive_file_id , is_gdrive_download_link = parse_url ( url , warning = not fuzzy ) \n if fuzzy and gdrive_file_id : \n url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) \n url_origin = url \n is_gdrive_download_link = True \n while True : \n res = sess . get ( url , stream = True , verify = verify ) \n if url == url_origin and res . status_code == <NUM_LIT> : \n url = \"<STR_LIT>\" . format ( id = gdrive_file_id ) \n continue \n if res . headers [ \"<STR_LIT>\" ] . startswith ( \"<STR_LIT>\" ) : \n m = re . search ( \"<STR_LIT>\" , res . text ) \n if m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif m and m . groups ( ) [ <NUM_LIT> ] . endswith ( \"<STR_LIT>\" ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n elif ( \n \"<STR_LIT>\" in res . headers \n and res . headers [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) \n and format not in { None , \"<STR_LIT>\" } \n ) : \n url = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( \n id = gdrive_file_id , \n format = \"<STR_LIT>\" if format is None else format , \n ) \n ) \n continue \n if use_cookies : \n if not osp . exists ( osp . dirname ( cookies_file ) ) : \n os . makedirs ( osp . dirname ( cookies_file ) ) \n with open ( cookies_file , \"<STR_LIT>\" ) as f : \n cookies = [ \n ( k , v ) \n for k , v in sess . cookies . items ( ) \n if not k . startswith ( \"<STR_LIT>\" ) \n ] \n json . dump ( cookies , f , indent = <NUM_LIT> ) \n if \"<STR_LIT>\" in res . headers : \n break \n if not ( gdrive_file_id and is_gdrive_download_link ) : \n break \n try : \n url = get_url_from_gdrive_confirmation ( res . text ) \n except FileURLRetrievalError as e : \n message = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) . format ( \n indent ( \"<STR_LIT>\" . join ( textwrap . wrap ( str ( e ) ) ) , prefix = \"<STR_LIT>\" ) , \n url_origin , \n ) \n raise FileURLRetrievalError ( message ) \n if gdrive_file_id and is_gdrive_download_link : \n content_disposition = six . moves . urllib_parse . unquote ( \n res . headers [ \"<STR_LIT>\" ] \n ) \n m = re . search ( r\"<STR_LIT>\" , content_disposition ) \n if not m : \n m = re . search ( r'<STR_LIT>' , content_disposition ) \n filename_from_url = m . groups ( ) [ <NUM_LIT> ] \n filename_from_url = filename_from_url . replace ( osp . sep , \"<STR_LIT>\" ) \n else : \n filename_from_url = osp . basename ( url ) \n if output is None : \n output = filename_from_url \n output_is_path = isinstance ( output , six . string_types ) \n if output_is_path and output . endswith ( osp . sep ) : \n if not osp . exists ( output ) : \n os . makedirs ( output ) \n output = osp . join ( output , filename_from_url ) \n if output_is_path : \n existing_tmp_files = [ ] \n for file in os . listdir ( osp . dirname ( output ) or \"<STR_LIT>\" ) : \n if file . startswith ( osp . basename ( output ) ) : \n existing_tmp_files . append ( osp . join ( osp . dirname ( output ) , file ) ) \n if resume and existing_tmp_files : \n if len ( existing_tmp_files ) != <NUM_LIT> : \n print ( \n \"<STR_LIT>\" , \n file = sys . stderr , \n ) \n print ( \"<STR_LIT>\" ) \n for file in existing_tmp_files : \n print ( \"<STR_LIT>\" , file , file = sys . stderr ) \n print ( \"<STR_LIT>\" ) \n print ( \n \"<STR_LIT>\" , \n file = sys . stderr , \n ) \n return \n tmp_file = existing_tmp_files [ <NUM_LIT> ] \n else : \n resume = False \n tmp_file = tempfile . mktemp ( \n suffix = tempfile . template , \n prefix = osp . basename ( output ) , \n dir = osp . dirname ( output ) ,", "output": ") \n f = open ( tmp_file , \"<STR_LIT>\" ) \n else : \n tmp_file = None \n f = output \n if tmp_file is not None and f . tell ( ) != <NUM_LIT> : \n headers = { \"<STR_LIT>\" : \"<STR_LIT>\" . format ( f . tell ( ) ) } \n res = sess . get ( url , headers = headers , stream = True , verify = verify ) \n if not quiet : \n if resume : \n print ( \"<STR_LIT>\" , tmp_file , file = sys . stderr ) \n print ( \n \"<STR_LIT>\" , \n osp . abspath ( output ) if output_is_path else output , \n file = sys . stderr , \n ) \n try : \n total = res . headers . get ( \"<STR_LIT>\" ) \n if total is not None : \n total = int ( total ) \n if not quiet : \n pbar = tqdm . tqdm ( total = total , unit = \"<STR_LIT>\" , unit_scale = True ) \n t_start = time . time ( ) \n for chunk in res . iter_content ( chunk_size = CHUNK_SIZE ) : \n f . write ( chunk ) \n if not quiet : \n pbar . update ( len ( chunk ) ) \n if speed is not None : \n elapsed_time_expected = <NUM_LIT> * pbar . n / speed \n elapsed_time = time . time ( ) - t_start \n if elapsed_time < elapsed_time_expected : \n time . sleep ( elapsed_time_expected - elapsed_time ) \n if not quiet : \n pbar . close ( ) \n if tmp_file : \n f . close ( ) \n shutil . move ( tmp_file , output ) \n finally : \n sess . close ( ) \n return output"}, {"input": "import mysql . connector \n from mysql . connector import errorcode \n config = { \n '<STR_LIT>' : \"<STR_LIT>\" , \n '<STR_LIT>' : \"<STR_LIT>\" , \n '<STR_LIT>' : \"<STR_LIT>\" , \n '<STR_LIT>' : '<STR_LIT>' \n } \n try : \n conn = mysql . connector . connect ( ** config ) \n print ( \"<STR_LIT>\" ) \n except mysql . connector . Error as err : \n if err . errno == errorcode . ER_DBACCESS_DENIED_ERROR : \n print ( \"<STR_LIT>\" ) \n elif err . errno == errorcode . ER_BAD_DB_ERROR : \n print ( \"<STR_LIT>\" ) \n else : \n print ( err ) \n else : \n mi_cursor = conn . cursor ( ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n registros = mi_cursor . fetchall ( ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n for row in registros : \n print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) \n mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" , '<STR_LIT>' ) ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" , ( <NUM_LIT> , \"<STR_LIT>\" ) ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n registros = mi_cursor . fetchall ( ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n for row in registros : \n print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) ) \n print ( \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" , { '<STR_LIT>' : \"<STR_LIT>\" } ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n mi_cursor . execute ( \"<STR_LIT>\" ) \n registros = mi_cursor . fetchall ( ) \n print ( \"<STR_LIT>\" , mi_cursor . rowcount , \"<STR_LIT>\" ) \n for row in registros : \n print ( \"<STR_LIT>\" , str ( row [ <NUM_LIT> ] ) , str ( row [ <NUM_LIT> ] ) )", "output": "print ( \"<STR_LIT>\" ) \n conn . commit ( ) \n mi_cursor . close ( ) \n conn . close ( ) \n print ( \"<STR_LIT>\" )"}, {"input": "import os \n from flask import Flask , request , jsonify \n from claude_api import Client \n from common . utils import * \n app = Flask ( __name__ ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def create_chat ( ) : \n data = request . get_json ( ) \n prompt = data [ '<STR_LIT>' ] \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n conversation = client . create_new_chat ( ) \n conversation_id = conversation [ '<STR_LIT>' ] \n response = client . send_message ( prompt , conversation_id ) \n return jsonify ( { '<STR_LIT>' : conversation_id , '<STR_LIT>' : response } ) \n @ app . route ( '<STR_LIT>' ) \n def get_chat_history ( conversation_id ) : \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n print ( isproxy ) \n client = Client ( cookie , isproxy ) \n history = client . chat_conversation_history ( conversation_id ) \n return jsonify ( history ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def send_message ( ) : \n data = request . get_json ( ) \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n response = client . send_message ( prompt , conversation_id ) \n return jsonify ( { '<STR_LIT>' : response } ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def send_message_attachment ( ) : \n conversation_id = request . form . get ( \"<STR_LIT>\" ) \n prompt = request . form . get ( \"<STR_LIT>\" ) \n file = request . files [ '<STR_LIT>' ] \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n file_path = None \n if file : \n file_path = save_upload_file ( file ) \n response = client . send_message ( prompt , conversation_id , file_path ) \n return jsonify ( { '<STR_LIT>' : response } ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def reset_conversations ( ) : \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n result = client . reset_all ( ) \n return jsonify ( { '<STR_LIT>' : result } ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def rename_conversation ( ) : \n data = request . get_json ( ) \n conversation_id = data [ '<STR_LIT>' ] \n title = data [ '<STR_LIT>' ] \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n result = client . rename_chat ( title , conversation_id ) \n return jsonify ( { '<STR_LIT>' : result } ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def upload_attachment ( ) : \n file = request . files [ '<STR_LIT>' ] \n if file : \n file_path = save_upload_file ( file ) \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n response = client . upload_attachment ( file_path ) \n return jsonify ( { '<STR_LIT>' : response } ) \n else : \n return jsonify ( { '<STR_LIT>' : '<STR_LIT>' } ) , <NUM_LIT> \n def save_upload_file ( file ) : \n uploads_dir = os . getenv ( '<STR_LIT>' ) \n print ( uploads_dir ) \n file_path = os . path . join ( uploads_dir , file . filename ) \n file . save ( file_path ) \n return file_path \n @ app . route ( '<STR_LIT>' )", "output": "def list_all_conversations ( ) : \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n conversations = client . list_all_conversations ( ) \n return jsonify ( conversations ) \n @ app . route ( '<STR_LIT>' ) \n def chat_conversation_history ( conversation_id ) : \n cookie = get_cookie ( ) \n isproxy = get_proxy ( ) \n client = Client ( cookie , isproxy ) \n history = client . chat_conversation_history ( conversation_id ) \n return jsonify ( history ) \n if __name__ == '<STR_LIT>' : \n app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = True , use_reloader = True ) \n app . default_encoding = '<STR_LIT>'"}, {"input": "import time \n import re \n import io \n import json \n import copy \n import logging \n from . . import config , utils \n from . . returnvalues import ReturnValue \n from . . storage import contact_change \n from . . utils import update_info_dict \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_contact ( core ) : \n core . update_chatroom = update_chatroom \n core . update_friend = update_friend \n core . get_contact = get_contact \n core . get_friends = get_friends \n core . get_chatrooms = get_chatrooms \n core . get_mps = get_mps \n core . set_alias = set_alias \n core . set_pinned = set_pinned \n core . accept_friend = accept_friend \n core . get_head_img = get_head_img \n core . create_chatroom = create_chatroom \n core . set_chatroom_name = set_chatroom_name \n core . delete_member_from_chatroom = delete_member_from_chatroom \n core . add_member_into_chatroom = add_member_into_chatroom \n def update_chatroom ( self , userName , detailedMember = False ) : \n if not isinstance ( userName , list ) : \n userName = [ userName ] \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , int ( time . time ( ) ) ) \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : len ( userName ) , \n '<STR_LIT>' : [ { \n '<STR_LIT>' : u , \n '<STR_LIT>' : '<STR_LIT>' , } for u in userName ] , } \n chatroomList = json . loads ( self . s . post ( url , data = json . dumps ( data ) , headers = headers \n ) . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) . get ( '<STR_LIT>' ) \n if not chatroomList : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n if detailedMember : \n def get_detailed_member_info ( encryChatroomId , memberList ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , int ( time . time ( ) ) ) \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , } \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : len ( memberList ) , \n '<STR_LIT>' : [ { \n '<STR_LIT>' : member [ '<STR_LIT>' ] , \n '<STR_LIT>' : encryChatroomId } \n for member in memberList ] , } \n return json . loads ( self . s . post ( url , data = json . dumps ( data ) , headers = headers \n ) . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) [ '<STR_LIT>' ] \n MAX_GET_NUMBER = <NUM_LIT> \n for chatroom in chatroomList : \n totalMemberList = [ ] \n for i in range ( int ( len ( chatroom [ '<STR_LIT>' ] ) / MAX_GET_NUMBER + <NUM_LIT> ) ) : \n memberList = chatroom [ '<STR_LIT>' ] [ i * \n MAX_GET_NUMBER : ( i + <NUM_LIT> ) * MAX_GET_NUMBER ] \n totalMemberList += get_detailed_member_info ( \n chatroom [ '<STR_LIT>' ] , memberList ) \n chatroom [ '<STR_LIT>' ] = totalMemberList \n update_local_chatrooms ( self , chatroomList ) \n r = [ self . storageClass . search_chatrooms ( userName = c [ '<STR_LIT>' ] ) \n for c in chatroomList ] \n return r if <NUM_LIT> < len ( r ) else r [ <NUM_LIT> ] \n def update_friend ( self , userName ) : \n if not isinstance ( userName , list ) : \n userName = [ userName ] \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , int ( time . time ( ) ) ) \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : len ( userName ) , \n '<STR_LIT>' : [ { \n '<STR_LIT>' : u , \n '<STR_LIT>' : '<STR_LIT>' , } for u in userName ] , } \n friendList = json . loads ( self . s . post ( url , data = json . dumps ( data ) , headers = headers \n ) . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) . get ( '<STR_LIT>' ) \n update_local_friends ( self , friendList ) \n r = [ self . storageClass . search_friends ( userName = f [ '<STR_LIT>' ] ) \n for f in friendList ] \n return r if len ( r ) != <NUM_LIT> else r [ <NUM_LIT> ] \n @ contact_change \n def update_local_chatrooms ( core , l ) : \n for chatroom in l : \n utils . emoji_formatter ( chatroom , '<STR_LIT>' ) \n for member in chatroom [ '<STR_LIT>' ] : \n if '<STR_LIT>' in member : \n utils . emoji_formatter ( member , '<STR_LIT>' ) \n if '<STR_LIT>' in member : \n utils . emoji_formatter ( member , '<STR_LIT>' ) \n if '<STR_LIT>' in member : \n utils . emoji_formatter ( member , '<STR_LIT>' ) \n oldChatroom = utils . search_dict_list ( \n core . chatroomList , '<STR_LIT>' , chatroom [ '<STR_LIT>' ] ) \n if oldChatroom : \n update_info_dict ( oldChatroom , chatroom ) \n memberList = chatroom . get ( '<STR_LIT>' , [ ] ) \n oldMemberList = oldChatroom [ '<STR_LIT>' ] \n if memberList : \n for member in memberList : \n oldMember = utils . search_dict_list ( \n oldMemberList , '<STR_LIT>' , member [ '<STR_LIT>' ] ) \n if oldMember : \n update_info_dict ( oldMember , member ) \n else : \n oldMemberList . append ( member ) \n else : \n core . chatroomList . append ( chatroom ) \n oldChatroom = utils . search_dict_list ( \n core . chatroomList , '<STR_LIT>' , chatroom [ '<STR_LIT>' ] ) \n if len ( chatroom [ '<STR_LIT>' ] ) != len ( oldChatroom [ '<STR_LIT>' ] ) and chatroom [ '<STR_LIT>' ] : \n existsUserNames = [ member [ '<STR_LIT>' ] \n for member in chatroom [ '<STR_LIT>' ] ] \n delList = [ ] \n for i , member in enumerate ( oldChatroom [ '<STR_LIT>' ] ) : \n if member [ '<STR_LIT>' ] not in existsUserNames : \n delList . append ( i ) \n delList . sort ( reverse = True ) \n for i in delList : \n del oldChatroom [ '<STR_LIT>' ] [ i ] \n if oldChatroom . get ( '<STR_LIT>' ) and oldChatroom . get ( '<STR_LIT>' ) : \n owner = utils . search_dict_list ( oldChatroom [ '<STR_LIT>' ] , \n '<STR_LIT>' , oldChatroom [ '<STR_LIT>' ] ) \n oldChatroom [ '<STR_LIT>' ] = ( owner or { } ) . get ( '<STR_LIT>' , <NUM_LIT> ) \n if '<STR_LIT>' in oldChatroom and oldChatroom [ '<STR_LIT>' ] != <NUM_LIT> : \n oldChatroom [ '<STR_LIT>' ] = oldChatroom [ '<STR_LIT>' ] == int ( core . loginInfo [ '<STR_LIT>' ] ) \n else : \n oldChatroom [ '<STR_LIT>' ] = None \n newSelf = utils . search_dict_list ( oldChatroom [ '<STR_LIT>' ] , \n '<STR_LIT>' , core . storageClass . userName ) \n oldChatroom [ '<STR_LIT>' ] = newSelf or copy . deepcopy ( core . loginInfo [ '<STR_LIT>' ] ) \n return { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : [ chatroom [ '<STR_LIT>' ] for chatroom in l ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : core . storageClass . userName , \n '<STR_LIT>' : core . storageClass . userName , } \n @ contact_change \n def update_local_friends ( core , l ) : \n fullList = core . memberList + core . mpList \n for friend in l : \n if '<STR_LIT>' in friend : \n utils . emoji_formatter ( friend , '<STR_LIT>' ) \n if '<STR_LIT>' in friend : \n utils . emoji_formatter ( friend , '<STR_LIT>' ) \n if '<STR_LIT>' in friend : \n utils . emoji_formatter ( friend , '<STR_LIT>' ) \n oldInfoDict = utils . search_dict_list ( \n fullList , '<STR_LIT>' , friend [ '<STR_LIT>' ] ) \n if oldInfoDict is None : \n oldInfoDict = copy . deepcopy ( friend ) \n if oldInfoDict [ '<STR_LIT>' ] & <NUM_LIT> == <NUM_LIT> : \n core . memberList . append ( oldInfoDict ) \n else : \n core . mpList . append ( oldInfoDict ) \n else : \n update_info_dict ( oldInfoDict , friend ) \n @ contact_change \n def update_local_uin ( core , msg ) : \n uins = re . search ( '<STR_LIT>' , msg [ '<STR_LIT>' ] ) \n usernameChangedList = [ ] \n r = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : usernameChangedList , \n '<STR_LIT>' : '<STR_LIT>' , } \n if uins : \n uins = uins . group ( <NUM_LIT> ) . split ( '<STR_LIT>' ) \n usernames = msg [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) \n if <NUM_LIT> < len ( uins ) == len ( usernames ) : \n for uin , username in zip ( uins , usernames ) : \n if not '<STR_LIT>' in username : \n continue \n fullContact = core . memberList + core . chatroomList + core . mpList \n userDicts = utils . search_dict_list ( fullContact , \n '<STR_LIT>' , username ) \n if userDicts : \n if userDicts . get ( '<STR_LIT>' , <NUM_LIT> ) == <NUM_LIT> : \n userDicts [ '<STR_LIT>' ] = uin \n usernameChangedList . append ( username ) \n logger . debug ( '<STR_LIT>' % ( username , uin ) ) \n else : \n if userDicts [ '<STR_LIT>' ] != uin : \n logger . debug ( '<STR_LIT>' % ( \n userDicts [ '<STR_LIT>' ] , uin ) ) \n else : \n if '<STR_LIT>' in username : \n core . storageClass . updateLock . release ( ) \n update_chatroom ( core , username ) \n core . storageClass . updateLock . acquire ( ) \n newChatroomDict = utils . search_dict_list ( \n core . chatroomList , '<STR_LIT>' , username ) \n if newChatroomDict is None : \n newChatroomDict = utils . struct_friend_info ( { \n '<STR_LIT>' : username , \n '<STR_LIT>' : uin , \n '<STR_LIT>' : copy . deepcopy ( core . loginInfo [ '<STR_LIT>' ] ) } ) \n core . chatroomList . append ( newChatroomDict ) \n else : \n newChatroomDict [ '<STR_LIT>' ] = uin \n elif '<STR_LIT>' in username : \n core . storageClass . updateLock . release ( ) \n update_friend ( core , username ) \n core . storageClass . updateLock . acquire ( ) \n newFriendDict = utils . search_dict_list ( \n core . memberList , '<STR_LIT>' , username ) \n if newFriendDict is None : \n newFriendDict = utils . struct_friend_info ( { \n '<STR_LIT>' : username , \n '<STR_LIT>' : uin , } ) \n core . memberList . append ( newFriendDict ) \n else : \n newFriendDict [ '<STR_LIT>' ] = uin \n usernameChangedList . append ( username ) \n logger . debug ( '<STR_LIT>' % ( username , uin ) ) \n else : \n logger . debug ( '<STR_LIT>' % ( \n len ( uins ) , len ( usernames ) ) ) \n else : \n logger . debug ( '<STR_LIT>' ) \n logger . debug ( msg [ '<STR_LIT>' ] ) \n return r \n def get_contact ( self , update = False ) : \n if not update : \n return utils . contact_deep_copy ( self , self . chatroomList ) \n def _get_contact ( seq = <NUM_LIT> ) : \n url = '<STR_LIT>' % ( self . loginInfo [ '<STR_LIT>' ] , \n int ( time . time ( ) ) , seq , self . loginInfo [ '<STR_LIT>' ] ) \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , }", "output": "try : \n r = self . s . get ( url , headers = headers ) \n except : \n logger . info ( \n '<STR_LIT>' ) \n for chatroom in self . get_chatrooms ( ) : \n self . update_chatroom ( chatroom [ '<STR_LIT>' ] , detailedMember = True ) \n return <NUM_LIT> , [ ] \n j = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n return j . get ( '<STR_LIT>' , <NUM_LIT> ) , j . get ( '<STR_LIT>' ) \n seq , memberList = <NUM_LIT> , [ ] \n while <NUM_LIT> : \n seq , batchMemberList = _get_contact ( seq ) \n memberList . extend ( batchMemberList ) \n if seq == <NUM_LIT> : \n break \n chatroomList , otherList = [ ] , [ ] \n for m in memberList : \n if m [ '<STR_LIT>' ] != <NUM_LIT> : \n otherList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n chatroomList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n otherList . append ( m ) \n if chatroomList : \n update_local_chatrooms ( self , chatroomList ) \n if otherList : \n update_local_friends ( self , otherList ) \n return utils . contact_deep_copy ( self , chatroomList ) \n def get_friends ( self , update = False ) : \n if update : \n self . get_contact ( update = True ) \n return utils . contact_deep_copy ( self , self . memberList ) \n def get_chatrooms ( self , update = False , contactOnly = False ) : \n if contactOnly : \n return self . get_contact ( update = True ) \n else : \n if update : \n self . get_contact ( True ) \n return utils . contact_deep_copy ( self , self . chatroomList ) \n def get_mps ( self , update = False ) : \n if update : \n self . get_contact ( update = True ) \n return utils . contact_deep_copy ( self , self . mpList ) \n def set_alias ( self , userName , alias ) : \n oldFriendInfo = utils . search_dict_list ( \n self . memberList , '<STR_LIT>' , userName ) \n if oldFriendInfo is None : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : userName , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : alias , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' ) , \n headers = headers ) \n r = ReturnValue ( rawResponse = r ) \n if r : \n oldFriendInfo [ '<STR_LIT>' ] = alias \n return r \n def set_pinned ( self , userName , isPinned = True ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : userName , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : int ( isPinned ) , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , json = data , headers = headers ) \n return ReturnValue ( rawResponse = r ) \n def accept_friend ( self , userName , v4 = '<STR_LIT>' , autoUpdate = True ) : \n url = f\"<STR_LIT>\" \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : [ { \n '<STR_LIT>' : userName , \n '<STR_LIT>' : v4 , } ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : [ <NUM_LIT> ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , headers = headers , \n data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if autoUpdate : \n self . update_friend ( userName ) \n return ReturnValue ( rawResponse = r ) \n def get_head_img ( self , userName = None , chatroomUserName = None , picDir = None ) : \n params = { \n '<STR_LIT>' : userName or chatroomUserName or self . storageClass . userName , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , } \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n if chatroomUserName is None : \n infoDict = self . storageClass . search_friends ( userName = userName ) \n if infoDict is None : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n else : \n if userName is None : \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n else : \n chatroom = self . storageClass . search_chatrooms ( \n userName = chatroomUserName ) \n if chatroomUserName is None : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n if '<STR_LIT>' in chatroom : \n params [ '<STR_LIT>' ] = chatroom [ '<STR_LIT>' ] \n params [ '<STR_LIT>' ] = params . get ( \n '<STR_LIT>' ) or chatroom [ '<STR_LIT>' ] \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . get ( url , params = params , stream = True , headers = headers ) \n tempStorage = io . BytesIO ( ) \n for block in r . iter_content ( <NUM_LIT> ) : \n tempStorage . write ( block ) \n if picDir is None : \n return tempStorage . getvalue ( ) \n with open ( picDir , '<STR_LIT>' ) as f : \n f . write ( tempStorage . getvalue ( ) ) \n tempStorage . seek ( <NUM_LIT> ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , } , \n '<STR_LIT>' : utils . get_image_postfix ( tempStorage . read ( <NUM_LIT> ) ) , } ) \n def create_chatroom ( self , memberList , topic = '<STR_LIT>' ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] , int ( time . time ( ) ) ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : len ( memberList . split ( '<STR_LIT>' ) ) , \n '<STR_LIT>' : [ { '<STR_LIT>' : member } for member in memberList . split ( '<STR_LIT>' ) ] , \n '<STR_LIT>' : topic , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , headers = headers , \n data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n return ReturnValue ( rawResponse = r ) \n def set_chatroom_name ( self , chatroomUserName , name ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : chatroomUserName , \n '<STR_LIT>' : name , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , headers = headers , \n data = json . dumps ( data , ensure_ascii = False ) . encode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n return ReturnValue ( rawResponse = r ) \n def delete_member_from_chatroom ( self , chatroomUserName , memberList ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : chatroomUserName , \n '<STR_LIT>' : '<STR_LIT>' . join ( [ member [ '<STR_LIT>' ] for member in memberList ] ) , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , data = json . dumps ( data ) , headers = headers ) \n return ReturnValue ( rawResponse = r ) \n def add_member_into_chatroom ( self , chatroomUserName , memberList , \n useInvitation = False ) : \n if not useInvitation : \n chatroom = self . storageClass . search_chatrooms ( \n userName = chatroomUserName ) \n if not chatroom : \n chatroom = self . update_chatroom ( chatroomUserName ) \n if len ( chatroom [ '<STR_LIT>' ] ) > self . loginInfo [ '<STR_LIT>' ] : \n useInvitation = True \n if useInvitation : \n fun , memberKeyName = '<STR_LIT>' , '<STR_LIT>' \n else : \n fun , memberKeyName = '<STR_LIT>' , '<STR_LIT>' \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , fun , self . loginInfo [ '<STR_LIT>' ] ) \n params = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : chatroomUserName , \n memberKeyName : memberList , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , data = json . dumps ( params ) , headers = headers ) \n return ReturnValue ( rawResponse = r )"}, {"input": "from . base import ma \n from . user import UserSchema \n from app . models import Message \n class MessageSchema ( ma . SQLAlchemySchema ) : \n class Meta : \n model = Message", "output": "ordered = True \n author = ma . Nested ( UserSchema ) \n time = ma . auto_field ( ) \n content = ma . auto_field ( )"}, {"input": "import numpy as np \n import os \n from datetime import datetime \n import isaacgym \n from legged_gym . envs import * \n from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger \n import torch \n def test_env ( args ) : \n env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) \n env_cfg . env . num_envs = min ( env_cfg . env . num_envs , <NUM_LIT> ) \n env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) \n for i in range ( int ( <NUM_LIT> * env . max_episode_length ) ) : \n actions = <NUM_LIT> * torch . ones ( env . num_envs , env . num_actions , device = env . device ) \n obs , _ , rew , done , info = env . step ( actions ) \n print ( \"<STR_LIT>\" )", "output": "if __name__ == '<STR_LIT>' : \n args = get_args ( ) \n test_env ( args )"}, {"input": "import sys \n try : \n archivo = open ( '<STR_LIT>' , \"<STR_LIT>\" ) \n texto = archivo . readline ( ) \n i = int ( texto . strip ( ) ) \n except OSError as err : \n print ( \"<STR_LIT>\" , err )", "output": "except ValueError : \n \"<STR_LIT>\" \n except Exception as err : \n print ( \"<STR_LIT>\" , err ) \n raise \n finally : \n archivo . close ( ) \n print ( texto )"}, {"input": "import ast \n import json \n from pathlib import Path \n from collections import OrderedDict \n def extract_i18n_strings ( node ) : \n i18n_strings = [ ] \n if ( \n isinstance ( node , ast . Call ) \n and isinstance ( node . func , ast . Name )", "output": "and node . func . id == \"<STR_LIT>\" \n ) : \n for arg in node . args : \n if isinstance ( arg , ast . Str ) : \n i18n_strings . append ( arg . s ) \n for child_node in ast . iter_child_nodes ( node ) : \n i18n_strings . extend ( extract_i18n_strings ( child_node ) ) \n return i18n_strings \n def process_file ( file_path ) : \n with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n code = file . read ( ) \n if \"<STR_LIT>\" in code : \n tree = ast . parse ( code ) \n i18n_strings = extract_i18n_strings ( tree ) \n print ( file_path , len ( i18n_strings ) ) \n return i18n_strings \n return [ ] \n py_files = Path ( \"<STR_LIT>\" ) . rglob ( \"<STR_LIT>\" ) \n code_keys = set ( ) \n for py_file in py_files : \n strings = process_file ( py_file ) \n code_keys . update ( strings ) \n print ( ) \n print ( \"<STR_LIT>\" , len ( code_keys ) ) \n standard_file = \"<STR_LIT>\" \n with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n standard_data = json . load ( file , object_pairs_hook = OrderedDict ) \n standard_keys = set ( standard_data . keys ( ) ) \n unused_keys = standard_keys - code_keys \n missing_keys = code_keys - standard_keys \n print ( \"<STR_LIT>\" , len ( unused_keys ) ) \n for unused_key in unused_keys : \n print ( \"<STR_LIT>\" , unused_key ) \n print ( \"<STR_LIT>\" , len ( missing_keys ) ) \n for missing_key in missing_keys : \n print ( \"<STR_LIT>\" , missing_key ) \n code_keys_dict = OrderedDict ( ( s , s ) for s in code_keys ) \n with open ( standard_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( code_keys_dict , file , ensure_ascii = False , indent = <NUM_LIT> , sort_keys = True ) \n file . write ( \"<STR_LIT>\" )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None", "output": "depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n def downgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( \"<STR_LIT>\" ) , nullable = True ) )"}, {"input": "import datetime \n import json \n import dateparser \n from bs4 import BeautifulSoup \n from feedi . requests import requests \n from feedi . scraping import CachingRequestsMixin \n def fetch ( feed_name , url ) : \n parser = None \n for cls in CustomParser . __subclasses__ ( ) : \n if cls . is_compatible ( url ) : \n parser = cls ( feed_name , url ) \n if not parser : \n raise ValueError ( \"<STR_LIT>\" , url ) \n return parser . fetch ( ) \n class CustomParser ( CachingRequestsMixin ) : \n BASE_URL = '<STR_LIT>' \n def __init__ ( self , feed_name , url ) : \n super ( ) . __init__ ( ) \n self . feed_name = feed_name \n self . url = url \n @ classmethod \n def is_compatible ( cls , feed_url ) : \n return cls . BASE_URL in feed_url \n def fetch ( self ) : \n raise NotImplementedError \n class AgendaBAParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n api_url = f'<STR_LIT>' \n response = requests . get ( api_url ) \n items = response . json ( ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n entry_values = [ ] \n for item in items : \n created = datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) . date ( ) \n content_url = f'<STR_LIT>' \n entry_values . append ( { \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n '<STR_LIT>' : created , \n '<STR_LIT>' : created , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] [ '<STR_LIT>' ] , \n '<STR_LIT>' : content_url , \n '<STR_LIT>' : json . dumps ( item ) \n } ) \n return entry_values \n class RevistaLenguaParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n soup = BeautifulSoup ( response . content , '<STR_LIT>' ) \n entry_values = [ ] \n for a in soup . select ( '<STR_LIT>' ) : \n article_html = self . request ( a [ '<STR_LIT>' ] ) \n article_soup = BeautifulSoup ( article_html , '<STR_LIT>' ) \n script = article_soup . find_all ( '<STR_LIT>' , type = \"<STR_LIT>\" ) [ <NUM_LIT> ] \n item = json . loads ( script . text ) \n del item [ '<STR_LIT>' ] \n entry_values . append ( { \n '<STR_LIT>' : json . dumps ( item ) , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , \n '<STR_LIT>' : datetime . datetime . fromisoformat ( item [ '<STR_LIT>' ] ) , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : item [ '<STR_LIT>' ] , \n '<STR_LIT>' : None , \n } ) \n return entry_values \n class EternaCadenciaParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n soup = BeautifulSoup ( response . content , '<STR_LIT>' ) \n entry_values = [ ] \n for article in soup . find_all ( class_ = '<STR_LIT>' ) : \n content_url = self . BASE_URL + article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] \n author = article . find ( class_ = '<STR_LIT>' ) \n if author : \n author = author . text \n date_str = article . find ( class_ = '<STR_LIT>' ) . text \n date = dateparser . parse ( date_str , languages = [ '<STR_LIT>' ] ) \n entry_values . append ( { \n '<STR_LIT>' : content_url . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , \n '<STR_LIT>' : author , \n '<STR_LIT>' : date , \n '<STR_LIT>' : date , \n '<STR_LIT>' : article . find ( class_ = '<STR_LIT>' ) . text , \n '<STR_LIT>' : article . find ( '<STR_LIT>' ) [ '<STR_LIT>' ] , \n '<STR_LIT>' : content_url ,", "output": "'<STR_LIT>' : content_url , \n } ) \n return entry_values \n class PioneerWorksParser ( CustomParser ) : \n BASE_URL = '<STR_LIT>' \n def fetch ( self ) : \n url = f'<STR_LIT>' \n response = requests . get ( url ) \n script = BeautifulSoup ( response . content , '<STR_LIT>' ) . find ( id = '<STR_LIT>' ) . text \n directory = json . loads ( script ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n entry_values = [ ] \n for article in directory : \n if not article . get ( '<STR_LIT>' ) or article . get ( '<STR_LIT>' ) != '<STR_LIT>' : \n continue \n pub_date = datetime . datetime . fromisoformat ( article . get ( '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) \n if datetime . datetime . now ( ) - pub_date > datetime . timedelta ( days = <NUM_LIT> ) : \n continue \n article_url = f'<STR_LIT>' \n entry_values . append ( { \n '<STR_LIT>' : json . dumps ( article ) , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : article [ '<STR_LIT>' ] , \n '<STR_LIT>' : pub_date , \n '<STR_LIT>' : pub_date , \n '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : self . fetch_meta ( article_url , '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : article_url , \n } ) \n return entry_values"}, {"input": "from flask import request , jsonify \n from app import app , mongo , photos \n import os \n import uuid \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def upload ( ) : \n if '<STR_LIT>' in request . files :", "output": "filename = photos . save ( request . files [ '<STR_LIT>' ] , name = str ( uuid . uuid4 ( ) ) + '<STR_LIT>' ) \n return jsonify ( { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : filename } ) \n return jsonify ( { \"<STR_LIT>\" : \"<STR_LIT>\" } ) , <NUM_LIT>"}, {"input": "import argparse \n import config \n from channel import channel_factory \n from common import log , const \n from multiprocessing import Pool \n from plugins . plugin_manager import PluginManager \n def start_process ( channel_type , config_path ) : \n try : \n config . load_config ( config_path ) \n model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n log . info ( \"<STR_LIT>\" , model_type , channel_type ) \n channel = channel_factory . create_channel ( channel_type ) \n channel . startup ( ) \n except Exception as e : \n log . error ( \"<STR_LIT>\" , channel_type , str ( e ) ) \n raise e \n def main ( ) : \n try : \n config . load_config ( args . config ) \n model_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n channel_type = config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n PluginManager ( ) \n if not isinstance ( channel_type , list ) : \n start_process ( channel_type , args . config ) \n exit ( <NUM_LIT> ) \n if len ( channel_type ) == <NUM_LIT> : \n start_process ( channel_type [ <NUM_LIT> ] , args . config ) \n exit ( <NUM_LIT> ) \n if const . TERMINAL in channel_type : \n index = channel_type . index ( const . TERMINAL ) \n terminal = channel_type . pop ( index ) \n else : \n terminal = None \n pool = Pool ( len ( channel_type ) ) \n for type_item in channel_type : \n log . info ( \"<STR_LIT>\" , model_type , type_item ) \n pool . apply_async ( start_process , args = [ type_item , args . config ] ) \n if terminal : \n start_process ( terminal , args . config ) \n pool . close ( )", "output": "pool . join ( ) \n except Exception as e : \n log . error ( \"<STR_LIT>\" ) \n log . exception ( e ) \n if __name__ == '<STR_LIT>' : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , help = \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" ) \n args = parser . parse_args ( ) \n main ( )"}, {"input": "import os \n import sys \n import faiss \n import numpy as np \n from sklearn . cluster import MiniBatchKMeans \n from multiprocessing import cpu_count \n exp_dir = sys . argv [ <NUM_LIT> ] \n version = sys . argv [ <NUM_LIT> ] \n try : \n if version == \"<STR_LIT>\" : \n feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) \n elif version == \"<STR_LIT>\" : \n feature_dir = os . path . join ( exp_dir , \"<STR_LIT>\" ) \n npys = [ ] \n listdir_res = sorted ( os . listdir ( feature_dir ) ) \n for name in listdir_res : \n file_path = os . path . join ( feature_dir , name ) \n phone = np . load ( file_path ) \n npys . append ( phone ) \n big_npy = np . concatenate ( npys , axis = <NUM_LIT> ) \n big_npy_idx = np . arange ( big_npy . shape [ <NUM_LIT> ] ) \n np . random . shuffle ( big_npy_idx ) \n big_npy = big_npy [ big_npy_idx ] \n if big_npy . shape [ <NUM_LIT> ] > <NUM_LIT> : \n big_npy = ( \n MiniBatchKMeans ( \n n_clusters = <NUM_LIT> , \n verbose = True , \n batch_size = <NUM_LIT> * cpu_count ( ) , \n compute_labels = False , \n init = \"<STR_LIT>\" , \n ) \n . fit ( big_npy ) \n . cluster_centers_ \n ) \n np . save ( os . path . join ( exp_dir , \"<STR_LIT>\" ) , big_npy ) \n n_ivf = min ( int ( <NUM_LIT> * np . sqrt ( big_npy . shape [ <NUM_LIT> ] ) ) , big_npy . shape [ <NUM_LIT> ] // <NUM_LIT> ) \n index_trained = faiss . index_factory ( \n <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" \n ) \n index_ivf_trained = faiss . extract_index_ivf ( index_trained ) \n index_ivf_trained . nprobe = <NUM_LIT> \n index_trained . train ( big_npy ) \n index_filename_trained = ( \n f\"<STR_LIT>\" \n ) \n index_filepath_trained = os . path . join ( exp_dir , index_filename_trained ) \n faiss . write_index ( index_trained , index_filepath_trained ) \n index_added = faiss . index_factory ( \n <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , f\"<STR_LIT>\" \n ) \n index_ivf_added = faiss . extract_index_ivf ( index_added ) \n index_ivf_added . nprobe = <NUM_LIT> \n index_added . train ( big_npy ) \n index_filename_added = ( \n f\"<STR_LIT>\" \n ) \n index_filepath_added = os . path . join ( exp_dir , index_filename_added ) \n batch_size_add = <NUM_LIT> \n for i in range ( <NUM_LIT> , big_npy . shape [ <NUM_LIT> ] , batch_size_add ) : \n index_added . add ( big_npy [ i : i + batch_size_add ] ) \n faiss . write_index ( index_added , index_filepath_added ) \n print ( f\"<STR_LIT>\" ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n if \"<STR_LIT>\" in str ( error ) : \n print ( \n \"<STR_LIT>\"", "output": ")"}, {"input": "import numpy as np \n class Slicer : \n def __init__ ( \n self , \n sr : int , \n threshold : float = - <NUM_LIT> , \n min_length : int = <NUM_LIT> , \n min_interval : int = <NUM_LIT> , \n hop_size : int = <NUM_LIT> , \n max_sil_kept : int = <NUM_LIT> , \n ) : \n if not min_length >= min_interval >= hop_size : \n raise ValueError ( \"<STR_LIT>\" ) \n if not max_sil_kept >= hop_size : \n raise ValueError ( \"<STR_LIT>\" ) \n min_interval = sr * min_interval / <NUM_LIT> \n self . threshold = <NUM_LIT> ** ( threshold / <NUM_LIT> ) \n self . hop_size = round ( sr * hop_size / <NUM_LIT> ) \n self . win_size = min ( round ( min_interval ) , <NUM_LIT> * self . hop_size ) \n self . min_length = round ( sr * min_length / <NUM_LIT> / self . hop_size ) \n self . min_interval = round ( min_interval / self . hop_size ) \n self . max_sil_kept = round ( sr * max_sil_kept / <NUM_LIT> / self . hop_size ) \n def _apply_slice ( self , waveform , begin , end ) : \n start_idx = begin * self . hop_size \n if len ( waveform . shape ) > <NUM_LIT> : \n end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) \n return waveform [ : , start_idx : end_idx ] \n else : \n end_idx = min ( waveform . shape [ <NUM_LIT> ] , end * self . hop_size ) \n return waveform [ start_idx : end_idx ] \n def slice ( self , waveform ) : \n samples = waveform . mean ( axis = <NUM_LIT> ) if len ( waveform . shape ) > <NUM_LIT> else waveform \n if samples . shape [ <NUM_LIT> ] <= self . min_length : \n return [ waveform ] \n rms_list = get_rms ( \n y = samples , frame_length = self . win_size , hop_length = self . hop_size \n ) . squeeze ( <NUM_LIT> ) \n sil_tags = [ ] \n silence_start , clip_start = None , <NUM_LIT> \n for i , rms in enumerate ( rms_list ) : \n if rms < self . threshold : \n if silence_start is None : \n silence_start = i \n continue \n if silence_start is None : \n continue \n is_leading_silence = silence_start == <NUM_LIT> and i > self . max_sil_kept \n need_slice_middle = ( \n i - silence_start >= self . min_interval \n and i - clip_start >= self . min_length \n ) \n if not is_leading_silence and not need_slice_middle : \n silence_start = None \n continue \n if i - silence_start <= self . max_sil_kept : \n pos = rms_list [ silence_start : i + <NUM_LIT> ] . argmin ( ) + silence_start \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos ) ) \n else :", "output": "sil_tags . append ( ( pos , pos ) ) \n clip_start = pos \n elif i - silence_start <= self . max_sil_kept * <NUM_LIT> : \n pos = rms_list [ \n i - self . max_sil_kept : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n pos += i - self . max_sil_kept \n pos_l = ( \n rms_list [ \n silence_start : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n + silence_start \n ) \n pos_r = ( \n rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) \n + i \n - self . max_sil_kept \n ) \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos_r ) ) \n clip_start = pos_r \n else : \n sil_tags . append ( ( min ( pos_l , pos ) , max ( pos_r , pos ) ) ) \n clip_start = max ( pos_r , pos ) \n else : \n pos_l = ( \n rms_list [ \n silence_start : silence_start + self . max_sil_kept + <NUM_LIT> \n ] . argmin ( ) \n + silence_start \n ) \n pos_r = ( \n rms_list [ i - self . max_sil_kept : i + <NUM_LIT> ] . argmin ( ) \n + i \n - self . max_sil_kept \n ) \n if silence_start == <NUM_LIT> : \n sil_tags . append ( ( <NUM_LIT> , pos_r ) ) \n else : \n sil_tags . append ( ( pos_l , pos_r ) ) \n clip_start = pos_r \n silence_start = None \n total_frames = rms_list . shape [ <NUM_LIT> ] \n if ( \n silence_start is not None \n and total_frames - silence_start >= self . min_interval \n ) : \n silence_end = min ( total_frames , silence_start + self . max_sil_kept ) \n pos = rms_list [ silence_start : silence_end + <NUM_LIT> ] . argmin ( ) + silence_start \n sil_tags . append ( ( pos , total_frames + <NUM_LIT> ) ) \n if not sil_tags : \n return [ waveform ] \n else : \n chunks = [ ] \n if sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] > <NUM_LIT> : \n chunks . append ( self . _apply_slice ( waveform , <NUM_LIT> , sil_tags [ <NUM_LIT> ] [ <NUM_LIT> ] ) ) \n for i in range ( len ( sil_tags ) - <NUM_LIT> ) : \n chunks . append ( \n self . _apply_slice ( waveform , sil_tags [ i ] [ <NUM_LIT> ] , sil_tags [ i + <NUM_LIT> ] [ <NUM_LIT> ] ) \n ) \n if sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] < total_frames : \n chunks . append ( \n self . _apply_slice ( waveform , sil_tags [ - <NUM_LIT> ] [ <NUM_LIT> ] , total_frames ) \n ) \n return chunks \n def get_rms ( \n y , \n frame_length = <NUM_LIT> , \n hop_length = <NUM_LIT> , \n pad_mode = \"<STR_LIT>\" , \n ) : \n padding = ( int ( frame_length // <NUM_LIT> ) , int ( frame_length // <NUM_LIT> ) ) \n y = np . pad ( y , padding , mode = pad_mode ) \n axis = - <NUM_LIT> \n out_strides = y . strides + tuple ( [ y . strides [ axis ] ] ) \n x_shape_trimmed = list ( y . shape ) \n x_shape_trimmed [ axis ] -= frame_length - <NUM_LIT> \n out_shape = tuple ( x_shape_trimmed ) + tuple ( [ frame_length ] ) \n xw = np . lib . stride_tricks . as_strided ( y , shape = out_shape , strides = out_strides ) \n if axis < <NUM_LIT> : \n target_axis = axis - <NUM_LIT> \n else : \n target_axis = axis + <NUM_LIT> \n xw = np . moveaxis ( xw , - <NUM_LIT> , target_axis ) \n slices = [ slice ( None ) ] * xw . ndim \n slices [ axis ] = slice ( <NUM_LIT> , None , hop_length ) \n x = xw [ tuple ( slices ) ] \n power = np . mean ( np . abs ( x ) ** <NUM_LIT> , axis = - <NUM_LIT> , keepdims = True ) \n return np . sqrt ( power )"}, {"input": "from typing import Iterable , Dict , Any , Callable , Union \n import logging \n import random \n import string \n import re \n from . const import WafFunc , PythonEnvironment , SET_STMT_PATTERNS \n from . options import Options \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Context = Dict [ str , Any ] \n ContextPayloads = Dict [ str , Context ] \n Waf = Callable [ [ str ] , bool ] \n context_payloads_stmts : ContextPayloads = { \n \"<STR_LIT>\" \n + \"<STR_LIT>\" : { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" : { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n } \n context_payloads_stmts_py3 = { \n ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" \n ) : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } \n context_payloads_exprs = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" \n ) : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } \n context_payloads_exprs_py3 = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" \n ) : \"<STR_LIT>\" , \n ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" \n + \"<STR_LIT>\" \n ) : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } \n digit_looks_similiar = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def digit_to_similiar_alpha ( s : str ) -> str : \n for d , c in digit_looks_similiar . items ( ) : \n s = s . replace ( d , c ) \n return s \n def filter_by_waf ( \n context_payloads : ContextPayloads , waf : Callable [ [ str ] , bool ] \n ) -> ContextPayloads : \n return { payload : d for payload , d in context_payloads . items ( ) if waf ( payload ) } \n def filter_by_used_context ( \n context_payloads : ContextPayloads , used_context : Iterable \n ) -> ContextPayloads : \n return { \n payload : d \n for payload , d in context_payloads . items ( ) \n if any ( var_name in used_context for var_name in d . keys ( ) ) \n } \n class ContextVariableManager : \n def __init__ ( self , waf : WafFunc , context_payloads : ContextPayloads ) : \n self . waf = waf \n self . context_payloads = context_payloads . copy ( ) \n self . payload_dependency = { } \n self . prepared = False \n def do_prepare ( self ) : \n if self . prepared : \n return \n self . context_payloads = filter_by_waf ( self . context_payloads , self . waf ) \n self . prepared = True \n def is_variable_exists ( self , var_name : str ) -> bool : \n all_vars = set ( v for d in self . context_payloads . values ( ) for v in d ) \n return var_name in all_vars \n def generate_related_variable_name ( self , value : Any ) -> Union [ str , None ] : \n value = \"<STR_LIT>\" . join ( re . findall ( \"<STR_LIT>\" , repr ( value ) ) ) . lower ( ) \n value = digit_to_similiar_alpha ( value ) \n if len ( value ) < <NUM_LIT> : \n return None \n for c in value [ <NUM_LIT> : ] : \n var_name = value [ <NUM_LIT> ] + c \n if self . is_variable_exists ( var_name ) : \n continue \n if not self . waf ( var_name ) : \n continue \n return var_name \n return None \n def generate_random_variable_name ( self ) -> Union [ str , None ] : \n var_name = None \n for i in range ( <NUM_LIT> ) : \n var_length = <NUM_LIT> if i < <NUM_LIT> else <NUM_LIT> \n name = \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = var_length ) ) \n if self . is_variable_exists ( name ) : \n continue \n if not self . waf ( name ) : \n continue \n var_name = name \n break \n return var_name \n def add_payload ( \n self , \n payload : str , \n variables : Context , \n depends_on : Union [ Context , None ] = None , \n check_waf : bool = True , \n ) -> bool : \n if not self . prepared : \n self . do_prepare ( ) \n if check_waf and not self . waf ( payload ) : \n return False \n if any ( self . is_variable_exists ( v ) for v in variables ) : \n return False \n if depends_on is not None : \n if not all ( self . is_variable_exists ( v ) for v in depends_on ) : \n notfound_vars = [ \n v for v in depends_on if not self . is_variable_exists ( v ) \n ] \n logger . warning ( \"<STR_LIT>\" , repr ( notfound_vars ) ) \n return False \n self . payload_dependency [ payload ] = depends_on \n self . context_payloads [ payload ] = variables \n return True \n def get_payload ( self , used_context : Context ) -> str : \n if not self . prepared : \n self . do_prepare ( ) \n answer = \"<STR_LIT>\" \n to_add_vars = list ( used_context . keys ( ) ) \n added_vars = set ( ) \n while to_add_vars : \n to_add = to_add_vars . pop ( <NUM_LIT> ) \n if to_add in added_vars : \n continue \n if not self . is_variable_exists ( to_add ) : \n raise RuntimeError ( f\"<STR_LIT>\" ) \n payload = next ( \n payload for payload , d in self . context_payloads . items ( ) if to_add in d \n ) \n if payload in self . payload_dependency : \n vars_name = list ( self . payload_dependency [ payload ] . keys ( ) ) \n assert all ( self . is_variable_exists ( v ) for v in vars_name ) \n if not all ( v in added_vars for v in vars_name ) : \n to_add_vars += list ( self . payload_dependency [ payload ] ) \n to_add_vars . append ( to_add ) \n continue \n answer += payload \n added_vars . add ( to_add ) \n return answer \n def get_context ( self ) -> Context : \n return { \n var_name : var_value \n for _ , d in self . context_payloads . items ( ) \n for var_name , var_value in d . items ( ) \n } \n def get_context_vars_manager ( waf : WafFunc , options : Options ) -> ContextVariableManager : \n context_payloads = context_payloads_stmts . copy ( ) \n if options . python_version == PythonEnvironment . PYTHON3 : \n context_payloads . update ( context_payloads_stmts_py3 ) \n manager = ContextVariableManager ( waf , context_payloads ) \n manager . do_prepare ( ) \n set_stmt_pattern = None \n for pattern , test_pattern in SET_STMT_PATTERNS : \n if waf ( test_pattern ) : \n set_stmt_pattern = pattern \n break \n if not set_stmt_pattern : \n return manager \n exprs = context_payloads_exprs . copy ( ) \n if options . python_version == PythonEnvironment . PYTHON3 : \n exprs . update ( context_payloads_exprs_py3 ) \n for expr , value in exprs . items ( ) : \n if not waf ( expr ) : \n continue \n name = manager . generate_random_variable_name ( ) \n if not name : \n continue \n stmt = set_stmt_pattern . replace ( \"<STR_LIT>\" , name ) . replace ( \"<STR_LIT>\" , expr ) \n _ = manager . add_payload ( stmt , { name : value } ) \n return manager"}, {"input": "import logging \n import os \n import mutagen \n from . import file \n from . import util \n from . import aac \n from . import aiff \n from . import apev2 \n from . import asf \n from . import dsf \n from . import flac \n from . import id3 \n from . import mp4 \n from . import smf \n from . import vorbis \n from . import wave \n from . file import Artwork , MetadataItem , NotAppendable , AudioFile \n __version__ = \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n log = logger \n def _subclass_spider_dfs ( kls , _lst = None ) : \n if _lst is None : \n _lst = [ ] \n for sub in kls . __subclasses__ ( ) : \n _subclass_spider_dfs ( sub , _lst = _lst ) \n _lst . append ( kls ) \n return _lst \n def load_file ( file_spec , err = '<STR_LIT>' ) : \n if isinstance ( file_spec , mutagen . FileType ) : \n mfile = file_spec \n filename = mfile . filename \n else : \n filename = file_spec", "output": "if not os . path . exists ( filename ) : \n if os . path . exists ( os . path . expanduser ( os . path . expandvars ( filename ) ) ) : \n filename = os . path . expanduser ( os . path . expandvars ( filename ) ) \n elif os . path . exists ( os . path . expanduser ( filename ) ) : \n filename = os . path . expanduser ( filename ) \n mfile = mutagen . File ( filename , easy = False ) \n ret = None \n for kls in _subclass_spider_dfs ( file . AudioFile ) : \n if kls . mutagen_kls is not None and isinstance ( mfile , kls . mutagen_kls ) : \n ret = kls ( filename , _mfile = mfile ) \n break \n if ret is None and err == '<STR_LIT>' : \n raise NotImplementedError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( type ( mfile ) ) ) \n return ret \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ]"}, {"input": "from datasets import load_dataset \n from tqdm import tqdm \n import torch \n def run_winoground ( model , processor ) : \n winoground = load_dataset ( \"<STR_LIT>\" ) [ '<STR_LIT>' ] \n winoground_clip_scores = [ ] \n with torch . no_grad ( ) : \n for example in tqdm ( winoground ) : \n input_c0_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) \n input_c1_i0 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) \n input_c0_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) \n input_c1_i1 = processor ( text = [ example [ \"<STR_LIT>\" ] ] , images = [ example [ \"<STR_LIT>\" ] . convert ( \"<STR_LIT>\" ) ] , return_tensors = \"<STR_LIT>\" ) \n output_c0_i0 = model ( ** input_c0_i0 . to ( model . device ) ) \n output_c1_i0 = model ( ** input_c1_i0 . to ( model . device ) ) \n output_c0_i1 = model ( ** input_c0_i1 . to ( model . device ) ) \n output_c1_i1 = model ( ** input_c1_i1 . to ( model . device ) ) \n clip_score_c0_i0 = output_c0_i0 . logits_per_image . item ( ) \n clip_score_c1_i0 = output_c1_i0 . logits_per_image . item ( ) \n clip_score_c0_i1 = output_c0_i1 . logits_per_image . item ( )", "output": "clip_score_c1_i1 = output_c1_i1 . logits_per_image . item ( ) \n winoground_clip_scores . append ( { \"<STR_LIT>\" : example [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : clip_score_c0_i0 , \"<STR_LIT>\" : clip_score_c0_i1 , \"<STR_LIT>\" : clip_score_c1_i0 , \"<STR_LIT>\" : clip_score_c1_i1 } ) \n def text_correct ( result ) : \n return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] \n def image_correct ( result ) : \n return result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] and result [ \"<STR_LIT>\" ] > result [ \"<STR_LIT>\" ] \n def group_correct ( result ) : \n return image_correct ( result ) and text_correct ( result ) \n text_correct_count = <NUM_LIT> \n image_correct_count = <NUM_LIT> \n group_correct_count = <NUM_LIT> \n for result in winoground_clip_scores : \n text_correct_count += <NUM_LIT> if text_correct ( result ) else <NUM_LIT> \n image_correct_count += <NUM_LIT> if image_correct ( result ) else <NUM_LIT> \n group_correct_count += <NUM_LIT> if group_correct ( result ) else <NUM_LIT> \n denominator = len ( winoground_clip_scores ) \n print ( \"<STR_LIT>\" , text_correct_count / denominator ) \n print ( \"<STR_LIT>\" , image_correct_count / denominator ) \n print ( \"<STR_LIT>\" , group_correct_count / denominator ) \n return { \n '<STR_LIT>' : text_correct_count / denominator , \n '<STR_LIT>' : image_correct_count / denominator , \n '<STR_LIT>' : group_correct_count / denominator , \n } \n if __name__ == '<STR_LIT>' : \n from transformers import CLIPProcessor , CLIPModel \n clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n run_winoground ( clip_model , clip_processor )"}, {"input": "import pytest \n import asyncio \n from profyle . application . profyle import profyle \n from tests . unit . repository import InMemoryTraceRepository \n def test_should_trace_a_process ( ) : \n trace_repo = InMemoryTraceRepository ( ) \n with profyle ( \n name = \"<STR_LIT>\" , \n repo = trace_repo , \n ) : \n print ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) \n @ pytest . mark . asyncio \n async def test_should_trace_an_async_process ( ) : \n trace_repo = InMemoryTraceRepository ( ) \n with profyle ( \n name = \"<STR_LIT>\" , \n repo = trace_repo , \n ) : \n await asyncio . sleep ( <NUM_LIT> ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) \n assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> \n @ pytest . mark . asyncio \n async def test_should_trace_a_process_with_min_duration ( ) : \n trace_repo = InMemoryTraceRepository ( ) \n with profyle ( \n name = \"<STR_LIT>\" , \n repo = trace_repo , \n min_duration = <NUM_LIT> \n ) : \n await asyncio . sleep ( <NUM_LIT> )", "output": "assert len ( trace_repo . traces ) == <NUM_LIT> \n assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) \n assert trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> > <NUM_LIT> \n @ pytest . mark . asyncio \n async def test_should_not_trace_a_process_if_min_duration_not_reached ( ) : \n trace_repo = InMemoryTraceRepository ( ) \n with profyle ( \n name = \"<STR_LIT>\" , \n repo = trace_repo , \n min_duration = <NUM_LIT> \n ) : \n await asyncio . sleep ( <NUM_LIT> ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert len ( trace_repo . traces [ <NUM_LIT> ] . data ) \n assert int ( trace_repo . traces [ <NUM_LIT> ] . duration / <NUM_LIT> ) == <NUM_LIT>"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None :", "output": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . VARCHAR ( ) , \n nullable = True ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . VARCHAR ( ) , \n nullable = False )"}, {"input": "import sys \n import os \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai \n from reliablegpt import reliableGPT \n import concurrent . futures \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n print ( openai . ChatCompletion . create ) \n good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) \n def test_single_call_bad_key ( ) : \n openai . api_key = \"<STR_LIT>\" \n model = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n temperature = <NUM_LIT> \n error_count = <NUM_LIT> \n failure_count = <NUM_LIT> \n try : \n print ( \"<STR_LIT>\" ) \n response = openai . ChatCompletion . create ( model = model , \n messages = messages , \n temperature = temperature ) \n print ( \"<STR_LIT>\" , response ) \n if response and \"<STR_LIT>\" in response : \n error_count += <NUM_LIT>", "output": "if response == \"<STR_LIT>\" : \n failure_count += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n error_count += <NUM_LIT> \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n if error_count == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n test_single_call_bad_key ( ) \n def test_embedding_bad_key ( ) : \n openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n user_token = '<STR_LIT>' , \n send_notification = True ) \n openai . api_key = \"<STR_LIT>\" \n def get_embedding ( text , model = \"<STR_LIT>\" ) : \n text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n return openai . Embedding . create ( input = [ text ] , \n model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n result = get_embedding ( \"<STR_LIT>\" ) \n print ( result ) \n test_embedding_bad_key ( ) \n def test_bad_open_ai_call ( ) : \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n test_bad_open_ai_call ( ) \n def test_bad_open_ai_call_with_q ( ) : \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n queue_requests = True ) \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n test_bad_open_ai_call_with_q ( ) \n def test_multiple_calls ( ) : \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> \n } , { \n \"<STR_LIT>\" : \n \"<STR_LIT>\" , \n \"<STR_LIT>\" : \n \"<STR_LIT>\" \n } , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } ] \n temperature = <NUM_LIT> \n error_count = <NUM_LIT> \n failure_count = <NUM_LIT> \n def call_reliable_openai ( ) : \n nonlocal error_count , failure_count \n try : \n print ( \"<STR_LIT>\" ) \n response = openai . ChatCompletion . create ( model = model , \n messages = messages , \n temperature = temperature ) \n print ( response ) \n if response and \"<STR_LIT>\" in response : \n error_count += <NUM_LIT> \n if response == \"<STR_LIT>\" : \n failure_count += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n error_count += <NUM_LIT> \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : \n future_calls = [ executor . submit ( call_reliable_openai ) for _ in range ( <NUM_LIT> ) ] \n concurrent . futures . wait ( future_calls ) \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n if error_count == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" )"}, {"input": "from typing import List \n import random \n import argparse \n from datasets import load_dataset \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n DATASET_ARGS = dict ( \n path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" \n ) \n PRETRAIN_PHRASES = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n def _write_convo ( idx , row ) -> List : \n example = { \n \"<STR_LIT>\" : [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] , \n } \n phrase = random . choice ( PRETRAIN_PHRASES ) \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : phrase , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] if \"<STR_LIT>\" in row else row [ \"<STR_LIT>\" ] , \n } , \n ] \n return example \n def main ( args ) : \n audio_dataset = load_dataset ( ** DATASET_ARGS ) \n def gen ( ) : \n i = <NUM_LIT> \n idxes = list ( range ( len ( audio_dataset ) ) ) \n random . shuffle ( idxes ) \n for k in idxes : \n try : \n yield _write_convo ( k , audio_dataset [ k ] ) \n except ValueError : \n pass \n else : \n i += <NUM_LIT> \n if i >= args . max_examples : \n break", "output": "ds = Dataset . from_generator ( gen ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from channel . http . http_channel import HttpChannel \n from channel . wechat . wechat_channel import WechatChannel \n import plugins \n from plugins import * \n from common import functions \n from config import channel_conf \n from config import channel_conf_val \n from common import const \n @ plugins . register ( name = \"<STR_LIT>\" , desire_priority = <NUM_LIT> , hidden = True , desc = \"<STR_LIT>\" , version = \"<STR_LIT>\" , author = \"<STR_LIT>\" ) \n class Createimg ( Plugin ) : \n def __init__ ( self ) : \n super ( ) . __init__ ( ) \n self . handles = { HttpChannel : self . handle_http } \n self . channel_types = { HttpChannel : const . HTTP , \n WechatChannel : const . WECHAT } \n self . handlers [ Event . ON_HANDLE_CONTEXT ] = self . handle_query \n self . handlers [ Event . ON_DECORATE_REPLY ] = self . send_images \n def get_events ( self ) : \n return self . handlers \n def handle_query ( self , e_context : EventContext ) : \n channel = e_context [ '<STR_LIT>' ] \n channel_type = self . channel_types . get ( type ( channel ) , None ) \n if ( channel_type ) : \n query = e_context [ '<STR_LIT>' ] \n if ( query ) : \n img_match_prefix = functions . check_prefix ( \n query , channel_conf_val ( channel_type , '<STR_LIT>' ) ) \n if img_match_prefix : \n if ( channel_type == const . HTTP ) and e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , False ) : \n e_context [ '<STR_LIT>' ] = channel . handle ( \n { '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] , '<STR_LIT>' : e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] } ) \n e_context . action = EventAction . BREAK_PASS \n else : \n query = query . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n e_context [ '<STR_LIT>' ] [ '<STR_LIT>' ] = '<STR_LIT>' \n if ( channel_type == const . WECHAT ) : \n channel . _do_send_img ( \n query , e_context [ '<STR_LIT>' ] ) \n e_context . action = EventAction . BREAK_PASS", "output": "else : \n e_context . action = EventAction . CONTINUE \n return e_context \n def handle_http ( self , e_context : EventContext ) : \n reply = e_context [ \"<STR_LIT>\" ] \n if e_context [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) == '<STR_LIT>' : \n if isinstance ( reply , list ) : \n images = \"<STR_LIT>\" \n for url in reply : \n images += f\"<STR_LIT>\" \n e_context [ \"<STR_LIT>\" ] = images \n return e_context \n def send_images ( self , e_context : EventContext ) : \n channel = e_context [ '<STR_LIT>' ] \n method = self . handles . get ( type ( channel ) , None ) \n if ( method ) : \n e_context = method ( e_context ) \n e_context . action = EventAction . BREAK_PASS \n return e_context"}, {"input": "from unittest . mock import Mock \n import pytest \n from fastapi . testclient import TestClient \n from simple_fastapi_app import Database , app , lifespan \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _client ( ) : \n with TestClient ( app ) as client : \n yield client \n def test_db_goes_boom ( client ) : \n db = Mock ( spec_set = Database ) \n db . get_user . side_effect = Exception ( \"<STR_LIT>\" )", "output": "lifespan . registry . register_value ( Database , db ) \n resp = client . get ( \"<STR_LIT>\" ) \n assert { \"<STR_LIT>\" : \"<STR_LIT>\" } == resp . json ( )"}, {"input": "from __future__ import annotations \n import os \n from typing import AsyncGenerator \n from starlette . applications import Starlette \n from starlette . middleware import Middleware \n from starlette . requests import Request \n from starlette . responses import JSONResponse \n from starlette . routing import Route \n import svcs \n config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } \n class Database :", "output": "@ classmethod \n async def connect ( cls , db_url : str ) -> Database : \n return Database ( ) \n async def get_user ( self , user_id : int ) -> dict [ str , str ] : \n return { } \n async def get_user ( request : Request ) -> JSONResponse : \n db = await svcs . starlette . aget ( request , Database ) \n try : \n return JSONResponse ( \n { \"<STR_LIT>\" : await db . get_user ( request . path_params [ \"<STR_LIT>\" ] ) } \n ) \n except Exception as e : \n return JSONResponse ( { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] } ) \n @ svcs . starlette . lifespan \n async def lifespan ( \n app : Starlette , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n async def connect_to_db ( ) -> Database : \n return await Database . connect ( config [ \"<STR_LIT>\" ] ) \n registry . register_factory ( Database , connect_to_db ) \n yield { \"<STR_LIT>\" : \"<STR_LIT>\" } \n app = Starlette ( \n lifespan = lifespan , \n middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , \n routes = [ Route ( \"<STR_LIT>\" , get_user ) ] , \n )"}, {"input": "import time \n from tensorboard import program \n log_path = \"<STR_LIT>\"", "output": "def launch_tensorboard_pipeline ( ) : \n tb = program . TensorBoard ( ) \n tb . configure ( argv = [ None , \"<STR_LIT>\" , log_path ] ) \n url = tb . launch ( ) \n print ( \n f\"<STR_LIT>\" \n ) \n while True : \n time . sleep ( <NUM_LIT> )"}, {"input": "class Status : \n HTTP_OK = <NUM_LIT> \n HTTP_CREATED = <NUM_LIT> \n HTTP_NO_CONTENT = <NUM_LIT> \n HTTP_MOVED_PERMANENTLY = <NUM_LIT> \n HTTP_FOUND = <NUM_LIT> \n HTTP_NOT_MODIFIED = <NUM_LIT> \n HTTP_BAD_REQUEST = <NUM_LIT> \n HTTP_UNAUTHORIZED = <NUM_LIT> \n HTTP_FORBIDDEN = <NUM_LIT> \n HTTP_NOT_FOUND = <NUM_LIT> \n HTTP_METHOD_NOT_ALLOWED = <NUM_LIT> \n HTTP_INTERNAL_SERVER_ERROR = <NUM_LIT> \n HTTP_NOT_IMPLEMENTED = <NUM_LIT> \n @ classmethod \n def get_message ( self , status_code ) : \n return {", "output": "self . HTTP_OK : \"<STR_LIT>\" , \n self . HTTP_CREATED : \"<STR_LIT>\" , \n self . HTTP_NO_CONTENT : \"<STR_LIT>\" , \n self . HTTP_MOVED_PERMANENTLY : \"<STR_LIT>\" , \n self . HTTP_FOUND : \"<STR_LIT>\" , \n self . HTTP_NOT_MODIFIED : \"<STR_LIT>\" , \n self . HTTP_BAD_REQUEST : \"<STR_LIT>\" , \n self . HTTP_UNAUTHORIZED : \"<STR_LIT>\" , \n self . HTTP_FORBIDDEN : \"<STR_LIT>\" , \n self . HTTP_NOT_FOUND : \"<STR_LIT>\" , \n self . HTTP_METHOD_NOT_ALLOWED : \"<STR_LIT>\" , \n self . HTTP_INTERNAL_SERVER_ERROR : \"<STR_LIT>\" , \n self . HTTP_NOT_IMPLEMENTED : \"<STR_LIT>\" , \n } . get ( status_code , \"<STR_LIT>\" )"}, {"input": "import sys \n sys . path . append ( '<STR_LIT>' ) \n from main import reliableGPT \n import openai \n openai . api_key = \"<STR_LIT>\" \n openai . ChatCompletion . create = reliableGPT ( openai . ChatCompletion . create , \n user_email = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def simple_openai_call ( prompt ) : \n print ( f\"<STR_LIT>\" ) \n model = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : prompt \n } ,", "output": "] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n list_questions = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] \n for question in list_questions : \n print ( \"<STR_LIT>\" ) \n print ( question ) \n result = simple_openai_call ( question ) \n print ( \"<STR_LIT>\" ) \n print ( result )"}, {"input": "import numpy as np \n import math \n from typing import List , Optional , Tuple , NewType , cast \n Xdm = NewType ( \"<STR_LIT>\" , int ) \n Ydm = NewType ( \"<STR_LIT>\" , int ) \n def mask_size ( mask : np . ndarray ) -> int : \n return np . sum ( mask * <NUM_LIT> ) \n def masks_overlap ( mask1 : np . ndarray , mask2 : np . ndarray ) -> bool : \n return np . any ( mask1 * <NUM_LIT> + mask2 * <NUM_LIT> == <NUM_LIT> ) \n def mask_union ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : \n return cast ( np . ndarray , mask1 * <NUM_LIT> + mask2 * <NUM_LIT> > <NUM_LIT> ) \n def all_mask_union ( masks : List [ np . ndarray ] ) -> np . ndarray : \n base = masks [ <NUM_LIT> ] \n for mask in masks [ <NUM_LIT> : ] : \n base = mask_union ( base , mask ) \n return base \n def subtract_mask ( mask1 : np . ndarray , mask2 : np . ndarray ) -> np . ndarray : \n return cast ( np . ndarray , mask1 * <NUM_LIT> - mask2 * <NUM_LIT> == <NUM_LIT> ) \n Point = Tuple [ Ydm , Xdm ] \n def point_in_box ( y : Ydm , x : Xdm , tlbr : Tuple [ Point , Point ] ) -> bool : \n ( t , l ) , ( b , r ) = tlbr \n return t <= y and y <= b and l <= x and x <= r \n class EfficientMask ( ) : \n def __init__ ( self , mask : np . ndarray , score : float , size : Optional [ int ] = None ) : \n self . mask = mask \n self . score = score \n self . _size : Optional [ int ] = size \n self . _tlbr : Optional [ Tuple [ Point , Point ] ] = None \n def __repr__ ( self ) -> str : \n return f\"<STR_LIT>\" \n def _reset_cache ( self ) : \n self . _tlbr = None \n self . _size = None \n def set_to ( self , other : \"<STR_LIT>\" ) : \n self . mask = other . mask \n self . score = other . score \n self . _size = other . _size \n self . _tlbr = other . _tlbr \n def get_tlbr ( self ) -> Tuple [ Point , Point ] : \n if self . _tlbr is None : \n try : \n np_where = np . where ( self . mask == True ) \n left = np . min ( np_where [ <NUM_LIT> ] ) \n right = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> \n top = np . min ( np_where [ <NUM_LIT> ] ) \n bottom = np . max ( np_where [ <NUM_LIT> ] ) + <NUM_LIT> \n except ValueError : \n top , left , bottom , right = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n self . _tlbr = ( ( cast ( Ydm , top ) , cast ( Xdm , left ) ) , ( cast ( Ydm , bottom ) , cast ( Xdm , right ) ) ) \n return self . _tlbr", "output": "def get_size ( self ) -> int : \n if self . _size is None : \n ( top , left ) , ( bottom , right ) = self . get_tlbr ( ) \n self . _size = np . sum ( self . mask [ top : bottom , left : right ] * <NUM_LIT> ) \n return self . _size \n def get_density ( self ) -> float : \n size = self . get_size ( ) \n ( t , l ) , ( b , r ) = self . get_tlbr ( ) \n area = ( b - t ) * ( r - l ) + <NUM_LIT> \n return size / area \n def dense_score ( self ) -> float : \n return self . score * math . sqrt ( self . get_density ( ) ) \n def _bbox_overlaps ( self , other : \"<STR_LIT>\" ) -> bool : \n ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n return ( \n point_in_box ( t1 , l1 , other . get_tlbr ( ) ) or \n point_in_box ( b1 , r1 , other . get_tlbr ( ) ) or \n point_in_box ( t2 , r2 , self . get_tlbr ( ) ) or \n point_in_box ( b2 , l2 , self . get_tlbr ( ) ) \n ) \n def _get_overlap_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : \n if not self . _bbox_overlaps ( other ) : \n return np . array ( [ ] ) \n ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) \n minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) \n return ( self . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> + other . mask [ maxt : minb , maxl : minr ] * <NUM_LIT> == <NUM_LIT> ) \n def _get_xor_submask ( self , other : \"<STR_LIT>\" ) -> np . ndarray : \n if not self . _bbox_overlaps ( other ) : \n return np . array ( [ ] ) \n ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n mint , minl = min ( t1 , t2 ) , min ( l1 , l2 ) \n maxb , maxr = max ( b1 , b2 ) , max ( r1 , r2 ) \n return ( self . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> + other . mask [ mint : maxb , minl : maxr ] * <NUM_LIT> == <NUM_LIT> ) \n def intersect ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : \n res = np . full ( self . mask . shape , False ) \n submask = self . _get_overlap_submask ( other ) \n if len ( submask ) != <NUM_LIT> : \n ( t1 , l1 ) , ( b1 , r1 ) = self . get_tlbr ( ) \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n maxt , maxl = max ( t1 , t2 ) , max ( l1 , l2 ) \n minb , minr = min ( b1 , b2 ) , min ( r1 , r2 ) \n res [ maxt : minb , maxl : minr ] = submask \n return EfficientMask ( res , ( self . score + other . score ) / <NUM_LIT> ) \n def mostly_contained_in ( self , out_mask : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : \n size_in = self . get_size ( ) + <NUM_LIT> \n overlap = mask_size ( self . _get_overlap_submask ( out_mask ) ) \n return overlap / size_in > thresh \n def overlaps_threshold ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : \n size_1 = self . get_size ( ) + <NUM_LIT> \n size_2 = other . get_size ( ) + <NUM_LIT> \n overlap = mask_size ( self . _get_overlap_submask ( other ) ) \n return overlap / size_1 > thresh or overlap / size_2 > thresh \n def near_equivalent_to ( self , other : \"<STR_LIT>\" , thresh : float = <NUM_LIT> ) -> bool : \n size_1 = self . get_size ( ) + <NUM_LIT> \n size_2 = other . get_size ( ) + <NUM_LIT> \n if size_1 / size_2 < thresh or size_2 / size_1 < thresh : \n return False \n difference = mask_size ( self . _get_xor_submask ( other ) ) \n if ( difference / size_1 ) > ( <NUM_LIT> - thresh ) or ( difference / size_2 ) > ( <NUM_LIT> - thresh ) : \n return False \n return True \n def union ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : \n new_mask = self . mask * <NUM_LIT> \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n new_mask [ t2 : b2 , l2 : r2 ] += other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT> \n return EfficientMask ( \n mask = cast ( np . ndarray , new_mask > <NUM_LIT> ) , \n score = ( self . score + other . score ) / <NUM_LIT> , \n ) \n def subtract ( self , other : \"<STR_LIT>\" ) -> \"<STR_LIT>\" : \n new_mask = self . mask * <NUM_LIT> \n ( t2 , l2 ) , ( b2 , r2 ) = other . get_tlbr ( ) \n new_mask [ t2 : b2 , l2 : r2 ] -= other . mask [ t2 : b2 , l2 : r2 ] * <NUM_LIT> \n return EfficientMask ( \n mask = cast ( np . ndarray , new_mask == <NUM_LIT> ) , \n score = self . score , \n )"}]