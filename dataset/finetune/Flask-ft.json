[{"input": "from profyle . infrastructure . middleware . flask import ProfyleMiddleware \n from tests . unit . repository import InMemoryTraceRepository \n def test_should_trace_all_requests ( flask_client , flask_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n flask_app . wsgi_app = ProfyleMiddleware ( \n flask_app . wsgi_app , \n trace_repo = trace_repo \n ) \n flask_client . post ( \"<STR_LIT>\" ) \n flask_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n def test_should_trace_filtered_requests ( flask_client , flask_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n flask_app . wsgi_app = ProfyleMiddleware ( \n flask_app . wsgi_app , \n trace_repo = trace_repo , \n pattern = \"<STR_LIT>\" , \n ) \n flask_client . post ( \"<STR_LIT>\" ) \n flask_client . get ( \"<STR_LIT>\" ) \n flask_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\"", "output": "assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n def test_should_no_trace_if_disabled ( flask_client , flask_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n flask_app . wsgi_app = ProfyleMiddleware ( \n flask_app . wsgi_app , \n trace_repo = trace_repo , \n enabled = False , \n ) \n flask_client . post ( \"<STR_LIT>\" ) \n flask_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT>"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) \n def downgrade ( ) -> None :", "output": "with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "pila = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n print ( pila ) \n pila . append ( <NUM_LIT> )", "output": "pila . append ( <NUM_LIT> ) \n print ( pila ) \n pila . pop ( ) \n print ( pila ) \n pila . pop ( ) \n print ( pila ) \n pila . pop ( ) \n print ( pila )"}, {"input": "from unittest . mock import Mock \n import pytest \n import svcs \n from tests . fake_factories import int_factory \n from tests . helpers import nop \n from tests . ifaces import AnotherService , Service \n try : \n import httpx \n from pyramid . config import Configurator \n from pyramid . view import view_config \n except ImportError : \n pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _config ( ) : \n config = Configurator ( settings = { } ) \n svcs . pyramid . init ( config ) \n config . add_route ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n config . add_route ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n config . scan ( ) \n return config \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _app ( config ) : \n return config . make_wsgi_app ( ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _client ( app ) : \n return httpx . Client ( app = app , base_url = \"<STR_LIT>\" ) \n @ pytest . fixture ( name = \"<STR_LIT>\" , params = ( <NUM_LIT> , <NUM_LIT> ) ) \n def _rh ( request , config , app ) : \n return ( config , app ) [ request . param ] \n def test_close_nop ( rh ) : \n svcs . pyramid . close_registry ( Mock ( registry = { } ) ) \n def test_close ( rh ) : \n orc = Mock ( ) \n svcs . pyramid . register_factory ( rh , int , int_factory , on_registry_close = orc ) \n svcs . pyramid . close_registry ( rh ) \n assert orc . called \n @ view_config ( route_name = \"<STR_LIT>\" , renderer = \"<STR_LIT>\" ) \n def tl_view ( request ) : \n svc = svcs . pyramid . get ( request , Service ) \n svcs . pyramid . get ( request , float ) \n assert ( \n svc \n is svcs . pyramid . get ( request , Service ) \n is svcs . pyramid . svcs_from ( request ) . get ( Service ) \n is svcs . pyramid . get_abstract ( request , Service ) \n ) \n assert ( \n request . registry [ \"<STR_LIT>\" ] \n is svcs . pyramid . get_registry ( ) \n is svcs . pyramid . get_registry ( request ) \n ) \n assert ( \n request . svcs_container \n is svcs . pyramid . svcs_from ( ) \n is svcs . pyramid . svcs_from ( request ) \n ) \n return { \"<STR_LIT>\" : svc } \n @ view_config ( route_name = \"<STR_LIT>\" , renderer = \"<STR_LIT>\" ) \n def health_view ( request ) : \n assert (", "output": "svcs . pyramid . get_pings ( request ) \n == svcs . pyramid . svcs_from ( request ) . get_pings ( ) \n ) \n return { \"<STR_LIT>\" : len ( svcs . pyramid . svcs_from ( request ) . get_pings ( ) ) } \n class TestIntergration : \n def test_get ( self , app , client , close_me ) : \n def closing_factory ( ) : \n yield <NUM_LIT> \n close_me . close ( ) \n svcs . pyramid . get_registry ( app ) . register_value ( Service , <NUM_LIT> ) \n svcs . pyramid . register_value ( app , AnotherService , <NUM_LIT> ) \n svcs . pyramid . register_factory ( app , float , closing_factory ) \n assert { \"<STR_LIT>\" : <NUM_LIT> } == client . get ( \"<STR_LIT>\" ) . json ( ) \n assert close_me . is_closed \n def test_get_pings ( self , app , client ) : \n svcs . pyramid . get_registry ( app ) . register_value ( Service , <NUM_LIT> , ping = nop ) \n assert { \"<STR_LIT>\" : <NUM_LIT> } == client . get ( \"<STR_LIT>\" ) . json ( )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n def _convert_convo ( convo ) -> List : \n msgs = [ ] \n for m in convo : \n msgs . append ( \n { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ROLE_ASSISTANT , \"<STR_LIT>\" : ROLE_USER } [ m [ \"<STR_LIT>\" ] ] , \n \"<STR_LIT>\" : m [ \"<STR_LIT>\" ] , \n } \n ) \n return msgs \n def main ( args ) : \n rows = [ ]", "output": "for json_fn in args . llava_json : \n with open ( json_fn ) as f : \n rows . extend ( json . load ( f ) ) \n def gen ( rows ) : \n for row in rows : \n img_path = row [ \"<STR_LIT>\" ] \n fn = os . path . join ( args . image_folder , img_path ) \n if not os . path . exists ( fn ) : \n print ( \"<STR_LIT>\" , fn ) \n continue \n yield { \n \"<STR_LIT>\" : str ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : [ fn ] , \n \"<STR_LIT>\" : _convert_convo ( row [ \"<STR_LIT>\" ] ) , \n } \n ds = Dataset . from_generator ( gen , gen_kwargs = { \"<STR_LIT>\" : rows } , num_proc = args . num_proc ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , action = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "def dividir ( a , b ) : \n try :", "output": "resultado = a / b \n print ( \"<STR_LIT>\" , resultado ) \n except ZeroDivisionError : \n print ( \"<STR_LIT>\" ) \n finally : \n print ( \"<STR_LIT>\" ) \n dividir ( <NUM_LIT> , <NUM_LIT> ) \n dividir ( <NUM_LIT> , <NUM_LIT> )"}, {"input": "import torch \n import torch . utils . data \n from librosa . filters import mel as librosa_mel_fn \n def dynamic_range_compression_torch ( x , C = <NUM_LIT> , clip_val = <NUM_LIT> ) : \n return torch . log ( torch . clamp ( x , min = clip_val ) * C ) \n def dynamic_range_decompression_torch ( x , C = <NUM_LIT> ) : \n return torch . exp ( x ) / C \n def spectral_normalize_torch ( magnitudes ) : \n return dynamic_range_compression_torch ( magnitudes )", "output": "def spectral_de_normalize_torch ( magnitudes ) : \n return dynamic_range_decompression_torch ( magnitudes ) \n mel_basis = { } \n hann_window = { } \n def spectrogram_torch ( y , n_fft , hop_size , win_size , center = False ) : \n global hann_window \n dtype_device = str ( y . dtype ) + \"<STR_LIT>\" + str ( y . device ) \n wnsize_dtype_device = str ( win_size ) + \"<STR_LIT>\" + dtype_device \n if wnsize_dtype_device not in hann_window : \n hann_window [ wnsize_dtype_device ] = torch . hann_window ( win_size ) . to ( \n dtype = y . dtype , device = y . device \n ) \n y = torch . nn . functional . pad ( \n y . unsqueeze ( <NUM_LIT> ) , \n ( int ( ( n_fft - hop_size ) / <NUM_LIT> ) , int ( ( n_fft - hop_size ) / <NUM_LIT> ) ) , \n mode = \"<STR_LIT>\" , \n ) \n y = y . squeeze ( <NUM_LIT> ) \n spec = torch . stft ( \n y , \n n_fft , \n hop_length = hop_size , \n win_length = win_size , \n window = hann_window [ wnsize_dtype_device ] , \n center = center , \n pad_mode = \"<STR_LIT>\" , \n normalized = False , \n onesided = True , \n return_complex = True , \n ) \n spec = torch . sqrt ( spec . real . pow ( <NUM_LIT> ) + spec . imag . pow ( <NUM_LIT> ) + <NUM_LIT> ) \n return spec \n def spec_to_mel_torch ( spec , n_fft , num_mels , sampling_rate , fmin , fmax ) : \n global mel_basis \n dtype_device = str ( spec . dtype ) + \"<STR_LIT>\" + str ( spec . device ) \n fmax_dtype_device = str ( fmax ) + \"<STR_LIT>\" + dtype_device \n if fmax_dtype_device not in mel_basis : \n mel = librosa_mel_fn ( \n sr = sampling_rate , n_fft = n_fft , n_mels = num_mels , fmin = fmin , fmax = fmax \n ) \n mel_basis [ fmax_dtype_device ] = torch . from_numpy ( mel ) . to ( \n dtype = spec . dtype , device = spec . device \n ) \n melspec = torch . matmul ( mel_basis [ fmax_dtype_device ] , spec ) \n melspec = spectral_normalize_torch ( melspec ) \n return melspec \n def mel_spectrogram_torch ( \n y , n_fft , num_mels , sampling_rate , hop_size , win_size , fmin , fmax , center = False \n ) : \n spec = spectrogram_torch ( y , n_fft , hop_size , win_size , center ) \n melspec = spec_to_mel_torch ( spec , n_fft , num_mels , sampling_rate , fmin , fmax ) \n return melspec"}, {"input": "import sys \n import traceback \n from instld . cli . traceback_cutting . traceback_utils import cut_base_of_traceback , cut_importlib_bug \n def create_cutting_excepthook ( old_hook , base_size ) : \n def new_hook ( exc_type , value , traceback_object ) :", "output": "traceback_object = cut_base_of_traceback ( traceback_object , base_size ) \n traceback_object = cut_importlib_bug ( traceback_object ) \n traceback . print_exception ( exc_type , value , traceback_object ) \n return new_hook \n def set_cutting_excepthook ( base_size ) : \n sys . excepthook = create_cutting_excepthook ( sys . excepthook , base_size )"}, {"input": "import validador as vl \n edad = <NUM_LIT> \n while True : \n try : \n edad = int ( input ( \"<STR_LIT>\" ) )", "output": "break \n except ValueError : \n print ( \"<STR_LIT>\" ) \n if vl . puedes_conducir ( edad ) : \n print ( \"<STR_LIT>\" ) \n vehiculo = input ( \"<STR_LIT>\" ) \n tipo = vl . tipo_licencia ( vehiculo ) \n if tipo : \n print ( \"<STR_LIT>\" , tipo ) \n else : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" )"}, {"input": "from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n from common . log import logger \n from config import conf \n import json \n import requests \n import io \n from wechatpy . enterprise . crypto import WeChatCrypto \n from wechatpy . enterprise import WeChatClient \n from wechatpy . exceptions import InvalidSignatureException \n from wechatpy . enterprise . exceptions import InvalidCorpIdException \n from wechatpy . enterprise import parse_message \n from flask import Flask , request , abort \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n app = Flask ( __name__ ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def handler_msg ( ) : \n return WechatEnterpriseChannel ( ) . handle ( ) \n class WechatEnterpriseChannel ( Channel ) : \n def __init__ ( self ) : \n self . CorpId = conf ( ) . get ( '<STR_LIT>' ) \n self . Secret = conf ( ) . get ( '<STR_LIT>' ) \n self . AppId = conf ( ) . get ( '<STR_LIT>' ) \n self . TOKEN = conf ( ) . get ( '<STR_LIT>' ) \n self . EncodingAESKey = conf ( ) . get ( '<STR_LIT>' ) \n self . crypto = WeChatCrypto ( self . TOKEN , self . EncodingAESKey , self . CorpId ) \n self . client = WeChatClient ( self . CorpId , self . Secret , self . AppId ) \n logger . info ( \"<STR_LIT>\" . format ( \n self . CorpId , self . Secret , self . AppId , self . TOKEN , self . EncodingAESKey ) ) \n def startup ( self ) : \n app . run ( host = '<STR_LIT>' , port = <NUM_LIT> ) \n def send ( self , msg , receiver ) : \n logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) \n self . client . message . send_text ( self . AppId , receiver , msg ) \n def _do_send ( self , query , reply_user_id ) : \n try : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = reply_user_id \n reply_text = super ( ) . build_reply_content ( query , context ) \n if reply_text : \n self . send ( reply_text , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def handle ( self ) : \n query_params = request . args \n signature = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n timestamp = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n nonce = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n if request . method == '<STR_LIT>' : \n echostr = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n try : \n echostr = self . crypto . check_signature ( signature , timestamp , nonce , echostr ) \n except InvalidSignatureException : \n abort ( <NUM_LIT> )", "output": "print ( echostr ) \n return echostr \n elif request . method == '<STR_LIT>' : \n try : \n message = self . crypto . decrypt_message ( \n request . data , \n signature , \n timestamp , \n nonce \n ) \n except ( InvalidSignatureException , InvalidCorpIdException ) : \n abort ( <NUM_LIT> ) \n msg = parse_message ( message ) \n if msg . type == '<STR_LIT>' : \n reply = '<STR_LIT>' \n thread_pool . submit ( self . _do_send , msg . content , msg . source ) \n else : \n reply = '<STR_LIT>' \n self . client . message . send_text ( self . AppId , msg . source , reply ) \n return '<STR_LIT>'"}, {"input": "import pickle , os \n import logging \n import requests \n from . . config import VERSION \n from . . returnvalues import ReturnValue \n from . . storage import templates \n from . contact import update_local_chatrooms , update_local_friends \n from . messages import produce_msg \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_hotreload ( core ) : \n core . dump_login_status = dump_login_status \n core . load_login_status = load_login_status \n def dump_login_status ( self , fileDir = None ) : \n fileDir = fileDir or self . hotReloadDir \n try : \n with open ( fileDir , '<STR_LIT>' ) as f : \n f . write ( '<STR_LIT>' ) \n os . remove ( fileDir ) \n except : \n raise Exception ( '<STR_LIT>' ) \n status = { \n '<STR_LIT>' : VERSION , \n '<STR_LIT>' : self . loginInfo , \n '<STR_LIT>' : self . s . cookies . get_dict ( ) , \n '<STR_LIT>' : self . storageClass . dumps ( ) } \n with open ( fileDir , '<STR_LIT>' ) as f : \n pickle . dump ( status , f ) \n logger . debug ( '<STR_LIT>' ) \n def load_login_status ( self , fileDir , \n loginCallback = None , exitCallback = None ) : \n try : \n with open ( fileDir , '<STR_LIT>' ) as f :", "output": "j = pickle . load ( f ) \n except Exception as e : \n logger . debug ( '<STR_LIT>' ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n if j . get ( '<STR_LIT>' , '<STR_LIT>' ) != VERSION : \n logger . debug ( ( '<STR_LIT>' + \n '<STR_LIT>' ) % ( \n j . get ( '<STR_LIT>' , '<STR_LIT>' ) , VERSION ) ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n self . loginInfo = j [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = templates . User ( self . loginInfo [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] . core = self \n self . s . cookies = requests . utils . cookiejar_from_dict ( j [ '<STR_LIT>' ] ) \n self . storageClass . loads ( j [ '<STR_LIT>' ] ) \n try : \n msgList , contactList = self . get_msg ( ) \n except : \n msgList = contactList = None \n if ( msgList or contactList ) is None : \n self . logout ( ) \n load_last_login_status ( self . s , j [ '<STR_LIT>' ] ) \n logger . debug ( '<STR_LIT>' ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n else : \n if contactList : \n for contact in contactList : \n if '<STR_LIT>' in contact [ '<STR_LIT>' ] : \n update_local_chatrooms ( self , [ contact ] ) \n else : \n update_local_friends ( self , [ contact ] ) \n if msgList : \n msgList = produce_msg ( self , msgList ) \n for msg in msgList : self . msgList . put ( msg ) \n self . start_receiving ( exitCallback ) \n logger . debug ( '<STR_LIT>' ) \n if hasattr ( loginCallback , '<STR_LIT>' ) : \n loginCallback ( ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , } } ) \n def load_last_login_status ( session , cookiesDict ) : \n try : \n session . cookies = requests . utils . cookiejar_from_dict ( { \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] + '<STR_LIT>' , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , } ) \n except : \n logger . info ( '<STR_LIT>' ) \n logger . info ( '<STR_LIT>' )"}, {"input": "from flask import render_template , jsonify , Response , abort \n from flask_login import login_required , current_user \n from app . models import Channel \n from app . schemas import ChannelSchema \n from . base import api \n from . utils import check_token , check_user \n @ api . route ( '<STR_LIT>' )", "output": "@ login_required \n def settings ( ) -> Response : \n token = current_user . generate_api_token ( ) \n return render_template ( '<STR_LIT>' , token = token ) \n @ api . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def show_channels ( ) -> Response : \n token = check_token ( ) \n user = check_user ( token ) \n channels = [ \n Channel . query . get ( allowed_record . channel_id ) \n for allowed_record in user . allowed_channels \n ] \n channel_schema = ChannelSchema ( ) \n return jsonify ( channel_schema . dump ( channels , many = True ) ) \n @ api . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def show_channel ( name : str ) -> Response : \n token = check_token ( ) \n user = check_user ( token ) \n channel = [ \n Channel . query . get ( allowed_record . channel_id ) \n for allowed_record in user . allowed_channels \n if Channel . query . get ( allowed_record . channel_id ) . name == name \n ] \n if not channel : \n abort ( <NUM_LIT> , description = f'<STR_LIT>' ) \n else : \n channel = channel [ <NUM_LIT> ] \n channel_schema = ChannelSchema ( ) \n return jsonify ( channel_schema . dump ( channel ) )"}, {"input": "TRANSLATE = '<STR_LIT>' \n class ReturnValue ( dict ) : \n def __init__ ( self , returnValueDict = { } , rawResponse = None ) : \n if rawResponse : \n try : \n returnValueDict = rawResponse . json ( ) \n except ValueError : \n returnValueDict = { \n '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' , } , \n '<STR_LIT>' : rawResponse . content , }", "output": "for k , v in returnValueDict . items ( ) : \n self [ k ] = v \n if not '<STR_LIT>' in self : \n self [ '<STR_LIT>' ] = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } \n if TRANSLATE : \n self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) \n self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = TRANSLATION [ TRANSLATE ] . get ( \n self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , '<STR_LIT>' ) ) or self [ '<STR_LIT>' ] . get ( '<STR_LIT>' , u'<STR_LIT>' ) \n self [ '<STR_LIT>' ] [ '<STR_LIT>' ] = self [ '<STR_LIT>' ] [ '<STR_LIT>' ] or self [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n def __nonzero__ ( self ) : \n return self [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) == <NUM_LIT> \n def __bool__ ( self ) : \n return self . __nonzero__ ( ) \n def __str__ ( self ) : \n return '<STR_LIT>' % '<STR_LIT>' . join ( \n [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % self . __str__ ( ) \n TRANSLATION = { \n '<STR_LIT>' : { \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n - <NUM_LIT> : u'<STR_LIT>' , \n <NUM_LIT> : u'<STR_LIT>' , \n } , \n }"}, {"input": "from setuptools import setup , find_packages \n setup ( \n name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n description = '<STR_LIT>' , \n author = '<STR_LIT>' , \n packages = [ \n '<STR_LIT>' \n ] , \n install_requires = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' \n ] ,", "output": ")"}, {"input": "import os \n import sys \n import wget \n import zipfile \n from bs4 import BeautifulSoup \n import requests \n from urllib . parse import unquote , urlencode , parse_qs , urlparse \n import re \n import shutil \n import six \n def find_folder_parent ( search_dir , folder_name ) : \n for dirpath , dirnames , _ in os . walk ( search_dir ) : \n if folder_name in dirnames : \n return os . path . abspath ( dirpath ) \n return None \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from rvc . lib . utils import format_title \n from rvc . lib . tools import gdown \n file_path = find_folder_parent ( now_dir , \"<STR_LIT>\" ) \n zips_path = os . getcwd ( ) + \"<STR_LIT>\" \n def search_pth_index ( folder ) : \n pth_paths = [ \n os . path . join ( folder , file ) \n for file in os . listdir ( folder ) \n if os . path . isfile ( os . path . join ( folder , file ) ) and file . endswith ( \"<STR_LIT>\" ) \n ] \n index_paths = [ \n os . path . join ( folder , file ) \n for file in os . listdir ( folder ) \n if os . path . isfile ( os . path . join ( folder , file ) ) and file . endswith ( \"<STR_LIT>\" ) \n ] \n return pth_paths , index_paths \n def get_mediafire_download_link ( url ) : \n response = requests . get ( url ) \n response . raise_for_status ( ) \n soup = BeautifulSoup ( response . text , \"<STR_LIT>\" ) \n download_button = soup . find ( \n \"<STR_LIT>\" , { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n if download_button : \n download_link = download_button . get ( \"<STR_LIT>\" ) \n return download_link \n else : \n return None \n def download_from_url ( url ) : \n os . makedirs ( zips_path , exist_ok = True ) \n if url != \"<STR_LIT>\" : \n if \"<STR_LIT>\" in url : \n if \"<STR_LIT>\" in url : \n file_id = url . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n elif \"<STR_LIT>\" in url : \n file_id = url . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n else : \n return None \n if file_id : \n os . chdir ( zips_path ) \n try : \n gdown . download ( \n f\"<STR_LIT>\" , \n quiet = True , \n fuzzy = True ,", "output": ") \n except Exception as error : \n error_message = str ( error ) \n if ( \n \"<STR_LIT>\" \n in error_message \n ) : \n os . chdir ( now_dir ) \n return \"<STR_LIT>\" \n elif ( \n \"<STR_LIT>\" in error_message \n ) : \n os . chdir ( now_dir ) \n return \"<STR_LIT>\" \n else : \n print ( error_message ) \n os . chdir ( now_dir ) \n return None \n elif \"<STR_LIT>\" in url : \n base_url = \"<STR_LIT>\" \n public_key = url \n final_url = base_url + urlencode ( dict ( public_key = public_key ) ) \n response = requests . get ( final_url ) \n download_url = response . json ( ) [ \"<STR_LIT>\" ] \n download_response = requests . get ( download_url ) \n if download_response . status_code == <NUM_LIT> : \n filename = parse_qs ( urlparse ( unquote ( download_url ) ) . query ) . get ( \n \"<STR_LIT>\" , [ \"<STR_LIT>\" ] \n ) [ <NUM_LIT> ] \n if filename : \n os . chdir ( zips_path ) \n with open ( filename , \"<STR_LIT>\" ) as f : \n f . write ( download_response . content ) \n else : \n print ( \"<STR_LIT>\" ) \n return None \n elif \"<STR_LIT>\" in url : \n try : \n file_id = url . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n os . chdir ( zips_path ) \n print ( file_id ) \n response = requests . get ( f\"<STR_LIT>\" ) \n if response . status_code == <NUM_LIT> : \n file_name = ( \n response . headers . get ( \"<STR_LIT>\" ) \n . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] \n . strip ( '<STR_LIT>' ) \n ) \n os . makedirs ( zips_path , exist_ok = True ) \n with open ( os . path . join ( zips_path , file_name ) , \"<STR_LIT>\" ) as newfile : \n newfile . write ( response . content ) \n os . chdir ( file_path ) \n return \"<STR_LIT>\" \n else : \n os . chdir ( file_path ) \n return None \n except Exception as e : \n print ( e ) \n os . chdir ( file_path ) \n return None \n elif \"<STR_LIT>\" in url : \n file = requests . get ( url ) \n os . chdir ( zips_path ) \n if file . status_code == <NUM_LIT> : \n name = url . split ( \"<STR_LIT>\" ) \n with open ( os . path . join ( name [ - <NUM_LIT> ] ) , \"<STR_LIT>\" ) as newfile : \n newfile . write ( file . content ) \n else : \n return None \n elif \"<STR_LIT>\" in url or \"<STR_LIT>\" in url : \n os . chdir ( zips_path ) \n if \"<STR_LIT>\" in url : \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n response = requests . get ( url , stream = True ) \n if response . status_code == <NUM_LIT> : \n content_disposition = six . moves . urllib_parse . unquote ( \n response . headers [ \"<STR_LIT>\" ] \n ) \n m = re . search ( r'<STR_LIT>' , content_disposition ) \n file_name = m . groups ( ) [ <NUM_LIT> ] \n file_name = file_name . replace ( os . path . sep , \"<STR_LIT>\" ) \n total_size_in_bytes = int ( response . headers . get ( \"<STR_LIT>\" , <NUM_LIT> ) ) \n block_size = <NUM_LIT> \n progress_bar_length = <NUM_LIT> \n progress = <NUM_LIT> \n with open ( os . path . join ( zips_path , file_name ) , \"<STR_LIT>\" ) as file : \n for data in response . iter_content ( block_size ) : \n file . write ( data ) \n progress += len ( data ) \n progress_percent = int ( ( progress / total_size_in_bytes ) * <NUM_LIT> ) \n num_dots = int ( \n ( progress / total_size_in_bytes ) * progress_bar_length \n ) \n progress_bar = ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" * num_dots \n + \"<STR_LIT>\" * ( progress_bar_length - num_dots ) \n + \"<STR_LIT>\" \n ) \n print ( \n f\"<STR_LIT>\" , \n end = \"<STR_LIT>\" , \n ) \n if progress_percent == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n os . chdir ( now_dir ) \n return None \n elif \"<STR_LIT>\" in url : \n os . chdir ( zips_path ) \n response = requests . get ( url ) \n soup = BeautifulSoup ( response . content , \"<STR_LIT>\" ) \n temp_url = \"<STR_LIT>\" \n for link in soup . find_all ( \"<STR_LIT>\" , href = True ) : \n if link [ \"<STR_LIT>\" ] . endswith ( \"<STR_LIT>\" ) : \n temp_url = link [ \"<STR_LIT>\" ] \n break \n if temp_url : \n url = temp_url \n url = url . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if \"<STR_LIT>\" not in url : \n url = \"<STR_LIT>\" + url \n wget . download ( url ) \n else : \n os . chdir ( now_dir ) \n return None \n elif \"<STR_LIT>\" in url : \n parts = url . split ( \"<STR_LIT>\" ) \n id_with_query = parts [ - <NUM_LIT> ] \n id_parts = id_with_query . split ( \"<STR_LIT>\" ) \n id_number = id_parts [ <NUM_LIT> ] \n url = \"<STR_LIT>\" \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n params = { \"<STR_LIT>\" : f\"<STR_LIT>\" } \n response = requests . get ( url , headers = headers , params = params ) \n if response . status_code == <NUM_LIT> : \n json_response = response . json ( ) \n print ( json_response ) \n if json_response : \n link = json_response [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n verify = download_from_url ( link ) \n if verify == \"<STR_LIT>\" : \n return \"<STR_LIT>\" \n else : \n return None \n else : \n return None \n else : \n try : \n os . chdir ( zips_path ) \n wget . download ( url ) \n except Exception as error : \n os . chdir ( now_dir ) \n print ( error ) \n return None \n for currentPath , _ , zipFiles in os . walk ( zips_path ) : \n for Files in zipFiles : \n filePart = Files . split ( \"<STR_LIT>\" ) \n extensionFile = filePart [ len ( filePart ) - <NUM_LIT> ] \n filePart . pop ( ) \n nameFile = \"<STR_LIT>\" . join ( filePart ) \n realPath = os . path . join ( currentPath , Files ) \n os . rename ( realPath , nameFile + \"<STR_LIT>\" + extensionFile ) \n os . chdir ( now_dir ) \n return \"<STR_LIT>\" \n os . chdir ( now_dir ) \n return None \n def extract_and_show_progress ( zipfile_path , unzips_path ) : \n try : \n with zipfile . ZipFile ( zipfile_path , \"<STR_LIT>\" ) as zip_ref : \n for file_info in zip_ref . infolist ( ) : \n zip_ref . extract ( file_info , unzips_path ) \n os . remove ( zipfile_path ) \n return True \n except Exception as error : \n print ( error ) \n return False \n def unzip_file ( zip_path , zip_file_name ) : \n zip_file_path = os . path . join ( zip_path , zip_file_name + \"<STR_LIT>\" ) \n extract_path = os . path . join ( file_path , zip_file_name ) \n with zipfile . ZipFile ( zip_file_path , \"<STR_LIT>\" ) as zip_ref : \n zip_ref . extractall ( extract_path ) \n os . remove ( zip_file_path ) \n def model_download_pipeline ( url ) : \n verify = download_from_url ( url ) \n if verify == \"<STR_LIT>\" : \n extract_folder_path = \"<STR_LIT>\" \n for filename in os . listdir ( zips_path ) : \n if filename . endswith ( \"<STR_LIT>\" ) : \n zipfile_path = os . path . join ( zips_path , filename ) \n print ( \"<STR_LIT>\" ) \n model_zip = os . path . basename ( zipfile_path ) \n model_name = format_title ( model_zip . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) \n extract_folder_path = os . path . join ( \n \"<STR_LIT>\" , \n os . path . normpath ( model_name ) , \n ) \n success = extract_and_show_progress ( zipfile_path , extract_folder_path ) \n subfolders = [ \n f \n for f in os . listdir ( extract_folder_path ) \n if os . path . isdir ( os . path . join ( extract_folder_path , f ) ) \n ] \n if len ( subfolders ) == <NUM_LIT> : \n subfolder_path = os . path . join ( extract_folder_path , subfolders [ <NUM_LIT> ] ) \n for item in os . listdir ( subfolder_path ) : \n s = os . path . join ( subfolder_path , item ) \n d = os . path . join ( extract_folder_path , item ) \n shutil . move ( s , d ) \n os . rmdir ( subfolder_path ) \n for item in os . listdir ( extract_folder_path ) : \n if \"<STR_LIT>\" in item : \n file_name = item . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n if file_name != model_name : \n os . rename ( \n os . path . join ( extract_folder_path , item ) , \n os . path . join ( extract_folder_path , model_name + \"<STR_LIT>\" ) , \n ) \n else : \n if \"<STR_LIT>\" not in item : \n file_name = item . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n if file_name != model_name : \n new_file_name = ( \n item . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n + \"<STR_LIT>\" \n + model_name \n + \"<STR_LIT>\" \n ) \n os . rename ( \n os . path . join ( extract_folder_path , item ) , \n os . path . join ( \n extract_folder_path , new_file_name + \"<STR_LIT>\" \n ) , \n ) \n else : \n file_name = item . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n if file_name != model_name : \n new_file_name = ( \n item . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n + \"<STR_LIT>\" \n + model_name \n + \"<STR_LIT>\" \n ) \n os . rename ( \n os . path . join ( extract_folder_path , item ) , \n os . path . join ( \n extract_folder_path , new_file_name + \"<STR_LIT>\" \n ) , \n ) \n if success : \n print ( f\"<STR_LIT>\" ) \n else : \n print ( f\"<STR_LIT>\" ) \n sys . exit ( ) \n if extract_folder_path == \"<STR_LIT>\" : \n print ( \"<STR_LIT>\" ) \n sys . exit ( ) \n result = search_pth_index ( extract_folder_path ) \n else : \n message = \"<STR_LIT>\""}, {"input": "from flask import Flask \n from flask_socketio import SocketIO , join_room , leave_room \n from . config import configure_app \n from . import models , main , login , api , bcrypt , login_manager , cli , schemas \n from app . sockets import add_message \n import app . cli . commands \n from . models import Channel , Message , db \n def create_app ( ) -> Flask : \n app = Flask ( __name__ ) \n configure_app ( app ) \n models . init_app ( app ) \n schemas . init_app ( app ) \n main . init_app ( app ) \n login . init_app ( app ) \n api . init_app ( app ) \n bcrypt . init_app ( app ) \n login_manager . init_app ( app ) \n cli . init_app ( app ) \n return app \n app = create_app ( ) \n socket_io = SocketIO ( app )", "output": "@ socket_io . on ( '<STR_LIT>' ) \n def join_r ( data : dict ) -> None : \n room = data [ '<STR_LIT>' ] \n join_room ( room ) \n @ socket_io . on ( '<STR_LIT>' ) \n def leave_r ( data : dict ) -> None : \n room = data [ '<STR_LIT>' ] \n leave_room ( room ) \n @ socket_io . on ( '<STR_LIT>' ) \n def add_message_socket ( data : dict ) -> None : \n add_message ( data )"}, {"input": "from flask import Flask \n from typing import Optional \n from . base import login_manager \n from app . models . user import User", "output": "def init_app ( app : Flask ) -> None : \n login_manager . init_app ( app ) \n login_manager . login_view = '<STR_LIT>' \n login_manager . login_message_category = '<STR_LIT>' \n @ login_manager . user_loader \n def load_user ( user_id : str ) -> Optional [ User ] : \n user : Optional [ User ] = User . query . get ( int ( user_id ) ) \n return user"}, {"input": "import subprocess , os \n import argparse \n import socket \n def main ( args ) : \n host_name = socket . gethostname ( ) \n logs_path_src = f\"<STR_LIT>\" \n logs_path_dst = \"<STR_LIT>\" \n folders = subprocess . check_output ( [ \"<STR_LIT>\" , f\"<STR_LIT>\" , \"<STR_LIT>\" , logs_path_src , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) . decode ( \"<STR_LIT>\" ) \n folder_list = folders . split ( \"<STR_LIT>\" ) \n for name in folder_list : \n if len ( name ) >= <NUM_LIT> : \n if name [ : <NUM_LIT> ] == args . exptid : \n exp_path_src = os . path . join ( logs_path_src , name ) \n break \n models = subprocess . check_output ( [ \"<STR_LIT>\" , f\"<STR_LIT>\" , \"<STR_LIT>\" , exp_path_src , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) . decode ( \"<STR_LIT>\" ) \n models = models . split ( \"<STR_LIT>\" )", "output": "models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) \n model = models [ - <NUM_LIT> ] \n if args . ckpt : \n model = f\"<STR_LIT>\" \n model_path_src = os . path . join ( exp_path_src , model ) \n model_path_dst = os . path . join ( logs_path_dst , name , model ) \n os . makedirs ( os . path . dirname ( model_path_dst ) , exist_ok = True ) \n print ( f\"<STR_LIT>\" ) \n p = subprocess . Popen ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , \n f\"<STR_LIT>\" + model_path_src , \n model_path_dst ] ) \n sts = os . waitpid ( p . pid , <NUM_LIT> ) \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( '<STR_LIT>' , type = str , required = True , default = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , required = True , default = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , required = False , default = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , required = False , default = '<STR_LIT>' ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "print ( \"<STR_LIT>\" ) \n for n in range ( <NUM_LIT> , <NUM_LIT> ) : \n es_primo = True \n for x in range ( <NUM_LIT> , n ) : \n if n % x == <NUM_LIT> : \n es_primo = False \n break \n if es_primo : \n print ( n , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n for num in range ( <NUM_LIT> , <NUM_LIT> ) : \n if num % <NUM_LIT> == <NUM_LIT> : \n print ( num , \"<STR_LIT>\" )", "output": "continue \n print ( num , \"<STR_LIT>\" )"}, {"input": "from typing import Dict , List \n import os \n import torch \n import torch . nn as nn \n from multi_token . modalities . base_modality import Modality \n from multi_token . modalities . projectors import build_mlp_vector_projector \n from multi_token . data_tools import with_local_files \n IMAGE_BIND_FORCE_CPU = \"<STR_LIT>\" \n IMAGE_BIND_EMBEDDING_SIZE = <NUM_LIT> \n class ImageBindModule ( nn . Module ) : \n def __init__ ( self ) : \n super ( ) . __init__ ( ) \n from imagebind . models import imagebind_model \n from imagebind import data \n data . BPE_PATH = os . path . join ( \n os . path . dirname ( data . __file__ ) , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" \n ) \n self . model = imagebind_model . imagebind_huge ( pretrained = True ) \n self . model . eval ( ) \n self . model . requires_grad_ ( False ) \n @ torch . no_grad ( ) \n def forward ( self , items : Dict ) -> torch . Tensor : \n forward_outs = self . model ( items ) \n return forward_outs \n @ property \n def embedding_size ( self ) : \n return IMAGE_BIND_EMBEDDING_SIZE \n class ImageBindModality ( Modality ) : \n def __init__ ( \n self ,", "output": "num_projector_layers : int = <NUM_LIT> , \n num_tokens : int = <NUM_LIT> , \n preprocess_device : str = \"<STR_LIT>\" , \n ) : \n self . module = ImageBindModule ( ) \n self . dtype = torch . float32 \n self . device = \"<STR_LIT>\" \n self . imagebind_device = \"<STR_LIT>\" \n self . preprocess_device = preprocess_device \n self . num_projector_layers = num_projector_layers \n self . num_tokens = num_tokens \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n return build_mlp_vector_projector ( \n self . module . embedding_size , \n lm_hidden_size , \n num_layers = self . num_projector_layers , \n num_tokens = self . num_tokens , \n ) \n @ property \n def name ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def data_key ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token_width ( self ) -> int : \n return self . num_tokens \n def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n self . device = device \n self . dtype = dtype \n if IMAGE_BIND_FORCE_CPU not in os . environ : \n self . module . to ( device = device ) \n self . imagebind_device = device \n return self \n def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ List [ Dict ] ] : \n from imagebind . models . imagebind_model import ModalityType \n from imagebind import data \n row_values = [ ] \n for row in rows : \n items = [ ] \n with with_local_files ( row [ self . data_key ] ) as item_paths : \n for item_path in item_paths : \n ib_modality = filename_to_imagebind_modality ( item_path ) \n if ib_modality == ModalityType . TEXT : \n items . append ( \n { \n ModalityType . TEXT : data . load_and_transform_text ( \n [ item_path ] , self . preprocess_device \n ) \n } \n ) \n elif ib_modality == ModalityType . VISION : \n items . append ( \n { \n ModalityType . VISION : data . load_and_transform_vision_data ( \n [ item_path ] , self . preprocess_device \n ) \n } \n ) \n elif ib_modality == ModalityType . AUDIO : \n items . append ( \n { \n ModalityType . AUDIO : data . load_and_transform_audio_data ( \n [ item_path ] , self . preprocess_device \n ) \n } \n ) \n else : \n raise ValueError ( f\"<STR_LIT>\" ) \n row_values . append ( items ) \n return row_values \n @ torch . no_grad ( ) \n def forward ( self , encoded_values : List [ List [ Dict ] ] ) -> List [ torch . Tensor ] : \n item_features = [ ] \n for item_batch in encoded_values : \n item_batch_emb = [ ] \n for item in item_batch : \n item = { \n k : v . to ( device = self . imagebind_device , dtype = torch . float32 ) \n for k , v in item . items ( ) \n } \n item_batch_emb . extend ( list ( self . module . forward ( item ) . values ( ) ) ) \n item_features . append ( \n torch . stack ( item_batch_emb ) . to ( device = self . device , dtype = self . dtype ) \n ) \n return item_features \n def filename_to_imagebind_modality ( fn : str ) -> str : \n from imagebind . models . imagebind_model import ModalityType \n _ , ext = os . path . splitext ( fn ) \n if ext in { \"<STR_LIT>\" } : \n return ModalityType . AUDIO \n elif ext in { \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" } : \n return ModalityType . VISION \n else : \n return ModalityType . TEXT"}, {"input": "import logging \n import time \n from urllib . parse import urlparse \n from typing import List , Dict , Tuple , Union , Any \n from enum import Enum \n from functools import partial \n from pathlib import Path \n import click \n from . const import ( \n DetectMode , \n TemplateEnvironment , \n PythonEnvironment , \n ReplacedKeywordStrategy , \n OS_POPEN_READ , \n CONFIG , \n EVAL , \n STRING , \n DEFAULT_USER_AGENT , \n ) \n from . colorize import colored , set_enable_coloring \n from . cracker import Cracker , EvalArgsModePayloadGen , guess_python_version \n from . form import Form , get_form \n from . full_payload_gen import FullPayloadGen \n from . requester import ( \n HTTPRequester , \n TCPRequester , \n check_line_break , \n fix_line_break , \n check_tail , \n fix_tail , \n ) \n from . submitter import ( \n Submitter , \n PathSubmitter , \n FormSubmitter , \n TCPSubmitter , \n shell_tamperer , \n ) \n from . scan_url import yield_form \n from . webui import main as webui_main \n from . interact import interact \n from . options import Options \n set_enable_coloring ( ) \n TITLE = colored ( \n \"<STR_LIT>\" , \n . strip ( \n \"<STR_LIT>\" \n ) , \n bold = True , \n ) \n LOGGING_FORMAT = \"<STR_LIT>\" \n logging . basicConfig ( level = logging . INFO , format = LOGGING_FORMAT ) \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n class EnumOption ( click . Option ) : \n def type_cast_value ( self , ctx : click . Context , value : Any ) : \n if not isinstance ( self . type , click . types . FuncParamType ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n clazz : type = self . type . func \n if not issubclass ( clazz , Enum ) : \n raise RuntimeError ( \"<STR_LIT>\" ) \n try : \n _ = self . type ( value ) \n except Exception as exc : \n raise click . exceptions . BadParameter ( \n f\"<STR_LIT>\" , \n ctx = ctx , \n param = self , \n ) from exc \n return super ( ) . type_cast_value ( ctx , value ) \n class RunFailed ( Exception ) : \n def do_submit_cmdexec ( \n cmd : str , \n submitter : Submitter , \n full_payload_gen_like : Union [ FullPayloadGen , EvalArgsModePayloadGen ] , \n ) -> str : \n payload , will_print = None , None \n if cmd [ <NUM_LIT> ] == \"<STR_LIT>\" : \n cmd = cmd [ <NUM_LIT> : ] \n if cmd . startswith ( \"<STR_LIT>\" ) : \n payload , will_print = full_payload_gen_like . generate ( CONFIG ) \n elif cmd . startswith ( \"<STR_LIT>\" ) : \n payload , will_print = full_payload_gen_like . generate ( \n EVAL , ( STRING , cmd [ <NUM_LIT> : ] . strip ( ) ) \n ) \n elif cmd . startswith ( \"<STR_LIT>\" ) : \n cmd = cmd . strip ( ) \n if len ( cmd ) == <NUM_LIT> : \n payload , will_print = full_payload_gen_like . generate ( \n EVAL , ( STRING , \"<STR_LIT>\" ) \n ) \n else : \n payload , will_print = full_payload_gen_like . generate ( \n EVAL , ( STRING , f\"<STR_LIT>\" ) \n ) \n elif cmd . startswith ( \"<STR_LIT>\" ) : \n filepath = cmd [ <NUM_LIT> : ] . strip ( ) \n payload , will_print = full_payload_gen_like . generate ( \n EVAL , ( STRING , f\"<STR_LIT>\" ) \n ) \n elif cmd . startswith ( \"<STR_LIT>\" ) : \n statements = cmd [ <NUM_LIT> : ] . strip ( ) \n payload , will_print = full_payload_gen_like . generate ( \n EVAL , ( STRING , f\"<STR_LIT>\" ) \n ) \n else : \n logging . warning ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n else : \n payload , will_print = full_payload_gen_like . generate ( OS_POPEN_READ , cmd ) \n if payload is None : \n logger . warning ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n return \"<STR_LIT>\" \n logger . info ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) ) \n if not will_print : \n payload_wont_print = ( \n \"<STR_LIT>\" \n ) \n logger . warning ( payload_wont_print , colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n result = submitter . submit ( payload ) \n assert result is not None \n return result . text \n def parse_headers_cookies ( headers_list : List [ str ] , cookies : str ) -> Dict [ str , str ] : \n headers = { } \n if headers_list : \n for header in headers_list : \n key , _ , value = header . partition ( \"<STR_LIT>\" ) \n if not key or not value : \n logger . warning ( \"<STR_LIT>\" , repr ( header ) ) \n continue \n if key . capitalize ( ) != key : \n logger . warning ( \"<STR_LIT>\" , key ) \n key = key . capitalize ( ) \n headers [ key ] = value \n if cookies : \n headers [ \"<STR_LIT>\" ] = cookies \n return headers \n def do_crack_form_pre ( \n url : str , \n form : Form , \n requester : HTTPRequester , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n tamper_cmd : Union [ str , None ] , \n ) -> Union [ Tuple [ FullPayloadGen , Submitter ] , None ] : \n python_version = guess_python_version ( url , requester ) \n for input_field in form [ \"<STR_LIT>\" ] : \n submitter = FormSubmitter ( \n url , \n form , \n input_field , \n requester , \n ) \n if tamper_cmd : \n tamperer = shell_tamperer ( tamper_cmd ) \n submitter . add_tamperer ( tamperer ) \n options = Options ( \n detect_mode = detect_mode , \n replaced_keyword_strategy = replaced_keyword_strategy , \n environment = environment , \n python_version = python_version , \n ) \n cracker = Cracker ( submitter = submitter , options = options ) \n if not cracker . has_respond ( ) : \n return None \n full_payload_gen = cracker . crack ( ) \n if full_payload_gen : \n return full_payload_gen , submitter \n return None \n def do_crack_form_eval_args_pre ( \n url : str , \n form : Form , \n requester : HTTPRequester , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n tamper_cmd : Union [ str , None ] , \n ) -> Union [ Tuple [ Submitter , EvalArgsModePayloadGen ] , None ] : \n python_version = guess_python_version ( url , requester )", "output": "for input_field in form [ \"<STR_LIT>\" ] : \n submitter = FormSubmitter ( \n url , \n form , \n input_field , \n requester , \n ) \n if tamper_cmd : \n tamperer = shell_tamperer ( tamper_cmd ) \n submitter . add_tamperer ( tamperer ) \n options = Options ( \n detect_mode = detect_mode , \n replaced_keyword_strategy = replaced_keyword_strategy , \n environment = environment , \n python_version = python_version , \n ) \n cracker = Cracker ( submitter = submitter , options = options ) \n if not cracker . has_respond ( ) : \n return None \n result = cracker . crack_eval_args ( ) \n if result : \n submitter , evalargs_payload_gen = result \n return submitter , evalargs_payload_gen \n return None \n def do_crack_path_pre ( \n url : str , \n requester : HTTPRequester , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n tamper_cmd : Union [ str , None ] , \n ) -> Union [ Tuple [ FullPayloadGen , Submitter ] , None ] : \n python_version = guess_python_version ( url , requester ) \n submitter = PathSubmitter ( url = url , requester = requester ) \n if tamper_cmd : \n tamperer = shell_tamperer ( tamper_cmd ) \n submitter . add_tamperer ( tamperer ) \n options = Options ( \n detect_mode = detect_mode , \n replaced_keyword_strategy = replaced_keyword_strategy , \n environment = environment , \n python_version = python_version , \n ) \n cracker = Cracker ( submitter = submitter , options = options ) \n if not cracker . has_respond ( ) : \n return None \n full_payload_gen = cracker . crack ( ) \n if full_payload_gen is None : \n return None \n return full_payload_gen , submitter \n def do_crack_request_pre ( \n submitter : TCPSubmitter , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n ) -> Union [ FullPayloadGen , None ] : \n options = Options ( \n detect_mode = detect_mode , \n replaced_keyword_strategy = replaced_keyword_strategy , \n environment = environment , \n python_version = PythonEnvironment . UNKNOWN , \n ) \n cracker = Cracker ( submitter = submitter , options = options ) \n if not cracker . has_respond ( ) : \n return None \n full_payload_gen = cracker . crack ( ) \n if full_payload_gen is None : \n return None \n return full_payload_gen \n def do_crack ( \n full_payload_gen : FullPayloadGen , submitter : Submitter , exec_cmd : Union [ str , None ] \n ) : \n cmd_exec_func = partial ( \n do_submit_cmdexec , \n submitter = submitter , \n full_payload_gen_like = full_payload_gen , \n ) \n if exec_cmd : \n print ( cmd_exec_func ( exec_cmd ) ) \n else : \n interact ( cmd_exec_func ) \n def do_crack_eval_args ( \n submitter : Submitter , \n eval_args_payloadgen : EvalArgsModePayloadGen , \n exec_cmd : Union [ str , None ] , \n ) : \n cmd_exec_func = partial ( \n do_submit_cmdexec , \n submitter = submitter , \n full_payload_gen_like = eval_args_payloadgen , \n ) \n if exec_cmd : \n print ( cmd_exec_func ( exec_cmd ) ) \n else : \n interact ( cmd_exec_func ) \n common_options_cli = [ \n click . option ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n default = \"<STR_LIT>\" , \n help = \"<STR_LIT>\" , \n ) , \n click . option ( \n \"<STR_LIT>\" , \n type = DetectMode , \n cls = EnumOption , \n default = DetectMode . ACCURATE , \n help = \"<STR_LIT>\" , \n ) , \n click . option ( \n \"<STR_LIT>\" , \n default = ReplacedKeywordStrategy . AVOID , \n type = ReplacedKeywordStrategy , \n cls = EnumOption , \n help = \"<STR_LIT>\" , \n ) , \n click . option ( \n \"<STR_LIT>\" , \n default = TemplateEnvironment . JINJA2 , \n type = TemplateEnvironment , \n cls = EnumOption , \n help = \"<STR_LIT>\" , \n ) , \n click . option ( \n \"<STR_LIT>\" , \n default = \"<STR_LIT>\" , \n help = \"<STR_LIT>\" , \n ) , \n click . option ( \"<STR_LIT>\" , default = <NUM_LIT> , help = \"<STR_LIT>\" ) , \n ] \n common_options_http = [ \n click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , help = \"<STR_LIT>\" ) , \n click . option ( \n \"<STR_LIT>\" , default = DEFAULT_USER_AGENT , help = \"<STR_LIT>\" \n ) , \n click . option ( \"<STR_LIT>\" , default = [ ] , multiple = True , help = \"<STR_LIT>\" ) , \n click . option ( \"<STR_LIT>\" , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) , \n click . option ( \"<STR_LIT>\" , default = None , help = \"<STR_LIT>\" ) , \n click . option ( \"<STR_LIT>\" , default = None , help = \"<STR_LIT>\" ) , \n click . option ( \"<STR_LIT>\" , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) , \n ] \n def add_options ( options ) : \n def decorator ( f ) : \n for option in options : \n f = option ( f ) \n return f \n return decorator \n @ click . group ( ) \n def main ( ) : \n @ main . command ( ) \n @ add_options ( common_options_http ) \n @ add_options ( common_options_cli ) \n @ click . option ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n default = None , \n help = \"<STR_LIT>\" , \n ) \n @ click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n @ click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , help = \"<STR_LIT>\" ) \n @ click . option ( \n \"<STR_LIT>\" , \n default = False , \n is_flag = True , \n help = \"<STR_LIT>\" , \n ) \n def crack ( \n url : str , \n action : str , \n method : str , \n inputs : str , \n exec_cmd : str , \n interval : float , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n eval_args_payload : bool , \n user_agent : str , \n header : tuple , \n cookies : str , \n extra_params : str , \n extra_data : str , \n proxy : str , \n tamper_cmd : str , \n ) : \n print ( TITLE ) \n assert all ( param is not None for param in [ url , inputs ] ) , \"<STR_LIT>\" \n form = get_form ( \n action = action or urlparse ( url ) . path , \n method = method , \n inputs = inputs . split ( \"<STR_LIT>\" ) , \n ) \n requester = HTTPRequester ( \n interval = interval , \n user_agent = user_agent , \n headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , \n extra_params_querystr = extra_params , \n extra_data_querystr = extra_data , \n proxy = proxy , \n ) \n if not eval_args_payload : \n result = do_crack_form_pre ( \n url , \n form , \n requester , \n detect_mode , \n replaced_keyword_strategy , \n environment , \n tamper_cmd , \n ) \n if not result : \n logger . warning ( \"<STR_LIT>\" ) \n raise RunFailed ( ) \n full_payload_gen , submitter = result \n do_crack ( full_payload_gen , submitter , exec_cmd ) \n else : \n result = do_crack_form_eval_args_pre ( \n url , \n form , \n requester , \n detect_mode , \n replaced_keyword_strategy , \n environment , \n tamper_cmd , \n ) \n if not result : \n logger . warning ( \"<STR_LIT>\" ) \n raise RunFailed ( ) \n submitter , evalargs_payload_gen = result \n do_crack_eval_args ( submitter , evalargs_payload_gen , exec_cmd ) \n @ main . command ( ) \n @ add_options ( common_options_http ) \n @ add_options ( common_options_cli ) \n def crack_path ( \n url : str , \n exec_cmd : str , \n interval : float , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n user_agent : str , \n header : tuple , \n cookies : str , \n extra_params : str , \n extra_data : str , \n proxy : str , \n tamper_cmd : str , \n ) : \n assert url is not None , \"<STR_LIT>\" \n print ( TITLE ) \n requester = HTTPRequester ( \n interval = interval , \n user_agent = user_agent , \n headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , \n extra_params_querystr = extra_params , \n extra_data_querystr = extra_data , \n proxy = proxy , \n ) \n result = do_crack_path_pre ( \n url , \n requester , \n detect_mode , \n replaced_keyword_strategy , \n environment , \n tamper_cmd , \n ) \n if not result : \n logger . warning ( \"<STR_LIT>\" ) \n raise RunFailed ( ) \n full_payload_gen , submitter = result \n do_crack ( full_payload_gen , submitter , exec_cmd ) \n @ main . command ( ) \n @ add_options ( common_options_http ) \n @ add_options ( common_options_cli ) \n def scan ( \n url , \n exec_cmd , \n interval , \n detect_mode , \n replaced_keyword_strategy , \n environment , \n user_agent , \n header , \n cookies , \n extra_params , \n extra_data , \n proxy , \n tamper_cmd : str , \n ) : \n print ( TITLE ) \n requester = HTTPRequester ( \n interval = interval , \n user_agent = user_agent , \n headers = parse_headers_cookies ( headers_list = list ( header ) , cookies = cookies ) , \n extra_params_querystr = extra_params , \n extra_data_querystr = extra_data , \n proxy = proxy , \n ) \n url_forms = ( \n ( page_url , form ) \n for ( page_url , forms ) in yield_form ( requester , url ) \n for form in forms \n ) \n for page_url , form in url_forms : \n logger . warning ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , repr ( form ) ) ) \n result = do_crack_form_pre ( \n page_url , \n form , \n requester , \n detect_mode , \n replaced_keyword_strategy , \n environment , \n tamper_cmd , \n ) \n if not result : \n continue \n full_payload_gen , submitter = result \n do_crack ( full_payload_gen , submitter , exec_cmd ) \n return \n logger . warning ( \"<STR_LIT>\" ) \n logger . warning ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" , \n url , \n ) \n raise RunFailed ( ) \n @ main . command ( ) \n @ add_options ( common_options_cli ) \n @ click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , help = \"<STR_LIT>\" ) \n @ click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , required = True , type = int , help = \"<STR_LIT>\" ) \n @ click . option ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n @ click . option ( \n \"<STR_LIT>\" , default = b\"<STR_LIT>\" , type = bytes , help = \"<STR_LIT>\" \n ) \n @ click . option ( \"<STR_LIT>\" , default = False , help = \"<STR_LIT>\" ) \n @ click . option ( \"<STR_LIT>\" , default = True , help = \"<STR_LIT>\" ) \n @ click . option ( \"<STR_LIT>\" , is_flag = True , default = False , help = \"<STR_LIT>\" ) \n @ click . option ( \"<STR_LIT>\" , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n def crack_request ( \n host : str , \n port : int , \n request_file : str , \n toreplace : bytes , \n ssl : bool , \n exec_cmd : str , \n urlencode_payload : bool , \n raw : bool , \n detect_mode : DetectMode , \n replaced_keyword_strategy : ReplacedKeywordStrategy , \n environment : TemplateEnvironment , \n retry_times : int , \n interval : float , \n tamper_cmd : str , \n ) : \n request_filepath = Path ( request_file ) \n if not request_filepath . is_file ( ) : \n logger . error ( \"<STR_LIT>\" , request_filepath ) \n request_pattern = request_filepath . read_bytes ( ) \n if not raw and not check_tail ( request_pattern ) : \n logger . warning ( \"<STR_LIT>\" ) \n logger . warning ( \"<STR_LIT>\" ) \n request_pattern = fix_tail ( request_pattern ) \n time . sleep ( <NUM_LIT> ) \n if not raw and not check_line_break ( request_pattern ) : \n logger . warning ( \"<STR_LIT>\" ) \n logger . warning ( \"<STR_LIT>\" ) \n request_pattern = fix_line_break ( request_pattern ) \n time . sleep ( <NUM_LIT> ) \n requester = TCPRequester ( \n host = host , port = port , use_ssl = ssl , retry_times = retry_times , interval = interval \n ) \n submitter = TCPSubmitter ( \n requester = requester , \n pattern = request_pattern , \n toreplace = toreplace , \n urlencode_payload = urlencode_payload , \n ) \n if tamper_cmd : \n tamperer = shell_tamperer ( tamper_cmd ) \n submitter . add_tamperer ( tamperer ) \n full_payload_gen = do_crack_request_pre ( \n submitter = submitter , \n detect_mode = detect_mode , \n replaced_keyword_strategy = replaced_keyword_strategy , \n environment = environment , \n ) \n if not full_payload_gen : \n logger . warning ( \"<STR_LIT>\" ) \n raise RunFailed ( ) \n do_crack ( full_payload_gen , submitter , exec_cmd ) \n @ main . command ( ) \n @ click . option ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" \n ) \n @ click . option ( \"<STR_LIT>\" , \"<STR_LIT>\" , default = <NUM_LIT> , help = \"<STR_LIT>\" ) \n @ click . option ( \n \"<STR_LIT>\" , default = True , help = \"<STR_LIT>\" \n ) \n def webui ( host , port , open_browser ) : \n webui_main ( host , port , open_browser ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "from . core import Core \n from . config import VERSION , ASYNC_COMPONENTS \n from . log import set_logging \n if ASYNC_COMPONENTS : \n from . async_components import load_components \n else : \n from . components import load_components \n __version__ = VERSION \n instanceList = [ ] \n def load_async_itchat ( ) -> Core : \n from . async_components import load_components \n load_components ( Core ) \n return Core ( ) \n def load_sync_itchat ( ) -> Core : \n from . components import load_components \n load_components ( Core ) \n return Core ( ) \n if ASYNC_COMPONENTS : \n instance = load_async_itchat ( ) \n else : \n instance = load_sync_itchat ( ) \n instanceList = [ instance ] \n login = instance . login \n get_QRuuid = instance . get_QRuuid \n get_QR = instance . get_QR \n check_login = instance . check_login \n web_init = instance . web_init \n show_mobile_login = instance . show_mobile_login \n start_receiving = instance . start_receiving \n get_msg = instance . get_msg \n logout = instance . logout \n update_chatroom = instance . update_chatroom \n update_friend = instance . update_friend \n get_contact = instance . get_contact \n get_friends = instance . get_friends \n get_chatrooms = instance . get_chatrooms \n get_mps = instance . get_mps \n set_alias = instance . set_alias \n set_pinned = instance . set_pinned", "output": "accept_friend = instance . accept_friend \n get_head_img = instance . get_head_img \n create_chatroom = instance . create_chatroom \n set_chatroom_name = instance . set_chatroom_name \n delete_member_from_chatroom = instance . delete_member_from_chatroom \n add_member_into_chatroom = instance . add_member_into_chatroom \n send_raw_msg = instance . send_raw_msg \n send_msg = instance . send_msg \n upload_file = instance . upload_file \n send_file = instance . send_file \n send_image = instance . send_image \n send_video = instance . send_video \n send = instance . send \n revoke = instance . revoke \n dump_login_status = instance . dump_login_status \n load_login_status = instance . load_login_status \n auto_login = instance . auto_login \n configured_reply = instance . configured_reply \n msg_register = instance . msg_register \n run = instance . run \n search_friends = instance . search_friends \n search_chatrooms = instance . search_chatrooms \n search_mps = instance . search_mps \n set_logging = set_logging"}, {"input": "from bridge . bridge import Bridge \n class Channel ( object ) : \n def startup ( self ) : \n raise NotImplementedError \n def handle ( self , msg ) : \n raise NotImplementedError \n def send ( self , msg , receiver ) : \n raise NotImplementedError \n def build_reply_content ( self , query , context = None ) :", "output": "return Bridge ( ) . fetch_reply_content ( query , context )"}, {"input": "from flask import request , abort \n from app . models import User \n def check_token ( ) -> str : \n token = request . values . get ( '<STR_LIT>' ) \n if not token : \n abort ( <NUM_LIT> , description = '<STR_LIT>' ) \n else : \n return token \n def check_user ( token : str ) -> User : \n user = User . verify_api_token ( token ) \n if not user : \n abort ( <NUM_LIT> , description = '<STR_LIT>' )", "output": "else : \n return user"}, {"input": "from typing import List \n import argparse \n import random \n import json \n import os \n from torch . distributions . categorical import Categorical \n from PIL import Image \n from datasets import Dataset \n import gymnasium as gym \n import torch . nn as nn \n import numpy as np \n import torch \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n LUNAR_LANDER_OPTIONS = ( \n \"<STR_LIT>\" . split ( \"<STR_LIT>\" ) \n ) \n MAX_STEPS = <NUM_LIT> \n def layer_init ( layer , std = np . sqrt ( <NUM_LIT> ) , bias_const = <NUM_LIT> ) : \n torch . nn . init . orthogonal_ ( layer . weight , std ) \n torch . nn . init . constant_ ( layer . bias , bias_const ) \n return layer", "output": "class Agent ( nn . Module ) : \n def __init__ ( self , envs ) : \n super ( ) . __init__ ( ) \n self . critic = nn . Sequential ( \n layer_init ( \n nn . Linear ( np . array ( envs . single_observation_space . shape ) . prod ( ) , <NUM_LIT> ) \n ) , \n nn . Tanh ( ) , \n layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) ) , \n nn . Tanh ( ) , \n layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) , std = <NUM_LIT> ) , \n ) \n self . actor = nn . Sequential ( \n layer_init ( \n nn . Linear ( np . array ( envs . single_observation_space . shape ) . prod ( ) , <NUM_LIT> ) \n ) , \n nn . Tanh ( ) , \n layer_init ( nn . Linear ( <NUM_LIT> , <NUM_LIT> ) ) , \n nn . Tanh ( ) , \n layer_init ( nn . Linear ( <NUM_LIT> , envs . single_action_space . n ) , std = <NUM_LIT> ) , \n ) \n def get_value ( self , x ) : \n return self . critic ( x ) \n def get_action_and_value ( self , x , action = None ) : \n logits = self . actor ( x ) \n probs = Categorical ( logits = logits ) \n if action is None : \n action = probs . sample ( ) \n return action , probs . log_prob ( action ) , probs . entropy ( ) , self . critic ( x ) \n def _gen_examples ( round_num , args ) : \n env = gym . make ( \"<STR_LIT>\" , render_mode = \"<STR_LIT>\" ) \n random . seed ( round_num ) \n np . random . seed ( round_num ) \n class EnvWrapper : \n single_observation_space = env . observation_space \n single_action_space = env . action_space \n model = Agent ( EnvWrapper ( ) ) . to ( \"<STR_LIT>\" ) \n model . load_state_dict ( \n torch . load ( args . pretrained_ppo_model_path , map_location = \"<STR_LIT>\" ) \n ) \n model . eval ( ) \n os . makedirs ( args . output_image_folder , exist_ok = True ) \n observation , info = env . reset ( seed = round_num ) \n for frame in range ( MAX_STEPS ) : \n img = env . render ( ) \n with torch . no_grad ( ) : \n action , logprob , _ , value = model . get_action_and_value ( \n torch . from_numpy ( observation ) \n ) \n action = action . cpu ( ) . numpy ( ) \n resp = \"<STR_LIT>\" \n if action == <NUM_LIT> : \n resp = \"<STR_LIT>\" \n elif action == <NUM_LIT> : \n resp = \"<STR_LIT>\" \n elif action == <NUM_LIT> : \n resp = \"<STR_LIT>\" \n elif action == <NUM_LIT> : \n resp = \"<STR_LIT>\" \n if random . random ( ) < args . sample_rate : \n random . shuffle ( LUNAR_LANDER_OPTIONS ) \n options_str = \"<STR_LIT>\" . join ( LUNAR_LANDER_OPTIONS ) \n img_fn = os . path . join ( args . output_image_folder , f\"<STR_LIT>\" ) \n messages = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } , \n { \"<STR_LIT>\" : ROLE_ASSISTANT , \"<STR_LIT>\" : resp } , \n ] \n Image . fromarray ( img ) . save ( img_fn ) \n example = { \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : [ img_fn ] , \n \"<STR_LIT>\" : messages , \n } \n yield example \n observation , reward , terminated , truncated , info = env . step ( action ) \n if terminated or truncated : \n break \n def main ( args ) : \n def gen ( idxs ) : \n for r in idxs : \n yield from _gen_examples ( r , args ) \n ds = Dataset . from_generator ( \n gen , gen_kwargs = { \"<STR_LIT>\" : list ( range ( args . rounds ) ) } , num_proc = args . num_proc \n ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , type = float , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from __future__ import annotations \n import json \n from pyramid . request import Request \n from pyramid . response import Response \n from pyramid . view import view_config \n import svcs \n @ view_config ( route_name = \"<STR_LIT>\" ) \n def healthy_view ( request : Request ) -> Response : \n ok : list [ str ] = [ ] \n failing : list [ dict [ str , str ] ] = [ ] \n status = <NUM_LIT> \n for svc in svcs . pyramid . get_pings ( request ) : \n try : \n svc . ping ( ) \n ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } )", "output": "status = <NUM_LIT> \n return Response ( \n content_type = \"<STR_LIT>\" , \n status = status , \n body = json . dumps ( { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } ) . encode ( \"<STR_LIT>\" ) , \n )"}, {"input": "from instld . errors import InstallingPackageError \n from instld . cli . parsing_comments . get_comment_string import get_comment_string_by_frame \n def get_options_from_comments ( comment_string ) : \n result = { } \n if comment_string is not None : \n options = ( x . strip ( ) for x in comment_string . split ( '<STR_LIT>' ) ) \n options = ( x for x in options if x ) \n for option in options : \n splitted_option = [ x for x in option . split ( ) if x ] \n if len ( splitted_option ) != <NUM_LIT> : \n raise InstallingPackageError ( '<STR_LIT>' ) \n option_name = splitted_option [ <NUM_LIT> ] . strip ( ) . lower ( ) \n option_value = splitted_option [ <NUM_LIT> ] . strip ( ) . lower ( ) \n result [ option_name ] = option_value \n result . pop ( '<STR_LIT>' , None ) \n result . pop ( '<STR_LIT>' , None ) \n return result \n def get_options_from_comments_by_frame ( frame ) : \n comment_string = get_comment_string_by_frame ( frame )", "output": "return get_options_from_comments ( comment_string )"}, {"input": "status = <NUM_LIT> \n match status : \n case <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n case <NUM_LIT> | <NUM_LIT> | <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n case <NUM_LIT> :", "output": "print ( \"<STR_LIT>\" ) \n case <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n case _ : \n print ( \"<STR_LIT>\" )"}, {"input": "import submitit \n import os \n from dataclasses import dataclass , field \n import hydra \n from omegaconf import MISSING \n from peft import PeftModel \n from hydra . core . config_store import ConfigStoreWithProvider \n from transformers import CLIPProcessor , CLIPModel \n from densely_captioned_images . dataset . utils import print_trainable_parameters \n from densely_captioned_images . repro . eval . run_aro import run_aro_evals \n from densely_captioned_images . repro . eval . run_vlc import run_vlc_on_model \n from densely_captioned_images . repro . eval . run_winoground import run_winoground \n from densely_captioned_images . repro . eval . run_elevater import run_elevater_on \n from densely_captioned_images . dataset . scripts . run_clip_dense_cap_eval import run_dense_cap_on_model \n from typing import Any , Optional \n HYDRA_CONFIG_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) \n @ dataclass \n class CLIPEvalConfig ( ) : \n run_aro : bool = field ( \n default = False , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n run_vlc : bool = field ( \n default = False , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n run_dense_cap : bool = field ( \n default = False , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n run_winoground : bool = field ( \n default = False , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n run_elevater : int = field ( \n default = - <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n elevater_dataset : Optional [ int ] = field ( \n default = None , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n lora_weight_location : str = field ( \n default = MISSING , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n model_name : str = field ( \n default = MISSING , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n config = ConfigStoreWithProvider ( \"<STR_LIT>\" ) \n config . store ( name = \"<STR_LIT>\" , node = CLIPEvalConfig ) \n class CLIPEvalJob : \n def __call__ ( self , * args , ** kwargs ) : \n run_eval_clip ( args [ <NUM_LIT> ] ) \n def checkpoint ( self , * args : Any , ** kwargs : Any ) -> submitit . helpers . DelayedSubmission : \n return submitit . helpers . DelayedSubmission ( self , * args , ** kwargs ) \n @ hydra . main ( \n config_path = HYDRA_CONFIG_PATH , config_name = \"<STR_LIT>\" , version_base = \"<STR_LIT>\" \n ) \n def run_eval_clip ( cfg : CLIPEvalConfig ) : \n print ( '<STR_LIT>' ) \n base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n print_trainable_parameters ( base_clip_model )", "output": "print ( f\"<STR_LIT>\" ) \n loaded = PeftModel . from_pretrained ( base_clip_model , cfg . lora_weight_location ) \n print_trainable_parameters ( loaded ) \n loaded = loaded . merge_and_unload ( ) . to ( '<STR_LIT>' ) \n if cfg . run_aro : \n print ( \"<STR_LIT>\" ) \n run_aro_evals ( loaded , processor ) \n if cfg . run_vlc : \n print ( \"<STR_LIT>\" ) \n run_vlc_on_model ( loaded , processor , cfg . model_name ) \n if cfg . run_winoground : \n print ( \"<STR_LIT>\" ) \n run_winoground ( loaded , processor ) \n if cfg . run_elevater != - <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n run_elevater_on ( \n loaded , \n cfg . model_name , \n run_full = True , \n do_finetune = True , \n shot_options = [ cfg . run_elevater ] , \n dataset_option = cfg . elevater_dataset , \n ) \n if cfg . run_dense_cap : \n print ( \"<STR_LIT>\" ) \n run_dense_cap_on_model ( loaded , processor ) \n if __name__ == '<STR_LIT>' : \n run_eval_clip ( )"}, {"input": "import ffmpeg \n import numpy as np \n import re \n import unicodedata \n def load_audio ( file , sampling_rate ) : \n try : \n file = file . strip ( \"<STR_LIT>\" ) . strip ( '<STR_LIT>' ) . strip ( \"<STR_LIT>\" ) . strip ( '<STR_LIT>' ) . strip ( \"<STR_LIT>\" ) \n out , _ = ( \n ffmpeg . input ( file , threads = <NUM_LIT> ) \n . output ( \"<STR_LIT>\" , format = \"<STR_LIT>\" , acodec = \"<STR_LIT>\" , ac = <NUM_LIT> , ar = sampling_rate ) \n . run ( cmd = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , capture_stdout = True , capture_stderr = True ) \n ) \n except Exception as error : \n raise RuntimeError ( f\"<STR_LIT>\" ) \n return np . frombuffer ( out , np . float32 ) . flatten ( ) \n def format_title ( title ) : \n formatted_title = ( \n unicodedata . normalize ( \"<STR_LIT>\" , title ) . encode ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . decode ( \"<STR_LIT>\" ) \n ) \n formatted_title = re . sub ( r\"<STR_LIT>\" , \"<STR_LIT>\" , formatted_title ) \n formatted_title = re . sub ( r\"<STR_LIT>\" , \"<STR_LIT>\" , formatted_title ) \n formatted_title = re . sub ( r\"<STR_LIT>\" , \"<STR_LIT>\" , formatted_title )", "output": "return formatted_title"}, {"input": "from bridge . bridge import Bridge \n class Channel ( object ) : \n def startup ( self ) : \n raise NotImplementedError \n def handle ( self , msg ) : \n raise NotImplementedError \n def send ( self , msg , receiver ) :", "output": "raise NotImplementedError \n def build_reply_content ( self , query , context = None ) : \n return Bridge ( ) . fetch_reply_content ( query , context ) \n async def build_reply_stream ( self , query , context = None ) : \n async for final , response in Bridge ( ) . fetch_reply_stream ( query , context ) : \n yield final , response"}, {"input": "from io import BytesIO \n from curl_cffi import Curl , CurlInfo , CurlOpt , requests \n def main_curl ( ) : \n buffer = BytesIO ( ) \n c = Curl ( ) \n c . setopt ( CurlOpt . CUSTOMREQUEST , b\"<STR_LIT>\" ) \n c . setopt ( CurlOpt . URL , b\"<STR_LIT>\" ) \n c . setopt ( CurlOpt . WRITEDATA , buffer ) \n c . perform ( ) \n body = buffer . getvalue ( ) \n print ( \"<STR_LIT>\" ) \n print ( body . decode ( ) ) \n print ( \"<STR_LIT>\" ) \n buffer = BytesIO ( ) \n c . setopt ( CurlOpt . WRITEDATA , buffer ) \n c . setopt ( CurlOpt . URL , b\"<STR_LIT>\" ) \n c . impersonate ( \"<STR_LIT>\" ) \n c . setopt ( CurlOpt . HTTPHEADER , [ b\"<STR_LIT>\" ] ) \n c . perform ( ) \n body = buffer . getvalue ( ) \n print ( \"<STR_LIT>\" ) \n print ( body . decode ( ) ) \n c . close ( ) \n def main_requests ( ) : \n r = requests . get ( \"<STR_LIT>\" ) \n print ( r . json ( ) ) \n r = requests . get ( \"<STR_LIT>\" , impersonate = \"<STR_LIT>\" ) \n print ( r . json ( ) ) \n async def async_main ( ) : \n async with requests . AsyncSession ( ) as s : \n r = await s . get ( \"<STR_LIT>\" ) \n print ( r . text ) \n r = await s . get ( \"<STR_LIT>\" , stream = True )", "output": "async for content in r . iter_content ( ) : \n print ( content ) \n if __name__ == \"<STR_LIT>\" : \n async_main ( )"}, {"input": "from __future__ import annotations \n from collections . abc import Callable \n from contextlib import suppress \n from typing import Any , Protocol , overload \n import attrs \n from pyramid . config import Configurator \n from pyramid . registry import Registry \n from pyramid . request import Request \n from pyramid . response import Response \n from pyramid . threadlocal import get_current_registry , get_current_request \n import svcs \n from . _core import ( \n _KEY_CONTAINER , \n _KEY_REGISTRY , \n T1 , \n T2 , \n T3 , \n T4 , \n T5 , \n T6 , \n T7 , \n T8 , \n T9 , \n T10 , \n ) \n def svcs_from ( request : Request | None = None ) -> svcs . Container : \n if request is None : \n request = get_current_request ( ) \n return getattr ( request , _KEY_CONTAINER ) \n def get_registry ( rh : PyramidRegistryHaver | None = None ) -> svcs . Registry : \n registry = rh . registry if rh else get_current_registry ( ) \n return registry [ _KEY_REGISTRY ] \n def init ( \n config : Configurator , \n * , \n registry : svcs . Registry | None = None , \n tween_under : Any = None , \n tween_over : Any = None , \n ) -> None : \n config . registry [ _KEY_REGISTRY ] = registry or svcs . Registry ( ) \n config . add_tween ( \n \"<STR_LIT>\" , over = tween_over , under = tween_under \n )", "output": "@ attrs . define \n class ServicesTween : \n handler : Callable [ [ Request ] , Response ] \n registry : Registry \n def __call__ ( self , request : Request ) -> Response : \n def make_container ( request : Request ) -> svcs . Container : \n con = svcs . Container ( self . registry [ _KEY_REGISTRY ] ) \n request . add_finished_callback ( lambda _ : con . close ( ) ) \n return con \n request . set_property ( make_container , _KEY_CONTAINER , reify = True ) \n return self . handler ( request ) \n def register_factory ( \n config : PyramidRegistryHaver , \n svc_type : type , \n factory : Callable , \n * , \n enter : bool = True , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n config . registry [ _KEY_REGISTRY ] . register_factory ( \n svc_type , \n factory , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n def register_value ( \n config : PyramidRegistryHaver , \n svc_type : type , \n value : object , \n * , \n enter : bool = False , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n config . registry [ _KEY_REGISTRY ] . register_value ( \n svc_type , \n value , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n def close_registry ( rh : PyramidRegistryHaver ) -> None : \n with suppress ( KeyError ) : \n get_registry ( rh ) . close ( ) \n class PyramidRegistryHaver ( Protocol ) : \n registry : dict [ str , Any ] \n def get_pings ( request : Request ) -> list [ svcs . ServicePing ] : \n return svcs_from ( request ) . get_pings ( ) \n def get_abstract ( request : Request , * svc_types : type ) -> Any : \n return svcs_from ( request ) . get ( * svc_types ) \n @ overload \n def get ( request : Request , svc_type : type [ T1 ] , / ) -> T1 : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n / , \n ) -> tuple [ T1 , T2 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... \n @ overload \n def get ( \n request : Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n svc_type10 : type [ T10 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... \n def get ( request : Request , * svc_types : type ) -> object : \n return svcs_from ( request ) . get ( * svc_types )"}, {"input": "from densely_captioned_images . repro . train . train_clip import CLIPAndNegConfig , CLIPTrainJob , get_dir_name \n from densely_captioned_images . repro . config import MODEL_PATH \n import itertools \n import submitit \n import time \n import os \n def makeGrid ( pars_dict ) : \n keys = pars_dict . keys ( ) \n combinations = itertools . product ( * pars_dict . values ( ) ) \n ds = [ dict ( zip ( keys , cc ) ) for cc in combinations ] \n return ds \n def main ( ) : \n base_only_sweep = { \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ False ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n } \n base_only_baselines = { \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ False ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] ,", "output": "\"<STR_LIT>\" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n } \n base_only_sweep_params = makeGrid ( base_only_sweep ) + makeGrid ( base_only_baselines ) \n base_only_sweep = [ CLIPAndNegConfig ( ** p ) for p in base_only_sweep_params ] \n final_sweep = { \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n } \n final_sweep_baseline = { \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> , <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n } \n final_sweep_params = makeGrid ( final_sweep_baseline ) \n final_sweep = [ CLIPAndNegConfig ( ** p ) for p in final_sweep_params ] \n log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f\"<STR_LIT>\" ) \n os . makedirs ( log_base_folder ) \n log_folder = f\"<STR_LIT>\" \n executor = submitit . AutoExecutor ( folder = log_folder ) \n executor . update_parameters ( \n slurm_partition = '<STR_LIT>' , \n nodes = <NUM_LIT> , \n timeout_min = <NUM_LIT> * <NUM_LIT> , \n tasks_per_node = <NUM_LIT> , \n gpus_per_node = <NUM_LIT> , \n cpus_per_task = <NUM_LIT> , \n slurm_mem = '<STR_LIT>' , \n slurm_constraint = '<STR_LIT>' , \n ) \n job_array = [ ] \n existing_sweeps = set ( ) \n for sweep_args in base_only_sweep + final_sweep : \n if get_dir_name ( sweep_args ) in existing_sweeps : \n continue \n existing_sweeps . add ( get_dir_name ( sweep_args ) ) \n job = executor . submit ( CLIPTrainJob ( ) , sweep_args ) \n job_array . append ( job ) \n print ( f\"<STR_LIT>\" ) \n for job in job_array : \n _ = job . result ( ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "def funcion ( ) : \n variable_local = \"<STR_LIT>\" \n print ( variable_local ) \n funcion ( ) \n def funcion_externa ( ) : \n variable_cierre = \"<STR_LIT>\" \n def funcion_interna ( ) : \n print ( variable_cierre ) \n funcion_interna ( ) \n funcion_externa ( )", "output": "variable_global = \"<STR_LIT>\" \n def funcion1 ( ) : \n print ( variable_global ) \n funcion1 ( ) \n print ( variable_global ) \n print ( len ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) )"}, {"input": "from typing import Type , List , Optional \n import logging \n from transformers import AutoTokenizer , AutoConfig , BitsAndBytesConfig \n from huggingface_hub import hf_hub_download \n from peft import PeftModel \n import torch \n import os \n from multi_token . model_utils import fix_tokenizer \n from multi_token . modalities . base_modality import Modality \n from multi_token . language_models . mistral import MistralForCausalLM \n from multi_token . language_models import LANGUAGE_MODEL_NAME_TO_CLASS \n from multi_token . modalities import MODALITY_BUILDERS \n def load_trained_lora_model ( \n model_name_or_path : str , \n model_lora_path : str , \n model_cls : Optional [ Type ] = None , \n modalities : Optional [ List [ Modality ] ] = None , \n load_bits : int = <NUM_LIT> , \n device_map : str = \"<STR_LIT>\" , \n ) : \n load_kwargs = { \"<STR_LIT>\" : device_map } \n if load_bits == <NUM_LIT> : \n load_kwargs [ \"<STR_LIT>\" ] = True", "output": "elif load_bits == <NUM_LIT> : \n load_kwargs [ \"<STR_LIT>\" ] = True \n load_kwargs [ \"<STR_LIT>\" ] = BitsAndBytesConfig ( \n load_in_4bit = True , \n bnb_4bit_compute_dtype = torch . float16 , \n bnb_4bit_use_double_quant = True , \n bnb_4bit_quant_type = \"<STR_LIT>\" , \n ) \n elif load_bits == <NUM_LIT> : \n load_kwargs [ \"<STR_LIT>\" ] = torch . float16 \n else : \n raise ValueError ( f\"<STR_LIT>\" ) \n tokenizer = AutoTokenizer . from_pretrained ( model_name_or_path , use_fast = False ) \n fix_tokenizer ( tokenizer ) \n cfg = AutoConfig . from_pretrained ( model_lora_path ) \n if model_cls is None : \n model_cls = LANGUAGE_MODEL_NAME_TO_CLASS [ cfg . model_cls ] \n if modalities is None : \n modalities = MODALITY_BUILDERS [ cfg . modality_builder ] ( ) \n logging . info ( f\"<STR_LIT>\" ) \n model = model_cls . from_pretrained ( \n model_name_or_path , low_cpu_mem_usage = True , config = cfg , ** load_kwargs \n ) \n model . modalities = modalities \n logging . info ( f\"<STR_LIT>\" ) \n if os . path . exists ( os . path . join ( model_lora_path , \"<STR_LIT>\" ) ) : \n non_lora_trainables = torch . load ( \n os . path . join ( model_lora_path , \"<STR_LIT>\" ) , map_location = \"<STR_LIT>\" \n ) \n else : \n local_fn = hf_hub_download ( \n repo_id = model_lora_path , \n filename = \"<STR_LIT>\" , \n repo_type = \"<STR_LIT>\" , \n ) \n non_lora_trainables = torch . load ( local_fn , map_location = \"<STR_LIT>\" ) \n model . get_model ( ) . initialize_pretrained_modules ( modalities , non_lora_trainables ) \n logging . info ( f\"<STR_LIT>\" ) \n model = PeftModel . from_pretrained ( model , model_lora_path ) \n if load_bits == <NUM_LIT> : \n model = model . merge_and_unload ( ) \n model . eval ( ) \n return model , tokenizer"}, {"input": "from aiocqhttp import CQHttp , Event , MessageSegment \n from channel . channel import Channel \n from common . log import logger \n from config import conf \n from bridge . bridge import Bridge \n from concurrent . futures import ThreadPoolExecutor", "output": "bot = CQHttp ( ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ bot . on_message ( '<STR_LIT>' ) \n async def _ ( event : Event ) : \n logger . info ( \"<STR_LIT>\" , event ) \n QqchaChannel ( ) . handle ( event ) \n @ bot . on_startup \n async def startup ( ) : \n logger . info ( \"<STR_LIT>\" ) \n class QqchaChannel ( Channel ) : \n def __init__ ( self ) : \n self . host = conf ( ) . get ( '<STR_LIT>' ) \n self . port = conf ( ) . get ( '<STR_LIT>' ) \n logger . info ( \"<STR_LIT>\" . format ( \n self . host , self . port ) ) \n def startup ( self ) : \n bot . run ( host = self . host , port = self . port ) \n def handle ( self , msg ) : \n thread_pool . submit ( self . _do_handle , msg ) \n def _do_handle ( self , msg ) : \n context = dict ( ) \n reply_text = self . build_reply_content ( msg . message , context ) \n bot . sync . send_private_msg ( user_id = msg . user_id , message = reply_text ) \n def send ( self , msg , receiver ) : \n logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) \n bot . send ( receiver , msg ) \n def build_reply_content ( self , query , context = None ) : \n return Bridge ( ) . fetch_reply_content ( query , context )"}, {"input": "from channel . channel import Channel \n from aiocqhttp import CQHttp , Event \n from common import log \n from concurrent . futures import ThreadPoolExecutor \n bot = CQHttp ( api_root = '<STR_LIT>' ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ bot . on_message ( '<STR_LIT>' ) \n def handle_private_msg ( event : Event ) : \n log . info ( \"<STR_LIT>\" , event ) \n QQChannel ( ) . handle ( event ) \n @ bot . on_message ( '<STR_LIT>' ) \n def handle_private_msg ( event : Event ) : \n log . info ( \"<STR_LIT>\" , event ) \n QQChannel ( ) . handle_group ( event ) \n class QQChannel ( Channel ) : \n def startup ( self ) : \n bot . run ( host = '<STR_LIT>' , port = <NUM_LIT> ) \n def handle ( self , msg ) : \n thread_pool . submit ( self . _do_handle , msg ) \n def _do_handle ( self , msg ) : \n context = dict ( ) \n log . info ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n context [ '<STR_LIT>' ] = msg . user_id \n reply_text = super ( ) . build_reply_content ( msg . message , context ) \n bot . sync . send_private_msg ( user_id = msg . user_id , message = reply_text ) \n def handle_group ( self , msg ) : \n thread_pool . submit ( self . _do_handle_group , msg ) \n def _do_handle_group ( self , msg ) : \n context = dict ( ) \n if msg . message and msg . message . find ( '<STR_LIT>' ) : \n receiver = msg . message . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n if receiver == str ( msg [ '<STR_LIT>' ] ) : \n text_list = msg . message . split ( '<STR_LIT>' , <NUM_LIT> ) \n if len ( text_list ) == <NUM_LIT> and len ( text_list [ <NUM_LIT> ] ) > <NUM_LIT> : \n query = text_list [ <NUM_LIT> ] . strip ( ) \n context [ '<STR_LIT>' ] = str ( msg . user_id )", "output": "reply_text = super ( ) . build_reply_content ( query , context ) \n reply_text = '<STR_LIT>' + str ( msg . user_id ) + '<STR_LIT>' + reply_text \n bot . sync . send_group_msg ( group_id = msg [ '<STR_LIT>' ] , message = reply_text )"}, {"input": "from typing import Dict , List , Optional \n import torch \n import torch . nn as nn \n from transformers import AutoFeatureExtractor , WhisperModel \n from multi_token . data_tools import load_audio \n from multi_token . modalities . base_modality import Modality \n from multi_token . modalities . projectors import ( \n build_mlp_vector_projector , \n ) \n OUTPUT_EMB_SIZE = <NUM_LIT> \n class WhisperAudioModule ( nn . Module ) : \n def __init__ ( self , model_name_or_path : str ) : \n super ( ) . __init__ ( ) \n self . model_name_or_path = model_name_or_path \n self . model = None \n self . feature_extractor = None \n self . load_model ( ) \n def load_model ( self ) : \n self . model = WhisperModel . from_pretrained ( self . model_name_or_path ) \n self . feature_extractor = AutoFeatureExtractor . from_pretrained ( \n self . model_name_or_path \n ) \n self . model . requires_grad_ ( False ) \n @ torch . no_grad ( ) \n def forward ( self , audios ) -> torch . Tensor : \n hidden_states = [ ] \n for i in range ( audios . shape [ <NUM_LIT> ] ) : \n decoder_input_ids = ( \n torch . tensor ( [ [ <NUM_LIT> ] ] ) * self . model . config . decoder_start_token_id \n ) \n last_hidden_state = self . model ( \n audios [ i ] . to ( device = self . device , dtype = self . dtype ) ,", "output": "decoder_input_ids = decoder_input_ids . to ( device = self . device ) , \n ) . last_hidden_state \n hidden_states . append ( last_hidden_state ) \n last_hidden_state = torch . stack ( hidden_states ) \n return last_hidden_state . view ( - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE ) \n @ property \n def dtype ( self ) : \n return self . model . dtype \n @ property \n def device ( self ) : \n return self . model . device \n class WhisperAudioModality ( Modality ) : \n def __init__ ( \n self , \n model_name_or_path : str = \"<STR_LIT>\" , \n num_projector_layers : int = <NUM_LIT> , \n num_tokens_output : int = <NUM_LIT> , \n ) : \n self . model_name_or_path = model_name_or_path \n self . module = WhisperAudioModule ( model_name_or_path = self . model_name_or_path ) \n self . num_projector_layers = num_projector_layers \n self . num_tokens_output = num_tokens_output \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n return build_mlp_vector_projector ( \n input_hidden_size = OUTPUT_EMB_SIZE , \n lm_hidden_size = lm_hidden_size , \n num_layers = self . num_projector_layers , \n num_tokens = self . num_tokens_output , \n ) \n @ property \n def name ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def data_key ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token_width ( self ) -> int : \n return self . num_tokens_output \n def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n self . module . to ( dtype = dtype , device = device ) \n return self \n def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ torch . Tensor ] ] : \n row_values = [ ] \n for row in rows : \n audios = [ ] \n for audio_dict in row [ self . data_key ] : \n audio_dict = load_audio ( \n audio_dict , \n target_sampling_rate = self . module . feature_extractor . sampling_rate , \n ) \n audio_processed = self . module . feature_extractor ( \n audio_dict [ \"<STR_LIT>\" ] , \n return_tensors = \"<STR_LIT>\" , \n sampling_rate = audio_dict [ \"<STR_LIT>\" ] , \n ) . input_features \n audios . append ( audio_processed ) \n row_values . append ( torch . stack ( audios ) if len ( audios ) > <NUM_LIT> else None ) \n return row_values \n @ torch . no_grad ( ) \n def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : \n audio_features = [ ] \n for audio_batch in encoded_values : \n audio_features . append ( self . module . forward ( audio_batch ) ) \n return audio_features"}, {"input": "import requests \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) -> list : \n try : \n url = f\"<STR_LIT>\"", "output": "response = requests . get ( url , headers = headers ) \n return response . json ( ) \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n return [ ] \n if __name__ == \"<STR_LIT>\" : \n print ( search ( title = \"<STR_LIT>\" ) )"}, {"input": "import requests \n import concurrent . futures \n import time \n import traceback \n url = \"<STR_LIT>\" \n params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n queries = [ ] \n for i in range ( <NUM_LIT> ) : \n query = f\"<STR_LIT>\" \n queries . append ( query ) \n embeddings = [ ] \n def make_request ( query ) : \n try : \n params [ \"<STR_LIT>\" ] = query \n response = requests . get ( url , params = params ) \n embeddings . append ( response ) \n return response . text \n except : \n print ( f\"<STR_LIT>\" ) \n start_time = time . time ( ) \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : \n futures = [ executor . submit ( make_request , query ) for query in queries ] \n concurrent . futures . wait ( futures ) \n results = [ future . result ( ) for future in futures ]", "output": "end_time = time . time ( ) \n print ( f\"<STR_LIT>\" )"}, {"input": "import os , time , copy \n from threading import Lock \n from . messagequeue import Queue \n from . templates import ( \n ContactList , AbstractUserDict , User , \n MassivePlatform , Chatroom , ChatroomMember ) \n def contact_change ( fn ) : \n def _contact_change ( core , * args , ** kwargs ) : \n with core . storageClass . updateLock : \n return fn ( core , * args , ** kwargs ) \n return _contact_change \n class Storage ( object ) : \n def __init__ ( self , core ) : \n self . userName = None \n self . nickName = None \n self . updateLock = Lock ( ) \n self . memberList = ContactList ( ) \n self . mpList = ContactList ( ) \n self . chatroomList = ContactList ( ) \n self . msgList = Queue ( - <NUM_LIT> ) \n self . lastInputUserName = None \n self . memberList . set_default_value ( contactClass = User ) \n self . memberList . core = core \n self . mpList . set_default_value ( contactClass = MassivePlatform ) \n self . mpList . core = core \n self . chatroomList . set_default_value ( contactClass = Chatroom ) \n self . chatroomList . core = core \n def dumps ( self ) : \n return { \n '<STR_LIT>' : self . userName , \n '<STR_LIT>' : self . nickName , \n '<STR_LIT>' : self . memberList , \n '<STR_LIT>' : self . mpList , \n '<STR_LIT>' : self . chatroomList , \n '<STR_LIT>' : self . lastInputUserName , } \n def loads ( self , j ) : \n self . userName = j . get ( '<STR_LIT>' , None ) \n self . nickName = j . get ( '<STR_LIT>' , None ) \n del self . memberList [ : ] \n for i in j . get ( '<STR_LIT>' , [ ] ) : \n self . memberList . append ( i ) \n del self . mpList [ : ] \n for i in j . get ( '<STR_LIT>' , [ ] ) : \n self . mpList . append ( i ) \n del self . chatroomList [ : ] \n for i in j . get ( '<STR_LIT>' , [ ] ) : \n self . chatroomList . append ( i ) \n for chatroom in self . chatroomList : \n if '<STR_LIT>' in chatroom : \n for member in chatroom [ '<STR_LIT>' ] : \n member . core = chatroom . core \n member . chatroom = chatroom \n if '<STR_LIT>' in chatroom : \n chatroom [ '<STR_LIT>' ] . core = chatroom . core \n chatroom [ '<STR_LIT>' ] . chatroom = chatroom \n self . lastInputUserName = j . get ( '<STR_LIT>' , None ) \n def search_friends ( self , name = None , userName = None , remarkName = None , nickName = None , \n wechatAccount = None ) : \n with self . updateLock : \n if ( name or userName or remarkName or nickName or wechatAccount ) is None : \n return copy . deepcopy ( self . memberList [ <NUM_LIT> ] ) \n elif userName : \n for m in self . memberList : \n if m [ '<STR_LIT>' ] == userName : \n return copy . deepcopy ( m ) \n else :", "output": "matchDict = { \n '<STR_LIT>' : remarkName , \n '<STR_LIT>' : nickName , \n '<STR_LIT>' : wechatAccount , } \n for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n if matchDict [ k ] is None : \n del matchDict [ k ] \n if name : \n contact = [ ] \n for m in self . memberList : \n if any ( [ m . get ( k ) == name for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : \n contact . append ( m ) \n else : \n contact = self . memberList [ : ] \n if matchDict : \n friendList = [ ] \n for m in contact : \n if all ( [ m . get ( k ) == v for k , v in matchDict . items ( ) ] ) : \n friendList . append ( m ) \n return copy . deepcopy ( friendList ) \n else : \n return copy . deepcopy ( contact ) \n def search_chatrooms ( self , name = None , userName = None ) : \n with self . updateLock : \n if userName is not None : \n for m in self . chatroomList : \n if m [ '<STR_LIT>' ] == userName : \n return copy . deepcopy ( m ) \n elif name is not None : \n matchList = [ ] \n for m in self . chatroomList : \n if name in m [ '<STR_LIT>' ] : \n matchList . append ( copy . deepcopy ( m ) ) \n return matchList \n def search_mps ( self , name = None , userName = None ) : \n with self . updateLock : \n if userName is not None : \n for m in self . mpList : \n if m [ '<STR_LIT>' ] == userName : \n return copy . deepcopy ( m ) \n elif name is not None : \n matchList = [ ] \n for m in self . mpList : \n if name in m [ '<STR_LIT>' ] : \n matchList . append ( copy . deepcopy ( m ) ) \n return matchList"}, {"input": "import logging \n import sys \n def _reset_logger ( log ) : \n for handler in log . handlers : \n handler . close ( ) \n log . removeHandler ( handler ) \n del handler \n log . handlers . clear ( ) \n log . propagate = False \n console_handle = logging . StreamHandler ( sys . stdout ) \n console_handle . setFormatter ( \n logging . Formatter ( \n \"<STR_LIT>\" , \n datefmt = \"<STR_LIT>\" , \n ) \n ) \n file_handle = logging . FileHandler ( \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) \n file_handle . setFormatter ( \n logging . Formatter ( \n \"<STR_LIT>\" , \n datefmt = \"<STR_LIT>\" , \n ) \n ) \n log . addHandler ( file_handle )", "output": "log . addHandler ( console_handle ) \n def _get_logger ( ) : \n log = logging . getLogger ( \"<STR_LIT>\" ) \n _reset_logger ( log ) \n log . setLevel ( logging . INFO ) \n return log \n logger = _get_logger ( )"}, {"input": "import os \n import asyncio \n import typer \n from rich import print \n from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository \n from profyle . application . trace . delete import delete_all_traces \n from profyle . application . trace . vacuum import vacuum \n from profyle . infrastructure . sqlite3 . get_connection import get_connection \n from profyle . infrastructure . http_server import start_server \n from profyle . settings import settings \n app = typer . Typer ( ) \n @ app . command ( ) \n def start ( port : int = <NUM_LIT> , host : str = \"<STR_LIT>\" ) : \n asyncio . run ( start_server ( port = port , host = host ) ) \n @ app . command ( ) \n def clean ( ) : \n db = get_connection ( ) \n sqlite_repo = SQLiteTraceRepository ( db ) \n removed_traces = delete_all_traces ( sqlite_repo ) \n vacuum ( sqlite_repo ) \n print ( f\"<STR_LIT>\" ) \n @ app . command ( ) \n def check ( ) : \n db_size_in_bytes = os . path . getsize ( settings . get_path ( \"<STR_LIT>\" ) ) \n db_size_in_megabytes = round ( db_size_in_bytes / <NUM_LIT> ** <NUM_LIT> , <NUM_LIT> ) \n db_size_in_gigabytes = round ( db_size_in_megabytes / <NUM_LIT> ** <NUM_LIT> , <NUM_LIT> ) \n if db_size_in_megabytes > <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n return", "output": "print ( f\"<STR_LIT>\" )"}, {"input": "from sqlalchemy import func \n from flask_wtf import FlaskForm \n from wtforms import StringField , PasswordField , SubmitField \n from wtforms . validators import Length , EqualTo , DataRequired , Email , ValidationError \n from app . models . user import User \n class RegistrationForm ( FlaskForm ) : \n username = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) \n email = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Email ( ) ] ) \n password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> ) ] ) \n confirm_password = PasswordField ( '<STR_LIT>' , validators = [ \n DataRequired ( ) , EqualTo ( '<STR_LIT>' , message = '<STR_LIT>' ) \n ] ) \n submit = SubmitField ( '<STR_LIT>' ) \n def validate_username ( self , username : StringField ) -> None : \n user = User . query . filter ( func . lower ( User . username ) == func . lower ( username . data ) ) . first ( ) \n if user : \n raise ValidationError ( '<STR_LIT>' ) \n def validate_email ( self , email : StringField ) -> None :", "output": "user = User . query . filter ( func . lower ( User . email ) == func . lower ( email . data ) ) . first ( ) \n if user : \n raise ValidationError ( '<STR_LIT>' )"}, {"input": "SIGNS = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : {", "output": "\"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n } \n def get_id_from_sign ( sign : str ) -> int : \n if not sign : \n return <NUM_LIT> , SIGNS [ '<STR_LIT>' ] \n sign = sign . lower ( ) \n if sign in SIGNS : \n index = SIGNS [ sign ] [ '<STR_LIT>' ] \n return index , SIGNS [ sign ] \n raise ValueError ( \"<STR_LIT>\" )"}, {"input": "from enum import Enum \n from . base import db \n class UserRole ( Enum ) : \n NORMAL_USER = <NUM_LIT> \n ADMIN = <NUM_LIT> \n class ChannelAllowList ( db . Model ) : \n __tablename__ = '<STR_LIT>' \n id = db . Column ( db . Integer , primary_key = True )", "output": "user_role = db . Column ( db . Integer , db . Enum ( UserRole ) , nullable = False , default = UserRole . NORMAL_USER . value ) \n channel_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) \n user_id = db . Column ( db . Integer , db . ForeignKey ( '<STR_LIT>' ) , nullable = False ) \n member = db . relationship ( '<STR_LIT>' , lazy = True ) \n def __repr__ ( self ) -> str : \n return ( \"<STR_LIT>\" + \n f\"<STR_LIT>\" + \n \"<STR_LIT>\" )"}, {"input": "import sys \n from isaacgym import gymapi \n from isaacgym import gymutil , gymtorch \n import numpy as np \n import torch \n import time \n class BaseTask ( ) : \n def __init__ ( self , cfg , sim_params , physics_engine , sim_device , headless ) : \n self . gym = gymapi . acquire_gym ( ) \n self . sim_params = sim_params \n self . physics_engine = physics_engine \n self . sim_device = sim_device \n sim_device_type , self . sim_device_id = gymutil . parse_device_str ( self . sim_device ) \n self . headless = headless \n if sim_device_type == '<STR_LIT>' and sim_params . use_gpu_pipeline : \n self . device = self . sim_device \n else : \n self . device = '<STR_LIT>' \n self . graphics_device_id = self . sim_device_id \n if self . headless == True : \n self . graphics_device_id = - <NUM_LIT> \n self . num_envs = cfg . env . num_envs \n self . num_obs = cfg . env . num_observations \n self . num_privileged_obs = cfg . env . num_privileged_obs \n self . num_actions = cfg . env . num_actions \n torch . _C . _jit_set_profiling_mode ( False ) \n torch . _C . _jit_set_profiling_executor ( False ) \n self . obs_buf = torch . zeros ( self . num_envs , self . num_obs , device = self . device , dtype = torch . float ) \n self . rew_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . float ) \n self . reset_buf = torch . ones ( self . num_envs , device = self . device , dtype = torch . long ) \n self . episode_length_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . long ) \n self . time_out_buf = torch . zeros ( self . num_envs , device = self . device , dtype = torch . bool ) \n if self . num_privileged_obs is not None : \n self . privileged_obs_buf = torch . zeros ( self . num_envs , self . num_privileged_obs , device = self . device , dtype = torch . float ) \n else : \n self . privileged_obs_buf = None \n self . extras = { } \n self . create_sim ( ) \n self . gym . prepare_sim ( self . sim ) \n self . enable_viewer_sync = True \n self . viewer = None \n if self . headless == False : \n self . viewer = self . gym . create_viewer ( \n self . sim , gymapi . CameraProperties ( ) ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_ESCAPE , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_V , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_F , \"<STR_LIT>\" ) \n for i in range ( <NUM_LIT> ) : \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , getattr ( gymapi , \"<STR_LIT>\" + str ( i ) ) , \"<STR_LIT>\" + str ( i ) ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_LEFT_BRACKET , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_RIGHT_BRACKET , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_SPACE , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_W , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_S , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_A , \"<STR_LIT>\" ) \n self . gym . subscribe_viewer_keyboard_event ( \n self . viewer , gymapi . KEY_D , \"<STR_LIT>\" ) \n self . free_cam = False \n self . lookat_id = <NUM_LIT> \n self . lookat_vec = torch . tensor ( [ - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , requires_grad = False , device = self . device ) \n def get_observations ( self ) : \n return self . obs_buf \n def get_privileged_observations ( self ) : \n return self . privileged_obs_buf \n def reset_idx ( self , env_ids ) :", "output": "raise NotImplementedError \n def reset ( self ) : \n self . reset_idx ( torch . arange ( self . num_envs , device = self . device ) ) \n obs , privileged_obs , _ , _ , _ = self . step ( torch . zeros ( self . num_envs , self . num_actions , device = self . device , requires_grad = False ) ) \n return obs , privileged_obs \n def step ( self , actions ) : \n raise NotImplementedError \n def lookat ( self , i ) : \n look_at_pos = self . root_states [ i , : <NUM_LIT> ] . clone ( ) \n cam_pos = look_at_pos + self . lookat_vec \n self . set_camera ( cam_pos , look_at_pos ) \n def render ( self , sync_frame_time = True ) : \n if self . viewer : \n if self . gym . query_viewer_has_closed ( self . viewer ) : \n sys . exit ( ) \n if not self . free_cam : \n self . lookat ( self . lookat_id ) \n for evt in self . gym . query_viewer_action_events ( self . viewer ) : \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n sys . exit ( ) \n elif evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . enable_viewer_sync = not self . enable_viewer_sync \n if not self . free_cam : \n for i in range ( <NUM_LIT> ) : \n if evt . action == \"<STR_LIT>\" + str ( i ) and evt . value > <NUM_LIT> : \n self . lookat ( i ) \n self . lookat_id = i \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . lookat_id = ( self . lookat_id - <NUM_LIT> ) % self . num_envs \n self . lookat ( self . lookat_id ) \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . lookat_id = ( self . lookat_id + <NUM_LIT> ) % self . num_envs \n self . lookat ( self . lookat_id ) \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . commands [ self . lookat_id , <NUM_LIT> ] += <NUM_LIT> \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . commands [ self . lookat_id , <NUM_LIT> ] -= <NUM_LIT> \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . commands [ self . lookat_id , <NUM_LIT> ] += <NUM_LIT> \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . commands [ self . lookat_id , <NUM_LIT> ] -= <NUM_LIT> \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . free_cam = not self . free_cam \n if self . free_cam : \n self . set_camera ( self . cfg . viewer . pos , self . cfg . viewer . lookat ) \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . pause = True \n while self . pause : \n time . sleep ( <NUM_LIT> ) \n self . gym . draw_viewer ( self . viewer , self . sim , True ) \n for evt in self . gym . query_viewer_action_events ( self . viewer ) : \n if evt . action == \"<STR_LIT>\" and evt . value > <NUM_LIT> : \n self . pause = False \n if self . gym . query_viewer_has_closed ( self . viewer ) : \n sys . exit ( ) \n if self . device != '<STR_LIT>' : \n self . gym . fetch_results ( self . sim , True ) \n self . gym . poll_viewer_events ( self . viewer ) \n if self . enable_viewer_sync : \n self . gym . step_graphics ( self . sim ) \n self . gym . draw_viewer ( self . viewer , self . sim , True ) \n if sync_frame_time : \n self . gym . sync_frame_time ( self . sim ) \n else : \n self . gym . poll_viewer_events ( self . viewer ) \n if not self . free_cam : \n p = self . gym . get_viewer_camera_transform ( self . viewer , None ) . p \n cam_trans = torch . tensor ( [ p . x , p . y , p . z ] , requires_grad = False , device = self . device ) \n look_at_pos = self . root_states [ self . lookat_id , : <NUM_LIT> ] . clone ( ) \n self . lookat_vec = cam_trans - look_at_pos"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>'", "output": "branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , server_default = sa . text ( \"<STR_LIT>\" ) , nullable = False ) )"}, {"input": "import json \n import pytest \n from flask import Flask \n from flask . testing import FlaskClient \n from app . controllers . horoscope_ctr_post import horoscope_blueprint_post \n from app . services . horoscope_service import HoroscopeService \n from app . utils . status_code import Status \n @ pytest . fixture \n def app ( ) : \n app = Flask ( __name__ ) \n app . register_blueprint ( horoscope_blueprint_post ) \n return app \n @ pytest . fixture \n def client ( app ) : \n return app . test_client ( ) \n def test_post_horoscope ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_OK \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_post_horoscope_when_date_is_future ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_post_horoscope_when_date_is_past ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_OK \n def test_post_horoscope_when_date_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_post_horoscope_when_lang_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_post_horoscope_when_sign_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_post_horoscope_when_sign_is_empty ( client ) : \n data = {", "output": "'<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_OK \n def test_post_horoscope_when_sign_is_none ( client ) : \n data = { \n '<STR_LIT>' : None , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . post ( '<STR_LIT>' , json = data ) \n assert response . status_code == Status . HTTP_OK"}, {"input": "from app . utils . translate import text_translate \n import pytest \n def test_text_translate_valid_country_code ( ) : \n content = \"<STR_LIT>\" \n target_lang = \"<STR_LIT>\" \n translated_text = text_translate ( content , target_lang ) \n assert isinstance ( translated_text , str ) \n assert len ( translated_text ) > <NUM_LIT> \n def test_text_translate_invalid_country_code ( ) : \n content = \"<STR_LIT>\" \n target_lang = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) : \n text_translate ( content , target_lang ) \n def test_text_translate_empty_content ( ) : \n content = \"<STR_LIT>\" \n target_lang = \"<STR_LIT>\" \n translated_text = text_translate ( content , target_lang ) \n assert isinstance ( translated_text , str ) \n assert len ( translated_text ) == <NUM_LIT> \n def test_text_translate_valid_input ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n translated_text = text_translate ( content , target_lang ) \n assert isinstance ( translated_text , str )", "output": "assert translated_text != \"<STR_LIT>\" \n def test_text_translate_empty_input ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n translated_text = text_translate ( content , target_lang ) \n assert isinstance ( translated_text , str ) \n assert translated_text == \"<STR_LIT>\" \n def test_text_translate_long_text ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n with pytest . raises ( ValueError ) as e : \n text_translate ( content , target_lang ) \n assert \"<STR_LIT>\" in str ( e . value ) \n def test_text_translate_invalid_country_code ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n with pytest . raises ( ValueError ) as e : \n text_translate ( content , target_lang ) \n assert \"<STR_LIT>\" in str ( e . value ) \n def test_text_translate_short_country_code ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n with pytest . raises ( ValueError ) as e : \n text_translate ( content , target_lang ) \n assert \"<STR_LIT>\" in str ( e . value ) \n def test_text_translate_long_country_code ( ) : \n content = \"<STR_LIT>\" \n target_lang = '<STR_LIT>' \n with pytest . raises ( ValueError ) as e : \n text_translate ( content , target_lang ) \n assert \"<STR_LIT>\" in str ( e . value )"}, {"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB \n from mephisto . tools . examine_utils import run_examine_or_review , print_results \n from mephisto . data_model . worker import Worker \n from mephisto . data_model . unit import Unit \n db = None \n def format_for_printing_data ( data ) : \n global db \n worker_name = Worker . get ( db , data [ \"<STR_LIT>\" ] ) . worker_name \n contents = data [ \"<STR_LIT>\" ] \n duration = data [ \"<STR_LIT>\" ] - data [ \"<STR_LIT>\" ] \n metadata_string = ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n outputs = contents [ \"<STR_LIT>\" ] \n output_string = f\"<STR_LIT>\" \n return f\"<STR_LIT>\" \n def main ( ) : \n global db \n db = LocalMephistoDB ( ) \n run_examine_or_review ( db , format_for_printing_data ) \n if __name__ == \"<STR_LIT>\" :", "output": "main ( )"}, {"input": "from typing import Generator \n import flask \n import svcs \n def factory_with_cleanup ( ) -> Generator [ int , None , None ] : \n yield <NUM_LIT> \n reg = svcs . Registry ( )", "output": "app = flask . Flask ( \"<STR_LIT>\" ) \n app = svcs . flask . init_app ( app , registry = reg ) \n app = svcs . flask . init_app ( app ) \n reg = svcs . flask . get_registry ( app ) \n svcs . flask . register_value ( app , int , <NUM_LIT> ) \n svcs . flask . register_value ( app , int , <NUM_LIT> , ping = lambda : None ) \n svcs . flask . register_factory ( app , str , str ) \n svcs . flask . register_factory ( app , int , factory_with_cleanup ) \n svcs . flask . register_value ( app , str , str , ping = lambda : None ) \n o1 : object = svcs . flask . get ( object ) \n a : int \n b : str \n c : bool \n d : tuple \n e : object \n f : float \n g : list \n h : dict \n i : set \n j : bytes \n a , b , c , d , e , f , g , h , i , j = svcs . flask . get ( \n int , str , bool , tuple , object , float , list , dict , set , bytes \n ) \n svcs . flask . close_registry ( app ) \n con : svcs . Container = svcs . flask . svcs_from ( ) \n con = svcs . flask . svcs_from ( flask . g ) \n class CustomApp ( flask . Flask ) : \n pass \n app = svcs . flask . init_app ( CustomApp ( \"<STR_LIT>\" ) ) \n reg = svcs . flask . get_registry ( CustomApp ( \"<STR_LIT>\" ) ) \n local_p : svcs . Container = svcs . flask . container \n local_r : svcs . Registry = svcs . flask . registry"}, {"input": "import os \n import time \n import re \n import io \n import threading \n import json \n import xml . dom . minidom \n import random \n import traceback \n import logging \n try : \n from httplib import BadStatusLine \n except ImportError : \n from http . client import BadStatusLine \n import requests \n from pyqrcode import QRCode \n from . . import config , utils", "output": "from . . returnvalues import ReturnValue \n from . . storage . templates import wrap_user_dict \n from . contact import update_local_chatrooms , update_local_friends \n from . messages import produce_msg \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_login ( core ) : \n core . login = login \n core . get_QRuuid = get_QRuuid \n core . get_QR = get_QR \n core . check_login = check_login \n core . web_init = web_init \n core . show_mobile_login = show_mobile_login \n core . start_receiving = start_receiving \n core . get_msg = get_msg \n core . logout = logout \n def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , \n loginCallback = None , exitCallback = None ) : \n if self . alive or self . isLogging : \n logger . warning ( '<STR_LIT>' ) \n return \n self . isLogging = True \n logger . info ( '<STR_LIT>' ) \n while self . isLogging : \n uuid = push_login ( self ) \n if uuid : \n qrStorage = io . BytesIO ( ) \n else : \n logger . info ( '<STR_LIT>' ) \n while not self . get_QRuuid ( ) : \n time . sleep ( <NUM_LIT> ) \n logger . info ( '<STR_LIT>' ) \n qrStorage = self . get_QR ( enableCmdQR = enableCmdQR , \n picDir = picDir , qrCallback = qrCallback ) \n isLoggedIn = False \n while not isLoggedIn : \n status = self . check_login ( ) \n if hasattr ( qrCallback , '<STR_LIT>' ) : \n qrCallback ( uuid = self . uuid , status = status , \n qrcode = qrStorage . getvalue ( ) ) \n if status == '<STR_LIT>' : \n isLoggedIn = True \n elif status == '<STR_LIT>' : \n if isLoggedIn is not None : \n logger . info ( '<STR_LIT>' ) \n isLoggedIn = None \n time . sleep ( <NUM_LIT> ) \n time . sleep ( <NUM_LIT> ) \n elif status != '<STR_LIT>' : \n break \n if isLoggedIn : \n break \n elif self . isLogging : \n logger . info ( '<STR_LIT>' ) \n else : \n return \n logger . info ( '<STR_LIT>' ) \n self . web_init ( ) \n self . show_mobile_login ( ) \n self . get_contact ( True ) \n if hasattr ( loginCallback , '<STR_LIT>' ) : \n r = loginCallback ( ) \n else : \n if os . path . exists ( picDir or config . DEFAULT_QR ) : \n os . remove ( picDir or config . DEFAULT_QR ) \n logger . info ( '<STR_LIT>' % self . storageClass . nickName ) \n self . start_receiving ( exitCallback ) \n self . isLogging = False \n def push_login ( core ) : \n cookiesDict = core . s . cookies . get_dict ( ) \n if '<STR_LIT>' in cookiesDict : \n url = '<STR_LIT>' % ( \n config . BASE_URL , cookiesDict [ '<STR_LIT>' ] ) \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = core . s . get ( url , headers = headers ) . json ( ) \n if '<STR_LIT>' in r and r . get ( '<STR_LIT>' ) in ( <NUM_LIT> , '<STR_LIT>' ) : \n core . uuid = r [ '<STR_LIT>' ] \n return r [ '<STR_LIT>' ] \n return False \n def get_QRuuid ( self ) : \n url = '<STR_LIT>' % config . BASE_URL \n params = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . get ( url , params = params , headers = headers ) \n regx = r'<STR_LIT>' \n data = re . search ( regx , r . text ) \n if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : \n self . uuid = data . group ( <NUM_LIT> ) \n return self . uuid \n def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : \n uuid = uuid or self . uuid \n picDir = picDir or config . DEFAULT_QR \n qrStorage = io . BytesIO ( ) \n qrCode = QRCode ( '<STR_LIT>' + uuid ) \n qrCode . png ( qrStorage , scale = <NUM_LIT> ) \n if hasattr ( qrCallback , '<STR_LIT>' ) : \n qrCallback ( uuid = uuid , status = '<STR_LIT>' , qrcode = qrStorage . getvalue ( ) ) \n else : \n with open ( picDir , '<STR_LIT>' ) as f : \n f . write ( qrStorage . getvalue ( ) ) \n if enableCmdQR : \n utils . print_cmd_qr ( qrCode . text ( <NUM_LIT> ) , enableCmdQR = enableCmdQR ) \n else : \n utils . print_qr ( picDir ) \n return qrStorage \n def check_login ( self , uuid = None ) : \n uuid = uuid or self . uuid \n url = '<STR_LIT>' % config . BASE_URL \n localTime = int ( time . time ( ) ) \n params = '<STR_LIT>' % ( \n uuid , int ( - localTime / <NUM_LIT> ) , localTime ) \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . get ( url , params = params , headers = headers ) \n regx = r'<STR_LIT>' \n data = re . search ( regx , r . text ) \n if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : \n if process_login_info ( self , r . text ) : \n return '<STR_LIT>' \n else : \n return '<STR_LIT>' \n elif data : \n return data . group ( <NUM_LIT> ) \n else : \n return '<STR_LIT>' \n def process_login_info ( core , loginContent ) : \n regx = r'<STR_LIT>' \n core . loginInfo [ '<STR_LIT>' ] = re . search ( regx , loginContent ) . group ( <NUM_LIT> ) \n headers = { '<STR_LIT>' : config . USER_AGENT , \n '<STR_LIT>' : config . UOS_PATCH_CLIENT_VERSION , \n '<STR_LIT>' : config . UOS_PATCH_EXTSPAM , \n '<STR_LIT>' : '<STR_LIT>' \n } \n r = core . s . get ( core . loginInfo [ '<STR_LIT>' ] , \n headers = headers , allow_redirects = False ) \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ : core . loginInfo [ '<STR_LIT>' ] . rfind ( \n '<STR_LIT>' ) ] \n for indexUrl , detailedUrl in ( \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) ) : \n fileUrl , syncUrl = [ '<STR_LIT>' % \n url for url in detailedUrl ] \n if indexUrl in core . loginInfo [ '<STR_LIT>' ] : \n core . loginInfo [ '<STR_LIT>' ] , core . loginInfo [ '<STR_LIT>' ] = fileUrl , syncUrl \n break \n else : \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] \n core . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] \n core . loginInfo [ '<STR_LIT>' ] = int ( time . time ( ) * <NUM_LIT> ) \n core . loginInfo [ '<STR_LIT>' ] = { } \n cookies = core . s . cookies . get_dict ( ) \n res = re . findall ( '<STR_LIT>' , r . text , re . S ) \n skey = res [ <NUM_LIT> ] if res else None \n res = re . findall ( \n '<STR_LIT>' , r . text , re . S ) \n pass_ticket = res [ <NUM_LIT> ] if res else None \n if skey is not None : \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = skey \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ \"<STR_LIT>\" ] \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ \"<STR_LIT>\" ] \n if pass_ticket is not None : \n core . loginInfo [ '<STR_LIT>' ] = pass_ticket \n if not all ( [ key in core . loginInfo for key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : \n logger . error ( \n '<STR_LIT>' % r . text ) \n core . isLogging = False \n return False \n return True \n def web_init ( self ) : \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n params = { \n '<STR_LIT>' : int ( - time . time ( ) / <NUM_LIT> ) , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n data = { '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , } \n r = self . s . post ( url , params = params , data = json . dumps ( data ) , headers = headers ) \n dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n utils . emoji_formatter ( dic [ '<STR_LIT>' ] , '<STR_LIT>' ) \n self . loginInfo [ '<STR_LIT>' ] = int ( dic [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] = wrap_user_dict ( \n utils . struct_friend_info ( dic [ '<STR_LIT>' ] ) ) \n self . memberList . append ( self . loginInfo [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) \n for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) \n self . storageClass . userName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n self . storageClass . nickName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n contactList = dic . get ( '<STR_LIT>' , [ ] ) \n chatroomList , otherList = [ ] , [ ] \n for m in contactList : \n if m [ '<STR_LIT>' ] != <NUM_LIT> : \n otherList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n m [ '<STR_LIT>' ] = [ ] \n chatroomList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n otherList . append ( m ) \n if chatroomList : \n update_local_chatrooms ( self , chatroomList ) \n if otherList : \n update_local_friends ( self , otherList ) \n return dic \n def show_mobile_login ( self ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : self . storageClass . userName , \n '<STR_LIT>' : self . storageClass . userName , \n '<STR_LIT>' : int ( time . time ( ) ) , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , } \n r = self . s . post ( url , data = json . dumps ( data ) , headers = headers ) \n return ReturnValue ( rawResponse = r ) \n def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : \n self . alive = True \n def maintain_loop ( ) : \n retryCount = <NUM_LIT> \n while self . alive : \n try : \n i = sync_check ( self ) \n if i is None : \n self . alive = False \n elif i == '<STR_LIT>' : \n pass \n else : \n msgList , contactList = self . get_msg ( ) \n if msgList : \n msgList = produce_msg ( self , msgList ) \n for msg in msgList : \n self . msgList . put ( msg ) \n if contactList : \n chatroomList , otherList = [ ] , [ ] \n for contact in contactList : \n if '<STR_LIT>' in contact [ '<STR_LIT>' ] : \n chatroomList . append ( contact ) \n else : \n otherList . append ( contact ) \n chatroomMsg = update_local_chatrooms ( \n self , chatroomList ) \n chatroomMsg [ '<STR_LIT>' ] = self . loginInfo [ '<STR_LIT>' ] \n self . msgList . put ( chatroomMsg ) \n update_local_friends ( self , otherList ) \n retryCount = <NUM_LIT> \n except requests . exceptions . ReadTimeout : \n pass \n except : \n retryCount += <NUM_LIT> \n logger . error ( traceback . format_exc ( ) ) \n if self . receivingRetryCount < retryCount : \n logger . error ( \"<STR_LIT>\" % ( \n retryCount ) + \"<STR_LIT>\" ) \n self . alive = False \n else : \n time . sleep ( <NUM_LIT> ) \n self . logout ( ) \n if hasattr ( exitCallback , '<STR_LIT>' ) : \n exitCallback ( ) \n else : \n logger . info ( '<STR_LIT>' ) \n if getReceivingFnOnly : \n return maintain_loop \n else : \n maintainThread = threading . Thread ( target = maintain_loop ) \n maintainThread . setDaemon ( True ) \n maintainThread . start ( ) \n def sync_check ( self ) : \n url = '<STR_LIT>' % self . loginInfo . get ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) \n params = { \n '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n self . loginInfo [ '<STR_LIT>' ] += <NUM_LIT> \n try : \n r = self . s . get ( url , params = params , headers = headers , \n timeout = config . TIMEOUT ) \n except requests . exceptions . ConnectionError as e : \n try : \n if not isinstance ( e . args [ <NUM_LIT> ] . args [ <NUM_LIT> ] , BadStatusLine ) : \n raise \n return '<STR_LIT>' \n except : \n raise \n r . raise_for_status ( ) \n regx = r'<STR_LIT>' \n pm = re . search ( regx , r . text ) \n if pm is None or pm . group ( <NUM_LIT> ) != '<STR_LIT>' : \n logger . error ( '<STR_LIT>' % r . text ) \n return None \n return pm . group ( <NUM_LIT> ) \n def get_msg ( self ) : \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] , \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : ~ int ( time . time ( ) ) , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , data = json . dumps ( data ) , \n headers = headers , timeout = config . TIMEOUT ) \n dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] != <NUM_LIT> : \n return None , None \n self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) \n for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) \n return dic [ '<STR_LIT>' ] , dic [ '<STR_LIT>' ] \n def logout ( self ) : \n if self . alive : \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n params = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n self . s . get ( url , params = params , headers = headers ) \n self . alive = False \n self . isLogging = False \n self . s . cookies . clear ( ) \n del self . chatroomList [ : ] \n del self . memberList [ : ] \n del self . mpList [ : ] \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , } } )"}, {"input": "from typing import Optional \n from profyle . application . profyle import profyle \n from profyle . domain . trace_repository import TraceRepository \n from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository \n class ProfyleMiddleware : \n def __init__ ( \n self , \n app , \n enabled : bool = True , \n pattern : Optional [ str ] = None , \n max_stack_depth : int = - <NUM_LIT> , \n min_duration : int = <NUM_LIT> , \n trace_repo : TraceRepository = SQLiteTraceRepository ( ) \n ) : \n self . app = app \n self . enabled = enabled \n self . pattern = pattern \n self . max_stack_depth = max_stack_depth \n self . min_duration = min_duration \n self . trace_repo = trace_repo", "output": "def __call__ ( self , environ , start_response ) : \n if environ . get ( \"<STR_LIT>\" ) == \"<STR_LIT>\" and self . enabled : \n method = environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . upper ( ) \n path = environ . get ( \"<STR_LIT>\" ) \n with profyle ( \n name = f\"<STR_LIT>\" , \n pattern = self . pattern , \n max_stack_depth = self . max_stack_depth , \n min_duration = self . min_duration , \n repo = self . trace_repo \n ) : \n return self . app ( environ , start_response ) \n return self . app ( environ , start_response )"}, {"input": "from common import const \n def create_channel ( channel_type ) : \n if channel_type == const . TERMINAL : \n from channel . terminal . terminal_channel import TerminalChannel \n return TerminalChannel ( ) \n if channel_type == const . WECHAT : \n from channel . wechat . wechat_channel import WechatChannel \n return WechatChannel ( ) \n elif channel_type == const . WECHAT_MP : \n from channel . wechat . wechat_mp_channel import WechatSubsribeAccount \n return WechatSubsribeAccount ( ) \n elif channel_type == const . WECHAT_MP_SERVICE : \n from channel . wechat . wechat_mp_service_channel import WechatServiceAccount \n return WechatServiceAccount ( ) \n elif channel_type == const . WECHAT_COM : \n from channel . wechat . wechat_com_channel import WechatEnterpriseChannel \n return WechatEnterpriseChannel ( ) \n elif channel_type == const . QQ : \n from channel . qq . qq_channel import QQChannel \n return QQChannel ( ) \n elif channel_type == const . GMAIL : \n from channel . gmail . gmail_channel import GmailChannel \n return GmailChannel ( ) \n elif channel_type == const . TELEGRAM : \n from channel . telegram . telegram_channel import TelegramChannel \n return TelegramChannel ( ) \n elif channel_type == const . SLACK : \n from channel . slack . slack_channel import SlackChannel \n return SlackChannel ( ) \n elif channel_type == const . HTTP : \n from channel . http . http_channel import HttpChannel \n return HttpChannel ( ) \n elif channel_type == const . DINGTALK : \n from channel . dingtalk . dingtalk_channel import DingTalkChannel \n return DingTalkChannel ( ) \n elif channel_type == const . FEISHU : \n from channel . feishu . feishu_channel import FeiShuChannel \n return FeiShuChannel ( ) \n elif channel_type == const . DISCORD : \n from channel . discord . discord_channel import DiscordChannel \n return DiscordChannel ( ) \n else :", "output": "raise RuntimeError ( \"<STR_LIT>\" + channel_type )"}, {"input": "import submitit \n import os \n from dataclasses import dataclass , field \n import hydra \n import math \n from hydra . core . config_store import ConfigStoreWithProvider \n from peft import get_peft_model , LoraConfig \n from transformers import CLIPProcessor , CLIPModel \n from transformers import TrainingArguments \n from densely_captioned_images . repro . train . trainer import compute_metrics , ClipAndNegTrainer \n from densely_captioned_images . dataset . utils import print_trainable_parameters \n from densely_captioned_images . dataset . impl import get_clip_ready_ds \n from densely_captioned_images . repro . train . coco_wrap import COCODataset , get_dataset_source as get_coco_dataset_source \n from densely_captioned_images . repro . train . localized_narratives_wrap import COCOLocalizedNarrativesDataset , get_dataset_source as get_loc_nar_dataset_source \n from densely_captioned_images . repro . config import MODEL_PATH \n from typing import Any \n HYDRA_CONFIG_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) \n @ dataclass \n class CLIPAndNegConfig ( ) : \n lora_r : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n lora_alpha : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n lora_dropout : float = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n use_base_images : bool = field ( \n default = True , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n use_subcaptions : bool = field ( \n default = True , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n caption_negative_source : str = field ( \n default = '<STR_LIT>' , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n caption_negative_strategy : str = field ( \n default = '<STR_LIT>' , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n train_count : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n valid_count : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n lr : float = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n bs : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n loss_alpha : float = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n loss_beta : float = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n caption_selection : str = field ( \n default = '<STR_LIT>' , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n sampler : str = field ( \n default = '<STR_LIT>' , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n loss_pool_type : str = field ( \n default = '<STR_LIT>' , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n datasource : str = field ( \n default = \"<STR_LIT>\" , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n epochs : int = field ( \n default = <NUM_LIT> , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n config = ConfigStoreWithProvider ( \"<STR_LIT>\" ) \n config . store ( name = \"<STR_LIT>\" , node = CLIPAndNegConfig ) \n def get_dir_name ( cfg ) : \n base = \"<STR_LIT>\" \n if cfg . datasource == '<STR_LIT>' : \n base = \"<STR_LIT>\" \n if cfg . datasource == '<STR_LIT>' : \n base = \"<STR_LIT>\" \n base += ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n ) \n if cfg . caption_selection . startswith ( '<STR_LIT>' ) and cfg . caption_selection != '<STR_LIT>' : \n base += f\"<STR_LIT>\" \n if cfg . use_subcaptions is False : \n base += f\"<STR_LIT>\" \n return base \n class CLIPTrainJob : \n def __call__ ( self , * args , ** kwargs ) : \n run_train_clip ( args [ <NUM_LIT> ] ) \n def checkpoint ( self , * args : Any , ** kwargs : Any ) -> submitit . helpers . DelayedSubmission : \n return submitit . helpers . DelayedSubmission ( self , * args , ** kwargs ) \n @ hydra . main ( \n config_path = HYDRA_CONFIG_PATH , config_name = \"<STR_LIT>\" , version_base = \"<STR_LIT>\" \n ) \n def run_train_clip ( cfg : CLIPAndNegConfig ) : \n print ( '<STR_LIT>' ) \n model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n print_trainable_parameters ( model ) \n print ( \"<STR_LIT>\" ) \n l_config = LoraConfig ( \n r = cfg . lora_r , \n lora_alpha = cfg . lora_alpha , \n target_modules = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] , \n lora_dropout = cfg . lora_dropout , \n bias = \"<STR_LIT>\" , \n ) \n lora_model = get_peft_model ( model , l_config )", "output": "print_trainable_parameters ( lora_model ) \n use_antonyms = cfg . caption_negative_source == '<STR_LIT>' \n caption_count = <NUM_LIT> \n if cfg . caption_selection . startswith ( '<STR_LIT>' ) : \n caption_count = int ( cfg . caption_selection [ <NUM_LIT> : ] ) \n if cfg . datasource == '<STR_LIT>' : \n print ( \"<STR_LIT>\" ) \n train_ds = get_clip_ready_ds ( \n split = '<STR_LIT>' , \n load_base_image = cfg . use_base_images , \n load_subcaptions = cfg . use_subcaptions , \n negative_source = cfg . caption_negative_source , \n negative_strategy = cfg . caption_negative_strategy , \n count = cfg . train_count , \n caption_bag_size = caption_count , \n ) \n eval_ds = get_clip_ready_ds ( \n split = '<STR_LIT>' , \n load_base_image = cfg . use_base_images , \n load_subcaptions = cfg . use_subcaptions , \n negative_source = cfg . caption_negative_source , \n negative_strategy = cfg . caption_negative_strategy , \n count = cfg . valid_count , \n caption_bag_size = caption_count , \n ) \n elif cfg . datasource == '<STR_LIT>' : \n print ( \"<STR_LIT>\" ) \n train_source = get_loc_nar_dataset_source ( '<STR_LIT>' , cfg . train_count , use_antonyms = use_antonyms ) \n valid_source = get_loc_nar_dataset_source ( '<STR_LIT>' , cfg . valid_count , use_antonyms = True ) \n train_ds = COCOLocalizedNarrativesDataset ( train_source ) \n eval_ds = COCOLocalizedNarrativesDataset ( valid_source ) \n elif cfg . datasource == '<STR_LIT>' : \n train_source = get_coco_dataset_source ( '<STR_LIT>' , cfg . train_count , use_antonyms = use_antonyms ) \n valid_source = get_coco_dataset_source ( '<STR_LIT>' , cfg . valid_count , use_antonyms = True ) \n train_ds = COCODataset ( train_source , caption_count ) \n eval_ds = COCODataset ( valid_source , <NUM_LIT> ) \n else : \n raise NotImplementedError ( f'<STR_LIT>' ) \n print ( f\"<STR_LIT>\" ) \n output_dir = os . path . join ( MODEL_PATH , get_dir_name ( cfg ) ) \n epochs = cfg . epochs \n training_args = TrainingArguments ( \n report_to = '<STR_LIT>' , \n output_dir = output_dir , \n learning_rate = cfg . lr , \n num_train_epochs = epochs , \n per_device_train_batch_size = cfg . bs , \n per_device_eval_batch_size = cfg . bs , \n save_total_limit = <NUM_LIT> , \n optim = \"<STR_LIT>\" , \n evaluation_strategy = '<STR_LIT>' , \n eval_steps = <NUM_LIT> / ( <NUM_LIT> * epochs ) , \n save_strategy = '<STR_LIT>' , \n save_steps = <NUM_LIT> / ( <NUM_LIT> * epochs ) , \n logging_steps = <NUM_LIT> , \n logging_dir = os . path . join ( MODEL_PATH , '<STR_LIT>' , get_dir_name ( cfg ) ) , \n remove_unused_columns = False , \n label_names = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n dataloader_drop_last = True , \n do_train = True , \n do_eval = True , \n include_inputs_for_metrics = True , \n load_best_model_at_end = True , \n ) \n trainer = ClipAndNegTrainer ( \n model = lora_model , \n args = training_args , \n train_dataset = train_ds , \n eval_dataset = eval_ds , \n compute_metrics = compute_metrics , \n loss_alpha = cfg . loss_alpha , \n loss_beta = cfg . loss_beta , \n sampler = cfg . sampler , \n loss_pool_type = cfg . loss_pool_type , \n ) \n if os . path . exists ( output_dir ) and len ( os . listdir ( output_dir ) ) > <NUM_LIT> : \n trainer . train ( resume_from_checkpoint = True ) \n else : \n trainer . train ( ) \n from densely_captioned_images . dataset . scripts . run_clip_dense_cap_eval import run_dense_cap_test \n run_dense_cap_test ( lora_model , processor ) \n if __name__ == '<STR_LIT>' : \n run_train_clip ( )"}, {"input": "import torch \n from transformers import Trainer \n import datasets \n from transformers . trainer_utils import seed_worker \n from torch . utils . data import DataLoader , SequentialSampler , RandomSampler \n from densely_captioned_images . dataset . loss import clip_loss , negatives_loss \n from densely_captioned_images . dataset . impl import DenseCaptionBatchSampler \n from typing import Optional \n available_gpus = [ torch . cuda . device ( i ) for i in range ( torch . cuda . device_count ( ) ) ] \n total_gpus = len ( available_gpus ) \n ALPHA = <NUM_LIT> \n BETA = <NUM_LIT> \n def compute_metrics ( eval_pred ) : \n global last_eval_pred \n with torch . no_grad ( ) : \n c_loss = eval_pred . predictions [ <NUM_LIT> ] . mean ( ) \n n_loss = eval_pred . predictions [ <NUM_LIT> ] . mean ( ) \n return { '<STR_LIT>' : c_loss , '<STR_LIT>' : n_loss } \n class ClipAndNegTrainer ( Trainer ) : \n def __init__ ( self , * args , ** kwargs ) : \n self . _la : float = kwargs . pop ( '<STR_LIT>' , ALPHA ) \n self . _lb : float = kwargs . pop ( '<STR_LIT>' , BETA ) \n self . _sampler_choice : str = kwargs . pop ( '<STR_LIT>' , '<STR_LIT>' ) \n self . _loss_pool_type : str = kwargs . pop ( '<STR_LIT>' , '<STR_LIT>' ) \n super ( ) . __init__ ( * args , ** kwargs ) \n def _a ( self ) : \n return self . _la \n def _b ( self ) : \n return self . _lb \n def get_train_dataloader ( self ) -> DataLoader : \n if self . train_dataset is None : \n raise ValueError ( \"<STR_LIT>\" )", "output": "train_dataset = self . train_dataset \n data_collator = self . data_collator \n if isinstance ( train_dataset , datasets . Dataset ) : \n train_dataset = self . _remove_unused_columns ( train_dataset , description = \"<STR_LIT>\" ) \n else : \n data_collator = self . _get_collator_with_removed_columns ( data_collator , description = \"<STR_LIT>\" ) \n dataloader_params = { \n \"<STR_LIT>\" : self . _train_batch_size , \n \"<STR_LIT>\" : data_collator , \n \"<STR_LIT>\" : self . args . dataloader_num_workers , \n \"<STR_LIT>\" : self . args . dataloader_pin_memory , \n } \n if not isinstance ( train_dataset , torch . utils . data . IterableDataset ) : \n sampler = self . _get_train_sampler ( ) \n if isinstance ( sampler , torch . utils . data . BatchSampler ) : \n dataloader_params [ \"<STR_LIT>\" ] = sampler \n del dataloader_params [ '<STR_LIT>' ] \n else : \n dataloader_params [ \"<STR_LIT>\" ] = sampler \n dataloader_params [ \"<STR_LIT>\" ] = self . args . dataloader_drop_last \n dataloader_params [ \"<STR_LIT>\" ] = seed_worker \n print ( dataloader_params ) \n return self . accelerator . prepare ( DataLoader ( train_dataset , ** dataloader_params ) ) \n def _get_train_sampler ( self ) -> Optional [ torch . utils . data . Sampler ] : \n if self . _sampler_choice == '<STR_LIT>' : \n return RandomSampler ( self . train_dataset ) \n elif self . _sampler_choice == '<STR_LIT>' : \n return SequentialSampler ( self . train_dataset ) \n elif self . _sampler_choice == '<STR_LIT>' : \n return DenseCaptionBatchSampler ( self . train_dataset , self . _train_batch_size ) \n else : \n raise NotImplementedError \n def compute_loss ( self , model , inputs , return_outputs = False ) : \n if '<STR_LIT>' in inputs : \n bs , n , t = inputs [ '<STR_LIT>' ] . shape \n unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n pos_inputs = { \n '<STR_LIT>' : unstacked_inputs . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : unstacked_attention . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , \n } \n else : \n pos_inputs = { \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , \n } \n outputs = model ( ** pos_inputs ) \n logits = outputs . logits_per_image \n c_loss = clip_loss ( logits , pool_type = self . _loss_pool_type ) \n if self . _b ( ) == <NUM_LIT> and not return_outputs : \n return self . _a ( ) * c_loss \n if '<STR_LIT>' in inputs : \n bs , n , t = inputs [ '<STR_LIT>' ] . shape \n unstacked_inputs = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n unstacked_attention = inputs [ '<STR_LIT>' ] . reshape ( bs * n , t ) \n neg_inputs = { \n '<STR_LIT>' : unstacked_inputs . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : unstacked_attention . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , \n } \n else : \n neg_inputs = { \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) . repeat ( total_gpus , <NUM_LIT> ) , \n '<STR_LIT>' : torch . squeeze ( inputs [ '<STR_LIT>' ] , axis = <NUM_LIT> ) , \n } \n neg_outputs = model ( ** neg_inputs ) \n n_loss = negatives_loss ( logits , neg_outputs . logits_per_image , pool_type = self . _loss_pool_type ) \n loss = self . _a ( ) * c_loss + self . _b ( ) * n_loss \n outputs [ '<STR_LIT>' ] = c_loss \n outputs [ '<STR_LIT>' ] = n_loss \n return ( loss , outputs ) if return_outputs else loss"}, {"input": "from __future__ import annotations \n from typing import Generator \n import pyramid \n from pyramid . config import Configurator \n import svcs \n def factory_with_cleanup ( ) -> Generator [ int , None , None ] : \n yield <NUM_LIT> \n config = Configurator ( settings = { } ) \n svcs . pyramid . init ( config ) \n svcs . pyramid . register_value ( config , int , <NUM_LIT> ) \n svcs . pyramid . register_value ( config , int , <NUM_LIT> , ping = lambda : None ) \n svcs . pyramid . register_factory ( config , str , str ) \n svcs . pyramid . register_factory ( config , int , factory_with_cleanup ) \n svcs . pyramid . register_value ( config , str , str , ping = lambda : None ) \n req = pyramid . request . Request ( ) \n o1 : object = svcs . pyramid . get ( req , object ) \n o2 : int = svcs . pyramid . get_abstract ( req , object ) \n a : int \n b : str \n c : bool", "output": "d : tuple \n e : object \n f : float \n g : list \n h : dict \n i : set \n j : bytes \n a , b , c , d , e , f , g , h , i , j = svcs . pyramid . get ( \n req , int , str , bool , tuple , object , float , list , dict , set , bytes \n ) \n pings : list [ svcs . ServicePing ] = svcs . pyramid . get_pings ( req ) \n reg : svcs . Registry = svcs . pyramid . get_registry ( config ) \n reg = svcs . pyramid . get_registry ( ) \n con : svcs . Container = svcs . pyramid . svcs_from ( ) \n con = svcs . pyramid . svcs_from ( req ) \n svcs . pyramid . close_registry ( config )"}, {"input": "import torch \n from torch . utils . data . sampler import BatchSampler , SubsetRandomSampler \n class ObsStorage : \n def __init__ ( self , num_envs , num_transitions_per_env , obs_shape , action_shape , device ) : \n self . device = device \n self . obs = torch . zeros ( num_transitions_per_env , num_envs , * obs_shape ) . to ( self . device ) \n self . expert = torch . zeros ( num_transitions_per_env , num_envs , * action_shape ) . to ( self . device ) \n self . device = device \n self . num_envs = num_envs \n self . num_transitions_per_env = num_transitions_per_env \n self . step = <NUM_LIT> \n def add_obs ( self , obs , expert_action ) : \n if self . step >= self . num_transitions_per_env : \n raise AssertionError ( \"<STR_LIT>\" ) \n self . obs [ self . step ] . copy_ ( torch . from_numpy ( obs ) . to ( self . device ) ) \n self . expert [ self . step ] . copy_ ( expert_action ) \n self . step += <NUM_LIT> \n def clear ( self ) : \n self . step = <NUM_LIT> \n def mini_batch_generator_shuffle ( self , num_mini_batches ) : \n batch_size = self . num_envs * self . num_transitions_per_env \n mini_batch_size = batch_size // num_mini_batches \n for indices in BatchSampler ( SubsetRandomSampler ( range ( batch_size ) ) , mini_batch_size , drop_last = True ) : \n obs_batch = self . obs . view ( - <NUM_LIT> , * self . obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] \n expert_action_batch = self . expert . view ( - <NUM_LIT> , * self . expert . size ( ) [ <NUM_LIT> : ] ) [ indices ] \n yield obs_batch , expert_actions_batch \n def mini_batch_generator_inorder ( self , num_mini_batches ) : \n batch_size = self . num_envs * self . num_transitions_per_env \n mini_batch_size = batch_size // num_mini_batches \n for batch_id in range ( num_mini_batches ) : \n yield self . obs . view ( - <NUM_LIT> , * self . obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . expert . view ( - <NUM_LIT> , * self . expert . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] \n class RolloutStorage : \n def __init__ ( self , num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , actions_shape , device ) : \n self . device = device \n self . critic_obs = torch . zeros ( num_transitions_per_env , num_envs , * actor_obs_shape ) . to ( self . device ) \n self . actor_obs = torch . zeros ( num_transitions_per_env , num_envs , * critic_obs_shape ) . to ( self . device ) \n self . rewards = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) \n self . actions = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape ) . to ( self . device ) \n self . dones = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . byte ( ) . to ( self . device ) \n self . actions_log_prob = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) \n self . values = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) \n self . returns = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) \n self . advantages = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> ) . to ( self . device ) \n self . num_transitions_per_env = num_transitions_per_env \n self . num_envs = num_envs \n self . device = device \n self . step = <NUM_LIT> \n def add_transitions ( self , actor_obs , critic_obs , actions , rewards , dones , values , actions_log_prob ) : \n if self . step >= self . num_transitions_per_env : \n raise AssertionError ( \"<STR_LIT>\" ) \n self . critic_obs [ self . step ] . copy_ ( torch . from_numpy ( critic_obs ) . to ( self . device ) ) \n self . actor_obs [ self . step ] . copy_ ( torch . from_numpy ( actor_obs ) . to ( self . device ) ) \n self . actions [ self . step ] . copy_ ( actions . to ( self . device ) ) \n self . rewards [ self . step ] . copy_ ( torch . from_numpy ( rewards ) . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) \n self . dones [ self . step ] . copy_ ( torch . from_numpy ( dones ) . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) \n self . values [ self . step ] . copy_ ( values . to ( self . device ) ) \n self . actions_log_prob [ self . step ] . copy_ ( actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) . to ( self . device ) ) \n self . step += <NUM_LIT> \n def clear ( self ) : \n self . step = <NUM_LIT> \n def compute_returns ( self , last_values , gamma , lam ) : \n advantage = <NUM_LIT> \n for step in reversed ( range ( self . num_transitions_per_env ) ) :", "output": "if step == self . num_transitions_per_env - <NUM_LIT> : \n next_values = last_values \n else : \n next_values = self . values [ step + <NUM_LIT> ] \n next_is_not_terminal = <NUM_LIT> - self . dones [ step ] . float ( ) \n delta = self . rewards [ step ] + next_is_not_terminal * gamma * next_values - self . values [ step ] \n advantage = delta + next_is_not_terminal * gamma * lam * advantage \n self . returns [ step ] = advantage + self . values [ step ] \n self . advantages = self . returns - self . values \n self . advantages = ( self . advantages - self . advantages . mean ( ) ) / ( self . advantages . std ( ) + <NUM_LIT> ) \n def mini_batch_generator_shuffle ( self , num_mini_batches ) : \n batch_size = self . num_envs * self . num_transitions_per_env \n mini_batch_size = batch_size // num_mini_batches \n for indices in BatchSampler ( SubsetRandomSampler ( range ( batch_size ) ) , mini_batch_size , drop_last = True ) : \n actor_obs_batch = self . actor_obs . view ( - <NUM_LIT> , * self . actor_obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] \n critic_obs_batch = self . critic_obs . view ( - <NUM_LIT> , * self . critic_obs . size ( ) [ <NUM_LIT> : ] ) [ indices ] \n actions_batch = self . actions . view ( - <NUM_LIT> , self . actions . size ( - <NUM_LIT> ) ) [ indices ] \n values_batch = self . values . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] \n returns_batch = self . returns . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] \n old_actions_log_prob_batch = self . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] \n advantages_batch = self . advantages . view ( - <NUM_LIT> , <NUM_LIT> ) [ indices ] \n yield actor_obs_batch , critic_obs_batch , actions_batch , values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch \n def mini_batch_generator_inorder ( self , num_mini_batches ) : \n batch_size = self . num_envs * self . num_transitions_per_env \n mini_batch_size = batch_size // num_mini_batches \n for batch_id in range ( num_mini_batches ) : \n yield self . actor_obs . view ( - <NUM_LIT> , * self . actor_obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . critic_obs . view ( - <NUM_LIT> , * self . critic_obs . size ( ) [ <NUM_LIT> : ] ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . actions . view ( - <NUM_LIT> , self . actions . size ( - <NUM_LIT> ) ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . values . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . advantages . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . returns . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ] , self . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) [ batch_id * mini_batch_size : ( batch_id + <NUM_LIT> ) * mini_batch_size ]"}, {"input": "from flask import render_template , flash , url_for , redirect , request \n from flask_login import login_user , current_user , logout_user , login_required \n from typing import Union \n from werkzeug . wrappers import Response \n from . base import login \n from . utils import add_user , is_valid_user , get_number_of_all_messages , get_number_of_all_channels , update_user , save_profile_picture , remove_old_profile_picture \n from app . forms . registration import RegistrationForm \n from app . forms . login import LoginForm \n from app . forms . update_profile import UpdateProfileForm \n from app . models . user import User \n @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def index ( ) -> Union [ Response , str ] : \n if current_user . is_authenticated : \n return redirect ( url_for ( '<STR_LIT>' ) ) \n form = LoginForm ( ) \n if form . validate_on_submit ( ) : \n user = User . query . filter_by ( email = form . email . data ) . first ( ) \n if is_valid_user ( user , form ) : \n login_user ( user = user , remember = form . remember ) \n next_page = request . args . get ( '<STR_LIT>' ) \n return redirect ( next_page ) if next_page else redirect ( url_for ( '<STR_LIT>' ) ) \n else : \n flash ( '<STR_LIT>' , '<STR_LIT>' ) \n return render_template ( '<STR_LIT>' , form = form ) \n @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def register ( ) -> Union [ Response , str ] : \n form = RegistrationForm ( ) \n if form . validate_on_submit ( ) : \n add_user ( form ) \n flash ( f'<STR_LIT>' , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n else : \n return render_template ( '<STR_LIT>' , form = form ) \n @ login . route ( '<STR_LIT>' ) \n @ login_required \n def log_out ( ) -> Response : \n logout_user ( ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n @ login . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n @ login_required \n def settings ( ) -> Union [ str , Response ] : \n all_messages = get_number_of_all_messages ( )", "output": "all_channels = get_number_of_all_channels ( ) \n form = UpdateProfileForm ( ) \n if form . validate_on_submit ( ) : \n if form . profile_picture . data : \n remove_old_profile_picture ( ) \n profile_picture = save_profile_picture ( form . profile_picture . data ) \n current_user . profile_picture = profile_picture \n update_user ( form . username . data , form . email . data ) \n flash ( '<STR_LIT>' , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n elif request . method == '<STR_LIT>' : \n form . username . data = current_user . username \n form . email . data = current_user . email \n return render_template ( '<STR_LIT>' , all_messages = all_messages , all_channels = all_channels , form = form )"}, {"input": "import datetime \n import pathlib \n import tempfile \n import flask \n import sqlalchemy as sa \n from flask import current_app as app \n from flask_login import current_user , login_required \n import feedi . models as models \n import feedi . tasks as tasks \n from feedi import scraping \n from feedi . models import db \n from feedi . parsers import mastodon , rss \n @ app . route ( \"<STR_LIT>\" ) \n @ app . route ( \"<STR_LIT>\" , defaults = { '<STR_LIT>' : True } , endpoint = '<STR_LIT>' ) \n @ app . route ( \"<STR_LIT>\" , defaults = { '<STR_LIT>' : True } , endpoint = '<STR_LIT>' ) \n @ app . route ( \"<STR_LIT>\" ) \n @ app . route ( \"<STR_LIT>\" ) \n @ app . route ( \"<STR_LIT>\" ) \n @ login_required \n def entry_list ( ** filters ) : \n page = flask . request . args . get ( '<STR_LIT>' ) \n hide_seen = flask . session . get ( '<STR_LIT>' , True ) \n ordering = flask . session . get ( '<STR_LIT>' , models . Entry . ORDER_FREQUENCY ) \n filters = dict ( ** filters ) \n text = flask . request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) . strip ( ) \n if text : \n filters [ '<STR_LIT>' ] = text \n is_mixed_feed_list = filters . get ( '<STR_LIT>' ) or ( \n flask . request . path == '<STR_LIT>' and not filters . get ( '<STR_LIT>' ) ) \n ( entries , next_page ) = fetch_entries_page ( page , current_user . id , ordering , hide_seen , is_mixed_feed_list , \n ** filters ) \n if page : \n return flask . render_template ( '<STR_LIT>' , \n entries = entries , \n filters = filters , \n next_page = next_page ) \n return flask . render_template ( '<STR_LIT>' , \n pinned = models . Entry . select_pinned ( current_user . id , ** filters ) , \n entries = entries , \n next_page = next_page , \n is_mixed_feed_view = is_mixed_feed_list , \n filters = filters ) \n def fetch_entries_page ( page_arg , \n user_id , \n ordering_setting , \n hide_seen_setting , \n is_mixed_feed_list , ** filters ) : \n filters [ '<STR_LIT>' ] = is_mixed_feed_list and hide_seen_setting \n ordering = ordering_setting if is_mixed_feed_list else models . Entry . ORDER_RECENCY \n if page_arg : \n start_at , page_num = page_arg . split ( '<STR_LIT>' ) \n page_num = int ( page_num ) \n start_at = datetime . datetime . fromtimestamp ( float ( start_at ) ) \n else : \n start_at = datetime . datetime . utcnow ( ) \n page_num = <NUM_LIT> \n query = models . Entry . sorted_by ( user_id , ordering , start_at , ** filters ) \n entry_page = db . paginate ( query , per_page = app . config [ '<STR_LIT>' ] , page = page_num ) \n next_page = f'<STR_LIT>' if entry_page . has_next else None \n if entry_page . has_prev : \n previous_ids = [ e . id for e in entry_page . prev ( ) . items ] \n update = db . update ( models . Entry ) . where ( models . Entry . id . in_ ( previous_ids ) ) . values ( viewed = datetime . datetime . utcnow ( ) ) \n db . session . execute ( update ) \n db . session . commit ( ) \n return entry_page , next_page \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def autocomplete ( ) : \n term = flask . request . args [ '<STR_LIT>' ] . strip ( ) \n options = [ ] \n if term . startswith ( '<STR_LIT>' ) or term . startswith ( '<STR_LIT>' ) : \n options += [ \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term , redirect = <NUM_LIT> ) , '<STR_LIT>' , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' ) , \n ] \n if current_user . has_kindle : \n options += [ ( '<STR_LIT>' , \n flask . url_for ( '<STR_LIT>' , url = term ) , '<STR_LIT>' , \n '<STR_LIT>' ) ] \n else : \n folders = db . session . scalars ( \n db . select ( models . Feed . folder ) \n . filter ( models . Feed . folder . icontains ( term ) , \n models . Feed . user_id == current_user . id ) . distinct ( ) \n ) . all ( ) \n options += [ ( f , flask . url_for ( '<STR_LIT>' , folder = f ) , '<STR_LIT>' ) \n for f in folders ] \n feed_names = db . session . scalars ( \n db . select ( models . Feed . name ) \n . filter ( models . Feed . name . icontains ( term ) , \n models . Feed . user_id == current_user . id \n ) . distinct ( ) \n ) . all ( ) \n options += [ ( f , flask . url_for ( '<STR_LIT>' , feed_name = f ) , '<STR_LIT>' ) \n for f in feed_names ] \n options . append ( ( '<STR_LIT>' + term , flask . url_for ( '<STR_LIT>' , q = term ) , '<STR_LIT>' ) ) \n options += [ ( '<STR_LIT>' + f , flask . url_for ( '<STR_LIT>' , feed_name = f ) , '<STR_LIT>' ) \n for f in feed_names ] \n static_options = [ \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , favorited = True ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' , favorited = True ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) , \n ( '<STR_LIT>' , flask . url_for ( '<STR_LIT>' ) , '<STR_LIT>' ) \n ] \n for so in static_options : \n if term . lower ( ) in so [ <NUM_LIT> ] . lower ( ) : \n options . append ( so ) \n return flask . render_template ( \"<STR_LIT>\" , options = options ) \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def entry_pin ( id ) : \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if entry . pinned : \n entry . pinned = None \n else : \n entry . fetch_content ( ) \n entry . pinned = datetime . datetime . utcnow ( ) \n entry . backlogged = None \n db . session . commit ( ) \n filters = dict ( ** flask . request . args ) \n pinned = models . Entry . select_pinned ( current_user . id , ** filters ) \n return flask . render_template ( \"<STR_LIT>\" , \n is_pinned_list = True , \n filters = filters , \n entries = pinned ) \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def entry_favorite ( id ) : \n \"<STR_LIT>\" \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if entry . favorited : \n entry . favorited = None \n else : \n entry . favorited = datetime . datetime . utcnow ( ) \n db . session . commit ( ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def entry_backlog_push ( id ) : \n \"<STR_LIT>\" \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n entry . backlog ( ) \n db . session . commit ( ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . delete ( \"<STR_LIT>\" ) \n @ login_required \n def entry_backlog_pop ( id ) : \n \"<STR_LIT>\" \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if entry . backlogged : \n entry . unbacklog ( ) \n db . session . commit ( ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def mastodon_favorite ( id ) : \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . feed . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if not entry . feed . is_mastodon : \n flask . abort ( <NUM_LIT> ) \n masto_acct = entry . feed . account \n mastodon . favorite ( masto_acct . app . api_base_url , \n masto_acct . access_token , \n entry . remote_id ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def mastodon_boost ( id ) : \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . feed . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if not entry . feed . is_mastodon : \n flask . abort ( <NUM_LIT> ) \n masto_acct = entry . feed . account \n mastodon . boost ( masto_acct . app . api_base_url , \n masto_acct . access_token , \n entry . remote_id ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . route ( \"<STR_LIT>\" ) \n @ login_required \n def feed_list ( ) : \n subquery = models . Feed . frequency_rank_query ( ) \n feeds = db . session . execute ( db . select ( models . Feed , subquery . c . rank , sa . func . count ( <NUM_LIT> ) , \n sa . func . max ( models . Entry . sort_date ) . label ( '<STR_LIT>' ) ) \n . filter ( models . Feed . user_id == current_user . id ) \n . join ( subquery , models . Feed . id == subquery . c . id , isouter = True ) \n . join ( models . Entry , models . Feed . id == models . Entry . feed_id , isouter = True ) \n . group_by ( models . Feed ) \n . order_by ( sa . text ( '<STR_LIT>' ) , sa . text ( '<STR_LIT>' ) ) ) \n return flask . render_template ( '<STR_LIT>' , feeds = feeds ) \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def feed_add ( ) : \n url = flask . request . args . get ( '<STR_LIT>' ) \n name = None \n error_msg = None \n if url : \n result = rss . discover_feed ( url ) \n if result : \n ( url , name ) = result \n if not result or not url : \n error_msg = \"<STR_LIT>\" \n folders = db . session . scalars ( \n db . select ( models . Feed . folder ) \n . filter ( models . Feed . folder . isnot ( None ) , \n models . Feed . folder . isnot ( '<STR_LIT>' ) ) \n . filter_by ( user_id = current_user . id ) . distinct ( ) ) \n return flask . render_template ( '<STR_LIT>' , \n url = url , \n name = name , \n folders = folders , \n error_msg = error_msg ) \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def feed_add_submit ( ) : \n values = { k : v . strip ( ) for k , v in flask . request . form . items ( ) if v } \n if not values . get ( '<STR_LIT>' ) : \n return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) \n if not values . get ( '<STR_LIT>' ) and not values . get ( '<STR_LIT>' , '<STR_LIT>' ) . startswith ( '<STR_LIT>' ) : \n return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) \n name = values . get ( '<STR_LIT>' ) \n feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( \n name = name , user_id = current_user . id ) ) \n if feed : \n return flask . render_template ( '<STR_LIT>' , error_msg = f\"<STR_LIT>\" , ** values ) \n feed_cls = models . Feed . resolve ( values [ '<STR_LIT>' ] ) \n if not values [ '<STR_LIT>' ] . startswith ( '<STR_LIT>' ) and values . get ( '<STR_LIT>' ) : \n del values [ '<STR_LIT>' ] \n feed = feed_cls ( ** values ) \n feed . user_id = current_user . id \n db . session . add ( feed ) \n db . session . flush ( ) \n feed . load_icon ( ) \n db . session . commit ( ) \n tasks . sync_feed ( feed . id , feed . name ) . get ( ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' , feed_name = feed . name ) ) \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def feed_edit ( feed_name ) : \n feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( \n name = feed_name , user_id = current_user . id ) ) \n if not feed : \n flask . abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n folders = db . session . scalars ( \n db . select ( models . Feed . folder ) \n . filter ( models . Feed . folder . isnot ( None ) , \n models . Feed . folder . isnot ( '<STR_LIT>' ) ) \n . filter_by ( user_id = current_user . id ) . distinct ( ) ) . all ( ) \n return flask . render_template ( '<STR_LIT>' , feed = feed , folders = folders ) \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def feed_edit_submit ( feed_name ) : \n feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( \n name = feed_name , user_id = current_user . id ) ) \n if not feed : \n flask . abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n values = flask . request . form \n if not values . get ( '<STR_LIT>' ) or not values . get ( '<STR_LIT>' ) : \n return flask . render_template ( '<STR_LIT>' , error_msg = '<STR_LIT>' , ** values ) \n for ( attr , value ) in values . items ( ) : \n setattr ( feed , attr , value . strip ( ) ) \n db . session . commit ( ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) \n @ app . delete ( \"<STR_LIT>\" ) \n @ login_required \n def feed_delete ( feed_name ) : \n \"<STR_LIT>\" \n feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( \n name = feed_name , user_id = current_user . id ) ) \n if not feed : \n flask . abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n update = db . update ( models . Entry ) . where ( ( models . Entry . feed_id == feed . id ) & ( \n models . Entry . favorited . isnot ( None ) | \n models . Entry . backlogged . isnot ( None ) | \n models . Entry . pinned . isnot ( None ) ) ) . values ( feed_id = None ) \n db . session . execute ( update ) \n db . session . delete ( feed ) \n db . session . commit ( ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def feed_sync ( feed_name ) :", "output": "\"<STR_LIT>\" \n feed = db . session . scalar ( db . select ( models . Feed ) . filter_by ( \n name = feed_name , user_id = current_user . id ) ) \n if not feed : \n flask . abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n task = tasks . sync_feed ( feed . id , feed . name , force = True ) \n task . get ( ) \n response = flask . make_response ( ) \n response . headers [ '<STR_LIT>' ] = flask . url_for ( '<STR_LIT>' , feed_name = feed . name ) \n return response \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def entry_add ( ) : \n url = flask . request . args [ '<STR_LIT>' ] \n redirect = flask . request . args . get ( '<STR_LIT>' ) \n try : \n entry = models . Entry . from_url ( current_user . id , url ) \n except Exception : \n if redirect : \n return redirect_response ( url ) \n else : \n return '<STR_LIT>' , <NUM_LIT> \n db . session . add ( entry ) \n db . session . commit ( ) \n if redirect : \n return redirect_response ( flask . url_for ( '<STR_LIT>' , id = entry . id ) ) \n else : \n return '<STR_LIT>' , <NUM_LIT> \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def entry_unwrap ( id ) : \n \"<STR_LIT>\" \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if entry . content_short : \n for link in entry . embedded_links ( ) : \n try : \n subentry = models . Entry . from_url ( current_user . id , link ) \n entry . viewed = datetime . datetime . now ( ) \n db . session . add ( subentry ) \n db . session . commit ( ) \n return flask . render_template ( '<STR_LIT>' , \n entries = [ subentry ] ) \n except Exception : \n continue \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def entry_view ( id ) : \n entry = db . get_or_404 ( models . Entry , id ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n if '<STR_LIT>' in flask . request . headers and '<STR_LIT>' not in flask . request . args and not entry . content_full : \n return flask . render_template ( \"<STR_LIT>\" , entry = entry , content = None ) \n else : \n if not entry . content_url and not entry . target_url : \n return \"<STR_LIT>\" , <NUM_LIT> \n if '<STR_LIT>' in entry . content_url or '<STR_LIT>' in entry . content_url : \n return redirect_response ( entry . target_url ) \n entry . fetch_content ( ) \n if entry . content_full : \n entry . viewed = entry . viewed or datetime . datetime . utcnow ( ) \n db . session . commit ( ) \n return flask . render_template ( \"<STR_LIT>\" , entry = entry , content = entry . content_full ) \n return redirect_response ( entry . target_url ) \n def redirect_response ( url ) : \n if '<STR_LIT>' in flask . request . headers : \n response = flask . make_response ( ) \n response . headers [ '<STR_LIT>' ] = url \n return response \n else : \n return flask . redirect ( url ) \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def send_to_kindle ( ) : \n if not current_user . has_kindle : \n return '<STR_LIT>' , <NUM_LIT> \n kindle = db . session . scalar ( db . select ( models . KindleDevice ) . filter_by ( \n user_id = current_user . id ) ) \n url = flask . request . args [ '<STR_LIT>' ] \n article = scraping . extract ( url ) \n with tempfile . NamedTemporaryFile ( mode = '<STR_LIT>' , delete = False ) as fp : \n scraping . compress ( fp . name , article ) \n kindle . send ( pathlib . Path ( fp . name ) , \n author = article [ '<STR_LIT>' ] , \n title = article [ '<STR_LIT>' ] ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . route ( \"<STR_LIT>\" ) \n @ login_required \n def raw_feed ( feed_name ) : \n feed = db . session . scalar ( \n db . select ( models . Feed ) \n . filter_by ( name = feed_name , user_id = current_user . id ) \n . options ( sa . orm . undefer ( models . Feed . raw_data ) ) \n ) \n if not feed : \n flask . abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n return app . response_class ( \n response = feed . raw_data , \n status = <NUM_LIT> , \n mimetype = '<STR_LIT>' \n ) \n @ app . route ( \"<STR_LIT>\" ) \n @ login_required \n def raw_entry ( id ) : \n entry = db . get_or_404 ( models . Entry , id , \n options = [ sa . orm . undefer ( models . Entry . raw_data ) ] ) \n if entry . user_id != current_user . id : \n flask . abort ( <NUM_LIT> ) \n return app . response_class ( \n response = entry . raw_data , \n status = <NUM_LIT> , \n mimetype = '<STR_LIT>' \n ) \n @ app . put ( \"<STR_LIT>\" ) \n @ login_required \n def update_setting ( setting , value ) : \n flask . session [ setting ] = value \n return '<STR_LIT>' , <NUM_LIT> \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def toggle_setting ( setting ) : \n flask . session [ setting ] = not flask . session . get ( setting , True ) \n return '<STR_LIT>' , <NUM_LIT> \n @ app . context_processor \n def sidebar_feeds ( ) : \n if current_user . is_authenticated : \n folders = db . session . scalars ( db . select ( models . Feed . folder ) \n . filter_by ( user_id = current_user . id ) \n . filter ( models . Feed . folder . isnot ( None ) , \n models . Feed . folder . isnot ( '<STR_LIT>' ) ) \n . group_by ( models . Feed . folder ) \n . order_by ( sa . func . count ( models . Feed . folder ) . desc ( ) ) ) . all ( ) \n return dict ( shortcut_folders = folders , filters = { } ) \n return { }"}, {"input": "import datetime as dt \n import re \n from tests . conftest import ( create_feed , extract_entry_ids , mock_feed , \n mock_request ) \n def test_feed_add ( client ) : \n feed_domain = '<STR_LIT>' \n response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n assert response . status_code == <NUM_LIT> \n assert response . request . path == f'<STR_LIT>' , '<STR_LIT>' \n assert '<STR_LIT>' in response . text , '<STR_LIT>' \n assert '<STR_LIT>' in response . text , '<STR_LIT>' \n assert response . text . find ( \n '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) , '<STR_LIT>' \n response = client . get ( '<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n assert '<STR_LIT>' in response . text , '<STR_LIT>' \n assert '<STR_LIT>' in response . text , '<STR_LIT>' \n assert response . text . find ( \n '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) , '<STR_LIT>' \n def test_folders ( client ) : \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , \n folder = '<STR_LIT>' ) \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , \n folder = '<STR_LIT>' ) \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , \n folder = '<STR_LIT>' ) \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n response = client . get ( '<STR_LIT>' ) \n assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) \n response = client . get ( '<STR_LIT>' ) \n assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) \n assert all ( [ feed not in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) \n response = client . get ( '<STR_LIT>' ) \n assert all ( [ feed in response . text for feed in [ '<STR_LIT>' , '<STR_LIT>' ] ] ) \n assert all ( [ feed not in response . text for feed in [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ] ) \n def test_home_sorting ( client ) : \n date12h = dt . datetime . now ( dt . timezone . utc ) - dt . timedelta ( hours = <NUM_LIT> ) \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date12h } ] ) \n items = [ ] \n for i in range ( <NUM_LIT> , <NUM_LIT> ) : \n items . append ( { '<STR_LIT>' : f'<STR_LIT>' , '<STR_LIT>' : date12h + dt . timedelta ( hours = <NUM_LIT> , minutes = i ) } ) \n create_feed ( client , '<STR_LIT>' , items ) \n response = client . get ( '<STR_LIT>' ) \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n date13h = date12h - dt . timedelta ( hours = <NUM_LIT> ) \n create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date13h } ] ) \n response = client . get ( '<STR_LIT>' ) \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n client . put ( '<STR_LIT>' ) \n response = client . get ( '<STR_LIT>' ) \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' not in response . text \n def test_home_pagination ( app , client ) : \n now = dt . datetime . now ( dt . timezone . utc ) \n items = [ ] \n per_page = app . config [ '<STR_LIT>' ] \n for i in range ( <NUM_LIT> , per_page * <NUM_LIT> ) : \n items . append ( { '<STR_LIT>' : f'<STR_LIT>' , '<STR_LIT>' : now - dt . timedelta ( hours = <NUM_LIT> , minutes = i ) } ) \n create_feed ( client , '<STR_LIT>' , items ) \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' not in response . text \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( f'<STR_LIT>' ) \n next_page = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) \n response = client . get ( f'<STR_LIT>' ) \n assert f'<STR_LIT>' not in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' not in response . text \n response = client . get ( '<STR_LIT>' ) \n assert f'<STR_LIT>' not in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' not in response . text \n response = client . post ( '<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert f'<STR_LIT>' in response . text \n assert f'<STR_LIT>' not in response . text \n def test_sync_old_entries ( client ) : \n pass \n def test_sync_updates ( client ) : \n feed_domain = '<STR_LIT>' \n response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n mock_feed ( feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n response = client . post ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n def test_sync_between_pages ( client ) : \n pass \n def test_favorites ( client ) : \n feed_domain = '<STR_LIT>' \n response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n entry_ids = extract_entry_ids ( response ) \n response = client . put ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . put ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert response . text . find ( '<STR_LIT>' ) < response . text . find ( '<STR_LIT>' ) \n def test_backlog ( client ) : \n feed_domain = '<STR_LIT>' \n response = create_feed ( client , feed_domain , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n entry_ids = extract_entry_ids ( response ) \n response = client . put ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . put ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' not in response . text \n response = client . delete ( f'<STR_LIT>' ) \n assert response . status_code == <NUM_LIT>", "output": "response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' in response . text \n def test_pinned ( client ) : \n response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] , \n folder = '<STR_LIT>' ) \n f1a2_pin_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) \n response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n f2_a2_pin_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n now = dt . datetime . now ( dt . timezone . utc ) \n for i in range ( <NUM_LIT> , <NUM_LIT> ) : \n date = now - dt . timedelta ( hours = <NUM_LIT> , minutes = <NUM_LIT> ) \n create_feed ( client , f'<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : date } ] , \n folder = '<STR_LIT>' ) \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' not in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n response = client . put ( f1a2_pin_url ) \n assert response . status_code == <NUM_LIT> \n response = client . put ( f2_a2_pin_url ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n def test_entries_not_mixed_between_users ( client ) : \n pass \n def test_view_entry_content ( client ) : \n with open ( '<STR_LIT>' ) as sample : \n body = sample . read ( ) \n response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : body } ] ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n entry_url = re . search ( r'<STR_LIT>' , response . text ) . group ( <NUM_LIT> ) \n response = client . get ( entry_url ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n def test_add_external_entry ( client ) : \n with open ( '<STR_LIT>' ) as sample : \n body = sample . read ( ) \n content_url = '<STR_LIT>' \n mock_request ( content_url , body = body ) \n response = client . post ( \n '<STR_LIT>' , query_string = { '<STR_LIT>' : content_url , '<STR_LIT>' : <NUM_LIT> } , follow_redirects = True ) \n assert response . status_code == <NUM_LIT> \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n previous_entry_url = response . request . path \n response = client . post ( \n '<STR_LIT>' , query_string = { '<STR_LIT>' : content_url , '<STR_LIT>' : <NUM_LIT> } , follow_redirects = True ) \n assert response . status_code == <NUM_LIT> \n assert response . request . path == previous_entry_url \n client . post ( '<STR_LIT>' ) \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n def test_discover_feed ( client ) : \n pass \n def test_feed_list ( client ) : \n pass \n def test_feed_edit ( client ) : \n pass \n def test_feed_delete ( client ) : \n response = create_feed ( client , '<STR_LIT>' , [ { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } , \n { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' } ] ) \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n entry_ids = extract_entry_ids ( response ) \n response = client . put ( '<STR_LIT>' + entry_ids [ <NUM_LIT> ] ) \n assert response . status_code == <NUM_LIT> \n response = client . put ( '<STR_LIT>' + entry_ids [ <NUM_LIT> ] ) \n assert response . status_code == <NUM_LIT> \n response = client . delete ( '<STR_LIT>' ) \n assert response . status_code == <NUM_LIT> \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' in response . text \n assert '<STR_LIT>' not in response . text \n response = client . get ( '<STR_LIT>' ) \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' not in response . text \n assert '<STR_LIT>' not in response . text \n def test_mastodon_feed ( client ) : \n pass"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class CassieRoughCfg ( LeggedRobotCfg ) : \n class env ( LeggedRobotCfg . env ) : \n num_envs = <NUM_LIT> \n num_observations = <NUM_LIT> \n num_actions = <NUM_LIT> \n class terrain ( LeggedRobotCfg . terrain ) : \n measured_points_x = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n measured_points_y = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> ,", "output": "'<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> \n } \n class control ( LeggedRobotCfg . control ) : \n stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset ( LeggedRobotCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = '<STR_LIT>' \n terminate_after_contacts_on = [ '<STR_LIT>' ] \n flip_visual_attachments = False \n self_collisions = <NUM_LIT> \n class rewards ( LeggedRobotCfg . rewards ) : \n soft_dof_pos_limit = <NUM_LIT> \n soft_dof_vel_limit = <NUM_LIT> \n soft_torque_limit = <NUM_LIT> \n max_contact_force = <NUM_LIT> \n only_positive_rewards = False \n class scales ( LeggedRobotCfg . rewards . scales ) : \n termination = - <NUM_LIT> \n tracking_ang_vel = <NUM_LIT> \n torques = - <NUM_LIT> \n dof_acc = - <NUM_LIT> \n lin_vel_z = - <NUM_LIT> \n feet_air_time = <NUM_LIT> \n dof_pos_limits = - <NUM_LIT> \n no_fly = <NUM_LIT> \n dof_vel = - <NUM_LIT> \n ang_vel_xy = - <NUM_LIT> \n feet_contact_forces = - <NUM_LIT> \n class CassieRoughCfgPPO ( LeggedRobotCfgPPO ) : \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>' \n class algorithm ( LeggedRobotCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT>"}, {"input": "from . bp import cli_bp \n from app . models . base import db \n @ cli_bp . cli . command ( '<STR_LIT>' ) \n def create_db ( ) -> None : \n db . create_all ( ) \n print ( '<STR_LIT>' ) \n @ cli_bp . cli . command ( '<STR_LIT>' )", "output": "def drop_db ( ) -> None : \n db . drop_all ( ) \n print ( '<STR_LIT>' )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None", "output": "depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) \n batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) \n batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) \n op . execute ( \n '<STR_LIT>' ) \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , nullable = False ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) \n batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) \n batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "import os \n import yaml \n DENSE_CAPS_REPRODUCTION_PACKAGE_DIR = os . path . dirname ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) \n DENSE_CAPS_DIR = os . path . dirname ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR ) \n CONFIG_FILE = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' ) \n def init_config ( ) : \n config = { '<STR_LIT>' : True } \n print ( \"<STR_LIT>\" ) \n default_data_dir = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) \n data_dir = input ( f\"<STR_LIT>\" ) \n if data_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = default_data_dir \n else : \n config [ '<STR_LIT>' ] = data_dir . strip ( ) \n coco_default = os . path . join ( data_dir , '<STR_LIT>' ) \n coco_dir = input ( f\"<STR_LIT>\" ) \n if coco_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = coco_default \n else : \n config [ '<STR_LIT>' ] = coco_dir . strip ( ) \n aro_default = os . path . join ( data_dir , '<STR_LIT>' ) \n aro_dir = input ( f\"<STR_LIT>\" ) \n if aro_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = aro_default \n else : \n config [ '<STR_LIT>' ] = aro_dir . strip ( ) \n vlc_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' , '<STR_LIT>' ) \n vlc_dir = input ( f\"<STR_LIT>\" ) \n if vlc_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = vlc_default \n else : \n config [ '<STR_LIT>' ] = vlc_dir . strip ( ) \n ln_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' , '<STR_LIT>' ) \n ln_dir = input ( f\"<STR_LIT>\" ) \n if ln_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = ln_default \n else : \n config [ '<STR_LIT>' ] = ln_dir . strip ( ) \n elevater_default = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' ) \n elevater_dir = input ( f\"<STR_LIT>\" ) \n if elevater_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = elevater_default \n else : \n config [ '<STR_LIT>' ] = elevater_dir . strip ( ) \n model_default = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) \n model_dir = input ( f\"<STR_LIT>\" ) \n if elevater_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = model_default \n else : \n config [ '<STR_LIT>' ] = model_dir . strip ( ) \n with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : \n yaml . dump ( config , config_file ) \n return config \n def get_config ( ) :", "output": "if not os . path . exists ( CONFIG_FILE ) : \n return init_config ( ) \n with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : \n config = yaml . safe_load ( config_file ) \n return config \n DCI_CONFIG = get_config ( ) \n EVAL_DATASET_PATH = DCI_CONFIG [ '<STR_LIT>' ] \n VLC_ROOT_PATH = DCI_CONFIG [ '<STR_LIT>' ] \n COCO_DIR = DCI_CONFIG [ '<STR_LIT>' ] \n ARO_DIR = DCI_CONFIG [ '<STR_LIT>' ] \n MODEL_PATH = DCI_CONFIG [ '<STR_LIT>' ] \n ELEVATER_EVAL_ROOT = DCI_CONFIG [ '<STR_LIT>' ] \n ELEVATER_MODEL_CONFIG = os . path . join ( ELEVATER_EVAL_ROOT , '<STR_LIT>' ) \n ELEVATER_DATASET_CONFIG_ROOT = os . path . join ( ELEVATER_EVAL_ROOT , '<STR_LIT>' ) \n ELEVATER_DATASET_ROOT = os . path . join ( EVAL_DATASET_PATH , '<STR_LIT>' ) \n LOCALIZED_NARRATIVES_DATAPATH = os . path . join ( DCI_CONFIG [ '<STR_LIT>' ] , '<STR_LIT>' ) \n COCO_TRAIN2017_DATAPATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) \n COCO_TRAIN2017_ANNOTATION_PATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) \n COCO_VALID2017_DATAPATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) \n COCO_VALID2017_ANNOTATION_PATH = os . path . join ( COCO_DIR , '<STR_LIT>' ) \n EVAL_LOG_PATH = os . path . join ( DENSE_CAPS_REPRODUCTION_PACKAGE_DIR , '<STR_LIT>' )"}, {"input": "import datetime \n import html \n import json \n import logging \n import pprint \n import time \n import traceback \n import urllib \n import feedparser \n from bs4 import BeautifulSoup \n from feedi import scraping \n from feedi . requests import USER_AGENT , requests \n from feedi . scraping import CachingRequestsMixin \n logger = logging . getLogger ( __name__ ) \n feedparser . USER_AGENT = USER_AGENT \n def fetch ( feed_name , url , skip_older_than , min_amount , \n previous_fetch , etag , modified , filters ) : \n parser_cls = RSSParser \n for cls in RSSParser . __subclasses__ ( ) : \n if cls . is_compatible ( url ) : \n parser_cls = cls \n parser = parser_cls ( feed_name , url , skip_older_than , min_amount ) \n return parser . fetch ( previous_fetch , etag , modified , filters ) \n def fetch_icon ( url ) : \n feed = feedparser . parse ( url ) \n feed_link = feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' , url ) \n icon_url = scraping . get_favicon ( feed_link ) \n if icon_url : \n logger . debug ( \"<STR_LIT>\" , icon_url ) \n return icon_url \n icon_url = feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' , feed [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) ) \n if icon_url and requests . get ( icon_url ) . ok : \n logger . debug ( \"<STR_LIT>\" , icon_url ) \n return icon_url \n logger . debug ( \"<STR_LIT>\" , url ) \n class RSSParser ( CachingRequestsMixin ) : \n FIELDS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ] \n @ staticmethod \n def is_compatible ( _feed_url ) : \n raise NotImplementedError \n def __init__ ( self , feed_name , url , skip_older_than , min_amount ) : \n super ( ) . __init__ ( ) \n self . feed_name = feed_name \n self . url = url \n self . skip_older_than = skip_older_than \n self . min_amount = min_amount \n def fetch ( self , previous_fetch , etag , modified , filters = None ) : \n feed = feedparser . parse ( self . url , etag = etag , modified = modified ) \n if feed . bozo : \n logger . warning ( \"<STR_LIT>\" , self . feed_name , feed . bozo_exception ) \n if not feed [ '<STR_LIT>' ] : \n logger . info ( '<STR_LIT>' , self . url , feed . get ( '<STR_LIT>' ) ) \n return None , [ ] , None , None \n etag = getattr ( feed , '<STR_LIT>' , None ) \n modified = getattr ( feed , '<STR_LIT>' , None ) \n entries = [ ] \n for item in feed [ '<STR_LIT>' ] : \n try : \n entry = self . parse ( item , len ( entries ) , previous_fetch , filters ) \n if entry : \n entry [ '<STR_LIT>' ] = json . dumps ( item ) \n entries . append ( entry ) \n except Exception as error : \n exc_desc_lines = traceback . format_exception_only ( type ( error ) , error ) \n exc_desc = '<STR_LIT>' . join ( exc_desc_lines ) . rstrip ( ) \n logger . error ( \"<STR_LIT>\" , \n self . feed_name , \n item . get ( '<STR_LIT>' ) , \n exc_desc ) \n logger . debug ( traceback . format_exc ( ) ) \n return feed [ '<STR_LIT>' ] , entries , etag , modified \n def parse ( self , item , parsed_count , previous_fetch , filters ) : \n if self . should_skip ( item ) : \n return \n is_first_load = previous_fetch is None \n published = item . get ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) ) \n if ( self . skip_older_than and published and to_datetime ( published ) < self . skip_older_than ) : \n if not is_first_load or not self . min_amount or parsed_count >= self . min_amount : \n logger . debug ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) ) \n return \n if filters and not self . _matches ( item , filters ) : \n logger . debug ( '<STR_LIT>' , item . get ( '<STR_LIT>' ) , filters ) \n return \n result = { } \n for field in self . FIELDS : \n method = '<STR_LIT>' + field \n result [ field ] = getattr ( self , method ) ( item ) \n return result \n @ staticmethod \n def should_skip ( _entry ) : \n return False \n @ staticmethod \n def _matches ( entry , filters ) : \n filters = filters . split ( '<STR_LIT>' ) \n for filter in filters : \n field , value = filter . strip ( ) . split ( '<STR_LIT>' ) \n field = field . lower ( ) . strip ( ) \n value = value . lower ( ) . strip ( ) \n if value not in entry . get ( field , '<STR_LIT>' ) . lower ( ) : \n return False \n return True \n def parse_title ( self , entry ) : \n return entry . get ( '<STR_LIT>' ) or self . fetch_meta ( self . parse_content_url ( entry ) , '<STR_LIT>' ) \n def parse_content_url ( self , entry ) : \n return entry [ '<STR_LIT>' ] \n def parse_target_url ( self , entry ) : \n return self . parse_content_url ( entry ) \n def parse_comments_url ( self , entry ) : \n return entry . get ( '<STR_LIT>' ) \n def parse_username ( self , entry ) : \n author = entry . get ( '<STR_LIT>' , '<STR_LIT>' ) \n if author : \n author = BeautifulSoup ( author , '<STR_LIT>' ) . text \n author = author . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n if '<STR_LIT>' in author : \n author = author . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n return author \n def parse_avatar_url ( self , entry ) : \n url = entry . get ( '<STR_LIT>' , { } ) . get ( '<STR_LIT>' ) \n if url and requests . get ( url ) . ok : \n logger . debug ( '<STR_LIT>' , url ) \n return url \n def parse_content_short ( self , entry ) : \n content_url = self . parse_content_url ( entry ) \n summary = entry . get ( '<STR_LIT>' ) \n if summary : \n footer = summary . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n if content_url . split ( '<STR_LIT>' ) [ <NUM_LIT> ] in footer : \n summary = summary . replace ( footer , '<STR_LIT>' ) . strip ( ) \n summary = html . unescape ( summary ) \n else : \n if not content_url : \n return \n summary = self . fetch_meta ( content_url , '<STR_LIT>' , '<STR_LIT>' ) \n if not summary : \n return \n soup = BeautifulSoup ( summary , '<STR_LIT>' ) \n for tag in soup ( '<STR_LIT>' ) : \n tag . decompose ( ) \n return str ( soup ) \n def parse_content_full ( self , _entry ) : \n return None \n def parse_media_url ( self , entry ) : \n if '<STR_LIT>' in entry : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n if '<STR_LIT>' in entry and entry [ '<STR_LIT>' ] [ <NUM_LIT> ] . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n if '<STR_LIT>' in entry : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n if soup . img : \n return soup . img [ '<STR_LIT>' ] \n parsed_dest_url = self . parse_content_url ( entry ) \n return self . fetch_meta ( parsed_dest_url , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def parse_remote_id ( self , entry ) : \n return entry . get ( '<STR_LIT>' , entry [ '<STR_LIT>' ] ) \n def parse_display_date ( self , entry ) : \n dt = to_datetime ( entry . get ( '<STR_LIT>' , entry . get ( '<STR_LIT>' ) ) ) \n if dt > datetime . datetime . utcnow ( ) : \n raise ValueError ( f\"<STR_LIT>\" ) \n return dt \n def parse_sort_date ( self , entry ) : \n dt = to_datetime ( entry [ '<STR_LIT>' ] ) \n if dt > datetime . datetime . utcnow ( ) : \n raise ValueError ( \"<STR_LIT>\" ) \n return dt \n def parse_header ( self , entry ) : \n return None \n def discover_feed ( url ) : \n res = requests . get ( url ) \n if not res . ok : \n logger . warn ( \"<STR_LIT>\" , url , res ) \n return \n parsed = feedparser . parse ( res . content ) \n if not parsed . bozo : \n title = parsed . feed . get ( '<STR_LIT>' ) \n return url , title \n soup = BeautifulSoup ( res . content , '<STR_LIT>' ) \n title = scraping . extract_meta ( soup , '<STR_LIT>' , '<STR_LIT>' ) \n if not title : \n title = soup . find ( '<STR_LIT>' ) \n if title : \n title = title . text \n link_types = [ \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ] \n feed_url = None \n for type in link_types : \n link = soup . find ( '<STR_LIT>' , type = type , href = True ) \n if link : \n feed_url = scraping . make_absolute ( url , link [ '<STR_LIT>' ] ) \n return feed_url , title \n common_paths = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for path in common_paths : \n rss_url = scraping . make_absolute ( url , path ) \n res = requests . get ( rss_url ) \n mime = res . headers . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n if res . ok and mime . endswith ( '<STR_LIT>' ) : \n return rss_url , title \n return None , title \n def pretty_print ( url ) : \n feed = feedparser . parse ( url ) \n pp = pprint . PrettyPrinter ( depth = <NUM_LIT> ) \n pp . pprint ( feed ) \n def to_datetime ( struct_time ) : \n try : \n return datetime . datetime . fromtimestamp ( time . mktime ( struct_time ) ) \n except Exception : \n logger . error ( \"<STR_LIT>\" , struct_time ) \n raise \n def short_date_handler ( date_str ) : \n return datetime . datetime . strptime ( date_str , '<STR_LIT>' ) . timetuple ( ) \n feedparser . registerDateHandler ( short_date_handler ) \n class RedditInboxParser ( RSSParser ) : \n \"<STR_LIT>\" \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n def parse_title ( self , entry ) : \n return entry [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . capitalize ( ) \n class RedditParser ( RSSParser ) : \n \"<STR_LIT>\" \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url and '<STR_LIT>' not in feed_url \n def parse_content_short ( self , entry ) : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n link_anchor = soup . find ( \"<STR_LIT>\" , string = \"<STR_LIT>\" ) \n comments_anchor = soup . find ( \"<STR_LIT>\" , string = \"<STR_LIT>\" ) \n if link_anchor [ '<STR_LIT>' ] == comments_anchor [ '<STR_LIT>' ] : \n link_anchor . decompose ( ) \n comments_anchor . decompose ( ) \n return str ( soup ) \n return self . fetch_meta ( link_anchor [ '<STR_LIT>' ] , '<STR_LIT>' , '<STR_LIT>' ) \n def parse_content_url ( self , entry ) : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n return soup . find ( \"<STR_LIT>\" , string = \"<STR_LIT>\" ) [ '<STR_LIT>' ] \n def parse_comments_url ( self , entry ) : \n return entry [ '<STR_LIT>' ] \n def parse_username ( self , entry ) :", "output": "if entry . get ( '<STR_LIT>' , [ ] ) : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n return super ( ) . parse_username ( entry ) \n class LobstersParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n if '<STR_LIT>' in entry [ '<STR_LIT>' ] : \n url = self . parse_content_url ( entry ) \n return self . fetch_meta ( url , '<STR_LIT>' , '<STR_LIT>' ) \n return entry [ '<STR_LIT>' ] \n def parse_username ( self , entry ) : \n username = super ( ) . parse_username ( entry ) \n return username . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n class HackerNewsParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url or '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n if '<STR_LIT>' in entry [ '<STR_LIT>' ] : \n url = self . parse_content_url ( entry ) \n return self . fetch_meta ( url , '<STR_LIT>' , '<STR_LIT>' ) \n return entry [ '<STR_LIT>' ] \n class GithubFeedParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n return entry [ '<STR_LIT>' ] \n def parse_username ( self , entry ) : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n def parse_title ( self , _entry ) : \n return None \n def parse_avatar_url ( self , entry ) : \n return entry [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n def parse_media_url ( self , _entry ) : \n return None \n def parse_content_url ( self , _entry ) : \n return None \n def parse_target_url ( self , _entry ) : \n return None \n class GoodreadsFeedParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n summary = html . unescape ( entry [ '<STR_LIT>' ] ) \n soup = BeautifulSoup ( summary , '<STR_LIT>' ) \n for img in soup ( '<STR_LIT>' ) : \n img . decompose ( ) \n for a in soup ( '<STR_LIT>' ) : \n a [ '<STR_LIT>' ] = urllib . parse . urljoin ( '<STR_LIT>' , a [ '<STR_LIT>' ] ) \n return str ( soup ) \n def parse_title ( self , _entry ) : \n return None \n def parse_media_url ( self , _entry ) : \n return None \n def parse_target_url ( self , entry ) : \n return entry [ '<STR_LIT>' ] \n def parse_content_url ( self , _entry ) : \n return None \n class RevistaCrisisParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url \n @ staticmethod \n def should_skip ( entry ) : \n return '<STR_LIT>' in entry [ '<STR_LIT>' ] or entry [ '<STR_LIT>' ] . lower ( ) . startswith ( '<STR_LIT>' ) \n def parse_content_short ( self , entry ) : \n return self . fetch_meta ( entry [ '<STR_LIT>' ] , '<STR_LIT>' , '<STR_LIT>' ) \n class ACMQueueParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n content = self . request ( entry [ '<STR_LIT>' ] ) \n soup = BeautifulSoup ( content , '<STR_LIT>' ) \n title = soup . find ( '<STR_LIT>' ) \n return str ( title . find_next ( '<STR_LIT>' ) ) \n def parse_username ( self , entry ) : \n content = self . request ( entry [ '<STR_LIT>' ] ) \n soup = BeautifulSoup ( content , '<STR_LIT>' ) \n title = soup . find ( '<STR_LIT>' ) \n author = title . find_next ( '<STR_LIT>' ) \n if author : \n return author . text . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n class WikiFeaturedParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( feed_url ) : \n return '<STR_LIT>' in feed_url and '<STR_LIT>' in feed_url \n def parse_content_short ( self , entry ) : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n return str ( soup . find ( '<STR_LIT>' ) ) \n def parse_title ( self , entry ) : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n return soup . find ( '<STR_LIT>' ) . find ( '<STR_LIT>' ) . text \n class IndieBlogParser ( RSSParser ) : \n @ staticmethod \n def is_compatible ( _feed_url ) : \n return '<STR_LIT>' in _feed_url \n def parse_content_short ( self , entry ) : \n soup = BeautifulSoup ( entry [ '<STR_LIT>' ] , '<STR_LIT>' ) \n body = soup . blockquote \n body . name = '<STR_LIT>' \n return str ( body )"}, {"input": "import pytest \n from app . utils . signs import SIGNS , get_id_from_sign", "output": "def test_get_month_from_sign_valid_sign ( ) : \n sign = '<STR_LIT>' \n expected_month = <NUM_LIT> \n expected_result = ( expected_month , SIGNS [ sign ] ) \n assert get_id_from_sign ( sign ) == expected_result \n def test_get_month_from_sign_empty_sign ( ) : \n sign = '<STR_LIT>' \n expected_month = <NUM_LIT> \n expected_result = ( expected_month , SIGNS [ '<STR_LIT>' ] ) \n assert get_id_from_sign ( sign ) == expected_result \n def test_get_month_from_sign_invalid_sign ( ) : \n sign = '<STR_LIT>' \n with pytest . raises ( ValueError ) : \n get_id_from_sign ( sign )"}, {"input": "import pytest \n from app . repositories . horoscope_repository import HoroscopeRepository \n class TestHoroscopeRepository : \n @ pytest . fixture \n def horoscope_repository ( self ) : \n return HoroscopeRepository ( ) \n def test_get_horoscope_info_without_date ( self , horoscope_repository ) : \n sign = <NUM_LIT> \n result = horoscope_repository . get_horoscope_info ( sign , None , None ) \n assert result is not None \n assert isinstance ( result , str ) \n def test_get_horoscope_info_with_date ( self , horoscope_repository ) : \n sign = <NUM_LIT> \n date = \"<STR_LIT>\" \n result = horoscope_repository . get_horoscope_info ( sign , date , None ) \n assert result is not None \n assert isinstance ( result , str ) \n def test_get_horoscope_info_with_invalid_date ( self , horoscope_repository ) : \n sign = <NUM_LIT> \n date = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) : \n horoscope_repository . get_horoscope_info ( sign , date , None ) \n def test_get_horoscope_info_with_nonexistent_sign ( self , horoscope_repository ) : \n sign = <NUM_LIT> \n with pytest . raises ( Exception ) : \n horoscope_repository . get_horoscope_info ( sign , None , None ) \n def test_get_horoscope_info_with_invalid_lang ( self , horoscope_repository ) : \n sign = <NUM_LIT> \n lang = \"<STR_LIT>\" \n with pytest . raises ( Exception ) :", "output": "horoscope_repository . get_horoscope_info ( sign , None , lang )"}, {"input": "numero1 = <NUM_LIT> \n numero2 = <NUM_LIT> \n numero3 = \"<STR_LIT>\" \n numero4 = \"<STR_LIT>\" \n print ( \"<STR_LIT>\" ) \n print ( numero1 + numero2 ) \n print ( numero3 + numero4 ) \n print ( \"<STR_LIT>\" ) \n print ( numero1 - numero2 ) \n print ( \"<STR_LIT>\" ) \n print ( numero1 * numero2 ) \n print ( \"<STR_LIT>\" ) \n print ( numero2 / numero1 ) \n print ( \"<STR_LIT>\" ) \n print ( numero2 % numero1 ) \n print ( \"<STR_LIT>\" ) \n print ( <NUM_LIT> ** <NUM_LIT> ) \n print ( numero2 ** numero1 ) \n print ( \"<STR_LIT>\" ) \n print ( numero2 // numero1 ) \n print ( \"<STR_LIT>\" ) \n print ( numero1 > numero2 ) \n print ( \"<STR_LIT>\" ) \n print ( numero1 < numero2 )", "output": "print ( \"<STR_LIT>\" ) \n print ( numero1 == numero2 ) \n print ( <NUM_LIT> == <NUM_LIT> ) \n print ( <NUM_LIT> == \"<STR_LIT>\" ) \n print ( <NUM_LIT> == <NUM_LIT> ) \n print ( \"<STR_LIT>\" ) \n print ( numero2 > numero1 ) \n print ( numero1 == numero2 ) \n print ( numero2 >= numero2 ) \n print ( numero2 >= numero1 ) \n print ( \"<STR_LIT>\" ) \n print ( numero2 < numero1 ) \n print ( numero1 == numero2 ) \n print ( numero2 <= numero2 ) \n print ( numero2 <= numero1 ) \n print ( numero1 <= numero2 ) \n print ( \"<STR_LIT>\" ) \n print ( numero1 != numero2 ) \n print ( <NUM_LIT> != <NUM_LIT> ) \n print ( <NUM_LIT> != \"<STR_LIT>\" ) \n print ( <NUM_LIT> != <NUM_LIT> ) \n a = <NUM_LIT> \n b = <NUM_LIT> \n print ( \"<STR_LIT>\" ) \n print ( a & b ) \n print ( \"<STR_LIT>\" ) \n print ( a | b ) \n print ( \"<STR_LIT>\" ) \n print ( a ^ b ) \n print ( \"<STR_LIT>\" ) \n print ( ~ a ) \n print ( \"<STR_LIT>\" ) \n print ( a >> b ) \n print ( \"<STR_LIT>\" ) \n print ( a << b ) \n print ( b << a ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n costo_cigarros = <NUM_LIT> \n dinero = <NUM_LIT> \n edad = <NUM_LIT> \n print ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) \n print ( True and False ) \n print ( \"<STR_LIT>\" ) \n costo_cigarros = <NUM_LIT> \n dinero = <NUM_LIT> \n edad = <NUM_LIT> \n print ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) \n print ( False or True ) \n print ( \"<STR_LIT>\" ) \n costo_cigarros = <NUM_LIT> \n dinero = <NUM_LIT> \n edad = <NUM_LIT> \n print ( not ( ( dinero >= costo_cigarros ) and ( edad >= <NUM_LIT> ) ) ) \n print ( not False ) \n print ( not True ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n print ( <NUM_LIT> in lista ) \n print ( <NUM_LIT> in lista ) \n print ( \"<STR_LIT>\" ) \n print ( <NUM_LIT> not in lista ) \n print ( <NUM_LIT> not in lista ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n a = <NUM_LIT> \n b = <NUM_LIT> \n c = <NUM_LIT> \n d = a \n print ( a is b ) \n print ( a is d ) \n print ( a is c ) \n print ( \"<STR_LIT>\" ) \n print ( a is not b ) \n print ( a is not d ) \n print ( a is not c ) \n a = a + a \n print ( a ) \n print ( d )"}, {"input": "import torch \n import torch . nn as nn \n import sys \n import torchvision \n class RecurrentDepthBackbone ( nn . Module ) : \n def __init__ ( self , base_backbone , env_cfg ) -> None : \n super ( ) . __init__ ( ) \n activation = nn . ELU ( ) \n last_activation = nn . Tanh ( ) \n self . base_backbone = base_backbone \n if env_cfg == None : \n self . combination_mlp = nn . Sequential ( \n nn . Linear ( <NUM_LIT> + <NUM_LIT> , <NUM_LIT> ) , \n activation , \n nn . Linear ( <NUM_LIT> , <NUM_LIT> ) \n ) \n else : \n self . combination_mlp = nn . Sequential ( \n nn . Linear ( <NUM_LIT> + env_cfg . env . n_proprio , <NUM_LIT> ) , \n activation , \n nn . Linear ( <NUM_LIT> , <NUM_LIT> ) \n ) \n self . rnn = nn . GRU ( input_size = <NUM_LIT> , hidden_size = <NUM_LIT> , batch_first = True )", "output": "self . output_mlp = nn . Sequential ( \n nn . Linear ( <NUM_LIT> , <NUM_LIT> + <NUM_LIT> ) , \n last_activation \n ) \n self . hidden_states = None \n def forward ( self , depth_image , proprioception ) : \n depth_image = self . base_backbone ( depth_image ) \n depth_latent = self . combination_mlp ( torch . cat ( ( depth_image , proprioception ) , dim = - <NUM_LIT> ) ) \n depth_latent , self . hidden_states = self . rnn ( depth_latent [ : , None , : ] , self . hidden_states ) \n depth_latent = self . output_mlp ( depth_latent . squeeze ( <NUM_LIT> ) ) \n return depth_latent \n def detach_hidden_states ( self ) : \n self . hidden_states = self . hidden_states . detach ( ) . clone ( ) \n class StackDepthEncoder ( nn . Module ) : \n def __init__ ( self , base_backbone , env_cfg ) -> None : \n super ( ) . __init__ ( ) \n activation = nn . ELU ( ) \n self . base_backbone = base_backbone \n self . combination_mlp = nn . Sequential ( \n nn . Linear ( <NUM_LIT> + env_cfg . env . n_proprio , <NUM_LIT> ) , \n activation , \n nn . Linear ( <NUM_LIT> , <NUM_LIT> ) \n ) \n self . conv1d = nn . Sequential ( nn . Conv1d ( in_channels = env_cfg . depth . buffer_len , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , \n activation , \n nn . Conv1d ( in_channels = <NUM_LIT> , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , \n activation ) \n self . mlp = nn . Sequential ( nn . Linear ( <NUM_LIT> * <NUM_LIT> , <NUM_LIT> ) , \n activation ) \n def forward ( self , depth_image , proprioception ) : \n depth_latent = self . base_backbone ( None , depth_image . flatten ( <NUM_LIT> , <NUM_LIT> ) , None ) \n depth_latent = depth_latent . reshape ( depth_image . shape [ <NUM_LIT> ] , depth_image . shape [ <NUM_LIT> ] , - <NUM_LIT> ) \n depth_latent = self . conv1d ( depth_latent ) \n depth_latent = self . mlp ( depth_latent . flatten ( <NUM_LIT> , <NUM_LIT> ) ) \n return depth_latent \n class DepthOnlyFCBackbone58x87 ( nn . Module ) : \n def __init__ ( self , prop_dim , scandots_output_dim , hidden_state_dim , output_activation = None , num_frames = <NUM_LIT> ) : \n super ( ) . __init__ ( ) \n self . num_frames = num_frames \n activation = nn . ELU ( ) \n self . image_compression = nn . Sequential ( \n nn . Conv2d ( in_channels = self . num_frames , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , \n nn . MaxPool2d ( kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , \n activation , \n nn . Conv2d ( in_channels = <NUM_LIT> , out_channels = <NUM_LIT> , kernel_size = <NUM_LIT> ) , \n activation , \n nn . Flatten ( ) , \n nn . Linear ( <NUM_LIT> * <NUM_LIT> * <NUM_LIT> , <NUM_LIT> ) , \n activation , \n nn . Linear ( <NUM_LIT> , scandots_output_dim ) \n ) \n if output_activation == \"<STR_LIT>\" : \n self . output_activation = nn . Tanh ( ) \n else : \n self . output_activation = activation \n def forward ( self , images : torch . Tensor ) : \n images_compressed = self . image_compression ( images . unsqueeze ( <NUM_LIT> ) ) \n latent = self . output_activation ( images_compressed ) \n return latent"}, {"input": "from long_captions . dense_image import DenseCaptionedImage , get_key_for \n from long_captions . prepare . remote_language_model import RemoteLanguageModel \n from long_captions . utils import get_clip_token_length \n from long_captions . config import OUTPUT_SUMMARY_PATH , OUTPUT_NEGATIVE_PATH \n import os \n import json \n import threading \n from queue import Queue \n from typing import Dict , Any , Optional , Tuple \n IDX_TARGET = <NUM_LIT>", "output": "DEBUG = False \n SKIP_EXISTING = False \n NUM_THREADS = <NUM_LIT> \n def get_n_negatives_per ( \n input_caption : str , \n rlm : RemoteLanguageModel , \n n : int = <NUM_LIT> , \n max_fails : int = <NUM_LIT> \n ) : \n if DEBUG : \n print ( input_caption ) \n res = { \n '<STR_LIT>' : [ ] , \n '<STR_LIT>' : [ ] , \n '<STR_LIT>' : [ ] , \n } \n targets = list ( res . keys ( ) ) * n \n failures = <NUM_LIT> \n caption_tokens = get_clip_token_length ( input_caption ) \n for target in targets : \n added = False \n while added is False : \n try : \n negative_caption = rlm . get_negative ( input_caption , target ) \n if DEBUG : \n print ( negative_caption ) \n assert get_clip_token_length ( negative_caption ) > caption_tokens - <NUM_LIT> , \"<STR_LIT>\" \n assert get_clip_token_length ( negative_caption ) < caption_tokens + <NUM_LIT> , \"<STR_LIT>\" \n negative_caption_reduced = rlm . reduce_length ( negative_caption , target_tokens = caption_tokens + <NUM_LIT> ) \n res [ target ] . append ( negative_caption_reduced ) \n added = True \n except Exception as e : \n if DEBUG : \n import traceback \n traceback . print_exc ( ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n failures += <NUM_LIT> \n if failures >= max_fails : \n raise \n if DEBUG : \n print ( res ) \n return res \n def negatives_len_fit ( negs ) : \n for subkeys in negs . values ( ) : \n for generated_negative in subkeys : \n if get_clip_token_length ( generated_negative ) > <NUM_LIT> : \n return False \n return True \n def gen_negatives_for ( \n idx : int , \n rlm : RemoteLanguageModel , \n n : int = <NUM_LIT> , \n max_fails : int = <NUM_LIT> , \n existing : Optional [ Dict [ str , Any ] ] = None \n ) -> Tuple [ Optional [ Dict [ str , Any ] ] , bool , bool ] : \n failures = <NUM_LIT> \n dci = DenseCaptionedImage ( idx ) \n had_edit = False \n if existing is None : \n negatives = { \n '<STR_LIT>' : None , \n } \n else : \n negatives = existing \n key = get_key_for ( idx ) \n summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) \n with open ( summary_path ) as jsonf : \n summaries = json . load ( jsonf ) \n while negatives [ '<STR_LIT>' ] is None or not negatives_len_fit ( negatives [ '<STR_LIT>' ] ) : \n try : \n negatives [ '<STR_LIT>' ] = get_n_negatives_per ( summaries [ '<STR_LIT>' ] , rlm , n ) \n had_edit = True \n except Exception as e : \n if DEBUG : \n import traceback \n traceback . print_exc ( ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n failures += <NUM_LIT> \n if failures >= max_fails : \n return negatives , had_edit , False \n all_masks = dci . get_all_masks ( ) \n for m in all_masks : \n entry = dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] \n key = entry [ '<STR_LIT>' ] \n caption = entry [ '<STR_LIT>' ] \n if key in negatives : \n if negatives_len_fit ( negatives [ key ] ) : \n continue \n if key in summaries : \n caption = summaries [ key ] \n else : \n continue \n res = None \n while res is None : \n try : \n res = get_n_negatives_per ( caption , rlm , <NUM_LIT> ) \n negatives [ key ] = res \n had_edit = True \n except Exception as e : \n if DEBUG : \n import traceback \n traceback . print_exc ( ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n failures += <NUM_LIT> \n if failures >= max_fails : \n return negatives , had_edit , False \n return negatives , had_edit , True \n def thread_entry ( idx_pool : Queue , rlm : RemoteLanguageModel ) : \n while not idx_pool . empty ( ) : \n i , existing = idx_pool . get ( ) \n key = get_key_for ( i ) \n if int ( i ) % <NUM_LIT> == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n target_path = os . path . join ( OUTPUT_NEGATIVE_PATH , key ) \n negatives , edited , success = gen_negatives_for ( i , rlm , existing = existing ) \n if edited : \n with open ( target_path , '<STR_LIT>' ) as jsonf : \n json . dump ( negatives , jsonf ) \n if success is False : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n else : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as neg_file : \n neg_file . write ( target_path + \"<STR_LIT>\" ) \n else : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n def run_gen_summary ( ) : \n rlm = RemoteLanguageModel ( '<STR_LIT>' ) \n idx_pool = Queue ( ) \n for i in range ( IDX_TARGET ) : \n key = get_key_for ( i ) \n target_path = os . path . join ( OUTPUT_NEGATIVE_PATH , key ) \n existing = None \n if os . path . exists ( target_path ) : \n if SKIP_EXISTING : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n else : \n with open ( target_path ) as jsonf : \n existing = json . load ( jsonf ) \n summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) \n if not os . path . exists ( summary_path ) : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n idx_pool . put ( ( i , existing ) ) \n thread_pool = [ ] \n for _ in range ( NUM_THREADS ) : \n t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) \n t . start ( ) \n thread_pool . append ( t ) \n for thread in thread_pool : \n thread . join ( ) \n if __name__ == '<STR_LIT>' : \n run_gen_summary ( )"}, {"input": "import time \n import sys \n from segment_anything import sam_model_registry , SamPredictor \n from . mask_creation_utils import compute_group_tree , FinalGrouping , FinalGroup \n from PIL import Image \n import numpy as np \n import os \n import base64 \n from io import BytesIO \n import cv2 \n import json \n LOW = <NUM_LIT> \n HIGH = <NUM_LIT> \n SETEV_MODEL_ROOT = '<STR_LIT>' \n ANNOTATE_ROOT = os . path . dirname ( os . path . dirname ( __file__ ) ) \n SOURCE_DIR = os . path . join ( ANNOTATE_ROOT , \"<STR_LIT>\" ) \n OUT_DIR = os . path . join ( ANNOTATE_ROOT , \"<STR_LIT>\" ) \n def fold_group_tree ( g : FinalGrouping ) : \n def fold_group ( subg : FinalGroup ) : \n outer_mask = subg [ '<STR_LIT>' ] \n mask_img = Image . fromarray ( np . uint8 ( outer_mask . mask * <NUM_LIT> ) ) \n mask_img = mask_img . convert ( '<STR_LIT>' ) \n maskbuf = BytesIO ( ) \n mask_img . save ( maskbuf , format = '<STR_LIT>' , bits = <NUM_LIT> , optimize = True ) \n mask_bytes = maskbuf . getvalue ( ) \n as_base64 = base64 . b64encode ( mask_bytes ) \n as_str = as_base64 . decode ( '<STR_LIT>' ) \n ( t , l ) , ( b , r ) = subg [ '<STR_LIT>' ] . get_tlbr ( ) \n return { \n '<STR_LIT>' : as_str ,", "output": "'<STR_LIT>' : outer_mask . get_size ( ) , \n '<STR_LIT>' : ( ( int ( t ) , int ( l ) ) , ( int ( b ) , int ( r ) ) ) , \n '<STR_LIT>' : { \n idx : fold_group ( subsubg ) for ( idx , subsubg ) in subg [ '<STR_LIT>' ] . items ( ) \n } \n } \n return { \n idx : fold_group ( subg ) for ( idx , subg ) in g . items ( ) \n } \n def main ( ) : \n all_images = os . listdir ( SOURCE_DIR ) \n target_images = all_images [ LOW : HIGH ] \n sam_checkpoint = SETEV_MODEL_ROOT \n model_type = \"<STR_LIT>\" \n device = \"<STR_LIT>\" \n sam = sam_model_registry [ model_type ] ( checkpoint = sam_checkpoint ) \n sam . to ( device = device ) \n predictor = SamPredictor ( sam ) \n for idx , img in enumerate ( target_images ) : \n start_time = time . time ( ) \n path = os . path . join ( SOURCE_DIR , img ) \n img_array = cv2 . imread ( path ) \n img_array = cv2 . cvtColor ( img_array , cv2 . COLOR_BGR2RGB ) \n result = compute_group_tree ( \n predictor , \n img_array , \n score_cutoff = <NUM_LIT> , \n outer_sim_thresh = <NUM_LIT> , \n mutual_sim_thresh = <NUM_LIT> , \n retain_best = False , \n ) \n folded = fold_group_tree ( result ) \n with open ( os . path . join ( OUT_DIR , img + \"<STR_LIT>\" ) , '<STR_LIT>' ) as json_outf : \n json . dump ( folded , json_outf ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n def _convert_convo ( convo ) -> List : \n msgs = [ ] \n for m in convo : \n msgs . append ( \n { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ROLE_ASSISTANT , \"<STR_LIT>\" : ROLE_USER } [ m [ \"<STR_LIT>\" ] ] , \n \"<STR_LIT>\" : m [ \"<STR_LIT>\" ] , \n } \n ) \n return msgs \n def _fix_path ( path ) : \n parts = path . split ( \"<STR_LIT>\" ) \n parts = [ parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , * parts [ <NUM_LIT> : ] ] \n new_path = os . path . join ( * parts ) \n return new_path \n def main ( args ) : \n rows = [ ] \n for json_fn in args . llava_json :", "output": "with open ( json_fn ) as f : \n rows . extend ( json . load ( f ) ) \n def gen ( rows ) : \n for row in rows : \n try : \n img_path = row [ \"<STR_LIT>\" ] \n except KeyError : \n continue \n fn = os . path . join ( args . image_folder , _fix_path ( img_path ) ) \n if not os . path . exists ( fn ) : \n print ( \"<STR_LIT>\" , fn ) \n continue \n yield { \n \"<STR_LIT>\" : str ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : [ fn ] , \n \"<STR_LIT>\" : _convert_convo ( row [ \"<STR_LIT>\" ] ) , \n } \n ds = Dataset . from_generator ( gen , gen_kwargs = { \"<STR_LIT>\" : rows } , num_proc = args . num_proc ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , action = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from flask import Flask , send_from_directory , jsonify , redirect \n from PIL import Image \n import json \n import base64 \n import io \n import os \n import sys \n from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_ANNOTATIONS_PATH \n ENTRIES = os . listdir ( DATASET_ANNOTATIONS_PATH ) \n ENTRIES_MAP = { str ( i ) : e for i , e in enumerate ( ENTRIES ) } \n ENTRIES_REVERSE_MAP = { str ( e ) : i for i , e in enumerate ( ENTRIES ) } \n app = Flask ( __name__ , static_folder = '<STR_LIT>' ) \n def extract_data ( entry_key ) : \n print ( entry_key ) \n if entry_key in ENTRIES_MAP : \n entry_key = ENTRIES_MAP [ entry_key ] \n assert entry_key in ENTRIES_REVERSE_MAP \n total_entries = len ( ENTRIES_REVERSE_MAP ) \n next = ( ENTRIES_REVERSE_MAP [ entry_key ] + <NUM_LIT> ) % total_entries \n next = ENTRIES_MAP [ str ( next ) ] \n prev = ( ENTRIES_REVERSE_MAP [ entry_key ] - <NUM_LIT> ) % total_entries \n prev = ENTRIES_MAP [ str ( prev ) ] \n with open ( os . path . join ( DATASET_ANNOTATIONS_PATH , entry_key ) ) as entry_file : \n base_data = json . load ( entry_file ) \n img = Image . open ( os . path . join ( DATASET_PHOTO_PATH , base_data [ '<STR_LIT>' ] ) ) \n width , height = img . size \n base_data [ '<STR_LIT>' ] = width \n base_data [ '<STR_LIT>' ] = height \n buffered = io . BytesIO ( ) \n img . save ( buffered , format = \"<STR_LIT>\" ) \n img_str = base64 . b64encode ( buffered . getvalue ( ) ) . decode ( ) \n base_data [ '<STR_LIT>' ] = \"<STR_LIT>\" + img_str \n return jsonify ( { \n '<STR_LIT>' : base_data , \n '<STR_LIT>' : next , \n '<STR_LIT>' : prev , \n '<STR_LIT>' : entry_key , \n } ) \n @ app . route ( '<STR_LIT>' ) \n def get_data_path ( path ) : \n try : \n return extract_data ( path ) \n except Exception as e : \n raise e \n @ app . route ( '<STR_LIT>' , defaults = { '<STR_LIT>' : None } ) \n def get_data ( path ) : \n return extract_data ( <NUM_LIT> ) \n @ app . route ( '<STR_LIT>' ) \n def serve_js ( ) :", "output": "return send_from_directory ( '<STR_LIT>' , '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' ) \n def serve_html ( ) : \n return redirect ( \"<STR_LIT>\" ) \n @ app . route ( '<STR_LIT>' ) \n def catch_all ( path ) : \n return send_from_directory ( app . static_folder , '<STR_LIT>' ) \n if __name__ == '<STR_LIT>' : \n if len ( sys . argv ) > <NUM_LIT> : \n print ( f'<STR_LIT>' ) \n sys . exit ( ) \n if len ( sys . argv ) > <NUM_LIT> : \n port = int ( sys . argv [ <NUM_LIT> ] ) \n else : \n port = <NUM_LIT> \n app . run ( debug = True , port = port )"}, {"input": "import logging \n import traceback \n import time \n import socket \n import ssl \n import re \n from urllib . parse import parse_qs \n from typing import Union , Tuple \n import requests \n from fenjing . colorize import colored \n from . const import DEFAULT_USER_AGENT \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Response = Tuple [ int , str ] \n def check_line_break ( req_pattern : bytes ) -> Union [ None , bool ] : \n linebreak_pos = req_pattern . find ( b\"<STR_LIT>\" ) \n if not linebreak_pos : \n return None \n linebreak = req_pattern [ linebreak_pos - <NUM_LIT> : linebreak_pos ] \n linebreak = bytes ( c for c in linebreak if c in b\"<STR_LIT>\" ) \n if linebreak == b\"<STR_LIT>\" or linebreak == b\"<STR_LIT>\" : \n return True \n elif linebreak == b\"<STR_LIT>\" : \n return False \n return None \n def fix_line_break ( req_pattern : bytes ) -> bytes : \n line_header , _ , body = req_pattern . partition ( b\"<STR_LIT>\" ) \n return line_header . replace ( b\"<STR_LIT>\" , b\"<STR_LIT>\" ) + b\"<STR_LIT>\" + body \n def get_tail ( req_pattern : bytes ) -> Tuple [ Union [ bytes , None ] , int ] : \n lbs = [ \n b\"<STR_LIT>\" , \n b\"<STR_LIT>\" , \n b\"<STR_LIT>\" , \n ] \n for lb in lbs : \n if req_pattern [ - len ( lb ) : ] == lb : \n count = <NUM_LIT> \n while req_pattern [ - count * len ( lb ) : ] == lb * count : \n count += <NUM_LIT> \n count -= <NUM_LIT> \n return lb , count \n return None , <NUM_LIT> \n def check_tail ( req_pattern : bytes ) -> bool : \n return get_tail ( req_pattern ) [ <NUM_LIT> ] == <NUM_LIT> \n def fix_tail ( req_pattern : bytes ) -> bytes : \n lb , count = get_tail ( req_pattern ) \n if lb is None : \n return req_pattern \n if count <= <NUM_LIT> : \n return req_pattern + lb * ( <NUM_LIT> - count ) \n return req_pattern [ : - len ( lb ) * <NUM_LIT> - count ] \n class TCPRequester : \n def __init__ ( \n self , \n host : str , \n port : int , \n use_ssl : bool , \n retry_times = <NUM_LIT> , \n interval = <NUM_LIT> , \n ) : \n self . host = host \n self . port = port \n self . use_ssl = use_ssl \n self . interval = interval \n self . retry_times = retry_times \n self . last_request_time = None \n def _get_socket ( self ) : \n sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n if self . use_ssl : \n ssl_context = ssl . create_default_context ( ssl . Purpose . SERVER_AUTH ) \n sock = ssl_context . wrap_socket ( sock ) \n sock . connect ( ( self . host , self . port ) ) \n return sock \n def _recv_all ( self , sock , bufsize = <NUM_LIT> ) : \n data = b\"<STR_LIT>\" \n while True : \n chunk = sock . recv ( bufsize ) \n if not chunk : \n break \n data += chunk \n return data \n def _request_once ( self , request : bytes ) : \n if self . last_request_time : \n duration = time . perf_counter ( ) - self . last_request_time \n if duration < self . interval : \n time . sleep ( self . interval - duration ) \n self . last_request_time = time . perf_counter ( ) \n try : \n sock = self . _get_socket ( ) \n except Exception as exception : \n logger . warning ( \"<STR_LIT>\" , repr ( exception ) ) \n logger . debug ( traceback . format_exc ( ) ) \n return None \n try : \n sock . sendall ( request ) \n except Exception as exception : \n logger . warning ( \"<STR_LIT>\" , repr ( exception ) ) \n logger . debug ( traceback . format_exc ( ) ) \n return None \n response = None \n try : \n response = self . _recv_all ( sock ) \n except Exception as exception : \n logger . warning ( \"<STR_LIT>\" , repr ( exception ) ) \n logger . debug ( traceback . format_exc ( ) ) \n return None \n response = response . decode ( ) \n status_code_result = re . search ( r\"<STR_LIT>\" , response . partition ( \"<STR_LIT>\" ) [ <NUM_LIT> ] ) \n assert status_code_result is not None , \"<STR_LIT>\" + response \n try : \n sock . close ( ) \n except Exception as exception : \n logger . warning ( \"<STR_LIT>\" , repr ( exception ) ) \n return int ( status_code_result . group ( <NUM_LIT> ) ) , response . partition ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n def request ( self , request : bytes ) -> Union [ Response , None ] : \n for _ in range ( self . retry_times ) : \n resp = self . _request_once ( request ) \n if resp is not None : \n return resp \n return None \n class HTTPRequester : \n def __init__ ( \n self , \n interval = <NUM_LIT> , \n timeout = <NUM_LIT> , \n retry_times = <NUM_LIT> , \n retry_interval = <NUM_LIT> , \n retry_status = ( <NUM_LIT> , ) , \n user_agent = DEFAULT_USER_AGENT , \n headers = None , \n extra_params_querystr = None , \n extra_data_querystr = None , \n proxy = None , \n ) : \n self . interval = interval \n self . timeout = timeout \n self . retry_times = retry_times \n self . retry_interval = retry_interval \n self . retry_status = retry_status \n self . session = requests . Session ( ) \n self . session . headers . update ( { \"<STR_LIT>\" : user_agent } ) \n self . last_request_time = <NUM_LIT> \n self . extra_params = { } \n self . extra_data = { } \n if interval > <NUM_LIT> : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n interval , \n ) \n if headers : \n self . session . headers . update ( headers ) \n if extra_params_querystr : \n self . extra_params = parse_qs ( extra_params_querystr ) \n if extra_data_querystr : \n self . extra_data = parse_qs ( extra_data_querystr ) \n if proxy : \n self . session . proxies = { \"<STR_LIT>\" : proxy , \"<STR_LIT>\" : proxy } \n def request_once ( self , ** kwargs ) : \n duration = time . perf_counter ( ) - self . last_request_time \n if duration < self . interval : \n time . sleep ( self . interval - duration ) \n if \"<STR_LIT>\" not in kwargs : \n kwargs [ \"<STR_LIT>\" ] = self . timeout \n resp = None \n try : \n resp = self . session . request ( ** kwargs ) \n except Exception as exception : \n logger . warning ( \"<STR_LIT>\" , repr ( exception ) ) \n logger . debug ( traceback . format_exc ( ) ) \n return None \n if resp . status_code in self . retry_status : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n colored ( \"<STR_LIT>\" , str ( resp . status_code ) ) , \n ) \n logger . warning ( \n \"<STR_LIT>\" \n ) \n time . sleep ( <NUM_LIT> ) \n return None \n if resp . status_code not in [ <NUM_LIT> , <NUM_LIT> ] : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , str ( resp . status_code ) ) , \n ) \n self . last_request_time = time . perf_counter ( ) \n return resp \n def request ( self , ** kwargs ) : \n if self . extra_params : \n params = self . extra_params . copy ( ) \n params . update ( kwargs . get ( \"<STR_LIT>\" , { } ) ) \n kwargs [ \"<STR_LIT>\" ] = params \n if self . extra_data : \n if kwargs [ \"<STR_LIT>\" ] not in ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n logger . warning ( \n \"<STR_LIT>\" , \n kwargs [ \"<STR_LIT>\" ] , \n ) \n data = self . extra_data . copy ( )", "output": "data . update ( kwargs . get ( \"<STR_LIT>\" , { } ) ) \n kwargs [ \"<STR_LIT>\" ] = data \n for _ in range ( self . retry_times ) : \n resp = self . request_once ( ** kwargs ) \n if resp is not None : \n return resp \n return None"}, {"input": "from typing import List \n import argparse \n import random \n import json \n import os \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n TYPES = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n REPLACEMENTS = { \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } \n TEMP_TOKEN = \"<STR_LIT>\" \n EXCLUDE_WORDS = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n def _convert_convo ( convo ) -> List : \n type_idx = TYPES . index ( random . choice ( TYPES ) ) \n msgs = [ ] \n for m in convo : \n content = m [ \"<STR_LIT>\" ] . replace ( \"<STR_LIT>\" , TEMP_TOKEN ) \n for k , v in REPLACEMENTS . items ( ) : \n content = content . replace ( k , v [ type_idx ] ) \n content = content . replace ( TEMP_TOKEN , \"<STR_LIT>\" ) \n msgs . append ( \n { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ROLE_ASSISTANT , \"<STR_LIT>\" : ROLE_USER } [ m [ \"<STR_LIT>\" ] ] , \n \"<STR_LIT>\" : content , \n } \n ) \n return msgs \n def _fix_path ( path ) : \n parts = path . split ( \"<STR_LIT>\" ) \n parts = [ parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , parts [ <NUM_LIT> ] , * parts [ <NUM_LIT> : ] ] \n new_path = os . path . join ( * parts ) \n return new_path \n def main ( args ) : \n rows = [ ] \n for json_fn in args . llava_json : \n with open ( json_fn ) as f : \n rows . extend ( json . load ( f ) ) \n def gen ( rows ) : \n for row in rows : \n try : \n img_path = row [ \"<STR_LIT>\" ] \n except KeyError : \n continue \n convo_text = repr ( row [ \"<STR_LIT>\" ] ) . lower ( ) \n if \"<STR_LIT>\" in img_path or any ( w in convo_text for w in EXCLUDE_WORDS ) : \n continue \n fn = os . path . join ( args . image_folder , _fix_path ( img_path ) ) \n if not os . path . exists ( fn ) : \n print ( \"<STR_LIT>\" , fn ) \n continue \n yield { \n \"<STR_LIT>\" : str ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : [ fn ] , \n \"<STR_LIT>\" : _convert_convo ( row [ \"<STR_LIT>\" ] ) , \n } \n ds = Dataset . from_generator ( gen , gen_kwargs = { \"<STR_LIT>\" : rows } , num_proc = args . num_proc ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , action = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str )", "output": "parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from dataclasses import dataclass , field \n import logging \n from flask import Flask , request , jsonify \n import transformers \n import torch \n from multi_token . training import ( \n ModelArguments , \n ) \n from multi_token . inference import load_trained_lora_model \n from multi_token . data_tools import encode_chat \n @ dataclass \n class ServeArguments ( ModelArguments ) : \n port : int = field ( default = <NUM_LIT> ) \n host : str = field ( default = \"<STR_LIT>\" ) \n load_bits : int = field ( default = <NUM_LIT> ) \n max_new_tokens : int = field ( default = <NUM_LIT> ) \n temperature : float = field ( default = <NUM_LIT> ) \n if __name__ == \"<STR_LIT>\" : \n logging . getLogger ( ) . setLevel ( logging . INFO ) \n parser = transformers . HfArgumentParser ( ( ServeArguments , ) ) \n serve_args , _ = parser . parse_args_into_dataclasses ( return_remaining_strings = True ) \n model , tokenizer = load_trained_lora_model ( \n model_name_or_path = serve_args . model_name_or_path , \n model_lora_path = serve_args . model_lora_path , \n load_bits = serve_args . load_bits ,", "output": ") \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" ] ) \n def generate ( ) : \n req_json = request . get_json ( ) \n encoded_dict = encode_chat ( req_json , tokenizer , model . modalities ) \n with torch . inference_mode ( ) : \n output_ids = model . generate ( \n input_ids = encoded_dict [ \"<STR_LIT>\" ] . unsqueeze ( <NUM_LIT> ) . to ( model . device ) , \n max_new_tokens = serve_args . max_new_tokens , \n use_cache = True , \n do_sample = True , \n temperature = serve_args . temperature , \n modality_inputs = { \n m . name : [ encoded_dict [ m . name ] ] for m in model . modalities \n } , \n ) \n outputs = tokenizer . decode ( \n output_ids [ <NUM_LIT> , encoded_dict [ \"<STR_LIT>\" ] . shape [ <NUM_LIT> ] : ] , \n skip_special_tokens = True , \n ) . strip ( ) \n return jsonify ( { \"<STR_LIT>\" : outputs } ) \n app . run ( host = serve_args . host , port = serve_args . port )"}, {"input": "from typing import Callable , Tuple , Dict , Union \n from . const import OS_POPEN_READ \n from . full_payload_gen import FullPayloadGen \n full_payload_store : Dict [ int , FullPayloadGen ] = { } \n def exec_cmd_payload ( \n waf_func : Callable [ \n [ \n str , \n ] , \n bool , \n ] , \n cmd : str , \n ) -> Tuple [ Union [ str , None ] , Union [ bool , None ] ] : \n full_payload = None \n if id ( waf_func ) not in full_payload_store :", "output": "full_payload = FullPayloadGen ( waf_func ) \n full_payload_store [ id ( waf_func ) ] = full_payload \n else : \n full_payload = full_payload_store [ id ( waf_func ) ] \n return full_payload . generate ( OS_POPEN_READ , cmd )"}, {"input": "from collections import namedtuple \n import hashlib \n import io \n import shutil \n import mutagen \n from mutagen . id3 import PictureType \n try : \n import PIL \n from PIL import Image \n BICUBIC = PIL . Image . BICUBIC \n _HAS_PIL = True \n except ImportError : \n BICUBIC = None \n _HAS_PIL = False \n from . import util \n def getter_not_implemented ( afile , norm_key ) : \n raise NotImplementedError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( norm_key , type ( afile ) ) ) \n def setter_not_implemented ( afile , norm_key , val ) : \n raise NotImplementedError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( norm_key , type ( afile ) ) ) \n def albumartist_from_comp ( afile , norm_key ) : \n ret = None \n if afile . get ( '<STR_LIT>' , default = None ) : \n ret = '<STR_LIT>' \n return ret \n def comp_from_albumartist ( afile , norm_key ) : \n ret = None \n albumartist = afile . get ( '<STR_LIT>' , default = None ) \n if albumartist : \n albumartist = albumartist . first . lower ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n if albumartist in ( '<STR_LIT>' , '<STR_LIT>' ) : \n ret = True \n else : \n ret = False \n return ret \n TAG_MAP_ENTRY = namedtuple ( '<STR_LIT>' , ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ) ) \n TAG_MAP_ENTRY . __new__ . __defaults__ = ( getter_not_implemented , \n setter_not_implemented , \n None , \n str , \n None , \n ) \n class MetadataItem ( object ) : \n def __init__ ( self , typ , sanitizer , val ) : \n self . _values = None \n if isinstance ( val , MetadataItem ) : \n val = val . values \n self . type = typ \n self . sanitizer = sanitizer \n self . values = val \n @ property \n def ismissing ( self ) : \n return bool ( self . values ) \n @ property \n def isna ( self ) : \n return bool ( self . values ) \n @ property \n def values ( self ) : \n return self . _values \n @ values . setter \n def values ( self , val ) : \n if isinstance ( val , ( list , tuple ) ) : \n self . _values = list ( val ) \n elif val is None : \n self . _values = [ ] \n else : \n self . _values = [ val ] \n for i , v in enumerate ( self . _values ) : \n if self . sanitizer is not None : \n v = self . sanitizer ( v ) \n if not ( self . type is None or v is None or isinstance ( v , self . type ) ) : \n v = self . type ( v ) \n self . _values [ i ] = v \n @ property \n def value ( self ) : \n try : \n if self . type is None : \n if len ( self . values ) == <NUM_LIT> : \n val = self . values [ <NUM_LIT> ] \n else : \n val = str ( self ) \n else : \n val = self . type ( self ) \n except TypeError : \n values = self . values \n if not values : \n raise ValueError ( \"<STR_LIT>\" ) \n elif len ( values ) > <NUM_LIT> : \n raise ValueError ( \"<STR_LIT>\" . format ( repr ( values ) ) ) \n val = values [ <NUM_LIT> ] \n return val \n @ property \n def val ( self ) : \n return self . value \n @ property \n def first ( self ) : \n try : \n return self . _values [ <NUM_LIT> ] \n except IndexError : \n return None \n def append ( self , val ) : \n if self . sanitizer is not None : \n val = self . sanitizer ( val ) \n if not ( self . type is None or val is None or isinstance ( val , self . type ) ) : \n val = self . type ( val ) \n if self . _values : \n self . _values . append ( val ) \n else : \n self . _values = [ val ] \n def __len__ ( self ) : \n return len ( self . _values ) \n def __str__ ( self ) : \n return '<STR_LIT>' . join ( str ( li ) for li in self . _values ) \n def __int__ ( self ) : \n if not self . _values : \n val = <NUM_LIT> \n elif len ( self . _values ) == <NUM_LIT> : \n val = int ( self . _values [ <NUM_LIT> ] ) \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n return val \n def __bool__ ( self ) : \n return any ( self . _values ) \n def __list__ ( self ) : \n return list ( self . _values ) \n def __tuple__ ( self ) : \n return tuple ( self . _values ) \n def __repr__ ( self ) : \n return '<STR_LIT>' . format ( self . __str__ ( ) ) \n class Artwork ( object ) : \n def __init__ ( self , raw , width = None , height = None , fmt = None , depth = None , \n pic_type = PictureType . COVER_FRONT ) : \n if isinstance ( raw , Artwork ) : \n orig = raw \n raw = orig . raw \n width , height = orig . width , orig . height \n fmt , depth = orig . fmt , orig . depth \n pic_type = orig . pic_type \n del orig \n if not isinstance ( raw , bytes ) : \n raise TypeError ( \"<STR_LIT>\" ) \n self . raw = raw \n if any ( v is None for v in ( width , height , fmt , depth ) ) : \n try : \n img = self . image \n width = img . width \n height = img . height \n fmt = img . format . lower ( ) \n mode2depth = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n depth = mode2depth [ img . mode ] \n except ImportError : \n width = None \n height = None \n fmt = None \n depth = None \n self . width = width \n self . height = height \n self . depth = depth \n self . format = fmt \n self . mime = \"<STR_LIT>\" . format ( self . format ) \n self . pic_type = pic_type \n @ property \n def data ( self ) : \n return self . raw \n @ property \n def image ( self ) : \n img = None \n if _HAS_PIL : \n img = Image . open ( io . BytesIO ( self . raw ) ) \n else : \n raise ImportError ( \"<STR_LIT>\" ) \n return img \n def thumbnail ( self , size , method = BICUBIC ) : \n image = self . image \n image . thumbnail ( size , method ) \n return image \n def raw_thumbnail ( self , size , method = BICUBIC , format = None , quality = <NUM_LIT> , \n return_info = False ) : \n thumb = self . thumbnail ( size , method = method ) \n if format is None : \n format = thumb . format \n with io . BytesIO ( ) as output : \n thumb . save ( output , format = format , quality = quality ) \n raw = output . getvalue ( ) \n if return_info : \n info = { '<STR_LIT>' : thumb . width , '<STR_LIT>' : thumb . height } \n return raw , info \n else : \n return raw \n def __str__ ( self ) : \n md5 = hashlib . md5 ( ) \n md5 . update ( self . data ) \n return \"<STR_LIT>\" . format ( self . mime , self . width , self . height , \n md5 . hexdigest ( ) ) \n class RawProxy ( object ) : \n def __init__ ( self , parent ) : \n self . parent = parent \n def resolve ( self , norm_key , default = None ) : \n return self . parent . resolve ( norm_key , default , typeless = True ) \n def get ( self , norm_key , default = None ) : \n raw_key = norm_key \n norm_key = self . parent . _normalize_norm_key ( norm_key ) \n if norm_key in self . parent . tag_map : \n md_item = self . parent . get ( norm_key , default = default , typeless = True ) \n return md_item \n else : \n return self . parent . mfile [ raw_key ] \n def set ( self , norm_key , val ) : \n raw_key = norm_key \n norm_key = self . parent . _normalize_norm_key ( norm_key ) \n if norm_key in self . parent . tag_map : \n self . parent . set ( norm_key , val , typeless = True ) \n else : \n self . parent . mfile [ raw_key ] = val \n def __getitem__ ( self , norm_key ) : \n return self . get ( norm_key , default = None ) \n def __setitem__ ( self , norm_key , val ) :", "output": "self . set ( norm_key , val ) \n class NotAppendable ( Exception ) : \n pass \n class AudioFile ( object ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = None \n appendable = True \n _DEFAULT_TAG_ALIASES = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n _DEFAULT_TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( type = Artwork ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = float ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , \n } \n _DEFAULT_RESOLVERS = { \n '<STR_LIT>' : ( '<STR_LIT>' , albumartist_from_comp , '<STR_LIT>' ) , \n '<STR_LIT>' : ( '<STR_LIT>' , '<STR_LIT>' ) , \n '<STR_LIT>' : ( '<STR_LIT>' , comp_from_albumartist ) , \n '<STR_LIT>' : ( '<STR_LIT>' , \n lambda afile , norm_key : <NUM_LIT> \n ) , \n '<STR_LIT>' : ( '<STR_LIT>' , \n lambda afile , norm_key : afile . get ( '<STR_LIT>' , <NUM_LIT> ) \n ) , \n } \n _DEFAULT_SINGULAR_KEYS = [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n ] \n _TAG_ALIASES = { } \n _TAG_MAP = { } \n _RESOLVERS = { } \n _SINGULAR_KEYS = [ ] \n def __init__ ( self , filename , _mfile = None ) : \n self . tag_aliases = self . _DEFAULT_TAG_ALIASES . copy ( ) \n self . tag_aliases . update ( self . _TAG_ALIASES ) \n self . tag_map = self . _DEFAULT_TAG_MAP . copy ( ) \n self . tag_map . update ( self . _TAG_MAP ) \n self . resolvers = self . _DEFAULT_RESOLVERS . copy ( ) \n self . resolvers . update ( self . _RESOLVERS ) \n self . singular_keys = self . _DEFAULT_SINGULAR_KEYS . copy ( ) \n self . singular_keys += self . _SINGULAR_KEYS \n self . filename = filename \n if _mfile is None : \n self . mfile = mutagen . File ( filename ) \n else : \n self . mfile = _mfile \n if self . mfile . tags is None : \n self . mfile . add_tags ( ) \n @ property \n def raw ( self ) : \n return RawProxy ( self ) \n def save ( self , filename = None , ** kwargs ) : \n if filename is None : \n self . mfile . save ( ** kwargs ) \n filename = self . filename \n else : \n shutil . copyfile ( self . filename , filename ) \n self . mfile . save ( filename , ** kwargs ) \n def _normalize_norm_key ( self , norm_key ) : \n norm_key = norm_key . replace ( '<STR_LIT>' , '<STR_LIT>' ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) . lower ( ) \n if self . tag_aliases and norm_key in self . tag_aliases : \n norm_key = self . tag_aliases [ norm_key ] \n return norm_key \n def resolve ( self , norm_key , default = None , typeless = False ) : \n norm_key = self . _normalize_norm_key ( norm_key ) \n tmap = self . tag_map [ norm_key ] \n md_type = None if typeless else tmap . type \n md_sanitizer = None if typeless else tmap . sanitizer \n ret = None \n if norm_key in self . resolvers : \n for resolver in self . resolvers [ norm_key ] : \n if hasattr ( resolver , '<STR_LIT>' ) : \n ret = resolver ( self , norm_key ) \n else : \n ret = self . get ( resolver , default = None , _raw_default = True , \n typeless = typeless ) \n if ret is not None : \n break \n else : \n ret = self . get ( norm_key , default = None , _raw_default = True , \n typeless = typeless ) \n if not ( ret is None or isinstance ( ret , MetadataItem ) ) : \n ret = MetadataItem ( md_type , md_sanitizer , ret ) \n if ret is None : \n ret = MetadataItem ( md_type , md_sanitizer , default ) \n return ret \n def _ft_getter ( self , key ) : \n return self . mfile . tags . get ( key , None ) \n def get ( self , norm_key , default = None , _raw_default = False , typeless = False ) : \n norm_key = self . _normalize_norm_key ( norm_key ) \n tmap = self . tag_map [ norm_key ] \n md_type = None if typeless else tmap . type \n md_sanitizer = None if typeless else tmap . sanitizer \n ret = None \n if hasattr ( tmap . getter , '<STR_LIT>' ) : \n val = tmap . getter ( self , norm_key ) \n ret = None if val is None else MetadataItem ( md_type , md_sanitizer , \n val ) \n elif norm_key . startswith ( '<STR_LIT>' ) : \n val = getattr ( self . mfile . info , tmap . getter ) \n if not typeless : \n val = tmap . type ( val ) \n ret = None if val is None else MetadataItem ( md_type , md_sanitizer , \n val ) \n elif isinstance ( tmap . getter , ( list , tuple ) ) : \n val = None \n for getter in tmap . getter : \n if val is not None : \n break \n if hasattr ( getter , '<STR_LIT>' ) : \n val = getter ( self , norm_key ) \n elif getter in self . mfile . tags : \n val = self . _ft_getter ( getter ) \n ret = None if val is None else MetadataItem ( md_type , md_sanitizer , \n val ) \n else : \n try : \n val = self . _ft_getter ( tmap . getter ) \n except KeyError : \n val = None \n ret = None if val is None else MetadataItem ( md_type , md_sanitizer , \n val ) \n if ret is None : \n if _raw_default : \n ret = default \n else : \n ret = MetadataItem ( md_type , md_sanitizer , default ) \n return ret \n def _ft_setter ( self , key , md_val , appendable = True ) : \n if self . appendable and appendable : \n self . mfile . tags [ key ] = md_val . values \n else : \n self . mfile . tags [ key ] = md_val . value \n def set_raw ( self , norm_key , key , md_val , appendable = True ) : \n if not isinstance ( md_val , MetadataItem ) : \n if isinstance ( md_val , ( list , tuple ) ) : \n md_val = MetadataItem ( type ( md_val [ <NUM_LIT> ] ) , None , md_val ) \n else : \n md_val = MetadataItem ( type ( md_val ) , None , md_val ) \n appendable = appendable and norm_key not in self . singular_keys \n if norm_key in self . singular_keys and len ( md_val . values ) > <NUM_LIT> : \n raise ValueError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( norm_key , md_val . values ) ) \n try : \n self . _ft_setter ( key , md_val , appendable = appendable ) \n except ( TypeError , ValueError ) : \n try : \n v = [ str ( vi ) for vi in md_val . values ] \n self . _ft_setter ( key , MetadataItem ( str , None , v ) , \n appendable = appendable ) \n except Exception : \n success = False \n else : \n success = True \n if not success : \n raise \n def set ( self , norm_key , val , typeless = False ) : \n norm_key = self . _normalize_norm_key ( norm_key ) \n tmap = self . tag_map [ norm_key ] \n md_type = None if typeless else tmap . type \n md_sanitizer = None if typeless else tmap . sanitizer \n if not isinstance ( val , MetadataItem ) : \n val = MetadataItem ( md_type , md_sanitizer , val ) \n if hasattr ( tmap . setter , '<STR_LIT>' ) : \n tmap . setter ( self , norm_key , val ) \n elif norm_key . startswith ( '<STR_LIT>' ) : \n raise KeyError ( \"<STR_LIT>\" ) \n elif isinstance ( tmap . setter , ( list , tuple ) ) : \n value_set = False \n for setter in tmap . setter : \n if value_set : \n break \n if hasattr ( tmap . setter , '<STR_LIT>' ) : \n tmap . setter ( self , norm_key , val ) \n value_set = True \n elif setter in self . mfile . tags : \n self . set_raw ( norm_key , setter , val ) \n value_set = True \n if not value_set : \n self . set_raw ( norm_key , tmap . setter [ <NUM_LIT> ] , val ) \n else : \n self . set_raw ( norm_key , tmap . setter , val ) \n def append_tag ( self , norm_key , val ) : \n norm_key = self . _normalize_norm_key ( norm_key ) \n if not self . appendable : \n raise NotAppendable ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( self . __class__ . __name__ ) ) \n if norm_key in self . singular_keys : \n raise NotAppendable ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( self . __class__ . __name__ , norm_key ) ) \n existing_val = self . get ( norm_key , default = None ) \n if existing_val is None : \n new_val = val \n else : \n existing_val . append ( val ) \n new_val = existing_val \n self . set ( norm_key , new_val ) \n def append ( self , norm_key , val ) : \n return self . append_tag ( norm_key , val ) \n def _ft_rmtag ( self , key ) : \n if key in self . mfile . tags : \n del self . mfile . tags [ key ] \n def remove_tag ( self , norm_key ) : \n norm_key = self . _normalize_norm_key ( norm_key ) \n if norm_key . startswith ( '<STR_LIT>' ) : \n raise KeyError ( \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n tmap = self . tag_map [ norm_key ] \n remover = None \n if tmap . remover : \n remover = tmap . remover \n if not remover : \n if isinstance ( tmap . getter , ( list , tuple ) ) : \n remover = [ g for g in tmap . getter if isinstance ( g , util . string_types ) ] \n if isinstance ( tmap . getter , util . string_types ) : \n remover = [ tmap . getter ] \n if not remover : \n if isinstance ( tmap . setter , ( list , tuple ) ) : \n remover = [ s for s in tmap . setter if isinstance ( s , util . string_types ) ] \n if isinstance ( tmap . setter , util . string_types ) : \n remover = [ tmap . setter ] \n if remover is not None : \n if hasattr ( remover , '<STR_LIT>' ) : \n remover ( self , norm_key ) \n elif isinstance ( remover , ( list , tuple ) ) : \n for key in remover : \n self . _ft_rmtag ( key ) \n elif isinstance ( remover , util . string_types ) : \n self . _ft_rmtag ( remover ) \n def info ( self , tags = None , show_empty = False , resolve = False ) : \n if not tags : \n tags = self . _TAG_MAP . keys ( ) \n t_lst = [ ] \n for tag in tags : \n if resolve : \n mdi = self . resolve ( tag , None ) \n else : \n mdi = self . get ( tag , None ) \n if mdi or show_empty : \n t_lst . append ( '<STR_LIT>' . format ( tag , str ( mdi ) ) ) \n return '<STR_LIT>' . join ( t_lst ) \n def __getitem__ ( self , norm_key ) : \n return self . get ( norm_key , default = None ) \n def __setitem__ ( self , norm_key , val ) : \n self . set ( norm_key , val ) \n def __contains__ ( self , key ) : \n return self [ key ] . values != [ ] \n def __delitem__ ( self , norm_key ) : \n self . remove_tag ( norm_key ) \n def __str__ ( self ) : \n return self . info ( show_empty = True )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n import random \n import openai \n from datasets import Dataset , load_dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n DATASET_ARGS = dict ( \n path = \"<STR_LIT>\" , name = \"<STR_LIT>\" , split = \"<STR_LIT>\" \n ) \n PROMPT = \n QUESTIONS = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n OPENAI_TOOLS = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } , \n } , \n } \n ] \n def _build_convo ( idx , row ) -> List : \n client = openai . Client ( ) \n captions = [ row [ \"<STR_LIT>\" ] ] \n speech_audios = [ { \"<STR_LIT>\" : DATASET_ARGS , \"<STR_LIT>\" : idx } ] \n captions_text = \"<STR_LIT>\" . join ( [ f'<STR_LIT>' for i , cap in enumerate ( captions ) ] ) \n prompt = PROMPT . format ( \n captions = captions_text , question = random . choice ( QUESTIONS ) \n ) . strip ( ) \n completion = client . chat . completions . create ( \n model = \"<STR_LIT>\" , \n messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : prompt } ] , \n tools = OPENAI_TOOLS , \n tool_choice = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n ) \n resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) \n if \"<STR_LIT>\" not in resp : \n print ( resp ) \n q = resp [ \"<STR_LIT>\" ] \n a = resp [ \"<STR_LIT>\" ] \n if random . choice ( [ True , False ] ) : \n q = \"<STR_LIT>\" * len ( captions ) + \"<STR_LIT>\" + q \n else : \n q = q + \"<STR_LIT>\" + \"<STR_LIT>\" * len ( captions ) \n example = { \n \"<STR_LIT>\" : speech_audios , \n \"<STR_LIT>\" : [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : q , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : a , \n } , \n ] , \n } \n return example \n def main ( args ) : \n data = load_dataset ( ** DATASET_ARGS ) \n data_idxs = list ( range ( len ( data ) ) ) \n os . makedirs ( args . cache_folder , exist_ok = True ) \n def gen ( seeds ) : \n r = random . Random ( seeds [ <NUM_LIT> ] + <NUM_LIT> ) \n cache = open ( \n os . path . join ( args . cache_folder , f\"<STR_LIT>\" ) , \"<STR_LIT>\" \n ) \n i = <NUM_LIT> \n while i < len ( seeds ) : \n selected_idx = r . sample ( data_idxs , k = <NUM_LIT> ) [ <NUM_LIT> ] \n selected_row = data [ selected_idx ] \n try : \n example = _build_convo ( selected_idx , selected_row ) \n cache . write ( json . dumps ( example ) + \"<STR_LIT>\" ) \n yield example \n i += <NUM_LIT> \n except Exception as e : \n print ( e ) \n continue \n cache . close ( ) \n ds = Dataset . from_generator ( \n gen , \n num_proc = args . num_proc , \n gen_kwargs = { \"<STR_LIT>\" : list ( range ( args . num_examples ) ) } , \n ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import numpy as np \n import code \n import torch \n import torch . nn as nn \n from torch . distributions import Normal \n from torch . nn . modules import rnn \n from torch . nn . modules . activation import ReLU \n class StateHistoryEncoder ( nn . Module ) : \n def __init__ ( self , activation_fn , input_size , tsteps , output_size , tanh_encoder_output = False ) : \n super ( StateHistoryEncoder , self ) . __init__ ( ) \n self . activation_fn = activation_fn \n self . tsteps = tsteps \n channel_size = <NUM_LIT> \n self . encoder = nn . Sequential ( \n nn . Linear ( input_size , <NUM_LIT> * channel_size ) , self . activation_fn , \n ) \n if tsteps == <NUM_LIT> : \n self . conv_layers = nn . Sequential ( \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Conv1d ( in_channels = channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , nn . Flatten ( ) ) \n elif tsteps == <NUM_LIT> : \n self . conv_layers = nn . Sequential ( \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Flatten ( ) ) \n elif tsteps == <NUM_LIT> : \n self . conv_layers = nn . Sequential ( \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = <NUM_LIT> * channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Conv1d ( in_channels = <NUM_LIT> * channel_size , out_channels = channel_size , kernel_size = <NUM_LIT> , stride = <NUM_LIT> ) , self . activation_fn , \n nn . Flatten ( ) ) \n else : \n raise ( ValueError ( \"<STR_LIT>\" ) ) \n self . linear_output = nn . Sequential ( \n nn . Linear ( channel_size * <NUM_LIT> , output_size ) , self . activation_fn \n ) \n def forward ( self , obs ) : \n nd = obs . shape [ <NUM_LIT> ] \n T = self . tsteps \n projection = self . encoder ( obs . reshape ( [ nd * T , - <NUM_LIT> ] ) ) \n output = self . conv_layers ( projection . reshape ( [ nd , T , - <NUM_LIT> ] ) . permute ( ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) ) \n output = self . linear_output ( output ) \n return output \n class Actor ( nn . Module ) : \n def __init__ ( self , num_prop , \n num_scan , \n num_actions , \n scan_encoder_dims , \n actor_hidden_dims , \n priv_encoder_dims , \n num_priv_latent , \n num_priv_explicit , \n num_hist , activation , \n tanh_encoder_output = False ) -> None : \n super ( ) . __init__ ( ) \n self . num_prop = num_prop \n self . num_scan = num_scan \n self . num_hist = num_hist \n self . num_actions = num_actions \n self . num_priv_latent = num_priv_latent \n self . num_priv_explicit = num_priv_explicit \n self . if_scan_encode = scan_encoder_dims is not None and num_scan > <NUM_LIT> \n if len ( priv_encoder_dims ) > <NUM_LIT> : \n priv_encoder_layers = [ ] \n priv_encoder_layers . append ( nn . Linear ( num_priv_latent , priv_encoder_dims [ <NUM_LIT> ] ) ) \n priv_encoder_layers . append ( activation ) \n for l in range ( len ( priv_encoder_dims ) - <NUM_LIT> ) : \n priv_encoder_layers . append ( nn . Linear ( priv_encoder_dims [ l ] , priv_encoder_dims [ l + <NUM_LIT> ] ) ) \n priv_encoder_layers . append ( activation ) \n self . priv_encoder = nn . Sequential ( * priv_encoder_layers ) \n priv_encoder_output_dim = priv_encoder_dims [ - <NUM_LIT> ] \n else : \n self . priv_encoder = nn . Identity ( ) \n priv_encoder_output_dim = num_priv_latent \n self . history_encoder = StateHistoryEncoder ( activation , num_prop , num_hist , priv_encoder_output_dim ) \n if self . if_scan_encode : \n scan_encoder = [ ] \n scan_encoder . append ( nn . Linear ( num_scan , scan_encoder_dims [ <NUM_LIT> ] ) ) \n scan_encoder . append ( activation ) \n for l in range ( len ( scan_encoder_dims ) - <NUM_LIT> ) : \n if l == len ( scan_encoder_dims ) - <NUM_LIT> : \n scan_encoder . append ( nn . Linear ( scan_encoder_dims [ l ] , scan_encoder_dims [ l + <NUM_LIT> ] ) ) \n scan_encoder . append ( nn . Tanh ( ) ) \n else : \n scan_encoder . append ( nn . Linear ( scan_encoder_dims [ l ] , scan_encoder_dims [ l + <NUM_LIT> ] ) ) \n scan_encoder . append ( activation ) \n self . scan_encoder = nn . Sequential ( * scan_encoder ) \n self . scan_encoder_output_dim = scan_encoder_dims [ - <NUM_LIT> ] \n else : \n self . scan_encoder = nn . Identity ( ) \n self . scan_encoder_output_dim = num_scan \n actor_layers = [ ] \n actor_layers . append ( nn . Linear ( num_prop + \n self . scan_encoder_output_dim + \n num_priv_explicit + \n priv_encoder_output_dim , \n actor_hidden_dims [ <NUM_LIT> ] ) ) \n actor_layers . append ( activation ) \n for l in range ( len ( actor_hidden_dims ) ) : \n if l == len ( actor_hidden_dims ) - <NUM_LIT> : \n actor_layers . append ( nn . Linear ( actor_hidden_dims [ l ] , num_actions ) ) \n else : \n actor_layers . append ( nn . Linear ( actor_hidden_dims [ l ] , actor_hidden_dims [ l + <NUM_LIT> ] ) ) \n actor_layers . append ( activation ) \n if tanh_encoder_output : \n actor_layers . append ( nn . Tanh ( ) ) \n self . actor_backbone = nn . Sequential ( * actor_layers ) \n def forward ( self , obs , hist_encoding : bool , eval = False , scandots_latent = None ) : \n if not eval : \n if self . if_scan_encode : \n obs_scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] \n if scandots_latent is None : \n scan_latent = self . scan_encoder ( obs_scan ) \n else : \n scan_latent = scandots_latent \n obs_prop_scan = torch . cat ( [ obs [ : , : self . num_prop ] , scan_latent ] , dim = <NUM_LIT> ) \n else : \n obs_prop_scan = obs [ : , : self . num_prop + self . num_scan ] \n obs_priv_explicit = obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] \n if hist_encoding : \n latent = self . infer_hist_latent ( obs ) \n else : \n latent = self . infer_priv_latent ( obs ) \n backbone_input = torch . cat ( [ obs_prop_scan , obs_priv_explicit , latent ] , dim = <NUM_LIT> ) \n backbone_output = self . actor_backbone ( backbone_input ) \n return backbone_output \n else : \n if self . if_scan_encode : \n obs_scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] \n if scandots_latent is None : \n scan_latent = self . scan_encoder ( obs_scan ) \n else : \n scan_latent = scandots_latent \n obs_prop_scan = torch . cat ( [ obs [ : , : self . num_prop ] , scan_latent ] , dim = <NUM_LIT> ) \n else : \n obs_prop_scan = obs [ : , : self . num_prop + self . num_scan ] \n obs_priv_explicit = obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] \n if hist_encoding : \n latent = self . infer_hist_latent ( obs ) \n else : \n latent = self . infer_priv_latent ( obs ) \n backbone_input = torch . cat ( [ obs_prop_scan , obs_priv_explicit , latent ] , dim = <NUM_LIT> ) \n backbone_output = self . actor_backbone ( backbone_input ) \n return backbone_output \n def infer_priv_latent ( self , obs ) : \n priv = obs [ : , self . num_prop + self . num_scan + self . num_priv_explicit : self . num_prop + self . num_scan + self . num_priv_explicit + self . num_priv_latent ] \n return self . priv_encoder ( priv ) \n def infer_hist_latent ( self , obs ) : \n hist = obs [ : , - self . num_hist * self . num_prop : ] \n return self . history_encoder ( hist . view ( - <NUM_LIT> , self . num_hist , self . num_prop ) ) \n def infer_scandots_latent ( self , obs ) : \n scan = obs [ : , self . num_prop : self . num_prop + self . num_scan ] \n return self . scan_encoder ( scan ) \n class ActorCriticRMA ( nn . Module ) : \n is_recurrent = False \n def __init__ ( self , num_prop , \n num_scan , \n num_critic_obs , \n num_priv_latent , \n num_priv_explicit , \n num_hist , \n num_actions , \n scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n activation = '<STR_LIT>' , \n init_noise_std = <NUM_LIT> , \n ** kwargs ) : \n if kwargs : \n print ( \"<STR_LIT>\" + str ( [ key for key in kwargs . keys ( ) ] ) ) \n super ( ActorCriticRMA , self ) . __init__ ( ) \n self . kwargs = kwargs \n priv_encoder_dims = kwargs [ '<STR_LIT>' ] \n activation = get_activation ( activation ) \n self . actor = Actor ( num_prop , num_scan , num_actions , scan_encoder_dims , actor_hidden_dims , priv_encoder_dims , num_priv_latent , num_priv_explicit , num_hist , activation , tanh_encoder_output = kwargs [ '<STR_LIT>' ] ) \n critic_layers = [ ] \n critic_layers . append ( nn . Linear ( num_critic_obs , critic_hidden_dims [ <NUM_LIT> ] ) ) \n critic_layers . append ( activation ) \n for l in range ( len ( critic_hidden_dims ) ) : \n if l == len ( critic_hidden_dims ) - <NUM_LIT> : \n critic_layers . append ( nn . Linear ( critic_hidden_dims [ l ] , <NUM_LIT> ) ) \n else : \n critic_layers . append ( nn . Linear ( critic_hidden_dims [ l ] , critic_hidden_dims [ l + <NUM_LIT> ] ) ) \n critic_layers . append ( activation ) \n self . critic = nn . Sequential ( * critic_layers ) \n self . std = nn . Parameter ( init_noise_std * torch . ones ( num_actions ) ) \n self . distribution = None", "output": "Normal . set_default_validate_args = False \n @ staticmethod \n def init_weights ( sequential , scales ) : \n [ torch . nn . init . orthogonal_ ( module . weight , gain = scales [ idx ] ) for idx , module in \n enumerate ( mod for mod in sequential if isinstance ( mod , nn . Linear ) ) ] \n def reset ( self , dones = None ) : \n pass \n def forward ( self ) : \n raise NotImplementedError \n @ property \n def action_mean ( self ) : \n return self . distribution . mean \n @ property \n def action_std ( self ) : \n return self . distribution . stddev \n @ property \n def entropy ( self ) : \n return self . distribution . entropy ( ) . sum ( dim = - <NUM_LIT> ) \n def update_distribution ( self , observations , hist_encoding ) : \n mean = self . actor ( observations , hist_encoding ) \n self . distribution = Normal ( mean , mean * <NUM_LIT> + self . std ) \n def act ( self , observations , hist_encoding = False , ** kwargs ) : \n self . update_distribution ( observations , hist_encoding ) \n return self . distribution . sample ( ) \n def get_actions_log_prob ( self , actions ) : \n return self . distribution . log_prob ( actions ) . sum ( dim = - <NUM_LIT> ) \n def act_inference ( self , observations , hist_encoding = False , eval = False , scandots_latent = None , ** kwargs ) : \n if not eval : \n actions_mean = self . actor ( observations , hist_encoding , eval , scandots_latent ) \n return actions_mean \n else : \n actions_mean , latent_hist , latent_priv = self . actor ( observations , hist_encoding , eval = True ) \n return actions_mean , latent_hist , latent_priv \n def evaluate ( self , critic_observations , ** kwargs ) : \n value = self . critic ( critic_observations ) \n return value \n def reset_std ( self , std , num_actions , device ) : \n new_std = std * torch . ones ( num_actions , device = device ) \n self . std . data = new_std . data \n def get_activation ( act_name ) : \n if act_name == \"<STR_LIT>\" : \n return nn . ELU ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . SELU ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . ReLU ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . ReLU ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . LeakyReLU ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . Tanh ( ) \n elif act_name == \"<STR_LIT>\" : \n return nn . Sigmoid ( ) \n else : \n print ( \"<STR_LIT>\" ) \n return None"}, {"input": "from __future__ import annotations \n from . import exceptions \n from . _core import ( \n Container , \n RegisteredService , \n Registry , \n ServicePing , \n ) \n __all__ = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ]", "output": "try : \n from . import aiohttp \n except ImportError : \n __all__ += [ \"<STR_LIT>\" ] \n try : \n from . import fastapi \n except ImportError : \n __all__ += [ \"<STR_LIT>\" ] \n try : \n from . import flask \n except ImportError : \n __all__ += [ \"<STR_LIT>\" ] \n try : \n from . import pyramid \n except ImportError : \n __all__ += [ \"<STR_LIT>\" ] \n try : \n from . import starlette \n except ImportError : \n __all__ += [ \"<STR_LIT>\" ]"}, {"input": "import re \n from slack_bolt import App \n from slack_bolt . adapter . socket_mode import SocketModeHandler \n from common import const \n from common . log import logger \n from channel . channel import Channel \n from config import channel_conf \n app = App ( token = channel_conf ( const . SLACK ) . get ( '<STR_LIT>' ) ) \n handler = SocketModeHandler ( app = app ,", "output": "app_token = channel_conf ( const . SLACK ) . get ( '<STR_LIT>' ) ) \n @ app . event ( \"<STR_LIT>\" ) \n def handle_mention ( event , say ) : \n if '<STR_LIT>' in event : \n ts = event [ \"<STR_LIT>\" ] \n else : \n ts = event [ \"<STR_LIT>\" ] \n reply_text = SlackChannel ( ) . handle ( event ) \n say ( text = f\"<STR_LIT>\" , thread_ts = ts ) \n class SlackChannel ( Channel ) : \n def startup ( self ) : \n handler . start ( ) \n def handle ( self , event ) : \n context = dict ( ) \n if '<STR_LIT>' in event : \n ts = event [ \"<STR_LIT>\" ] \n else : \n ts = event [ \"<STR_LIT>\" ] \n context [ '<STR_LIT>' ] = str ( ts ) \n plain_text = re . sub ( r\"<STR_LIT>\" , \"<STR_LIT>\" , event [ \"<STR_LIT>\" ] ) \n return super ( ) . build_reply_content ( plain_text , context )"}, {"input": "from channel . wechat . wechat_channel import WechatChannel \n from channel . wechatcom . wechatenterprise_channel import WechatEnterpriseChannel \n from channel . qqchat . qqchat_channel import QqchaChannel \n from channel . dingtalk . dingtalk_channel import DingTalkChannel \n from channel . feishu . feishu_channel import FeiShuChannel \n from channel . webchatmp . wechat_mp_channel import WechatSubsribeAccount \n def create_channel ( channel_type ) : \n if channel_type == '<STR_LIT>' :", "output": "return WechatChannel ( ) \n if channel_type == '<STR_LIT>' : \n return WechatEnterpriseChannel ( ) \n if channel_type == '<STR_LIT>' : \n return QqchaChannel ( ) \n if channel_type == '<STR_LIT>' : \n return DingTalkChannel ( ) \n if channel_type == '<STR_LIT>' : \n return FeiShuChannel ( ) \n if channel_type == '<STR_LIT>' : \n return WechatSubsribeAccount ( ) \n raise RuntimeError"}, {"input": "import numpy as np \n import torch \n import torch . nn as nn \n from torch . distributions import Normal \n from torch . nn . modules import rnn \n from . actor_critic import ActorCritic , get_activation \n from rsl_rl . utils import unpad_trajectories \n class ActorCriticRecurrent ( ActorCritic ) : \n is_recurrent = True \n def __init__ ( self , num_actor_obs , \n num_critic_obs , \n num_actions , \n actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n activation = '<STR_LIT>' , \n rnn_type = '<STR_LIT>' , \n rnn_hidden_size = <NUM_LIT> , \n rnn_num_layers = <NUM_LIT> , \n init_noise_std = <NUM_LIT> , \n ** kwargs ) : \n if kwargs : \n print ( \"<STR_LIT>\" + str ( kwargs . keys ( ) ) , ) \n super ( ) . __init__ ( num_actor_obs = rnn_hidden_size , \n num_critic_obs = rnn_hidden_size , \n num_actions = num_actions , \n actor_hidden_dims = actor_hidden_dims , \n critic_hidden_dims = critic_hidden_dims , \n activation = activation , \n init_noise_std = init_noise_std , \n ** kwargs ) \n activation = get_activation ( activation ) \n self . memory_a = Memory ( num_actor_obs , type = rnn_type , num_layers = rnn_num_layers , hidden_size = rnn_hidden_size ) \n self . memory_c = Memory ( num_critic_obs , type = rnn_type , num_layers = rnn_num_layers , hidden_size = rnn_hidden_size ) \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n def reset ( self , dones = None ) : \n self . memory_a . reset ( dones ) \n self . memory_c . reset ( dones ) \n def act ( self , observations , masks = None , hidden_states = None ) : \n input_a = self . memory_a ( observations , masks , hidden_states ) \n return super ( ) . act ( input_a . squeeze ( <NUM_LIT> ) ) \n def act_inference ( self , observations , ** kwargs ) : \n input_a = self . memory_a ( observations , ** kwargs ) \n return super ( ) . act_inference ( input_a . squeeze ( <NUM_LIT> ) ) \n def evaluate ( self , critic_observations , masks = None , hidden_states = None ) : \n input_c = self . memory_c ( critic_observations , masks , hidden_states ) \n return super ( ) . evaluate ( input_c . squeeze ( <NUM_LIT> ) ) \n def get_hidden_states ( self ) : \n return self . memory_a . hidden_states , self . memory_c . hidden_states \n class Memory ( torch . nn . Module ) : \n def __init__ ( self , input_size , type = '<STR_LIT>' , num_layers = <NUM_LIT> , hidden_size = <NUM_LIT> ) : \n super ( ) . __init__ ( ) \n rnn_cls = nn . GRU if type . lower ( ) == '<STR_LIT>' else nn . LSTM \n self . rnn = rnn_cls ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers ) \n self . hidden_states = None \n def forward ( self , input , masks = None , hidden_states = None ) : \n batch_mode = masks is not None \n if batch_mode : \n if hidden_states is None : \n raise ValueError ( \"<STR_LIT>\" ) \n out , _ = self . rnn ( input , hidden_states )", "output": "out = unpad_trajectories ( out , masks ) \n else : \n out , self . hidden_states = self . rnn ( input . unsqueeze ( <NUM_LIT> ) , self . hidden_states ) \n return out \n def reset ( self , dones = None ) : \n for hidden_state in self . hidden_states : \n hidden_state [ ... , dones , : ] = <NUM_LIT>"}, {"input": "from mephisto . abstractions . databases . local_database import LocalMephistoDB \n from mephisto . tools . data_browser import DataBrowser \n from tqdm import tqdm \n import os \n import shutil \n import json \n from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_ANNOTATIONS_PATH \n TARGET_TASKS = [ \n '<STR_LIT>' \n ] \n ASSET_PATH = os . path . join ( os . path . dirname ( __file__ ) , '<STR_LIT>' ) \n def extract_final_data ( m_data ) : \n inputs = m_data [ '<STR_LIT>' ] \n outputs = m_data [ '<STR_LIT>' ] \n reconstructed_masks = inputs [ '<STR_LIT>' ] \n for mask_key in reconstructed_masks . keys ( ) : \n reconstructed_masks [ mask_key ] . update ( outputs [ '<STR_LIT>' ] [ mask_key ] ) \n return { \n '<STR_LIT>' : outputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : outputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] [ '<STR_LIT>' ] , \n '<STR_LIT>' : reconstructed_masks , \n '<STR_LIT>' : list ( outputs [ '<STR_LIT>' ] . keys ( ) ) , \n } \n def main ( ) : \n db = LocalMephistoDB ( ) \n browser = DataBrowser ( db ) \n approved_units = [ ] \n print ( \"<STR_LIT>\" ) \n for target in TARGET_TASKS : \n print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) \n units = browser . get_units_for_task_name ( target ) \n print ( f\"<STR_LIT>\" , end = \"<STR_LIT>\" , flush = True ) \n units = [ u for u in units if u . get_assigned_agent ( ) is not None and u . get_assigned_agent ( ) . get_status ( ) == '<STR_LIT>' ]", "output": "print ( f\"<STR_LIT>\" , flush = True ) \n approved_units += units \n print ( f\"<STR_LIT>\" ) \n for u in tqdm ( approved_units ) : \n try : \n data = u . get_assigned_agent ( ) . state . get_data ( ) \n formatted = extract_final_data ( data ) \n source_image_path = os . path . join ( ASSET_PATH , formatted [ '<STR_LIT>' ] ) \n target_image_path = os . path . join ( DATASET_PHOTO_PATH , formatted [ '<STR_LIT>' ] ) \n output_data_path = os . path . join ( DATASET_ANNOTATIONS_PATH , f\"<STR_LIT>\" ) \n shutil . copy2 ( source_image_path , target_image_path ) \n with open ( output_data_path , '<STR_LIT>' ) as output_file : \n json . dump ( formatted , output_file ) \n except OSError : \n print ( f\"<STR_LIT>\" ) \n continue \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "import argparse \n import random \n import requests \n import os \n from PIL import Image \n import gymnasium as gym \n from multi_token . constants import ROLE_USER \n LUNAR_LANDER_OPTIONS = ( \n \"<STR_LIT>\" . split ( \"<STR_LIT>\" ) \n ) \n MAX_STEPS = <NUM_LIT> \n def main ( args ) : \n env = gym . make ( \"<STR_LIT>\" , render_mode = \"<STR_LIT>\" ) \n env = gym . wrappers . RecordVideo ( env , args . video_folder ) \n env . reset ( ) \n for _ in range ( MAX_STEPS ) : \n img = env . render ( ) \n random . shuffle ( LUNAR_LANDER_OPTIONS ) \n options_str = \"<STR_LIT>\" . join ( LUNAR_LANDER_OPTIONS ) \n img_fn = os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n messages = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } , \n ] \n Image . fromarray ( img ) . save ( img_fn ) \n example = { \n \"<STR_LIT>\" : [ img_fn ] , \n \"<STR_LIT>\" : messages , \n } \n output = requests . post ( \n args . server_endpoint , \n json = example , \n ) . json ( ) [ \"<STR_LIT>\" ] \n print ( \"<STR_LIT>\" + output ) \n if output == \"<STR_LIT>\" : \n action = <NUM_LIT> \n elif output == \"<STR_LIT>\" : \n action = <NUM_LIT> \n elif output == \"<STR_LIT>\" : \n action = <NUM_LIT> \n else : \n action = <NUM_LIT> \n observation , reward , terminated , truncated , info = env . step ( action ) \n if terminated or truncated : \n break \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( )", "output": "parser . add_argument ( \n \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" \n ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )", "output": "op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n def downgrade ( ) -> None : \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}, {"input": "from legged_gym import LEGGED_GYM_ROOT_DIR \n import os \n import code \n import isaacgym \n from legged_gym . envs import * \n from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger \n from isaacgym import gymtorch , gymapi , gymutil \n import numpy as np \n import torch \n import cv2 \n from collections import deque \n import statistics \n import faulthandler \n from copy import deepcopy \n import matplotlib . pyplot as plt \n from time import time , sleep \n from legged_gym . utils import webviewer \n def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = \"<STR_LIT>\" ) : \n if checkpoint == - <NUM_LIT> : \n models = [ file for file in os . listdir ( root ) if model_name_include in file ] \n models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) \n model = models [ - <NUM_LIT> ] \n checkpoint = model . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n return model , checkpoint", "output": "def play ( args ) : \n if args . web : \n web_viewer = webviewer . WebViewer ( ) \n faulthandler . enable ( ) \n exptid = args . exptid \n log_pth = \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid \n env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) \n if args . nodelay : \n env_cfg . domain_rand . action_delay_view = <NUM_LIT> \n env_cfg . env . num_envs = <NUM_LIT> if not args . save else <NUM_LIT> \n env_cfg . env . episode_length_s = <NUM_LIT> \n env_cfg . commands . resampling_time = <NUM_LIT> \n env_cfg . terrain . num_rows = <NUM_LIT> \n env_cfg . terrain . num_cols = <NUM_LIT> \n env_cfg . terrain . height = [ <NUM_LIT> , <NUM_LIT> ] \n env_cfg . terrain . terrain_dict = { \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> } \n env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) \n env_cfg . terrain . curriculum = False \n env_cfg . terrain . max_difficulty = True \n env_cfg . depth . angle = [ <NUM_LIT> , <NUM_LIT> ] \n env_cfg . noise . add_noise = True \n env_cfg . domain_rand . randomize_friction = True \n env_cfg . domain_rand . push_robots = False \n env_cfg . domain_rand . push_interval_s = <NUM_LIT> \n env_cfg . domain_rand . randomize_base_mass = False \n env_cfg . domain_rand . randomize_base_com = False \n depth_latent_buffer = [ ] \n env : LeggedRobot \n env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) \n obs = env . get_observations ( ) \n if args . web : \n web_viewer . setup ( env ) \n train_cfg . runner . resume = True \n ppo_runner , train_cfg , log_pth = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args , train_cfg = train_cfg , return_log_dir = True ) \n if args . use_jit : \n path = os . path . join ( log_pth , \"<STR_LIT>\" ) \n model , checkpoint = get_load_path ( root = path , checkpoint = args . checkpoint ) \n path = os . path . join ( path , model ) \n print ( \"<STR_LIT>\" , path ) \n policy_jit = torch . jit . load ( path , map_location = env . device ) \n else : \n policy = ppo_runner . get_inference_policy ( device = env . device ) \n estimator = ppo_runner . get_estimator_inference_policy ( device = env . device ) \n if env . cfg . depth . use_camera : \n depth_encoder = ppo_runner . get_depth_encoder_inference_policy ( device = env . device ) \n actions = torch . zeros ( env . num_envs , <NUM_LIT> , device = env . device , requires_grad = False ) \n infos = { } \n infos [ \"<STR_LIT>\" ] = env . depth_buffer . clone ( ) . to ( ppo_runner . device ) [ : , - <NUM_LIT> ] if ppo_runner . if_depth else None \n for i in range ( <NUM_LIT> * int ( env . max_episode_length ) ) : \n if args . use_jit : \n if env . cfg . depth . use_camera : \n if infos [ \"<STR_LIT>\" ] is not None : \n depth_latent = torch . ones ( ( env_cfg . env . num_envs , <NUM_LIT> ) , device = env . device ) \n actions , depth_latent = policy_jit ( obs . detach ( ) , True , infos [ \"<STR_LIT>\" ] , depth_latent ) \n else : \n depth_buffer = torch . ones ( ( env_cfg . env . num_envs , <NUM_LIT> , <NUM_LIT> ) , device = env . device ) \n actions , depth_latent = policy_jit ( obs . detach ( ) , False , depth_buffer , depth_latent ) \n else : \n obs_jit = torch . cat ( ( obs . detach ( ) [ : , : env_cfg . env . n_proprio + env_cfg . env . n_priv ] , obs . detach ( ) [ : , - env_cfg . env . history_len * env_cfg . env . n_proprio : ] ) , dim = <NUM_LIT> ) \n actions = policy ( obs_jit ) \n else : \n if env . cfg . depth . use_camera : \n if infos [ \"<STR_LIT>\" ] is not None : \n obs_student = obs [ : , : env . cfg . env . n_proprio ] . clone ( ) \n obs_student [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> \n depth_latent_and_yaw = depth_encoder ( infos [ \"<STR_LIT>\" ] , obs_student ) \n depth_latent = depth_latent_and_yaw [ : , : - <NUM_LIT> ] \n yaw = depth_latent_and_yaw [ : , - <NUM_LIT> : ] \n obs [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> * yaw \n else : \n depth_latent = None \n if hasattr ( ppo_runner . alg , \"<STR_LIT>\" ) : \n actions = ppo_runner . alg . depth_actor ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) \n else : \n actions = policy ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) \n obs , _ , rews , dones , infos = env . step ( actions . detach ( ) ) \n if args . web : \n web_viewer . render ( fetch_results = True , \n step_graphics = True , \n render_all_camera_sensors = True , \n wait_for_page_load = True ) \n print ( \"<STR_LIT>\" , env . episode_length_buf [ env . lookat_id ] . item ( ) / <NUM_LIT> , \n \"<STR_LIT>\" , env . commands [ env . lookat_id , <NUM_LIT> ] . item ( ) , \n \"<STR_LIT>\" , env . base_lin_vel [ env . lookat_id , <NUM_LIT> ] . item ( ) , ) \n id = env . lookat_id \n if __name__ == '<STR_LIT>' : \n EXPORT_POLICY = False \n RECORD_FRAMES = False \n MOVE_CAMERA = False \n args = get_args ( ) \n play ( args )"}, {"input": "", "output": "class Figura : \n def __init__ ( self , base , altura ) : \n self . base = base \n self . altura = altura \n class Poligono : \n def dar_nombre ( self , nombre ) : \n return f\"<STR_LIT>\" \n class Rectangulo ( Figura , Poligono ) : \n def calcular_area ( self ) : \n return self . base * self . altura \n class Triangulo ( Figura ) : \n def calcular_area ( self ) : \n return ( self . base * self . altura ) // <NUM_LIT> \n rectangulo = Rectangulo ( <NUM_LIT> , <NUM_LIT> ) \n triangulo = Triangulo ( <NUM_LIT> , <NUM_LIT> ) \n print ( \"<STR_LIT>\" , rectangulo . calcular_area ( ) ) \n print ( \"<STR_LIT>\" , triangulo . calcular_area ( ) ) \n print ( rectangulo . dar_nombre ( \"<STR_LIT>\" ) )"}, {"input": "import sys \n import importlib \n import copy \n from io import StringIO \n from contextlib import redirect_stdout \n from tempfile import TemporaryDirectory \n import pytest \n import instld \n def test_polog_install_and_import ( ) : \n with instld ( '<STR_LIT>' ) : \n import polog \n importlib . reload ( polog ) \n def test_polog_install_and_import_with_logger_none ( ) : \n with instld ( '<STR_LIT>' , logger = None ) : \n import polog \n importlib . reload ( polog ) \n def test_polog_install_two_and_import ( ) : \n with instld ( '<STR_LIT>' , '<STR_LIT>' ) : \n import polog \n import astrologic \n importlib . reload ( polog ) \n importlib . reload ( astrologic ) \n def test_polog_install_two_contexts_and_import ( ) : \n with instld ( '<STR_LIT>' ) : \n with instld ( '<STR_LIT>' ) : \n import polog \n import astrologic \n importlib . reload ( polog ) \n importlib . reload ( astrologic ) \n def test_deleting_contexts ( ) : \n with instld ( '<STR_LIT>' ) : \n with instld ( '<STR_LIT>' ) : \n pass \n with pytest . raises ( ModuleNotFoundError ) : \n import polog \n importlib . reload ( polog ) \n def test_sys_path_lenth ( ) : \n number_before = len ( sys . path ) \n sys_path_copy = copy . copy ( sys . path ) \n with instld ( '<STR_LIT>' ) : \n assert len ( sys . path ) == number_before + <NUM_LIT> \n assert sys . path [ <NUM_LIT> : ] == sys_path_copy \n assert len ( sys . path ) == number_before \n def test_fazy_install_and_autoimport ( ) : \n with instld ( '<STR_LIT>' ) as package : \n f = package . import_here ( '<STR_LIT>' ) \n assert f ( '<STR_LIT>' ) == '<STR_LIT>' \n def test_super_project ( ) : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package :", "output": "module = package . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : \n module = package . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : \n module = package . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package : \n module = package . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> \n def test_super_project_nested ( ) : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_1 : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_2 : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) as package_3 : \n module_1 = package_1 . import_here ( '<STR_LIT>' ) \n module_2 = package_2 . import_here ( '<STR_LIT>' ) \n module_3 = package_3 . import_here ( '<STR_LIT>' ) \n assert module_1 . version == '<STR_LIT>' \n assert module_2 . version == '<STR_LIT>' \n assert module_3 . version == '<STR_LIT>' \n def test_catch_output_default ( ) : \n with redirect_stdout ( StringIO ( ) ) as context : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' ) : \n pass \n assert len ( context . getvalue ( ) ) > <NUM_LIT> \n def test_catch_output_false ( ) : \n with redirect_stdout ( StringIO ( ) ) as context : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' , catch_output = False ) : \n pass \n assert len ( context . getvalue ( ) ) > <NUM_LIT> \n def test_catch_output_true ( ) : \n with redirect_stdout ( StringIO ( ) ) as context : \n with instld ( '<STR_LIT>' , index_url = '<STR_LIT>' , catch_output = True ) : \n pass \n assert len ( context . getvalue ( ) ) == <NUM_LIT> \n def test_install_from_requirements_file ( ) : \n with instld ( index_url = '<STR_LIT>' , r = '<STR_LIT>' ) as context : \n module = context . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n def test_set_where_and_check_path ( ) : \n with TemporaryDirectory ( ) as where : \n with instld ( where = where ) : \n assert sys . path [ <NUM_LIT> ] . startswith ( where ) \n assert not sys . path [ <NUM_LIT> ] . startswith ( where ) \n def test_install_after ( ) : \n with instld ( ) as context : \n context . install ( '<STR_LIT>' , index_url = '<STR_LIT>' ) \n module = context . import_here ( '<STR_LIT>' ) \n assert module . version == '<STR_LIT>' \n assert module . function ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT>"}, {"input": "from densely_captioned_images . repro . eval . run_full_evals import CLIPEvalConfig , CLIPEvalJob \n from densely_captioned_images . repro . config import MODEL_PATH , DENSE_CAPS_DIR \n import submitit \n import os \n def main ( ) : \n with open ( os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) ) as f : \n MODEL_FILES = [ \n os . path . join ( MODEL_PATH , m . strip ( ) ) \n for m in f . readlines ( ) \n ] \n print ( \"<STR_LIT>\" , len ( MODEL_FILES ) ) \n eval_sweep = [ \n CLIPEvalConfig ( \n run_aro = True , \n run_vlc = True , \n run_dense_cap = True , \n run_winoground = True , \n run_elevater = - <NUM_LIT> , \n lora_weight_location = model_path ,", "output": "model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n ) for model_path in MODEL_FILES \n ] \n log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f\"<STR_LIT>\" ) \n os . makedirs ( log_base_folder , exist_ok = True ) \n log_folder = f\"<STR_LIT>\" \n executor = submitit . AutoExecutor ( folder = log_folder ) \n executor . update_parameters ( \n slurm_partition = '<STR_LIT>' , \n nodes = <NUM_LIT> , \n timeout_min = <NUM_LIT> * <NUM_LIT> , \n tasks_per_node = <NUM_LIT> , \n gpus_per_node = <NUM_LIT> , \n cpus_per_task = <NUM_LIT> , \n slurm_mem = '<STR_LIT>' , \n ) \n job_array = [ ] \n for sweep_args in eval_sweep : \n job = executor . submit ( CLIPEvalJob ( ) , sweep_args ) \n job_array . append ( job ) \n print ( f\"<STR_LIT>\" ) \n for job in job_array : \n _ = job . result ( ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "import io \n import os \n from dotenv import load_dotenv \n from PIL import Image \n def fsize ( file ) : \n if isinstance ( file , io . BytesIO ) : \n return file . getbuffer ( ) . nbytes \n elif isinstance ( file , str ) : \n return os . path . getsize ( file ) \n elif hasattr ( file , \"<STR_LIT>\" ) and hasattr ( file , \"<STR_LIT>\" ) : \n pos = file . tell ( ) \n file . seek ( <NUM_LIT> , os . SEEK_END ) \n size = file . tell ( ) \n file . seek ( pos ) \n return size \n else : \n raise TypeError ( \"<STR_LIT>\" ) \n def compress_imgfile ( file , max_size ) : \n if fsize ( file ) <= max_size : \n return file \n file . seek ( <NUM_LIT> ) \n img = Image . open ( file ) \n rgb_image = img . convert ( \"<STR_LIT>\" )", "output": "quality = <NUM_LIT> \n while True : \n out_buf = io . BytesIO ( ) \n rgb_image . save ( out_buf , \"<STR_LIT>\" , quality = quality ) \n if fsize ( out_buf ) <= max_size : \n return out_buf \n quality -= <NUM_LIT> \n def split_string_by_utf8_length ( string , max_length , max_split = <NUM_LIT> ) : \n encoded = string . encode ( \"<STR_LIT>\" ) \n start , end = <NUM_LIT> , <NUM_LIT> \n result = [ ] \n while end < len ( encoded ) : \n if max_split > <NUM_LIT> and len ( result ) >= max_split : \n result . append ( encoded [ start : ] . decode ( \"<STR_LIT>\" ) ) \n break \n end = min ( start + max_length , len ( encoded ) ) \n while end < len ( encoded ) and ( encoded [ end ] & <NUM_LIT> ) == <NUM_LIT> : \n end -= <NUM_LIT> \n result . append ( encoded [ start : end ] . decode ( \"<STR_LIT>\" ) ) \n start = end \n return result \n load_dotenv ( ) \n def get_cookie ( ) : \n cookie = os . getenv ( '<STR_LIT>' ) \n print ( cookie ) \n if not cookie : \n raise ValueError ( \"<STR_LIT>\" ) \n return cookie \n def get_proxy ( ) -> bool : \n isproxy = os . getenv ( '<STR_LIT>' ) \n print ( isproxy ) \n if not isproxy : \n return False \n else : \n return True if isproxy . lower ( ) == '<STR_LIT>' else False"}, {"input": "from channel . channel import Channel \n from common . log import logger \n from config import conf , common_conf_val , channel_conf \n import ssl \n import discord \n from discord . ext import commands \n class DiscordChannel ( Channel ) : \n def __init__ ( self ) : \n config = conf ( ) \n self . token = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) \n self . discord_channel_name = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) \n self . discord_channel_session = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' , '<STR_LIT>' ) \n self . voice_enabled = channel_conf ( '<STR_LIT>' ) . get ( '<STR_LIT>' , False ) \n self . cmd_clear_session = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) [ <NUM_LIT> ] \n self . sessions = [ ] \n self . intents = discord . Intents . default ( ) \n self . intents . message_content = True \n self . intents . guilds = True \n self . intents . members = True \n self . intents . messages = True \n self . intents . voice_states = True \n context = ssl . create_default_context ( ) \n context . load_verify_locations ( common_conf_val ( '<STR_LIT>' ) ) \n self . bot = commands . Bot ( command_prefix = '<STR_LIT>' , intents = self . intents , ssl = context ) \n self . bot . add_listener ( self . on_ready ) \n logger . debug ( '<STR_LIT>' , self . cmd_clear_session ) \n def startup ( self ) : \n self . bot . add_listener ( self . on_message ) \n self . bot . add_listener ( self . on_guild_channel_delete ) \n self . bot . add_listener ( self . on_guild_channel_create ) \n self . bot . add_listener ( self . on_private_channel_delete ) \n self . bot . add_listener ( self . on_private_channel_create ) \n self . bot . add_listener ( self . on_channel_delete ) \n self . bot . add_listener ( self . on_channel_create ) \n self . bot . add_listener ( self . on_thread_delete ) \n self . bot . add_listener ( self . on_thread_create ) \n self . bot . run ( self . token ) \n async def on_ready ( self ) : \n logger . info ( '<STR_LIT>' . format ( self . bot . user ) ) \n if self . voice_enabled == False : \n logger . debug ( '<STR_LIT>' ) \n await self . bot . remove_cog ( \"<STR_LIT>\" ) \n async def join ( self , ctx ) : \n logger . debug ( '<STR_LIT>' , repr ( ctx ) ) \n channel = ctx . author . voice . channel \n await channel . connect ( ) \n async def _do_on_channel_delete ( self , channel ) : \n if not self . discord_channel_name or channel . name != self . discord_channel_name : \n logger . debug ( '<STR_LIT>' , channel . name ) \n return \n for name in self . sessions : \n try : \n response = self . send_text ( name , self . cmd_clear_session ) \n logger . debug ( '<STR_LIT>' , channel . name , response ) \n except Exception as e : \n logger . warn ( '<STR_LIT>' , name ) \n self . sessions . clear ( ) \n async def on_guild_channel_delete ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n await self . _do_on_channel_delete ( channel ) \n async def on_guild_channel_create ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n async def on_private_channel_delete ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n await self . _do_on_channel_delete ( channel ) \n async def on_private_channel_create ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n async def on_channel_delete ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n async def on_channel_create ( self , channel ) : \n logger . debug ( '<STR_LIT>' , repr ( channel ) ) \n async def on_thread_delete ( self , thread ) : \n print ( '<STR_LIT>' , thread ) \n if self . discord_channel_session != '<STR_LIT>' or thread . parent . name != self . discord_channel_name : \n logger . debug ( '<STR_LIT>' , thread . id ) \n return \n try : \n response = self . send_text ( thread . id , self . cmd_clear_session ) \n if thread . id in self . sessions : \n self . sessions . remove ( thread . id ) \n logger . debug ( '<STR_LIT>' , thread . id , response ) \n except Exception as e : \n logger . warn ( '<STR_LIT>' , thread . id ) \n raise e \n async def on_thread_create ( self , thread ) : \n logger . debug ( '<STR_LIT>' , thread . id ) \n if self . discord_channel_session != '<STR_LIT>' or thread . parent . name != self . discord_channel_name : \n logger . debug ( '<STR_LIT>' , repr ( thread ) ) \n return \n self . sessions . append ( thread . id ) \n async def on_message ( self , message ) : \n await self . bot . wait_until_ready ( ) \n if not self . check_message ( message ) : \n return \n prompt = message . content . strip ( ) ; \n logger . debug ( '<STR_LIT>' , message . author ) \n logger . debug ( '<STR_LIT>' , prompt ) \n session_id = message . author \n if self . discord_channel_session == '<STR_LIT>' and isinstance ( message . channel , discord . Thread ) : \n logger . debug ( '<STR_LIT>' , message . channel . id ) \n session_id = message . channel . id \n await message . channel . send ( '<STR_LIT>' ) \n response = response = self . send_text ( session_id , prompt ) \n await message . channel . send ( response ) \n def check_message ( self , message ) : \n if message . author == self . bot . user : \n return False \n prompt = message . content . strip ( ) ; \n if not prompt : \n logger . debug ( '<STR_LIT>' , message . author ) \n return False", "output": "if self . discord_channel_name : \n if isinstance ( message . channel , discord . Thread ) and message . channel . parent . name == self . discord_channel_name : \n return True \n if not isinstance ( message . channel , discord . Thread ) and self . discord_channel_session != '<STR_LIT>' and message . channel . name == self . discord_channel_name : \n return True \n logger . debug ( \"<STR_LIT>\" ) \n return False \n else : \n return True \n def send_text ( self , id , content ) : \n context = dict ( ) \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n context [ '<STR_LIT>' ] = id \n context [ '<STR_LIT>' ] = content \n return super ( ) . build_reply_content ( content , context )"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class A1RoughCfg ( LeggedRobotCfg ) : \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class control ( LeggedRobotCfg . control ) : \n control_type = '<STR_LIT>' \n stiffness = { '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset ( LeggedRobotCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ]", "output": "terminate_after_contacts_on = [ \"<STR_LIT>\" ] \n self_collisions = <NUM_LIT> \n class rewards ( LeggedRobotCfg . rewards ) : \n soft_dof_pos_limit = <NUM_LIT> \n base_height_target = <NUM_LIT> \n class scales ( LeggedRobotCfg . rewards . scales ) : \n torques = <NUM_LIT> \n dof_pos_limits = - <NUM_LIT> \n class A1RoughCfgPPO ( LeggedRobotCfgPPO ) : \n class algorithm ( LeggedRobotCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT> \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>'"}, {"input": "from typing import Dict , List , Any , Union , Optional \n from collections import Counter \n from functools import cache \n import contextlib \n import tempfile \n import shutil \n import random \n import subprocess \n import json \n import re \n import io \n import os \n import torch \n import requests \n import transformers \n import numpy as np \n from datasets import load_dataset , Dataset \n from PIL import Image \n from multi_token . constants import IGNORE_INDEX \n def encode_chat ( \n item : Dict , \n tokenizer : transformers . PreTrainedTokenizer , \n modalities : List [ \"<STR_LIT>\" ] , \n ) -> Dict : \n messages = list ( item [ \"<STR_LIT>\" ] ) \n chat_as_string = tokenizer . apply_chat_template ( messages , tokenize = False ) \n token_to_modality = { m . token : m for m in modalities } \n modality_token_counts = Counter ( ) \n instruct_pattern = r\"<STR_LIT>\" \n pattern = \"<STR_LIT>\" + \"<STR_LIT>\" . join ( re . escape ( m . token ) for m in modalities ) + \"<STR_LIT>\" \n chat_part = re . split ( instruct_pattern , chat_as_string ) \n input_ids = [ ] \n labels = [ ] \n for part in chat_part : \n if \"<STR_LIT>\" in part : \n is_instruction = True \n else : \n is_instruction = False \n for subpart in re . split ( pattern , part ) : \n if not subpart : \n continue \n if subpart in token_to_modality : \n assert ( \n is_instruction \n ) , \"<STR_LIT>\"", "output": "m = token_to_modality [ subpart ] \n modality_token_counts [ m . name ] += <NUM_LIT> \n input_ids . extend ( [ m . token_idx ] * m . token_width ) \n labels . extend ( [ IGNORE_INDEX ] * m . token_width ) \n elif is_instruction : \n part_ids = tokenizer ( subpart , add_special_tokens = False ) . input_ids \n input_ids . extend ( part_ids ) \n labels . extend ( [ IGNORE_INDEX ] * len ( part_ids ) ) \n else : \n part_ids = tokenizer ( subpart , add_special_tokens = False ) . input_ids \n input_ids . extend ( part_ids ) \n labels . extend ( part_ids ) \n input_ids = torch . tensor ( input_ids , dtype = torch . long ) \n labels = torch . tensor ( labels , dtype = torch . long ) \n data_dict = dict ( \n input_ids = input_ids , \n labels = labels , \n ) \n for m in modalities : \n data_dict [ m . name ] = m . preprocess_rows ( [ item ] ) [ <NUM_LIT> ] \n return data_dict \n def load_image ( value : Any ) -> Image . Image : \n img = None \n if isinstance ( value , str ) : \n if value . startswith ( \"<STR_LIT>\" ) or value . startswith ( \"<STR_LIT>\" ) : \n response = requests . get ( value ) \n img = Image . open ( io . BytesIO ( response . content ) ) \n elif os . path . exists ( value ) : \n img = Image . open ( value ) \n elif isinstance ( value , Image . Image ) : \n img = value \n if img is None : \n raise ValueError ( f\"<STR_LIT>\" ) \n img = img . convert ( \"<STR_LIT>\" ) \n return img \n @ contextlib . contextmanager \n def with_local_files ( fn_or_urls : List [ Any ] ) : \n local_fns = [ ] \n fps = [ ] \n for fn_or_url in fn_or_urls : \n if isinstance ( fn_or_url , Image . Image ) : \n fp = tempfile . NamedTemporaryFile ( suffix = \"<STR_LIT>\" , mode = \"<STR_LIT>\" ) \n fn_or_url . convert ( \"<STR_LIT>\" ) . save ( fp ) \n fps . append ( fp ) \n local_fns . append ( fp . name ) \n elif fn_or_url . startswith ( \"<STR_LIT>\" ) or fn_or_url . startswith ( \"<STR_LIT>\" ) : \n suffix = os . path . splitext ( fn_or_url ) [ - <NUM_LIT> ] \n with requests . get ( fn_or_url , stream = True ) as r : \n fp = tempfile . NamedTemporaryFile ( suffix = suffix , mode = \"<STR_LIT>\" ) \n shutil . copyfileobj ( r . raw , fp ) \n fps . append ( fp ) \n local_fns . append ( fp . name ) \n else : \n local_fns . append ( fn_or_url ) \n try : \n yield local_fns \n finally : \n for fp in fps : \n fp . close ( ) \n @ cache \n def _get_dataset ( dataset_args : str ) -> Dataset : \n return load_dataset ( ** json . loads ( dataset_args ) ) \n def get_dataset_cached ( dataset_args : Dict ) -> Dataset : \n return _get_dataset ( json . dumps ( dataset_args ) ) \n def load_audio ( input_ : Union [ Dict , str ] , target_sampling_rate : int = None ) -> Dict : \n import soundfile as sf \n import librosa \n if isinstance ( input_ , dict ) and \"<STR_LIT>\" in input_ and \"<STR_LIT>\" in input_ : \n array = input_ [ \"<STR_LIT>\" ] \n sampling_rate = input_ [ \"<STR_LIT>\" ] \n elif isinstance ( input_ , dict ) and \"<STR_LIT>\" in input_ : \n item = get_dataset_cached ( input_ [ \"<STR_LIT>\" ] ) [ input_ [ \"<STR_LIT>\" ] ] \n array = item [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n sampling_rate = item [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n elif isinstance ( input_ , dict ) and \"<STR_LIT>\" in input_ : \n with with_local_files ( [ input_ [ \"<STR_LIT>\" ] ] ) as local_fns : \n array , sampling_rate = sf . read ( local_fns [ <NUM_LIT> ] ) \n elif isinstance ( input_ , str ) : \n with with_local_files ( [ input_ ] ) as local_fns : \n array , sampling_rate = sf . read ( local_fns [ <NUM_LIT> ] ) \n else : \n raise ValueError ( f\"<STR_LIT>\" ) \n if array . ndim == <NUM_LIT> : \n array = array . mean ( axis = <NUM_LIT> ) \n if target_sampling_rate is not None and sampling_rate != target_sampling_rate : \n array = librosa . resample ( \n array , orig_sr = sampling_rate , target_sr = target_sampling_rate \n ) \n sampling_rate = target_sampling_rate \n return { \"<STR_LIT>\" : list ( array ) , \"<STR_LIT>\" : sampling_rate } \n def _download_yt_video ( url : str ) -> str : \n from pytube import YouTube \n youtube = YouTube ( url ) \n video = youtube . streams . first ( ) \n fn = \"<STR_LIT>\" . join ( random . choices ( \"<STR_LIT>\" , k = <NUM_LIT> ) ) \n file_path = video . download ( output_path = tempfile . gettempdir ( ) , filename = fn ) \n return file_path \n def _read_video_pyav ( container , indices ) : \n frames = [ ] \n container . seek ( <NUM_LIT> ) \n start_index = indices [ <NUM_LIT> ] \n end_index = indices [ - <NUM_LIT> ] \n for i , frame in enumerate ( container . decode ( video = <NUM_LIT> ) ) : \n if i > end_index : \n break \n if i >= start_index and i in indices : \n frames . append ( frame ) \n return np . stack ( [ x . to_ndarray ( format = \"<STR_LIT>\" ) for x in frames ] ) \n def _sample_frame_indices ( clip_len , frame_sample_rate , seg_len ) : \n converted_len = int ( clip_len * frame_sample_rate ) \n end_idx = np . random . randint ( converted_len , seg_len ) \n start_idx = end_idx - converted_len \n indices = np . linspace ( start_idx , end_idx , num = clip_len ) \n indices = np . clip ( indices , start_idx , end_idx - <NUM_LIT> ) . astype ( np . int64 ) \n return indices \n def load_video ( \n input_ : str , \n frames : int = <NUM_LIT> , \n frame_sample_rate : int = <NUM_LIT> , \n start_time : Optional [ int ] = None , \n end_time : Optional [ int ] = None , \n ) -> np . ndarray : \n import av \n delete_file = False \n if isinstance ( input_ , dict ) and \"<STR_LIT>\" and input_ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n file_path = _download_yt_video ( input_ [ \"<STR_LIT>\" ] ) \n delete_file = True \n elif isinstance ( input_ , str ) and \"<STR_LIT>\" in input_ : \n file_path = _download_yt_video ( input_ ) \n delete_file = True \n elif isinstance ( input_ , str ) : \n file_path = input_ \n else : \n raise ValueError ( f\"<STR_LIT>\" ) \n if start_time is not None or end_time is not None : \n start_time = start_time if start_time is not None else <NUM_LIT> \n end_time = end_time if end_time is not None else \"<STR_LIT>\" \n trim_file_path = f\"<STR_LIT>\" \n subprocess . run ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n file_path , \n \"<STR_LIT>\" , \n str ( start_time ) , \n \"<STR_LIT>\" , \n str ( end_time ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n trim_file_path , \n ] \n ) \n file_path = trim_file_path \n container = av . open ( file_path ) \n indices = _sample_frame_indices ( \n clip_len = frames , \n frame_sample_rate = frame_sample_rate , \n seg_len = container . streams . video [ <NUM_LIT> ] . frames , \n ) \n video = _read_video_pyav ( container , indices ) \n if delete_file : \n os . remove ( file_path ) \n return video"}, {"input": "import base64 \n import mutagen . apev2 \n import mutagen . wavpack \n import mutagen . musepack \n import mutagen . monkeysaudio \n import mutagen . optimfrog \n from mutagen . id3 import PictureType \n from . import util \n from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY \n pic_type2tag = { \n PictureType . COVER_FRONT : '<STR_LIT>' , \n PictureType . COVER_BACK : '<STR_LIT>' , \n } \n pic_tag2type = { } \n for key , val in pic_type2tag . items ( ) : \n pic_tag2type [ val ] = key \n del key , val \n def get_tracknum ( afile , norm_key ) : \n return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_tracknum ( afile , norm_key , val ) : \n return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaltracks ( afile , norm_key ) : \n return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaltracks ( afile , norm_key , val ) : \n return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_discnum ( afile , norm_key ) : \n return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_discnum ( afile , norm_key , val ) : \n return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaldiscs ( afile , norm_key ) : \n return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaldiscs ( afile , norm_key , val ) : \n return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_pictures ( afile , norm_key ) : \n artworks = [ ] \n for pic_tag , pic_type in pic_tag2type . items ( ) :", "output": "if pic_tag in afile . mfile . tags : \n p = afile . mfile . tags [ pic_tag ] . value \n try : \n artwork = Artwork ( p , pic_type = pic_type ) \n except OSError : \n artwork = Artwork ( p . split ( b'<STR_LIT>' , <NUM_LIT> ) [ <NUM_LIT> ] , pic_type = pic_type ) \n artworks . append ( artwork ) \n return MetadataItem ( Artwork , None , artworks ) \n def set_pictures ( afile , norm_key , artworks ) : \n for art in artworks . values : \n pic_tag = pic_type2tag [ art . pic_type ] \n raw = ( pic_tag + '<STR_LIT>' ) . encode ( '<STR_LIT>' ) + b'<STR_LIT>' + art . raw \n afile . mfile . tags [ pic_tag ] = raw \n class Apev2File ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . apev2 . APEv2File \n _TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknum , \n setter = set_tracknum , \n remover = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracks , \n setter = set_totaltracks , \n remover = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnum , \n setter = set_discnum , \n remover = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscs , \n setter = set_totaldiscs , \n remover = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int , sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n remover = list ( pic_tag2type . keys ( ) ) , \n type = Artwork ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n } \n def _ft_getter ( self , key ) : \n val = self . mfile . tags . get ( key , None ) \n if val is not None : \n val = str ( val ) \n return val \n def _ft_setter ( self , key , md_val , appendable = True ) : \n self . mfile . tags [ key ] = str ( md_val ) \n class WavePackFile ( Apev2File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . wavpack . WavPack \n _TAG_MAP = Apev2File . _TAG_MAP . copy ( ) \n _TAG_MAP . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n } ) \n class MusepackFile ( Apev2File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . musepack . Musepack \n class MonkeysAudioFile ( Apev2File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . monkeysaudio . MonkeysAudio \n class OptimFrogFile ( Apev2File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . optimfrog . OptimFROG"}, {"input": "from __future__ import annotations \n from collections . abc import Callable \n from typing import Any , TypeVar , cast , overload \n from flask import Flask , current_app , g , has_app_context \n from flask . ctx import _AppCtxGlobals \n from werkzeug . local import LocalProxy \n from . _core import ( \n _KEY_CONTAINER , \n _KEY_REGISTRY , \n T1 , \n T2 , \n T3 , \n T4 , \n T5 , \n T6 , \n T7 , \n T8 , \n T9 , \n T10 , \n Container , \n Registry , \n ServicePing , \n ) \n def svcs_from ( g : _AppCtxGlobals = g ) -> Container : \n if ( con := g . get ( _KEY_CONTAINER , None ) ) is None : \n con = Container ( current_app . extensions [ _KEY_REGISTRY ] ) \n setattr ( g , _KEY_CONTAINER , con ) \n return con \n def get_registry ( app : Flask | None = None ) -> Registry : \n if app is None : \n app = current_app \n return app . extensions [ _KEY_REGISTRY ] \n registry = cast ( Registry , LocalProxy ( get_registry ) ) \n container = cast ( Container , LocalProxy ( svcs_from ) ) \n FlaskAppT = TypeVar ( \"<STR_LIT>\" , bound = Flask ) \n def init_app ( app : FlaskAppT , * , registry : Registry | None = None ) -> FlaskAppT : \n app . extensions [ _KEY_REGISTRY ] = registry or Registry ( ) \n app . teardown_appcontext ( teardown ) \n return app \n def get_abstract ( * svc_types : type ) -> Any : \n return get ( * svc_types ) \n def register_factory ( \n app : Flask , \n svc_type : type , \n factory : Callable , \n * , \n enter : bool = True , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n app . extensions [ _KEY_REGISTRY ] . register_factory ( \n svc_type , \n factory , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n def register_value ( \n app : Flask , \n svc_type : type , \n value : object , \n * , \n enter : bool = False , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n app . extensions [ _KEY_REGISTRY ] . register_value ( \n svc_type , \n value ,", "output": "enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n def overwrite_factory ( \n svc_type : type , \n factory : Callable , \n * , \n enter : bool = True , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n container = svcs_from ( ) \n container . registry . register_factory ( \n svc_type , \n factory , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n container . close ( ) \n def overwrite_value ( \n svc_type : type , \n value : object , \n * , \n enter : bool = True , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n container = svcs_from ( ) \n container . registry . register_value ( \n svc_type , \n value , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n container . close ( ) \n def get_pings ( ) -> list [ ServicePing ] : \n return svcs_from ( g ) . get_pings ( ) \n def teardown ( exc : BaseException | None ) -> None : \n if has_app_context ( ) and ( container := g . pop ( _KEY_CONTAINER , None ) ) : \n container . close ( ) \n def close_registry ( app : Flask ) -> None : \n if reg := app . extensions . pop ( _KEY_REGISTRY , None ) : \n reg . close ( ) \n @ overload \n def get ( svc_type : type [ T1 ] , / ) -> T1 : ... \n @ overload \n def get ( svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / ) -> tuple [ T1 , T2 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , svc_type3 : type [ T3 ] , / \n ) -> tuple [ T1 , T2 , T3 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... \n @ overload \n def get ( \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n svc_type10 : type [ T10 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... \n def get ( * svc_types : type ) -> object : \n return svcs_from ( g ) . get ( * svc_types )"}, {"input": "import os \n import json \n from transformers import CLIPProcessor , CLIPModel \n from long_captions . dense_image import DenseCaptionedImage , get_key_for , get_dci_count \n from long_captions . config import DATASET_COMPLETE_PATH \n from tqdm import tqdm \n from typing import Tuple \n clip_processor = None", "output": "clip_model = None \n def get_clip ( ) -> Tuple [ CLIPModel , CLIPProcessor ] : \n global clip_processor , clip_model \n if clip_processor is None : \n clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n if clip_model is None : \n clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n return clip_model , clip_processor \n def get_clip_scores ( dci : DenseCaptionedImage ) : \n did_nothing = True \n scores = dci . _data . get ( '<STR_LIT>' , { } ) \n summaries = dci . get_summaries ( ) \n negatives = dci . get_negatives ( ) \n mask_exs = [ dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] for m in dci . filter_masks_by_size ( ) ] \n exs = dci . get_formatted_complete_description ( ) + mask_exs \n clip_model , clip_processor = get_clip ( ) \n for ex in exs : \n key = ex [ '<STR_LIT>' ] \n if key in scores : \n continue \n summary = summaries [ key ] \n negs = negatives [ key ] \n subkeys = [ '<STR_LIT>' ] \n texts = [ summary ] \n for neg_type , typed_negs in negs . items ( ) : \n for idx , typed_neg in enumerate ( typed_negs ) : \n texts . append ( typed_neg ) \n subkeys . append ( f\"<STR_LIT>\" ) \n inputs = clip_processor ( \n text = texts , \n images = [ ex [ '<STR_LIT>' ] ] , \n return_tensors = \"<STR_LIT>\" , \n padding = True \n ) \n outputs = clip_model ( ** inputs ) \n logits_per_image = outputs . logits_per_image . detach ( ) . cpu ( ) . numpy ( ) [ <NUM_LIT> ] . tolist ( ) \n scores [ key ] = { sk : logit for sk , logit in zip ( subkeys , logits_per_image ) } \n did_nothing = False \n assert not did_nothing \n return scores \n def generate_and_write_clip_scores ( ) : \n count = get_dci_count ( ) \n for i in tqdm ( range ( count ) ) : \n key = get_key_for ( i ) \n source_path = os . path . join ( DATASET_COMPLETE_PATH , key ) \n if not os . path . exists ( source_path ) : \n continue \n dci = DenseCaptionedImage ( i ) \n try : \n clip_scores = get_clip_scores ( dci ) \n except Exception : \n continue \n with open ( source_path ) as jsonf : \n base_data = json . load ( jsonf ) \n base_data [ '<STR_LIT>' ] = clip_scores \n with open ( source_path , '<STR_LIT>' ) as jsonf : \n json . dump ( base_data , jsonf ) \n if __name__ == \"<STR_LIT>\" : \n generate_and_write_clip_scores ( )"}, {"input": "def pretrained_selector ( pitch_guidance ) : \n if pitch_guidance : \n return { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n } , \n } \n else : \n return { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": ") , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n } , \n }"}, {"input": "from typing import Dict , Union \n from . const import CONFIG , WafFunc \n from . full_payload_gen import FullPayloadGen \n full_payload_store : Dict [ int , FullPayloadGen ] = { } \n def config_payload ( waf_func : WafFunc ) -> Union [ str , None ] : \n full_payload = None \n if id ( waf_func ) not in full_payload_store : \n full_payload = FullPayloadGen ( waf_func ) \n full_payload_store [ id ( waf_func ) ] = full_payload \n else : \n full_payload = full_payload_store [ id ( waf_func ) ] \n payload , will_print = full_payload . generate ( CONFIG )", "output": "if not will_print : \n return None \n return payload"}, {"input": "import hashlib \n from mod . auth import webui \n from mod . auth . authentication import require_auth \n from . import * \n import os \n import requests \n from urllib . parse import urlparse \n from flask import request , render_template_string , send_from_directory \n from werkzeug . utils import secure_filename \n from mod . tools import calculate_md5 \n class Wget : \n def __init__ ( self , url : str , headers : dict = None , save_file : str = None , chunk_size : int = <NUM_LIT> * <NUM_LIT> ) : \n self . url = url \n self . headers = headers or { '<STR_LIT>' : '<STR_LIT>' } \n self . save_file = save_file or os . path . join ( os . getcwd ( ) , os . path . basename ( urlparse ( url ) . path ) ) \n self . chunk_size = chunk_size \n self . temp_file = calculate_md5 ( self . url ) + '<STR_LIT>' \n def __enter__ ( self ) : \n self . file = open ( self . temp_file , '<STR_LIT>' ) \n return self \n def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n self . file . close ( ) \n if exc_type is not None : \n os . remove ( self . temp_file ) \n else : \n os . rename ( self . temp_file , self . save_file ) \n def download ( self ) : \n response = requests . get ( self . url , headers = self . headers , stream = True ) \n response . raise_for_status ( ) \n for chunk in response . iter_content ( chunk_size = self . chunk_size ) : \n self . file . write ( chunk ) \n @ v1_bp . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" ] ) \n def file_api_download ( ) : \n match require_auth ( request = request , permission = \"<STR_LIT>\" ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n data = request . json \n if not data : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n url = data . get ( \"<STR_LIT>\" ) \n headers = data . get ( \"<STR_LIT>\" ) or { \"<STR_LIT>\" : \"<STR_LIT>\" } \n save_file = data . get ( \"<STR_LIT>\" ) \n chunk_size = data . get ( \"<STR_LIT>\" ) or <NUM_LIT> * <NUM_LIT>", "output": "if not url or not save_file : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n if os . path . exists ( save_file ) : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n try : \n with Wget ( url , headers , save_file , chunk_size ) as wget : \n wget . download ( ) \n return { \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n except requests . RequestException as e : \n return { \"<STR_LIT>\" : str ( e ) , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def upload_file ( ) : \n match require_auth ( request = request , permission = \"<STR_LIT>\" ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n if '<STR_LIT>' not in request . files : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n file = request . files [ '<STR_LIT>' ] \n filename = secure_filename ( file . filename ) \n directory = request . form . get ( '<STR_LIT>' ) \n chunk_size = int ( request . form . get ( '<STR_LIT>' ) ) \n start_position = int ( request . form . get ( '<STR_LIT>' ) ) \n chunk_hash = request . form . get ( '<STR_LIT>' ) \n if filename == '<STR_LIT>' : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n file_content = file . read ( ) \n if hashlib . md5 ( file_content ) . hexdigest ( ) != chunk_hash : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n file . seek ( start_position ) \n with open ( os . path . join ( directory , filename ) , '<STR_LIT>' ) as f : \n f . write ( file_content [ : chunk_size ] ) \n return { \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def list_file ( ) : \n match require_auth ( request = request , permission = \"<STR_LIT>\" ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n path = request . args . get ( '<STR_LIT>' , os . getcwd ( ) ) \n row = request . args . get ( '<STR_LIT>' , <NUM_LIT> ) \n page = request . args . get ( '<STR_LIT>' , <NUM_LIT> ) \n if not os . path . exists ( path ) : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n if not os . path . isdir ( path ) : \n return { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } , <NUM_LIT> \n data = { \n \"<STR_LIT>\" : [ page , row ] , \n \"<STR_LIT>\" : [ ] \n } \n for i in os . listdir ( path ) : \n if os . path . isdir ( os . path . join ( path , i ) ) : \n data [ \"<STR_LIT>\" ] . append ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : i , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : int ( os . path . getctime ( os . path . join ( path , i ) ) ) , \n \"<STR_LIT>\" : int ( os . path . getmtime ( os . path . join ( path , i ) ) ) , \n \"<STR_LIT>\" : os . stat ( os . path . join ( path , i ) ) . st_uid , \n \"<STR_LIT>\" : oct ( os . stat ( os . path . join ( path , i ) ) . st_mode ) [ - <NUM_LIT> : ] \n } ) \n else : \n data [ \"<STR_LIT>\" ] . append ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : i , \n \"<STR_LIT>\" : os . path . getsize ( os . path . join ( path , i ) ) , \n \"<STR_LIT>\" : int ( os . path . getctime ( os . path . join ( path , i ) ) ) , \n \"<STR_LIT>\" : int ( os . path . getmtime ( os . path . join ( path , i ) ) ) , \n \"<STR_LIT>\" : os . stat ( os . path . join ( path , i ) ) . st_uid , \n \"<STR_LIT>\" : oct ( os . stat ( os . path . join ( path , i ) ) . st_mode ) [ - <NUM_LIT> : ] \n } ) \n return { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : data [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : data [ \"<STR_LIT>\" ] [ row * ( page - <NUM_LIT> ) : row * page ] \n } \n }"}, {"input": "import sys \n import os \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n class InstallationError ( Exception ) : \n def __init__ ( self , message = \"<STR_LIT>\" ) : \n self . message = message \n super ( ) . __init__ ( self . message ) \n def check_installation ( ) : \n try : \n system_drive = os . getenv ( \"<STR_LIT>\" ) \n current_drive = os . path . splitdrive ( now_dir ) [ <NUM_LIT> ] \n if current_drive . upper ( ) != system_drive . upper ( ) : \n raise InstallationError ( \n f\"<STR_LIT>\" \n ) \n except : \n pass \n else : \n if \"<STR_LIT>\" in now_dir :", "output": "raise InstallationError ( \n \"<STR_LIT>\" \n ) \n elif \"<STR_LIT>\" in now_dir : \n raise InstallationError ( \n \"<STR_LIT>\" \n ) \n try : \n now_dir . encode ( \"<STR_LIT>\" ) \n except UnicodeEncodeError : \n raise InstallationError ( \n \"<STR_LIT>\" \n )"}, {"input": "import torch \n import numpy as np \n from tqdm import tqdm \n class AROtoHFCLIPWrap ( ) : \n def __init__ ( self , model , processor , device = None ) : \n self . model = model \n self . processor = processor \n if device is None : \n device = model . device \n self . device = device \n @ torch . no_grad ( ) \n def process_batch ( self , b ) : \n width = len ( b [ '<STR_LIT>' ] ) \n bs = len ( b [ '<STR_LIT>' ] [ <NUM_LIT> ] ) \n all_entries = [ ] \n for cap_tuple in b [ '<STR_LIT>' ] : \n all_entries += list ( cap_tuple ) \n entries_tokenized = self . processor . tokenizer ( all_entries , return_tensors = '<STR_LIT>' , padding = True ) . to ( self . device ) \n pixel_values = b [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ <NUM_LIT> ] \n all_logits = self . model ( input_ids = entries_tokenized [ '<STR_LIT>' ] , attention_mask = entries_tokenized [ '<STR_LIT>' ] , pixel_values = pixel_values . to ( self . device ) ) \n def do_keep ( a ) : \n rowsize = width * bs \n curr_row = a // rowsize \n curr_col = a % bs \n return curr_col == curr_row \n index_np = np . arange ( width * bs * bs ) . reshape ( ( bs , width * bs ) ) \n grouped = all_logits . logits_per_image . cpu ( ) . numpy ( ) [ do_keep ( index_np ) ] \n scores = grouped . reshape ( ( bs , <NUM_LIT> , width ) )", "output": "return scores \n @ torch . no_grad ( ) \n def get_retrieval_scores_batched ( self , joint_loader ) : \n scores = [ ] \n tqdm_loader = tqdm ( joint_loader ) \n tqdm_loader . set_description ( \"<STR_LIT>\" ) \n for batch in tqdm_loader : \n batch_score = self . process_batch ( batch ) \n scores . append ( batch_score ) \n all_scores = np . concatenate ( scores , axis = <NUM_LIT> ) \n return all_scores"}, {"input": "import os , sys \n import torch \n import json \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n from tabs . settings . restart import restart_applio \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir )", "output": "i18n = I18nAuto ( ) \n ngpu = torch . cuda . device_count ( ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def gpu_available ( ) : \n if torch . cuda . is_available ( ) or ngpu != <NUM_LIT> : \n return True \n def load_fake_gpu ( ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n return config [ \"<STR_LIT>\" ] \n def save_config ( value ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n config [ \"<STR_LIT>\" ] = value \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( config , file , indent = <NUM_LIT> ) \n def fake_gpu_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n presence = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n value = load_fake_gpu ( ) , \n ) \n presence . change ( \n fn = toggle , \n inputs = [ presence ] , \n outputs = [ ] , \n ) \n def toggle ( checkbox ) : \n save_config ( bool ( checkbox ) ) \n restart_applio ( )"}, {"input": "from mephisto . operations . operator import Operator \n from mephisto . tools . scripts import task_script , build_custom_bundle \n from mephisto . data_model . qualification import QUAL_NOT_EXIST , QUAL_EXISTS \n from mephisto . utils . qualifications import make_qualification_dict \n from mephisto . abstractions . blueprints . abstract . static_task . static_blueprint import ( \n SharedStaticTaskState , \n ) \n from omegaconf import DictConfig \n NUM_TASKS = <NUM_LIT> \n PILOT_QUALIFICATION = '<STR_LIT>' \n ALLOWLIST_QUALIFICATION = '<STR_LIT>' \n @ task_script ( default_config_file = \"<STR_LIT>\" ) \n def main ( operator : Operator , cfg : DictConfig ) -> None : \n shared_state = SharedStaticTaskState ( \n static_task_data = [ { } ] * NUM_TASKS , \n ) \n shared_state . qualifications = [ \n make_qualification_dict ( \n PILOT_QUALIFICATION , \n QUAL_NOT_EXIST , \n None , \n ) , \n make_qualification_dict ( \n ALLOWLIST_QUALIFICATION , \n QUAL_NOT_EXIST , \n None ,", "output": ") , \n ] \n shared_state . mturk_specific_qualifications = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ <NUM_LIT> ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n ] \n task_dir = cfg . task_dir \n build_custom_bundle ( \n task_dir , \n force_rebuild = cfg . mephisto . task . force_rebuild , \n post_install_script = cfg . mephisto . task . post_install_script , \n ) \n operator . launch_task_run ( cfg . mephisto , shared_state ) \n operator . wait_for_runs_then_shutdown ( skip_input = True , log_rate = <NUM_LIT> ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None", "output": "def upgrade ( ) -> None : \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = True ) \n def downgrade ( ) -> None : \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' )"}, {"input": "from reliablegpt . IndividualRequest import IndividualRequest \n from reliablegpt . Model import Model \n from reliablegpt . Alerting import Alerting \n import requests \n import asyncio \n from posthog import Posthog \n from flask import Flask , request \n posthog = Posthog ( \n project_api_key = '<STR_LIT>' , \n host = '<STR_LIT>' ) \n alerting = Alerting ( ) \n def save_exception ( type , user_email , result = \"<STR_LIT>\" , original_error = \"<STR_LIT>\" , error2 = \"<STR_LIT>\" , function_name = \"<STR_LIT>\" , kwargs = { } ) : \n try : \n url = '<STR_LIT>' \n data = { \n '<STR_LIT>' : type , \n '<STR_LIT>' : user_email , \n '<STR_LIT>' : result , \n '<STR_LIT>' : original_error , \n '<STR_LIT>' : error2 , \n '<STR_LIT>' : function_name , \n '<STR_LIT>' : kwargs \n } \n response = requests . post ( url , json = data ) \n except Exception as e : \n pass \n return response \n def save_request ( user_email , \n graceful_string , \n posthog_event = \"<STR_LIT>\" , \n result = \"<STR_LIT>\" , \n posthog_metadata = { } , \n errors = [ ] , function_name = \"<STR_LIT>\" , kwargs = { } ) : \n try : \n if posthog_event != \"<STR_LIT>\" : \n posthog . capture ( user_email , posthog_event , \n posthog_metadata ) \n if posthog_event == '<STR_LIT>' : \n original_error = \"<STR_LIT>\" \n if '<STR_LIT>' in posthog_metadata : \n original_error = posthog_metadata [ '<STR_LIT>' ] \n save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , function_name = function_name , kwargs = kwargs ) \n if posthog_event == '<STR_LIT>' : \n original_error = \"<STR_LIT>\" \n if '<STR_LIT>' in posthog_metadata : \n original_error = posthog_metadata [ '<STR_LIT>' ] \n save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , function_name = function_name , kwargs = kwargs ) \n if posthog_event == '<STR_LIT>' : \n original_error = \"<STR_LIT>\" \n error2 = \"<STR_LIT>\" \n if '<STR_LIT>' in posthog_metadata : \n original_error = posthog_metadata [ '<STR_LIT>' ] \n if '<STR_LIT>' in posthog_metadata : \n error2 = posthog_metadata [ '<STR_LIT>' ] \n save_exception ( type = '<STR_LIT>' , user_email = user_email , result = result , original_error = original_error , error2 = error2 , function_name = function_name , kwargs = kwargs ) \n if result == graceful_string or len ( \n errors ) == <NUM_LIT> : \n for error in errors : \n alerting . add_error ( error ) \n except : \n pass \n return \n def reliableGPT ( openai_create_function , \n fallback_strategy = [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' \n ] , \n azure_fallback_strategy = None , \n graceful_string = \"<STR_LIT>\" , \n user_email = \"<STR_LIT>\" , \n user_token = \"<STR_LIT>\" , \n send_notification = True , \n caching = False , \n max_threads = None , \n backup_openai_key = None , \n _test = False , \n verbose = False ) :", "output": "primary_email = \"<STR_LIT>\" \n if isinstance ( user_email , str ) : \n if user_email == \"<STR_LIT>\" : \n raise ValueError ( \"<STR_LIT>\" ) \n else : \n alerting . add_emails ( user_email ) \n primary_email = user_email \n elif isinstance ( user_email , list ) : \n primary_email = user_email [ <NUM_LIT> ] \n for email in user_email : \n alerting . add_emails ( email ) \n model = Model ( openai_create_function ) \n return IndividualRequest ( model , \n fallback_strategy = fallback_strategy , \n azure_fallback_strategy = azure_fallback_strategy , \n graceful_string = graceful_string , \n user_email = primary_email , \n user_token = user_token , \n logging_fn = save_request , \n send_notification = send_notification , \n backup_openai_key = backup_openai_key , \n caching = caching , \n max_threads = max_threads , \n alerting = alerting , \n _test = _test , \n verbose = verbose )"}, {"input": "import os \n import sys \n import json \n import subprocess \n import shutil \n import pytest \n @ pytest . mark . timeout ( <NUM_LIT> ) \n def test_cli_where ( main_runner ) : \n strings = [ \n rf'<STR_LIT>' , \n rf'<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( '<STR_LIT>' . join ( strings ) ) \n for runner in ( main_runner , subprocess . run ) : \n result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n result . check_returncode ( ) \n base_libs_paths = { \n os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : '<STR_LIT>' , \n os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : '<STR_LIT>' , \n } \n for path , library_name in base_libs_paths . items ( ) : \n full_path_to_the_lib = os . path . join ( path , '<STR_LIT>' ) \n if sys . platform . lower ( ) not in ( '<STR_LIT>' , ) : \n full_path_to_the_lib = os . path . join ( full_path_to_the_lib , os . path . basename ( os . listdir ( path = full_path_to_the_lib ) [ <NUM_LIT> ] ) , '<STR_LIT>' ) \n full_path_to_the_lib = os . path . join ( full_path_to_the_lib , library_name ) \n assert os . path . isdir ( full_path_to_the_lib ) \n shutil . rmtree ( path ) \n os . remove ( script ) \n def test_run_command_with_arguments ( main_runner ) : \n strings = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ]", "output": "script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( os . linesep . join ( strings ) ) \n extra_arguments_options = ( \n [ ] , \n [ '<STR_LIT>' ] , \n [ '<STR_LIT>' , '<STR_LIT>' ] , \n [ '<STR_LIT>' , '<STR_LIT>' ] , \n ) \n for runner in ( main_runner , subprocess . run ) : \n for extra_arguments in extra_arguments_options : \n expected_arguments_without_command = [ script ] + extra_arguments \n result = runner ( [ '<STR_LIT>' , * expected_arguments_without_command ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n result . check_returncode ( ) \n assert result . stderr . decode ( '<STR_LIT>' ) == '<STR_LIT>' \n arguments_from_file = json . loads ( result . stdout . decode ( '<STR_LIT>' ) ) \n arguments_from_file_without_command = arguments_from_file [ <NUM_LIT> : ] \n assert arguments_from_file_without_command == expected_arguments_without_command \n os . remove ( script ) \n def test_exceptions_are_similar_with_just_python_command ( main_runner ) : \n errors = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n for runner in ( subprocess . run , main_runner ) : \n for error in errors : \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( f'<STR_LIT>' ) \n result_1 = runner ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n result_2 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n assert result_1 . returncode == result_2 . returncode \n assert result_1 . stdout == result_2 . stdout \n assert result_1 . stderr == result_2 . stderr \n os . remove ( script ) \n def test_exceptions_are_similar_with_just_python_command_2 ( ) : \n errors = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n for error in errors : \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( f'<STR_LIT>' ) \n result_1 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n result_2 = subprocess . run ( [ '<STR_LIT>' , os . path . abspath ( script ) ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> ) \n assert result_1 . returncode == result_2 . returncode \n assert result_1 . stdout == result_2 . stdout \n assert result_1 . stderr == result_2 . stderr \n os . remove ( script ) \n @ pytest . mark . skip ( reason = \"<STR_LIT>\" ) \n def test_install_package_from_another_repository ( main_runner ) : \n strings = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( '<STR_LIT>' . join ( strings ) ) \n for runner in ( subprocess . run , main_runner ) : \n result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) \n result . check_returncode ( ) \n assert result . stdout == '<STR_LIT>' \n os . remove ( script ) \n def test_install_package_from_another_repository_only_command ( ) : \n strings = [ \n '<STR_LIT>' , \n '<STR_LIT>' , \n ] \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( '<STR_LIT>' . join ( strings ) ) \n for runner in ( subprocess . run , ) : \n result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) \n result . check_returncode ( ) \n assert result . stdout == '<STR_LIT>' \n os . remove ( script ) \n def test_run_script_and_check_the___name__ ( ) : \n strings = [ \n '<STR_LIT>' , \n ] \n script = os . path . join ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) \n with open ( script , '<STR_LIT>' ) as file : \n file . write ( '<STR_LIT>' . join ( strings ) ) \n for runner in ( subprocess . run , ) : \n result = runner ( [ '<STR_LIT>' , script ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) \n result . check_returncode ( ) \n assert result . stdout == '<STR_LIT>' \n os . remove ( script )"}, {"input": "import dataclasses \n from typing import Protocol , runtime_checkable \n @ runtime_checkable \n class Interface ( Protocol ) : \n pass \n @ dataclasses . dataclass \n class Service : \n pass \n @ dataclasses . dataclass \n class AnotherService : \n pass \n @ dataclasses . dataclass \n class YetAnotherService :", "output": "pass"}, {"input": "from typing import Optional , List \n from dataclasses import field , dataclass \n import logging \n import subprocess \n import pathlib \n import torch \n import shutil \n import glob \n import os \n import transformers \n from transformers . trainer_utils import PREFIX_CHECKPOINT_DIR \n from transformers import Trainer \n from multi_token . training_data import ( \n DataArguments , \n LMMDataset , \n DataCollatorForSupervisedLMMDataset , \n ) \n from multi_token . model_utils import ( \n make_model_lora , \n get_peft_state , \n get_peft_state_non_lora , \n fix_tokenizer , \n ) \n from multi_token . modalities . base_modality import Modality \n README_TEMPLATE = \n @ dataclass \n class TrainingArguments ( transformers . TrainingArguments ) : \n cache_dir : Optional [ str ] = field ( default = None ) \n remove_unused_columns : bool = field ( default = False ) \n optim : str = field ( default = \"<STR_LIT>\" ) \n model_max_length : int = field ( \n default = <NUM_LIT> , \n metadata = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ) \n double_quant : bool = field ( \n default = True , \n metadata = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ) \n quant_type : str = field ( \n default = \"<STR_LIT>\" , \n metadata = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ) \n pretrain_projectors : bool = field ( default = False ) \n pretrained_projectors_path : Optional [ str ] = field ( default = None ) \n bits : int = field ( default = <NUM_LIT> , metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n lora_enable : bool = False \n lora_r : int = <NUM_LIT> \n lora_alpha : int = <NUM_LIT> \n lora_dropout : float = <NUM_LIT> \n lora_weight_path : str = \"<STR_LIT>\" \n lora_bias : str = \"<STR_LIT>\" \n @ dataclass \n class ModelArguments : \n model_name_or_path : str = field ( default = \"<STR_LIT>\" ) \n model_cls : str = field ( default = \"<STR_LIT>\" ) \n modality_builder : str = field ( default = \"<STR_LIT>\" ) \n model_lora_path : Optional [ str ] = field ( default = None ) \n class LMMTrainer ( Trainer ) : \n def _save_checkpoint ( self , model , trial , metrics = None ) : \n checkpoint_folder = f\"<STR_LIT>\" \n run_dir = self . _get_output_dir ( trial = trial ) \n output_dir = os . path . join ( run_dir , checkpoint_folder ) \n self . _save_extras ( output_dir ) \n super ( LMMTrainer , self ) . _save_checkpoint ( model , trial , metrics ) \n def _save ( self , output_dir : Optional [ str ] = None , state_dict = None ) : \n self . _save_extras ( output_dir ) \n super ( LMMTrainer , self ) . _save ( output_dir , state_dict ) \n for unused_dir in glob . iglob ( os . path . join ( output_dir , \"<STR_LIT>\" ) ) : \n shutil . rmtree ( unused_dir ) \n def _save_extras ( self , output_dir : Optional [ str ] = None ) : \n self . model . config . save_pretrained ( output_dir ) \n non_lora_state_dict = get_peft_state_non_lora ( self . model . named_parameters ( ) ) \n torch . save ( \n non_lora_state_dict , \n os . path . join ( output_dir , \"<STR_LIT>\" ) , \n ) \n def _get_training_devices_dump ( ) -> str : \n out = subprocess . check_output ( \n [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n ) \n return out . decode ( \"<STR_LIT>\" ) . strip ( ) \n def train_for_modalities ( \n model_cls , \n training_args : TrainingArguments , \n model_args : ModelArguments , \n data_args : DataArguments , \n modalities : List [ Modality ] , \n ) : \n for m in modalities : \n m . to ( \n dtype = torch . bfloat16 if training_args . bf16 else torch . float16 , \n device = training_args . device , \n ) \n tokenizer = transformers . AutoTokenizer . from_pretrained ( \n model_args . model_name_or_path , \n cache_dir = training_args . cache_dir , \n model_max_length = training_args . model_max_length , \n padding_side = \"<STR_LIT>\" , \n use_fast = False , \n ) \n fix_tokenizer ( tokenizer ) \n dataset = LMMDataset ( data_args , tokenizer , modalities ) \n collator = DataCollatorForSupervisedLMMDataset ( tokenizer , modalities ) \n model = model_cls . from_pretrained ( \n model_args . model_name_or_path , \n cache_dir = training_args . cache_dir , \n )", "output": "model . modalities = modalities \n model . config . use_cache = False \n model . config . model_cls = model_cls . __name__ \n model . config . modality_builder = model_args . modality_builder \n if training_args . gradient_checkpointing : \n if hasattr ( model , \"<STR_LIT>\" ) : \n model . enable_input_require_grads ( ) \n else : \n def make_inputs_require_grad ( module , input , output ) : \n output . requires_grad_ ( True ) \n model . get_input_embeddings ( ) . register_forward_hook ( make_inputs_require_grad ) \n if model_args . model_lora_path : \n raise ValueError ( \n \"<STR_LIT>\" \n ) \n if training_args . lora_enable : \n logging . info ( \"<STR_LIT>\" ) \n model = make_model_lora ( model , training_args ) \n if training_args . pretrained_projectors_path : \n projector_weights = torch . load ( \n training_args . pretrained_projectors_path , map_location = \"<STR_LIT>\" \n ) \n projector_weights = { \n k : v for k , v in projector_weights . items ( ) if \"<STR_LIT>\" in k \n } \n else : \n projector_weights = { } \n model . get_model ( ) . initialize_modules ( modalities , projector_weights ) \n if training_args . pretrain_projectors : \n model . requires_grad_ ( False ) \n for m in modalities : \n proj = getattr ( model . get_model ( ) , m . name + \"<STR_LIT>\" ) \n for p in proj . parameters ( ) : \n p . requires_grad = True \n os . makedirs ( training_args . output_dir , exist_ok = True ) \n with open ( \n os . path . join ( training_args . output_dir , \"<STR_LIT>\" ) , \"<STR_LIT>\" \n ) as f : \n for name , param in model . named_parameters ( ) : \n f . write ( f\"<STR_LIT>\" ) \n with open ( os . path . join ( training_args . output_dir , \"<STR_LIT>\" ) , \"<STR_LIT>\" ) as f : \n modalities_text = [ \n f\"<STR_LIT>\" \n for m in modalities \n ] \n readme_text = README_TEMPLATE . format ( \n base_model = model_args . model_name_or_path , \n dataset = data_args . dataset_path , \n dataset_example = repr ( dataset . get_example ( ) ) , \n num_examples = len ( dataset ) , \n modalities = \"<STR_LIT>\" . join ( modalities_text ) , \n training_devices_dump = _get_training_devices_dump ( ) , \n repr_model = f\"<STR_LIT>\" , \n ) \n f . write ( readme_text ) \n trainer = LMMTrainer ( \n model = model , \n tokenizer = tokenizer , \n args = training_args , \n data_collator = collator , \n train_dataset = dataset , \n eval_dataset = None , \n ) \n if list ( pathlib . Path ( training_args . output_dir ) . glob ( f\"<STR_LIT>\" ) ) : \n trainer . train ( resume_from_checkpoint = True ) \n else : \n trainer . train ( ) \n trainer . save_state ( ) \n model . config . use_cache = True \n model . config . save_pretrained ( training_args . output_dir ) \n state_dict = get_peft_state ( model . named_parameters ( ) , training_args . lora_bias ) \n model . save_pretrained ( training_args . output_dir , state_dict = state_dict ) \n non_lora_state_dict = get_peft_state_non_lora ( model . named_parameters ( ) ) \n torch . save ( \n non_lora_state_dict , \n os . path . join ( training_args . output_dir , \"<STR_LIT>\" ) , \n )"}, {"input": "import os \n import sys \n import gradio as gr \n import json \n from assets . i18n . i18n import I18nAuto \n from assets . discord_presence import RPCManager \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n i18n = I18nAuto ( ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def load_config_presence ( ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n return config [ \"<STR_LIT>\" ] \n def save_config ( value ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n config [ \"<STR_LIT>\" ] = value \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( config , file , indent = <NUM_LIT> ) \n def presence_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n presence = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n value = load_config_presence ( ) , \n ) \n presence . change ( \n fn = toggle , \n inputs = [ presence ] , \n outputs = [ ] ,", "output": ") \n def toggle ( checkbox ) : \n save_config ( bool ( checkbox ) ) \n if load_config_presence ( ) == True : \n try : \n RPCManager . start_presence ( ) \n except KeyboardInterrupt : \n RPCManager . stop_presence ( ) \n else : \n RPCManager . stop_presence ( )"}, {"input": "import torch \n from torch import Tensor \n import numpy as np \n from isaacgym . torch_utils import quat_apply , normalize \n from typing import Tuple \n def quat_apply_yaw ( quat , vec ) : \n quat_yaw = quat . clone ( ) . view ( - <NUM_LIT> , <NUM_LIT> ) \n quat_yaw [ : , : <NUM_LIT> ] = <NUM_LIT>", "output": "quat_yaw = normalize ( quat_yaw ) \n return quat_apply ( quat_yaw , vec ) \n def wrap_to_pi ( angles ) : \n angles %= <NUM_LIT> * np . pi \n angles -= <NUM_LIT> * np . pi * ( angles > np . pi ) \n return angles \n def torch_rand_sqrt_float ( lower , upper , shape , device ) : \n r = <NUM_LIT> * torch . rand ( * shape , device = device ) - <NUM_LIT> \n r = torch . where ( r < <NUM_LIT> , - torch . sqrt ( - r ) , torch . sqrt ( r ) ) \n r = ( r + <NUM_LIT> ) / <NUM_LIT> \n return ( upper - lower ) * r + lower \n def torch_rand_int ( lower , upper , shape , device ) : \n return ( ( upper - lower ) * torch . rand ( * shape , device = device ) . squeeze ( <NUM_LIT> ) + lower ) . long ( ) . float ( ) \n def sample_unit_vector ( n , dim , device ) : \n tensor = torch . randn ( n , dim , device = device ) \n unit_vector = tensor / torch . norm ( tensor , dim = - <NUM_LIT> , keepdim = True ) \n return unit_vector"}, {"input": "import json \n import aiohttp \n import asyncio \n import base64 \n import random \n import string \n import time \n from mod import textcompare \n from mod import tools \n headers = { '<STR_LIT>' : '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' , } \n async def get_cover ( session , m_hash , m_id ) : \n def _dfid ( num ) : \n random_str = '<STR_LIT>' . join ( random . sample ( ( string . ascii_letters + string . digits ) , num ) ) \n return random_str \n def _mid ( num ) : \n random_str = '<STR_LIT>' . join ( random . sample ( ( string . ascii_letters [ : <NUM_LIT> ] + string . digits ) , num ) ) \n return random_str \n music_url = '<STR_LIT>' \n parameter = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : m_hash , \n '<STR_LIT>' : _dfid ( <NUM_LIT> ) , \n '<STR_LIT>' : _mid ( <NUM_LIT> ) , \n '<STR_LIT>' : m_id , \n '<STR_LIT>' : str ( round ( time . time ( ) * <NUM_LIT> ) ) \n } \n json_data_r = await session . get ( music_url , headers = headers , params = parameter ) \n json_data = json . loads ( await json_data_r . text ( ) ) \n if json_data . get ( \"<STR_LIT>\" ) : \n return json_data [ '<STR_LIT>' ] . get ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n async def a_search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) : \n if not any ( ( title , artist , album ) ) : \n return None \n result_list = [ ] \n limit = <NUM_LIT> \n async with aiohttp . ClientSession ( ) as session : \n async with session . get ( \n f\"<STR_LIT>\" , \n headers = headers ) as response : \n if response . status == <NUM_LIT> : \n song_info_t = await response . text ( ) \n song_info = json . loads ( song_info_t ) \n song_info = song_info [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n if len ( song_info ) >= <NUM_LIT> : \n for song_item in song_info : \n song_name = song_item [ \"<STR_LIT>\" ] \n singer_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n song_hash = song_item [ \"<STR_LIT>\" ] \n album_id = song_item [ \"<STR_LIT>\" ] \n album_name = song_item . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n title_conform_ratio = textcompare . association ( title , song_name ) \n artist_conform_ratio = textcompare . assoc_artists ( artist , singer_name ) \n ratio = ( title_conform_ratio * ( artist_conform_ratio + <NUM_LIT> ) / <NUM_LIT> ) ** <NUM_LIT> \n if ratio >= <NUM_LIT> : \n async with session . get ( \n f\"<STR_LIT>\" , \n headers = headers ) as response2 : \n lyrics_info = await response2 . json ( ) \n lyrics_id = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n lyrics_key = lyrics_info [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n async with session . get ( \n f\"<STR_LIT>\" , \n headers = headers ) as response3 : \n lyrics_data = await response3 . json ( ) \n lyrics_encode = lyrics_data [ \"<STR_LIT>\" ] \n lrc_text = tools . standard_lrc ( base64 . b64decode ( lyrics_encode ) . decode ( '<STR_LIT>' ) ) \n music_json_data = { \n \"<STR_LIT>\" : song_name , \n \"<STR_LIT>\" : album_name , \n \"<STR_LIT>\" : singer_name , \n \"<STR_LIT>\" : lrc_text , \n \"<STR_LIT>\" : await get_cover ( session , song_hash , album_id ) , \n \"<STR_LIT>\" : tools . calculate_md5 ( \n f\"<STR_LIT>\" ) \n } \n result_list . append ( { \n \"<STR_LIT>\" : music_json_data , \n \"<STR_LIT>\" : ratio \n } ) \n if len ( result_list ) > limit : \n break \n else : \n return None \n sort_li = sorted ( result_list , key = lambda x : x [ '<STR_LIT>' ] , reverse = True ) \n return [ i . get ( '<STR_LIT>' ) for i in sort_li ] \n def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) : \n return asyncio . run ( a_search ( title = title , artist = artist , album = album ) )", "output": "if __name__ == \"<STR_LIT>\" : \n print ( search ( album = \"<STR_LIT>\" ) )"}, {"input": "import json \n import os \n import uuid \n import requests \n from curl_cffi import requests , Curl , CurlOpt \n from dotenv import load_dotenv \n from common . log import logger \n import PyPDF2 \n import docx \n import re \n from io import BytesIO \n load_dotenv ( ) \n class Client : \n def __init__ ( self , cookie , use_proxy = False ) : \n self . cookie = cookie \n self . use_proxy = use_proxy \n self . proxies = self . load_proxies_from_env ( ) \n self . organization_id = self . get_organization_id ( ) \n def load_proxies_from_env ( self ) : \n proxies = { } \n if self . use_proxy : \n http_proxy = os . getenv ( '<STR_LIT>' ) \n https_proxy = os . getenv ( '<STR_LIT>' ) \n socks5_proxy = os . getenv ( '<STR_LIT>' ) \n if http_proxy : \n proxies [ '<STR_LIT>' ] = http_proxy \n if https_proxy : \n proxies [ '<STR_LIT>' ] = https_proxy \n if socks5_proxy : \n proxies [ '<STR_LIT>' ] = socks5_proxy \n return proxies \n def get_organization_id ( self ) : \n url = \"<STR_LIT>\" \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers ) \n if response . status_code == <NUM_LIT> : \n res = json . loads ( response . text ) \n uuid = res [ <NUM_LIT> ] [ '<STR_LIT>' ] \n return uuid \n else : \n print ( f\"<STR_LIT>\" ) \n def get_content_type ( self , file_path ) : \n extension = os . path . splitext ( file_path ) [ - <NUM_LIT> ] . lower ( ) \n if extension == '<STR_LIT>' : \n return '<STR_LIT>' \n elif extension == '<STR_LIT>' : \n return '<STR_LIT>' \n elif extension == '<STR_LIT>' : \n return '<STR_LIT>' \n else : \n return '<STR_LIT>' \n def list_all_conversations ( self ) : \n url = f\"<STR_LIT>\" \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers ) \n conversations = response . json ( ) \n if response . status_code == <NUM_LIT> : \n return conversations \n else : \n print ( f\"<STR_LIT>\" ) \n def send_message ( self , prompt , conversation_id , attachment = None ) : \n url = \"<STR_LIT>\" \n attachments = [ ] \n if attachment : \n attachment_response = self . upload_attachment ( attachment ) \n if attachment_response : \n attachments = [ attachment_response ] \n else : \n return { \"<STR_LIT>\" } \n if not attachment : \n attachments = [ ] \n payload = json . dumps ( { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : attachments \n } ) \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n buffer = BytesIO ( ) \n c = Curl ( ) \n def stream_callback ( data ) : \n json_str = data . decode ( '<STR_LIT>' ) \n decoded_data = re . sub ( '<STR_LIT>' , '<STR_LIT>' , json_str ) . strip ( ) \n data_strings = decoded_data . split ( '<STR_LIT>' ) \n for data_string in data_strings : \n json_str = data_string [ <NUM_LIT> : ] . strip ( ) \n _data = json . loads ( json_str ) \n if '<STR_LIT>' in _data : \n buffer . write ( str ( _data [ '<STR_LIT>' ] ) . encode ( '<STR_LIT>' ) ) \n print ( _data [ '<STR_LIT>' ] , end = \"<STR_LIT>\" ) \n c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) \n c . setopt ( CurlOpt . WRITEFUNCTION , stream_callback ) \n c . setopt ( CurlOpt . HTTPHEADER , headers ) \n c . setopt ( CurlOpt . POSTFIELDS , payload ) \n c . impersonate ( \"<STR_LIT>\" ) \n c . perform ( ) \n c . close ( ) \n body = buffer . getvalue ( ) \n print ( body . decode ( ) ) \n return body \n def delete_conversation ( self , conversation_id ) : \n url = f\"<STR_LIT>\" \n payload = json . dumps ( f\"<STR_LIT>\" ) \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) \n if response . status_code == <NUM_LIT> : \n return True \n else : \n return False \n def chat_conversation_history ( self , conversation_id ) : \n url = f\"<STR_LIT>\" \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers , params = { '<STR_LIT>' : '<STR_LIT>' } ) \n print ( type ( response ) ) \n return response . json ( ) \n def generate_uuid ( self ) : \n random_uuid = uuid . uuid4 ( ) \n random_uuid_str = str ( random_uuid ) \n formatted_uuid = f\"<STR_LIT>\" \n return formatted_uuid \n def create_new_chat ( self ) : \n url = f\"<STR_LIT>\" \n uuid = self . generate_uuid ( ) \n payload = json . dumps ( { \"<STR_LIT>\" : uuid , \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : self . cookie , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) \n return response . json ( ) \n def reset_all ( self ) : \n conversations = self . list_all_conversations ( ) \n for conversation in conversations : \n conversation_id = conversation [ '<STR_LIT>' ] \n delete_id = self . delete_conversation ( conversation_id ) \n return True \n def upload_attachment ( self , file_path ) : \n if file_path . endswith ( ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ) : \n file_name = os . path . basename ( file_path ) \n file_size = os . path . getsize ( file_path ) \n file_type = \"<STR_LIT>\" \n file_content = \"<STR_LIT>\" \n if file_path . endswith ( '<STR_LIT>' ) : \n with open ( file_path , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file : \n file_content = file . read ( ) \n elif file_path . endswith ( '<STR_LIT>' ) : \n with open ( file_path , '<STR_LIT>' ) as file : \n pdf_reader = PyPDF2 . PdfFileReader ( file ) \n for page_num in range ( pdf_reader . numPages ) : \n page = pdf_reader . getPage ( page_num ) \n file_content += page . extractText ( ) \n elif file_path . endswith ( ( '<STR_LIT>' , '<STR_LIT>' ) ) : \n doc = docx . Document ( file_path ) \n paragraphs = doc . paragraphs \n for paragraph in paragraphs : \n file_content += paragraph . text \n return { \n \"<STR_LIT>\" : file_name , \n \"<STR_LIT>\" : file_type , \n \"<STR_LIT>\" : file_size , \n \"<STR_LIT>\" : file_content \n } \n url = '<STR_LIT>' \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n file_name = os . path . basename ( file_path ) \n content_type = self . get_content_type ( file_path ) \n files = { \n '<STR_LIT>' : ( file_name , open ( file_path , '<STR_LIT>' ) , content_type ) , \n '<STR_LIT>' : ( None , self . organization_id ) \n } \n response = self . send_request ( url , \"<STR_LIT>\" , headers = headers , files = files )", "output": "if response . status_code == <NUM_LIT> : \n return response . json ( ) \n else : \n return False \n def rename_chat ( self , title , conversation_id ) : \n url = \"<STR_LIT>\" \n payload = json . dumps ( { \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" \n } ) \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = self . send_request ( \"<STR_LIT>\" , url , headers = headers , data = payload ) \n if response . status_code == <NUM_LIT> : \n return True \n else : \n return False \n def send_request ( self , method , url , headers , data = None , files = None , params = None , stream = False ) : \n if self . use_proxy : \n return requests . request ( method , url , headers = headers , data = data , files = files , params = params , impersonate = \"<STR_LIT>\" , proxies = self . proxies , timeout = <NUM_LIT> ) \n else : \n return requests . request ( method , url , headers = headers , data = data , files = files , params = params , impersonate = \"<STR_LIT>\" , timeout = <NUM_LIT> )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n import random \n import openai \n from datasets import Dataset , load_dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n PROMPT = \n QUESTIONS = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n OPENAI_TOOLS = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } , \n } , \n } \n ] \n def _build_convo ( pretrain_examples ) -> List : \n client = openai . Client ( ) \n captions = [ e [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] for e in pretrain_examples ] \n paths = [ e [ \"<STR_LIT>\" ] [ <NUM_LIT> ] for e in pretrain_examples ] \n captions_text = \"<STR_LIT>\" . join ( \n [ f\"<STR_LIT>\" for i , cap in enumerate ( captions ) ] \n ) \n prompt = PROMPT . format ( \n captions = captions_text , question = random . choice ( QUESTIONS ) \n ) . strip ( ) \n completion = client . chat . completions . create ( \n model = \"<STR_LIT>\" , \n messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : prompt } ] , \n tools = OPENAI_TOOLS , \n tool_choice = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n ) \n resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) \n if \"<STR_LIT>\" not in resp : \n print ( resp ) \n q = resp [ \"<STR_LIT>\" ] \n a = resp [ \"<STR_LIT>\" ] \n if random . choice ( [ True , False ] ) : \n q = \"<STR_LIT>\" * len ( captions ) + \"<STR_LIT>\" + q \n else : \n q = q + \"<STR_LIT>\" + \"<STR_LIT>\" * len ( captions ) \n example = { \n \"<STR_LIT>\" : paths , \n \"<STR_LIT>\" : [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : q , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : a , \n } , \n ] , \n } \n return example \n def main ( args ) : \n data = load_dataset ( \"<STR_LIT>\" , split = \"<STR_LIT>\" , data_files = \"<STR_LIT>\" ) \n data_idxs = list ( range ( len ( data ) ) ) \n os . makedirs ( args . cache_folder , exist_ok = True ) \n def gen ( seeds ) : \n r = random . Random ( seeds [ <NUM_LIT> ] ) \n cache = open ( \n os . path . join ( args . cache_folder , f\"<STR_LIT>\" ) , \"<STR_LIT>\" \n ) \n i = <NUM_LIT> \n while i < len ( seeds ) : \n k = r . randint ( <NUM_LIT> , args . max_images ) \n selected_idxs = r . sample ( data_idxs , k = k ) \n selected_examples = [ data [ i ] for i in selected_idxs ] \n try : \n example = _build_convo ( selected_examples ) \n cache . write ( json . dumps ( example ) + \"<STR_LIT>\" ) \n yield example \n i += <NUM_LIT> \n except Exception as e : \n print ( e ) \n continue \n cache . close ( ) \n ds = Dataset . from_generator ( \n gen , \n num_proc = args . num_proc , \n gen_kwargs = { \"<STR_LIT>\" : list ( range ( args . num_examples ) ) } , \n ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> )", "output": "parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import sys \n import typing \n import pytest \n import svcs \n @ pytest . mark . skipif ( sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) , reason = \"<STR_LIT>\" ) \n def test_get_type_hints ( ) :", "output": "typing . get_type_hints ( svcs . Registry ) \n typing . get_type_hints ( svcs . Container ) \n typing . get_type_hints ( svcs . ServicePing ) \n typing . get_type_hints ( svcs . RegisteredService )"}, {"input": "from concurrent import futures \n from mod . searchx import api , kugou \n def search_all ( title , artist , album , timeout = <NUM_LIT> ) : \n funcs = [ api , kugou ] \n results = [ ] \n def request ( task ) : \n res : list = task . search ( title , artist , album ) \n if isinstance ( res , list ) : \n results . extend ( res ) \n with futures . ThreadPoolExecutor ( ) as executor : \n _futures = [ ] \n for func in funcs : \n _futures . append ( executor . submit ( request , func ) ) \n for future in futures . as_completed ( _futures , timeout = timeout ) : \n future . result ( ) \n for future in _futures : \n if future . done ( ) and future . exception ( ) :", "output": "future . result ( ) \n else : \n future . cancel ( ) \n return results \n if __name__ == \"<STR_LIT>\" : \n print ( search_all ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) )"}, {"input": "from abc import ABC , abstractmethod \n from typing import Optional \n from profyle . domain . trace import Trace , TraceCreate \n class TraceRepository ( ABC ) :", "output": "@ abstractmethod \n def create_trace_selected_table ( self ) -> None : \n ... \n @ abstractmethod \n def create_trace_table ( self ) -> None : \n ... \n @ abstractmethod \n def delete_all_traces ( self ) -> int : \n ... \n @ abstractmethod \n def vacuum ( self ) -> None : \n ... \n @ abstractmethod \n def store_trace_selected ( self , trace_id : int ) -> None : \n ... \n @ abstractmethod \n def store_trace ( self , new_trace : TraceCreate ) -> None : \n ... \n @ abstractmethod \n def get_all_traces ( self ) -> list [ Trace ] : \n ... \n @ abstractmethod \n def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : \n ... \n @ abstractmethod \n def get_trace_selected ( self ) -> Optional [ int ] : \n ... \n @ abstractmethod \n def delete_trace_by_id ( self , trace_id : int ) : \n ..."}, {"input": "import torch \n import os \n from tqdm import tqdm \n from typing import Dict , List , Any , Callable , Optional , Tuple \n from densely_captioned_images . repro . eval . localized_narratives . localized_narratives import DataLoader as LocNarDataLoader \n from PIL import Image \n from densely_captioned_images . dataset . utils import get_clip_processor , get_clip_token_length \n from densely_captioned_images . repro . config import COCO_TRAIN2017_DATAPATH , COCO_VALID2017_DATAPATH , LOCALIZED_NARRATIVES_DATAPATH \n from densely_captioned_images . dataset . spacy_negs import get_spacy_negative \n def get_dataset_source ( split = '<STR_LIT>' , count = <NUM_LIT> , use_antonyms = False ) : \n if split == '<STR_LIT>' : \n source_dir = COCO_TRAIN2017_DATAPATH \n ln_split = '<STR_LIT>' \n elif split == '<STR_LIT>' : \n source_dir = COCO_VALID2017_DATAPATH \n ln_split = '<STR_LIT>' \n else : \n raise NotImplementedError ( '<STR_LIT>' ) \n loader = LocNarDataLoader ( LOCALIZED_NARRATIVES_DATAPATH ) \n res = [ ] \n count_so_far = <NUM_LIT> \n skipped = <NUM_LIT> \n for n in tqdm ( loader . load_annotations ( ln_split ) ) : \n toks = get_clip_token_length ( n . caption ) \n if toks > <NUM_LIT> : \n skipped += <NUM_LIT> \n continue \n image_path = os . path . join ( source_dir , f\"<STR_LIT>\" ) \n res . append ( { \n \"<STR_LIT>\" : image_path , \n \"<STR_LIT>\" : n . caption , \n \"<STR_LIT>\" : get_spacy_negative ( n . caption , use_antonyms = use_antonyms ) , \n } ) \n count_so_far += <NUM_LIT> \n if count_so_far == count : \n break \n return res \n class COCOLocalizedNarrativesDataset ( torch . utils . data . Dataset ) : \n def __init__ ( self , dataset_source ) : \n self . data_list = dataset_source \n self . processor = get_clip_processor ( ) \n def __getitem__ ( self , idx ) :", "output": "item = self . data_list [ idx ] \n image = Image . open ( item [ '<STR_LIT>' ] ) \n inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) \n negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) \n res = { \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : negatives [ '<STR_LIT>' ] , \n '<STR_LIT>' : negatives [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n } \n return res \n def __len__ ( self ) : \n return len ( self . data_list )"}, {"input": "import argparse \n from datasets import load_dataset , concatenate_datasets \n def main ( args ) : \n dss = [ ] \n for dataset_path in args . dataset : \n dataset = load_dataset ( dataset_path , split = \"<STR_LIT>\" , data_files = \"<STR_LIT>\" ) \n dss . append ( dataset ) \n ds = concatenate_datasets ( dss ) \n ds = ds . shuffle ( ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , action = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str )", "output": "args = parser . parse_args ( ) \n main ( args )"}, {"input": "import base64 \n import os \n import io \n from PIL import Image \n from mod import music_tag \n TAG_MAP = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n def dump_b64 ( album_art : music_tag . file . MetadataItem ) -> str : \n artwork = album_art . values [ <NUM_LIT> ] \n img_data = artwork . data \n img_format = artwork . format \n img = Image . open ( io . BytesIO ( img_data ) ) \n img_byte_arr = io . BytesIO ( ) \n img . save ( img_byte_arr , format = img_format ) \n img_base64 = base64 . b64encode ( img_byte_arr . getvalue ( ) ) \n return img_base64 . decode ( ) \n def write ( tags : dict , file : any ) -> None : \n if not isinstance ( tags , dict ) : \n raise TypeError ( f'<STR_LIT>' ) \n file_path = file if isinstance ( file , str ) else ( file . name if hasattr ( file , '<STR_LIT>' ) else None ) \n if not file_path or not os . path . exists ( file_path ) : \n raise FileNotFoundError ( f'<STR_LIT>' ) \n music_file_obj = music_tag . load_file ( file ) \n for tag_name , tag_value in tags . items ( ) :", "output": "if tag_name == \"<STR_LIT>\" and tag_value : \n artwork_raw : bytes = base64 . b64decode ( tag_value ) \n artwork = music_tag . file . Artwork ( artwork_raw ) \n music_file_obj [ tag_name ] = artwork \n elif tag_name in TAG_MAP and tag_value : \n music_file_obj [ tag_name ] = tag_value \n elif tag_value is False : \n del music_file_obj [ tag_name ] \n else : \n continue \n music_file_obj . save ( ) \n def read ( file : any ) -> dict : \n file_path = file if isinstance ( file , str ) else ( file . name if hasattr ( file , '<STR_LIT>' ) else None ) \n if not file_path or not os . path . exists ( file_path ) : \n return { } \n result = { } \n for tag_name , tag_func in TAG_MAP . items ( ) : \n if tag_name == \"<STR_LIT>\" : \n result [ tag_name ] = dump_b64 ( music_tag . load_file ( file_path ) . resolve ( tag_name ) ) \n else : \n result [ tag_name ] = str ( music_tag . load_file ( file_path ) . resolve ( tag_name ) ) \n return result \n if __name__ == '<STR_LIT>' : \n val_tags = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' \n } \n print ( read ( r'<STR_LIT>' ) )"}, {"input": "from flask import Flask \n import time \n import openai \n import sys \n import traceback \n import dotenv \n import random \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai \n from main import reliableGPT \n import os \n openai . api_key = os . getenv ( '<STR_LIT>' ) \n def logging_fn ( * args , ** kwargs ) : \n pass \n app = Flask ( __name__ ) \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , app = app , \n user_email = [ \"<STR_LIT>\" ] , verbose = True ) \n def throw_random_error ( ) : \n if random . random ( ) < <NUM_LIT> : \n raise Exception ( \"<STR_LIT>\" ) \n @ app . route ( \"<STR_LIT>\" ) \n def test_fn ( ) : \n print ( \"<STR_LIT>\" ) \n throw_random_error ( ) \n result = openai . ChatCompletion . create ( model = \"<STR_LIT>\" , messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } ] ) \n print ( f\"<STR_LIT>\" ) \n return result \n @ app . route ( '<STR_LIT>' )", "output": "def index ( ) : \n return '<STR_LIT>' \n if __name__ == \"<STR_LIT>\" : \n from waitress import serve \n serve ( app , host = \"<STR_LIT>\" , port = <NUM_LIT> , threads = <NUM_LIT> )"}, {"input": "from app import app \n def test_home_route ( ) : \n with app . test_client ( ) as client : \n response = client . get ( '<STR_LIT>' ) \n assert response . status_code < <NUM_LIT>", "output": "def test_source_route ( ) : \n with app . test_client ( ) as client : \n response = client . get ( '<STR_LIT>' ) \n assert response . status_code < <NUM_LIT> \n def test_lyrics_route ( ) : \n with app . test_client ( ) as client : \n response = client . get ( '<STR_LIT>' ) \n assert response . status_code < <NUM_LIT>"}, {"input": "from importlib import metadata \n suppress_warnings = [ \"<STR_LIT>\" ] \n extensions = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n myst_enable_extensions = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ogp_image = \"<STR_LIT>\" \n templates_path = [ \"<STR_LIT>\" ] \n source_suffix = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n master_doc = \"<STR_LIT>\" \n project = \"<STR_LIT>\" \n author = \"<STR_LIT>\" \n copyright = f\"<STR_LIT>\" \n release = metadata . version ( \"<STR_LIT>\" ) \n version = release . rsplit ( \"<STR_LIT>\" , <NUM_LIT> ) [ <NUM_LIT> ] \n if \"<STR_LIT>\" in release : \n release = version = \"<STR_LIT>\" \n exclude_patterns = [ \"<STR_LIT>\" ] \n nitpick_ignore = [ \n * [ ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) for i in range ( <NUM_LIT> , <NUM_LIT> ) ] , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] \n add_function_parentheses = True \n autodoc_typehints = \"<STR_LIT>\" \n autodoc_typehints_description_target = \"<STR_LIT>\" \n html_theme = \"<STR_LIT>\" \n html_theme_options = {", "output": "\"<STR_LIT>\" : None , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n \"<STR_LIT>\" , \n } , \n } \n html_logo = \"<STR_LIT>\" \n html_static_path = [ \"<STR_LIT>\" ] \n html_css_files = [ \"<STR_LIT>\" ] \n htmlhelp_basename = \"<STR_LIT>\" \n _descr = f\"<STR_LIT>\" \n _title = \"<STR_LIT>\" \n rst_epilog = \n linkcheck_ignore = [ \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" , \n ] \n intersphinx_mapping = { \n \"<STR_LIT>\" : ( \"<STR_LIT>\" , None ) , \n \"<STR_LIT>\" : ( \"<STR_LIT>\" , None ) , \n \"<STR_LIT>\" : ( \"<STR_LIT>\" , None ) , \n \"<STR_LIT>\" : ( \"<STR_LIT>\" , None ) , \n \"<STR_LIT>\" : ( \n \"<STR_LIT>\" , \n None , \n ) , \n }"}, {"input": "import gc \n from unittest . mock import Mock \n import pytest \n import svcs \n from . fake_factories import ( \n async_bool_cm_factory , \n async_str_gen_factory , \n bool_cm_factory , \n str_gen_factory , \n ) \n from . ifaces import AnotherService , Service , YetAnotherService \n class TestContainer : \n def test_get_pings_empty ( self , container ) : \n assert [ ] == container . get_pings ( ) \n @ pytest . mark . asyncio ( ) \n async def test_repr ( self , registry , container ) : \n registry . register_factory ( Service , str_gen_factory ) \n registry . register_factory ( bool , bool_cm_factory ) \n registry . register_factory ( AnotherService , async_str_gen_factory ) \n registry . register_factory ( YetAnotherService , async_bool_cm_factory ) \n container . get ( Service ) \n container . get ( bool ) \n await container . aget ( AnotherService ) \n await container . aget ( YetAnotherService ) \n assert \"<STR_LIT>\" == repr ( container ) \n await container . aclose ( ) \n def test_contains ( self , container ) : \n container . registry . register_value ( int , <NUM_LIT> ) \n assert int not in container \n container . get ( int ) \n assert int in container \n def test_context_manager ( self , container , close_me ) : \n def factory ( ) : \n yield <NUM_LIT> \n close_me . close ( ) \n container . registry . register_factory ( int , factory ) \n with container : \n assert <NUM_LIT> == container . get ( int ) \n assert close_me . is_closed \n @ pytest . mark . asyncio ( ) \n async def test_async_context_manager ( self , container , close_me ) : \n async def factory ( ) : \n yield <NUM_LIT> \n await close_me . aclose ( ) \n container . registry . register_factory ( int , factory ) \n async with container : \n assert <NUM_LIT> == await container . aget ( int ) \n assert close_me . is_aclosed \n def test_gc_warning ( self , recwarn , registry ) : \n def scope ( ) : \n container = svcs . Container ( registry )", "output": "registry . register_factory ( str , str_gen_factory ) \n container . get ( str ) \n scope ( ) \n gc . collect ( ) \n assert ( \n \"<STR_LIT>\" , \n ) == recwarn . list [ <NUM_LIT> ] . message . args \n class TestServicePing : \n def test_ping ( self , registry , container , close_me ) : \n def factory ( ) : \n yield Service ( ) \n close_me . close ( ) \n ping = Mock ( spec_set = [ \"<STR_LIT>\" ] ) \n registry . register_factory ( Service , factory , ping = ping ) \n ( svc_ping , ) = container . get_pings ( ) \n svc_ping . ping ( ) \n ping . assert_called_once ( ) \n assert not close_me . is_closed \n container . close ( ) \n assert close_me . is_closed \n assert not container . _instantiated \n assert not container . _on_close"}, {"input": "import os \n import json \n import pathlib \n from random import shuffle \n from rvc . configs . config import Config \n config = Config ( ) \n current_directory = os . getcwd ( ) \n def generate_config ( rvc_version , sampling_rate , model_path ) : \n if rvc_version == \"<STR_LIT>\" or sampling_rate == \"<STR_LIT>\" : \n config_path = f\"<STR_LIT>\" \n else : \n config_path = f\"<STR_LIT>\" \n config_save_path = os . path . join ( model_path , \"<STR_LIT>\" ) \n if not pathlib . Path ( config_save_path ) . exists ( ) : \n with open ( config_save_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : \n json . dump ( \n config . json_config [ config_path ] , \n f , \n ensure_ascii = False , \n indent = <NUM_LIT> , \n sort_keys = True , \n ) \n f . write ( \"<STR_LIT>\" ) \n def generate_filelist ( f0_method , model_path , rvc_version , sampling_rate ) : \n gt_wavs_dir = f\"<STR_LIT>\" \n feature_dir = ( \n f\"<STR_LIT>\" \n if rvc_version == \"<STR_LIT>\" \n else f\"<STR_LIT>\" \n ) \n if f0_method : \n f0_dir = f\"<STR_LIT>\" \n f0nsf_dir = f\"<STR_LIT>\" \n names = ( \n set ( [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( gt_wavs_dir ) ] ) \n & set ( [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( feature_dir ) ] ) \n & set ( [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( f0_dir ) ] ) \n & set ( [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( f0nsf_dir ) ] ) \n ) \n else : \n names = set ( [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( gt_wavs_dir ) ] ) & set ( \n [ name . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] for name in os . listdir ( feature_dir ) ] \n ) \n options = [ ] \n for name in names : \n if f0_method : \n options . append ( \n f\"<STR_LIT>\" \n ) \n else : \n options . append ( f\"<STR_LIT>\" ) \n fea_dim = <NUM_LIT> if rvc_version == \"<STR_LIT>\" else <NUM_LIT> \n if f0_method : \n for _ in range ( <NUM_LIT> ) : \n options . append ( \n f\"<STR_LIT>\"", "output": ") \n else : \n for _ in range ( <NUM_LIT> ) : \n options . append ( \n f\"<STR_LIT>\" \n ) \n shuffle ( options ) \n with open ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n f . write ( \"<STR_LIT>\" . join ( options ) )"}, {"input": "from termcolor import colored \n import requests \n import copy \n import posthog \n import openai \n from openai import ChatCompletion \n import traceback \n from uuid import uuid4 \n from waitress import serve \n from flask import Flask , request \n from uuid import uuid4 \n import traceback \n from threading import active_count \n import random \n import time \n import asyncio \n import signal \n from posthog import Posthog \n posthog = Posthog ( \n project_api_key = '<STR_LIT>' , \n host = '<STR_LIT>' ) \n class CustomError ( Exception ) : \n def __init__ ( self , error ) : \n self . error = error \n class IndividualRequest : \n def __init__ ( self , \n model = None , \n fallback_strategy = [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' \n ] , \n azure_fallback_strategy = None , \n graceful_string = \"<STR_LIT>\" , \n user_email = \"<STR_LIT>\" , \n user_token = \"<STR_LIT>\" , \n send_notification = False , \n logging_fn = None , \n backup_openai_key = \"<STR_LIT>\" , \n caching = False ,", "output": "alerting = None , \n max_threads = None , \n _test = False , \n verbose = False ) : \n self . model = model \n self . model_function = model . get_model_function ( ) \n self . verbose = verbose \n self . graceful_string = graceful_string \n self . fallback_strategy = fallback_strategy \n self . user_email = user_email \n self . user_token = user_token \n self . save_request = logging_fn \n self . backup_openai_key = backup_openai_key \n self . _test = _test \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . caching = caching \n self . max_threads = max_threads \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . alerting = alerting \n self . azure_fallback_strategy = azure_fallback_strategy \n self . backup_model = None \n self . set_cooldown = False \n self . cooldown_start_time = time . time ( ) \n def handle_unhandled_exception ( self , e ) : \n self . print_verbose ( colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n if self . alerting : \n self . alerting . add_error ( error_type = \"<STR_LIT>\" , error_description = traceback . format_exc ( ) ) \n def print_verbose ( self , print_statement ) : \n posthog . capture ( '<STR_LIT>' , '<STR_LIT>' , { '<STR_LIT>' : str ( print_statement ) [ : <NUM_LIT> ] } ) \n if self . verbose : \n print ( colored ( \"<STR_LIT>\" + str ( print_statement ) , \"<STR_LIT>\" ) ) \n def start_cooldown ( self ) : \n self . set_cooldown = True \n self . cooldown_start_time = time . time ( ) \n def call_model ( self , args , kwargs ) : \n try : \n if self . _test : \n error = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n raise CustomError ( error ) \n if self . set_cooldown : \n if time . time ( ) - self . cooldown_start_time > <NUM_LIT> : \n error = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n raise ( CustomError ( error = error ) ) \n else : \n self . set_cooldown = False \n result = self . model_function ( * args , ** kwargs ) \n if result == None : \n self . print_verbose ( f\"<STR_LIT>\" ) \n return \n error = { \"<STR_LIT>\" : f\"<STR_LIT>\" } \n raise CustomError ( error ) \n if \"<STR_LIT>\" in kwargs : \n if \"<STR_LIT>\" in kwargs : \n self . curr_azure_model = kwargs [ \"<STR_LIT>\" ] \n if self . caching : \n self . print_verbose ( kwargs [ \"<STR_LIT>\" ] ) \n input_prompt = \"<STR_LIT>\" . join ( message [ \"<STR_LIT>\" ] \n for message in kwargs [ \"<STR_LIT>\" ] ) \n extracted_result = result [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n self . print_verbose ( f'<STR_LIT>' ) \n self . add_cache ( \n input_prompt , extracted_result \n ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n return result \n except Exception as e : \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . print_verbose ( \"<STR_LIT>\" ) \n self . start_cooldown ( ) \n return self . handle_exception ( args , kwargs , e ) \n async def async_call_model ( self , args , kwargs ) : \n try : \n if self . _test : \n error = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n raise CustomError ( error ) \n if self . set_cooldown : \n if time . time ( ) - self . cooldown_start_time > <NUM_LIT> : \n error = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n raise ( CustomError ( error = error ) ) \n else : \n self . set_cooldown = False \n result = await self . model_function ( * args , ** kwargs ) \n if \"<STR_LIT>\" in kwargs : \n if \"<STR_LIT>\" in kwargs : \n self . curr_azure_model = kwargs [ \"<STR_LIT>\" ] \n if self . caching : \n self . print_verbose ( kwargs [ \"<STR_LIT>\" ] ) \n input_prompt = \"<STR_LIT>\" . join ( message [ \"<STR_LIT>\" ] \n for message in kwargs [ \"<STR_LIT>\" ] ) \n extracted_result = result [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n self . print_verbose ( f'<STR_LIT>' ) \n self . add_cache ( \n input_prompt , extracted_result \n ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n return result \n except Exception as e : \n self . print_verbose ( \"<STR_LIT>\" ) \n self . start_cooldown ( ) \n return self . handle_exception ( args , kwargs , e ) \n def __call__ ( self , * args , ** kwargs ) : \n try : \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n try : \n self . save_request ( \n user_email = self . user_email , \n graceful_string = self . graceful_string , \n posthog_event = '<STR_LIT>' , \n ) \n except : \n self . print_verbose ( \"<STR_LIT>\" ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n if self . max_threads and self . caching : \n self . print_verbose ( f'<STR_LIT>' ) \n thread_utilization = active_count ( ) / self . max_threads \n self . print_verbose ( f\"<STR_LIT>\" ) \n if thread_utilization > <NUM_LIT> : \n if \"<STR_LIT>\" in kwargs and self . caching : \n self . print_verbose ( kwargs [ \"<STR_LIT>\" ] ) \n input_prompt = \"<STR_LIT>\" . join ( message [ \"<STR_LIT>\" ] \n for message in kwargs [ \"<STR_LIT>\" ] ) \n self . print_verbose ( \n f\"<STR_LIT>\" ) \n result = self . try_cache_request ( query = input_prompt ) \n if self . alerting : \n self . alerting . add_error ( error_type = \"<STR_LIT>\" , error_description = \"<STR_LIT>\" ) \n if result == None : \n pass \n else : \n self . print_verbose ( f\"<STR_LIT>\" ) \n self . save_request ( \n user_email = self . user_email , \n posthog_event = '<STR_LIT>' , \n graceful_string = self . graceful_string , \n result = result , \n posthog_metadata = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : result , \n } , \n errors = [ '<STR_LIT>' ] , \n function_name = str ( self . model_function ) , \n kwargs = kwargs \n ) \n return result \n if asyncio . iscoroutinefunction ( self . model_function ) : \n return self . async_call_model ( args = args , kwargs = kwargs ) \n else : \n return self . call_model ( args = args , kwargs = kwargs ) \n except Exception as e : \n self . print_verbose ( f\"<STR_LIT>\" ) \n def add_cache ( self , input_prompt , response ) : \n try : \n if self . caching : \n if request : \n if request . args and request . args . get ( \"<STR_LIT>\" ) : \n customer_id = request . args . get ( \"<STR_LIT>\" ) \n if request . args . get ( \"<STR_LIT>\" ) : \n instance_id = request . args . get ( \"<STR_LIT>\" ) \n else : \n instance_id = <NUM_LIT> \n user_email = self . user_email \n url = \"<STR_LIT>\" \n querystring = { \n \"<STR_LIT>\" : customer_id , \n \"<STR_LIT>\" : instance_id , \n \"<STR_LIT>\" : user_email , \n \"<STR_LIT>\" : input_prompt , \n \"<STR_LIT>\" : response \n } \n response = requests . post ( url , params = querystring ) \n except : \n pass \n def try_cache_request ( self , query = None ) : \n try : \n if query : \n self . print_verbose ( \"<STR_LIT>\" ) \n if request : \n if request . args and request . args . get ( \"<STR_LIT>\" ) : \n customer_id = request . args . get ( \"<STR_LIT>\" ) \n if request . args . get ( \"<STR_LIT>\" ) : \n instance_id = request . args . get ( \"<STR_LIT>\" ) \n else : \n instance_id = <NUM_LIT> \n user_email = self . user_email \n url = \"<STR_LIT>\" \n querystring = { \n \"<STR_LIT>\" : customer_id , \n \"<STR_LIT>\" : instance_id , \n \"<STR_LIT>\" : user_email , \n \"<STR_LIT>\" : query , \n } \n response = requests . get ( url , params = querystring ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n extracted_result = response . json ( ) [ \"<STR_LIT>\" ] \n results = { \"<STR_LIT>\" : [ { \"<STR_LIT>\" : { \"<STR_LIT>\" : extracted_result } } ] } \n return results \n except : \n traceback . print_exc ( ) \n pass \n self . print_verbose ( f\"<STR_LIT>\" ) \n return None \n def fallback_request ( self , args , kwargs , fallback_strategy ) : \n try : \n self . print_verbose ( \"<STR_LIT>\" ) \n result = None \n new_kwargs = copy . deepcopy ( kwargs ) \n if self . backup_openai_key and len ( \n self . backup_openai_key \n ) > <NUM_LIT> : \n new_kwargs [ \"<STR_LIT>\" ] = openai . __dict__ . copy ( \n ) \n if \"<STR_LIT>\" in str ( self . model_function ) : \n fallback_strategy = [ \"<STR_LIT>\" ] \n if self . azure_fallback_strategy : \n for engine in self . azure_fallback_strategy : \n new_kwargs [ \"<STR_LIT>\" ] = engine \n new_kwargs [ \"<STR_LIT>\" ] = True \n self . print_verbose ( f\"<STR_LIT>\" ) \n result = self . make_LLM_request ( new_kwargs ) \n if result != None : \n return result \n for model in fallback_strategy : \n new_kwargs [ '<STR_LIT>' ] = model \n result = self . make_LLM_request ( new_kwargs ) \n if result != None : \n return result \n return None \n except : \n self . print_verbose ( traceback . format_exc ( ) ) \n return None \n def make_LLM_request ( self , new_kwargs ) : \n embedding_model = self . model . get_original_embeddings ( ) \n chat_model = self . model . get_original_chat ( ) \n completion_model = self . model . get_original_completion ( ) \n try : \n self . print_verbose ( f\"<STR_LIT>\" ) \n if \"<STR_LIT>\" in new_kwargs : \n new_kwargs_except_azure_fallback_flag = { \n k : v \n for k , v in new_kwargs . items ( ) if k != \"<STR_LIT>\" \n } \n return chat_model ( ** new_kwargs_except_azure_fallback_flag ) \n if \"<STR_LIT>\" in new_kwargs : \n openai . api_type = \"<STR_LIT>\" \n openai . api_base = \"<STR_LIT>\" \n openai . api_version = None \n openai . api_key = self . backup_openai_key \n new_kwargs_except_openai_attributes = { \n k : v \n for k , v in new_kwargs . items ( ) if k != \"<STR_LIT>\" \n } \n new_kwargs_except_engine = { \n k : v \n for k , v in new_kwargs_except_openai_attributes . items ( ) \n if k != \"<STR_LIT>\" \n } \n completion = self . model_function ( ** new_kwargs_except_engine ) \n openai . api_type = new_kwargs [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n openai . api_base = new_kwargs [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n openai . api_version = new_kwargs [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n openai . api_key = new_kwargs [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n return completion \n if \"<STR_LIT>\" in str ( self . model_function ) : \n self . print_verbose ( colored ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n return embedding_model ( ** new_kwargs ) \n model = str ( new_kwargs [ '<STR_LIT>' ] ) \n self . print_verbose ( \n colored ( f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n if \"<STR_LIT>\" in model or \"<STR_LIT>\" in model : \n self . print_verbose ( \n colored ( \n f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n return chat_model ( ** new_kwargs ) \n else : \n self . print_verbose ( \n colored ( f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n new_kwargs [ '<STR_LIT>' ] = \"<STR_LIT>\" . join ( \n [ message [ \"<STR_LIT>\" ] for message in new_kwargs [ '<STR_LIT>' ] ] ) \n new_kwargs . pop ( '<STR_LIT>' , \n None ) \n return completion_model ( ** new_kwargs ) \n except Exception as e : \n self . print_verbose ( colored ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n raise ValueError ( e ) \n def api_key_handler ( self , args , kwargs , fallback_strategy , user_email , \n user_token ) : \n try : \n url = f\"<STR_LIT>\" \n response = requests . get ( url ) \n if response . status_code == <NUM_LIT> : \n result = response . json ( ) \n if result [ '<STR_LIT>' ] == '<STR_LIT>' : \n self . print_verbose ( \n colored ( \n f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n return None \n fallback_keys = result [ '<STR_LIT>' ] [ \n '<STR_LIT>' ] \n if len ( fallback_keys ) == <NUM_LIT> : \n return None \n for fallback_key in fallback_keys : \n openai . api_key = fallback_key \n result = self . make_LLM_request ( kwargs ) \n if result != None : \n return result \n else : \n self . print_verbose ( \n colored ( \n f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n return None \n except Exception as e : \n raise ValueError ( e ) \n def handle_openAI_error ( self , \n args , \n kwargs , \n openAI_error , \n fallback_strategy , \n graceful_string , \n user_email = \"<STR_LIT>\" , \n user_token = \"<STR_LIT>\" ) : \n self . print_verbose ( \n colored ( f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n if openAI_error != None : \n openAI_error = openAI_error . error \n error_type = None \n if openAI_error != None and '<STR_LIT>' in openAI_error : \n error_type = openAI_error [ '<STR_LIT>' ] \n if error_type == '<STR_LIT>' or error_type == '<STR_LIT>' : \n if openAI_error . code == '<STR_LIT>' : \n self . print_verbose ( \n colored ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n fallback_strategy = [ '<STR_LIT>' ] + fallback_strategy \n result = self . fallback_request ( args = args , \n kwargs = kwargs , \n fallback_strategy = fallback_strategy ) \n if result == None : \n return graceful_string \n else : \n return result \n if openAI_error . code == \"<STR_LIT>\" : \n self . print_verbose ( \n colored ( \"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n result = self . api_key_handler ( args = args , \n kwargs = kwargs , \n fallback_strategy = fallback_strategy , \n user_email = user_email , \n user_token = user_token ) \n if result == None : \n return graceful_string \n else : \n return result \n elif error_type == '<STR_LIT>' or error_type == '<STR_LIT>' : \n self . print_verbose ( colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n return graceful_string \n result = self . fallback_request ( args = args , \n kwargs = kwargs , \n fallback_strategy = fallback_strategy ) \n if result == None : \n return graceful_string \n else : \n return result \n return graceful_string \n def handle_exception ( self , args , kwargs , e ) : \n result = self . graceful_string \n try : \n self . print_verbose ( colored ( f\"<STR_LIT>\" , '<STR_LIT>' ) ) \n result = self . handle_openAI_error ( \n args = args , \n kwargs = kwargs , \n openAI_error = e , \n fallback_strategy = self . fallback_strategy , \n graceful_string = self . graceful_string , \n user_email = self . user_email , \n user_token = self . user_token ) \n self . print_verbose ( \n colored ( f\"<STR_LIT>\" , \n \"<STR_LIT>\" ) ) \n if result == self . graceful_string : \n if \"<STR_LIT>\" in kwargs and self . caching : \n self . print_verbose ( kwargs [ \"<STR_LIT>\" ] ) \n input_prompt = \"<STR_LIT>\" . join ( message [ \"<STR_LIT>\" ] \n for message in kwargs [ \"<STR_LIT>\" ] ) \n cached_response = self . try_cache_request ( query = input_prompt ) \n if cached_response == None : \n pass \n else : \n self . save_request ( \n user_email = self . user_email , \n posthog_event = '<STR_LIT>' , \n graceful_string = self . graceful_string , \n result = cached_response , \n posthog_metadata = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : cached_response , \n } , \n errors = [ '<STR_LIT>' ] , \n function_name = str ( self . model_function ) , \n kwargs = kwargs \n ) \n return cached_response \n self . save_request ( \n user_email = self . user_email , \n graceful_string = self . graceful_string , \n posthog_event = '<STR_LIT>' , \n result = result , \n posthog_metadata = { \n '<STR_LIT>' : str ( e ) , \n '<STR_LIT>' : result \n } , \n errors = [ e ] , \n function_name = str ( self . model_function ) , \n kwargs = kwargs ) \n else : \n self . save_request ( user_email = self . user_email , \n graceful_string = self . graceful_string , \n posthog_event = \"<STR_LIT>\" , \n result = result , \n posthog_metadata = { \n '<STR_LIT>' : str ( e ) , \n '<STR_LIT>' : result \n } , \n errors = [ e ] , \n function_name = str ( self . model_function ) , \n kwargs = kwargs ) \n except Exception as e2 : \n traceback . print_exc ( ) \n self . print_verbose ( \"<STR_LIT>\" , e2 ) \n self . save_request ( \n user_email = self . user_email , \n graceful_string = self . graceful_string , \n posthog_event = '<STR_LIT>' , \n result = \"<STR_LIT>\" , \n posthog_metadata = { \n '<STR_LIT>' : str ( e ) , \n '<STR_LIT>' : str ( e2 ) , \n '<STR_LIT>' : self . graceful_string \n } , \n errors = [ e , e2 ] , \n function_name = str ( self . model_function ) , \n kwargs = kwargs ) \n raise e \n return result"}, {"input": "from lib import itchat \n from lib . itchat . content import * \n import json \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n from common . log import logger \n from config import conf \n import requests \n import io \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ itchat . msg_register ( TEXT ) \n def handler_single_msg ( msg ) : \n WechatChannel ( ) . handle ( msg ) \n return None \n @ itchat . msg_register ( TEXT , isGroupChat = True ) \n def handler_group_msg ( msg ) : \n WechatChannel ( ) . handle_group ( msg ) \n return None \n class WechatChannel ( Channel ) : \n def __init__ ( self ) : \n pass \n def startup ( self ) : \n itchat . auto_login ( enableCmdQR = <NUM_LIT> ) \n itchat . run ( ) \n def handle ( self , msg ) : \n logger . debug ( \"<STR_LIT>\" + json . dumps ( msg , ensure_ascii = False ) ) \n from_user_id = msg [ '<STR_LIT>' ] \n to_user_id = msg [ '<STR_LIT>' ] \n other_user_id = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n content = msg [ '<STR_LIT>' ] \n match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) \n if from_user_id == other_user_id and match_prefix is not None : \n if match_prefix != '<STR_LIT>' : \n str_list = content . split ( match_prefix , <NUM_LIT> ) \n if len ( str_list ) == <NUM_LIT> : \n content = str_list [ <NUM_LIT> ] . strip ( ) \n img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) \n if img_match_prefix : \n content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n thread_pool . submit ( self . _do_send_img , content , from_user_id ) \n else : \n thread_pool . submit ( self . _do_send , content , from_user_id ) \n elif to_user_id == other_user_id and match_prefix : \n str_list = content . split ( match_prefix , <NUM_LIT> ) \n if len ( str_list ) == <NUM_LIT> : \n content = str_list [ <NUM_LIT> ] . strip ( ) \n img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) \n if img_match_prefix : \n content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n thread_pool . submit ( self . _do_send_img , content , to_user_id ) \n else : \n thread_pool . submit ( self . _do_send , content , to_user_id ) \n def handle_group ( self , msg ) :", "output": "logger . debug ( \"<STR_LIT>\" + json . dumps ( msg , ensure_ascii = False ) ) \n group_name = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) \n group_id = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) \n if not group_name : \n return \"<STR_LIT>\" \n origin_content = msg [ '<STR_LIT>' ] \n content = msg [ '<STR_LIT>' ] \n content_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) \n context_special_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) \n if len ( context_special_list ) == <NUM_LIT> : \n content = context_special_list [ <NUM_LIT> ] \n elif len ( content_list ) == <NUM_LIT> : \n content = content_list [ <NUM_LIT> ] \n config = conf ( ) \n match_prefix = ( msg [ '<STR_LIT>' ] and not config . get ( \"<STR_LIT>\" , False ) ) or self . check_prefix ( origin_content , config . get ( '<STR_LIT>' ) ) or self . check_contain ( origin_content , config . get ( '<STR_LIT>' ) ) \n if ( '<STR_LIT>' in config . get ( '<STR_LIT>' ) or group_name in config . get ( '<STR_LIT>' ) or self . check_contain ( group_name , config . get ( '<STR_LIT>' ) ) ) and match_prefix : \n img_match_prefix = self . check_prefix ( content , conf ( ) . get ( '<STR_LIT>' ) ) \n if img_match_prefix : \n content = content . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n thread_pool . submit ( self . _do_send_img , content , group_id ) \n else : \n thread_pool . submit ( self . _do_send_group , content , msg ) \n def send ( self , msg , receiver ) : \n logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) \n itchat . send ( msg , toUserName = receiver ) \n def _do_send ( self , query , reply_user_id ) : \n try : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = reply_user_id \n reply_text = super ( ) . build_reply_content ( query , context ) \n if reply_text : \n self . send ( conf ( ) . get ( \"<STR_LIT>\" ) + reply_text , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def _do_send_img ( self , query , reply_user_id ) : \n try : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n img_url = super ( ) . build_reply_content ( query , context ) \n if not img_url : \n return \n pic_res = requests . get ( img_url , stream = True ) \n image_storage = io . BytesIO ( ) \n for block in pic_res . iter_content ( <NUM_LIT> ) : \n image_storage . write ( block ) \n image_storage . seek ( <NUM_LIT> ) \n logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) \n itchat . send_image ( image_storage , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def _do_send_group ( self , query , msg ) : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] \n reply_text = super ( ) . build_reply_content ( query , context ) \n if reply_text : \n reply_text = '<STR_LIT>' + msg [ '<STR_LIT>' ] + '<STR_LIT>' + reply_text . strip ( ) \n self . send ( conf ( ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) + reply_text , msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n def check_prefix ( self , content , prefix_list ) : \n for prefix in prefix_list : \n if content . startswith ( prefix ) : \n return prefix \n return None \n def check_contain ( self , content , keyword_list ) : \n if not keyword_list : \n return None \n for ky in keyword_list : \n if content . find ( ky ) != - <NUM_LIT> : \n return True \n return None"}, {"input": "import torch \n from torch . nn import functional as F \n import numpy as np \n DEFAULT_MIN_BIN_WIDTH = <NUM_LIT> \n DEFAULT_MIN_BIN_HEIGHT = <NUM_LIT> \n DEFAULT_MIN_DERIVATIVE = <NUM_LIT> \n def piecewise_rational_quadratic_transform ( \n inputs , \n unnormalized_widths , \n unnormalized_heights , \n unnormalized_derivatives , \n inverse = False , \n tails = None , \n tail_bound = <NUM_LIT> , \n min_bin_width = DEFAULT_MIN_BIN_WIDTH , \n min_bin_height = DEFAULT_MIN_BIN_HEIGHT , \n min_derivative = DEFAULT_MIN_DERIVATIVE , \n ) : \n if tails is None : \n spline_fn = rational_quadratic_spline \n spline_kwargs = { } \n else : \n spline_fn = unconstrained_rational_quadratic_spline \n spline_kwargs = { \"<STR_LIT>\" : tails , \"<STR_LIT>\" : tail_bound } \n outputs , logabsdet = spline_fn ( \n inputs = inputs , \n unnormalized_widths = unnormalized_widths , \n unnormalized_heights = unnormalized_heights , \n unnormalized_derivatives = unnormalized_derivatives , \n inverse = inverse , \n min_bin_width = min_bin_width , \n min_bin_height = min_bin_height , \n min_derivative = min_derivative , \n ** spline_kwargs \n ) \n return outputs , logabsdet \n def searchsorted ( bin_locations , inputs , eps = <NUM_LIT> ) : \n bin_locations [ ... , - <NUM_LIT> ] += eps \n return torch . sum ( inputs [ ... , None ] >= bin_locations , dim = - <NUM_LIT> ) - <NUM_LIT> \n def unconstrained_rational_quadratic_spline ( \n inputs , \n unnormalized_widths , \n unnormalized_heights , \n unnormalized_derivatives , \n inverse = False , \n tails = \"<STR_LIT>\" , \n tail_bound = <NUM_LIT> , \n min_bin_width = DEFAULT_MIN_BIN_WIDTH , \n min_bin_height = DEFAULT_MIN_BIN_HEIGHT , \n min_derivative = DEFAULT_MIN_DERIVATIVE , \n ) : \n inside_interval_mask = ( inputs >= - tail_bound ) & ( inputs <= tail_bound ) \n outside_interval_mask = ~ inside_interval_mask \n outputs = torch . zeros_like ( inputs ) \n logabsdet = torch . zeros_like ( inputs ) \n if tails == \"<STR_LIT>\" : \n unnormalized_derivatives = F . pad ( unnormalized_derivatives , pad = ( <NUM_LIT> , <NUM_LIT> ) ) \n constant = np . log ( np . exp ( <NUM_LIT> - min_derivative ) - <NUM_LIT> ) \n unnormalized_derivatives [ ... , <NUM_LIT> ] = constant \n unnormalized_derivatives [ ... , - <NUM_LIT> ] = constant \n outputs [ outside_interval_mask ] = inputs [ outside_interval_mask ] \n logabsdet [ outside_interval_mask ] = <NUM_LIT> \n else : \n raise RuntimeError ( \"<STR_LIT>\" . format ( tails ) ) \n ( \n outputs [ inside_interval_mask ] , \n logabsdet [ inside_interval_mask ] , \n ) = rational_quadratic_spline ( \n inputs = inputs [ inside_interval_mask ] , \n unnormalized_widths = unnormalized_widths [ inside_interval_mask , : ] , \n unnormalized_heights = unnormalized_heights [ inside_interval_mask , : ] , \n unnormalized_derivatives = unnormalized_derivatives [ inside_interval_mask , : ] , \n inverse = inverse , \n left = - tail_bound , \n right = tail_bound , \n bottom = - tail_bound , \n top = tail_bound , \n min_bin_width = min_bin_width , \n min_bin_height = min_bin_height , \n min_derivative = min_derivative , \n ) \n return outputs , logabsdet \n def rational_quadratic_spline ( \n inputs , \n unnormalized_widths , \n unnormalized_heights , \n unnormalized_derivatives , \n inverse = False , \n left = <NUM_LIT> , \n right = <NUM_LIT> , \n bottom = <NUM_LIT> , \n top = <NUM_LIT> , \n min_bin_width = DEFAULT_MIN_BIN_WIDTH , \n min_bin_height = DEFAULT_MIN_BIN_HEIGHT , \n min_derivative = DEFAULT_MIN_DERIVATIVE , \n ) : \n if torch . min ( inputs ) < left or torch . max ( inputs ) > right : \n raise ValueError ( \"<STR_LIT>\" ) \n num_bins = unnormalized_widths . shape [ - <NUM_LIT> ] \n if min_bin_width * num_bins > <NUM_LIT> : \n raise ValueError ( \"<STR_LIT>\" ) \n if min_bin_height * num_bins > <NUM_LIT> : \n raise ValueError ( \"<STR_LIT>\" ) \n widths = F . softmax ( unnormalized_widths , dim = - <NUM_LIT> ) \n widths = min_bin_width + ( <NUM_LIT> - min_bin_width * num_bins ) * widths \n cumwidths = torch . cumsum ( widths , dim = - <NUM_LIT> ) \n cumwidths = F . pad ( cumwidths , pad = ( <NUM_LIT> , <NUM_LIT> ) , mode = \"<STR_LIT>\" , value = <NUM_LIT> ) \n cumwidths = ( right - left ) * cumwidths + left \n cumwidths [ ... , <NUM_LIT> ] = left \n cumwidths [ ... , - <NUM_LIT> ] = right \n widths = cumwidths [ ... , <NUM_LIT> : ] - cumwidths [ ... , : - <NUM_LIT> ] \n derivatives = min_derivative + F . softplus ( unnormalized_derivatives ) \n heights = F . softmax ( unnormalized_heights , dim = - <NUM_LIT> ) \n heights = min_bin_height + ( <NUM_LIT> - min_bin_height * num_bins ) * heights \n cumheights = torch . cumsum ( heights , dim = - <NUM_LIT> ) \n cumheights = F . pad ( cumheights , pad = ( <NUM_LIT> , <NUM_LIT> ) , mode = \"<STR_LIT>\" , value = <NUM_LIT> ) \n cumheights = ( top - bottom ) * cumheights + bottom \n cumheights [ ... , <NUM_LIT> ] = bottom \n cumheights [ ... , - <NUM_LIT> ] = top \n heights = cumheights [ ... , <NUM_LIT> : ] - cumheights [ ... , : - <NUM_LIT> ] \n if inverse : \n bin_idx = searchsorted ( cumheights , inputs ) [ ... , None ] \n else : \n bin_idx = searchsorted ( cumwidths , inputs ) [ ... , None ] \n input_cumwidths = cumwidths . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n input_bin_widths = widths . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n input_cumheights = cumheights . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n delta = heights / widths \n input_delta = delta . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n input_derivatives = derivatives . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n input_derivatives_plus_one = derivatives [ ... , <NUM_LIT> : ] . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n input_heights = heights . gather ( - <NUM_LIT> , bin_idx ) [ ... , <NUM_LIT> ] \n if inverse : \n a = ( inputs - input_cumheights ) * ( \n input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta \n ) + input_heights * ( input_delta - input_derivatives ) \n b = input_heights * input_derivatives - ( inputs - input_cumheights ) * ( \n input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta \n ) \n c = - input_delta * ( inputs - input_cumheights ) \n discriminant = b . pow ( <NUM_LIT> ) - <NUM_LIT> * a * c \n assert ( discriminant >= <NUM_LIT> ) . all ( ) \n root = ( <NUM_LIT> * c ) / ( - b - torch . sqrt ( discriminant ) ) \n outputs = root * input_bin_widths + input_cumwidths \n theta_one_minus_theta = root * ( <NUM_LIT> - root ) \n denominator = input_delta + ( \n ( input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta ) \n * theta_one_minus_theta \n ) \n derivative_numerator = input_delta . pow ( <NUM_LIT> ) * ( \n input_derivatives_plus_one * root . pow ( <NUM_LIT> ) \n + <NUM_LIT> * input_delta * theta_one_minus_theta \n + input_derivatives * ( <NUM_LIT> - root ) . pow ( <NUM_LIT> ) \n ) \n logabsdet = torch . log ( derivative_numerator ) - <NUM_LIT> * torch . log ( denominator ) \n return outputs , - logabsdet \n else : \n theta = ( inputs - input_cumwidths ) / input_bin_widths \n theta_one_minus_theta = theta * ( <NUM_LIT> - theta ) \n numerator = input_heights * ( \n input_delta * theta . pow ( <NUM_LIT> ) + input_derivatives * theta_one_minus_theta \n ) \n denominator = input_delta + ( \n ( input_derivatives + input_derivatives_plus_one - <NUM_LIT> * input_delta ) \n * theta_one_minus_theta \n ) \n outputs = input_cumheights + numerator / denominator \n derivative_numerator = input_delta . pow ( <NUM_LIT> ) * ( \n input_derivatives_plus_one * theta . pow ( <NUM_LIT> ) \n + <NUM_LIT> * input_delta * theta_one_minus_theta \n + input_derivatives * ( <NUM_LIT> - theta ) . pow ( <NUM_LIT> ) \n )", "output": "logabsdet = torch . log ( derivative_numerator ) - <NUM_LIT> * torch . log ( denominator ) \n return outputs , logabsdet"}, {"input": "import psycopg2 \n from configparser import ConfigParser \n def leer_config ( archivo = \"<STR_LIT>\" , seccion = '<STR_LIT>' ) : \n parser = ConfigParser ( ) \n parser . read ( archivo ) \n bd = { } \n if parser . has_section ( seccion ) : \n params = parser . items ( seccion ) \n for param in params : \n bd [ param [ <NUM_LIT> ] ] = param [ <NUM_LIT> ] \n return bd \n def conectar ( ) : \n conexion = None \n try : \n params = leer_config ( ) \n print ( params ) \n print ( \"<STR_LIT>\" ) \n conexion = psycopg2 . connect ( ** params ) \n cursor = conexion . cursor ( ) \n cursor . execute ( \"<STR_LIT>\" ) \n version = cursor . fetchone ( ) \n print ( version ) \n cursor . close ( ) \n except ( Exception , psycopg2 . DatabaseError ) as error :", "output": "print ( error ) \n finally : \n if conexion is not None : \n conexion . close ( ) \n print ( \"<STR_LIT>\" ) \n conectar ( )"}, {"input": "from flask import Blueprint , jsonify , request \n from app . repositories . horoscope_repository import HoroscopeRepository \n from app . services . horoscope_service import HoroscopeService \n from app . utils . status_code import Status \n horoscope_blueprint_post = Blueprint ( '<STR_LIT>' , __name__ ) \n @ horoscope_blueprint_post . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def post_horoscope ( ) : \n sign = request . json . get ( '<STR_LIT>' , None ) \n date = request . json . get ( '<STR_LIT>' , None ) \n lang = request . json . get ( '<STR_LIT>' , None ) \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n try : \n horoscope = service . get_horoscope_info ( sign , date , lang ) \n url = f\"<STR_LIT>\"", "output": "horoscope . icon = horoscope . icon . format ( path = f\"<STR_LIT>\" ) \n return jsonify ( horoscope . __dict__ ) , Status . HTTP_OK \n except ValueError as e : \n return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_BAD_REQUEST \n except Exception as e : \n return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_INTERNAL_SERVER_ERROR"}, {"input": "from setuptools import find_packages \n from distutils . core import setup \n setup ( \n name = '<STR_LIT>' , \n version = '<STR_LIT>' , \n author = '<STR_LIT>' , \n license = \"<STR_LIT>\" , \n packages = find_packages ( ) , \n author_email = '<STR_LIT>' , \n description = '<STR_LIT>' , \n install_requires = [ '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' ]", "output": ")"}, {"input": "import logging \n import threading \n import time \n import webbrowser \n import uuid \n from urllib . parse import urlparse \n from typing import Union \n from os import environ \n from platform import system \n from flask import Flask , render_template , request , jsonify \n from . const import ( \n DetectMode , \n ReplacedKeywordStrategy , \n TemplateEnvironment , \n CALLBACK_GENERATE_FULLPAYLOAD , \n CALLBACK_GENERATE_PAYLOAD , \n CALLBACK_PREPARE_FULLPAYLOADGEN , \n CALLBACK_SUBMIT , \n CALLBACK_TEST_FORM_INPUT , \n APICODE_OK , \n APICODE_WRONG_INPUT , \n DEFAULT_USER_AGENT , \n OS_POPEN_READ , \n ) \n from . cracker import Cracker \n from . options import Options \n from . form import get_form , Form \n from . full_payload_gen import FullPayloadGen \n from . requester import HTTPRequester \n from . scan_url import yield_form \n from . submitter import Submitter , FormSubmitter , PathSubmitter \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n app = Flask ( __name__ ) \n tasks = { } \n class CallBackLogger : \n def __init__ ( self , flash_messages , messages ) : \n self . flash_messages = flash_messages \n self . messages = messages \n def callback_prepare_fullpayloadgen ( self , data ) : \n self . messages . append ( \"<STR_LIT>\" ) \n if data [ \"<STR_LIT>\" ] : \n context_repr = \"<STR_LIT>\" . join ( \n f\"<STR_LIT>\" for k , v in data [ \"<STR_LIT>\" ] . items ( ) \n ) \n self . messages . append ( f\"<STR_LIT>\" ) \n else : \n self . messages . append ( \"<STR_LIT>\" ) \n if not data [ \"<STR_LIT>\" ] : \n self . messages . append ( \"<STR_LIT>\" ) \n def callback_generate_fullpayload ( self , data ) : \n payload = ( \n data [ \"<STR_LIT>\" ] \n if len ( data [ \"<STR_LIT>\" ] ) < <NUM_LIT> \n else data [ \"<STR_LIT>\" ] [ : <NUM_LIT> ] + \"<STR_LIT>\" \n ) \n self . messages . append ( f\"<STR_LIT>\" ) \n if not data [ \"<STR_LIT>\" ] : \n self . messages . append ( \"<STR_LIT>\" ) \n def callback_generate_payload ( self , data ) : \n payload_repr = data [ \"<STR_LIT>\" ] \n if len ( payload_repr ) > <NUM_LIT> : \n payload_repr = payload_repr [ : <NUM_LIT> ] + \"<STR_LIT>\" \n req = f\"<STR_LIT>\" \n self . flash_messages . append ( f\"<STR_LIT>\" ) \n def callback_submit ( self , data ) : \n if not data [ \"<STR_LIT>\" ] : \n if data . get ( \"<STR_LIT>\" , None ) == \"<STR_LIT>\" : \n self . flash_messages . append ( \n f\"<STR_LIT>\" \n ) \n elif data . get ( \"<STR_LIT>\" , None ) == \"<STR_LIT>\" : \n self . flash_messages . append ( \n f\"<STR_LIT>\" \n ) \n else : \n self . flash_messages . append ( \"<STR_LIT>\" ) \n elif data . get ( \"<STR_LIT>\" , None ) == \"<STR_LIT>\" : \n self . flash_messages . append ( \n f\"<STR_LIT>\" \n ) \n else : \n self . flash_messages . append ( \n f\"<STR_LIT>\" \n ) \n def callback_test_form_input ( self , data ) : \n if not data [ \"<STR_LIT>\" ] : \n return \n testsuccess_msg = ( \n \"<STR_LIT>\" if data [ \"<STR_LIT>\" ] else \"<STR_LIT>\" \n ) \n will_print_msg = \"<STR_LIT>\" if data [ \"<STR_LIT>\" ] else \"<STR_LIT>\" \n self . messages . append ( testsuccess_msg + will_print_msg ) \n def __call__ ( self , callback_type , data ) : \n def default_handler ( _ ) : \n return logger . warning ( \"<STR_LIT>\" , callback_type ) \n return { \n CALLBACK_PREPARE_FULLPAYLOADGEN : self . callback_prepare_fullpayloadgen , \n CALLBACK_GENERATE_FULLPAYLOAD : self . callback_generate_fullpayload , \n CALLBACK_GENERATE_PAYLOAD : self . callback_generate_payload , \n CALLBACK_SUBMIT : self . callback_submit , \n CALLBACK_TEST_FORM_INPUT : self . callback_test_form_input , \n } . get ( callback_type , default_handler ) ( data ) \n class BaseCrackTaskThread ( threading . Thread ) : \n def __init__ ( self ) : \n super ( ) . __init__ ( ) \n self . flash_messages = [ ] \n self . messages = [ ] \n self . callback = CallBackLogger ( self . flash_messages , self . messages ) \n self . success = False \n class CrackTaskThread ( BaseCrackTaskThread ) : \n def __init__ ( self , url , form : Form , interval : float , options : Options ) : \n super ( ) . __init__ ( ) \n self . form = form \n self . url = url \n self . options = options \n self . submitter : Union [ Submitter , None ] = None \n self . full_payload_gen : Union [ FullPayloadGen , None ] = None \n self . cracker : Union [ Cracker , None ] = None \n self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) \n def run ( self ) : \n for input_field in self . form [ \"<STR_LIT>\" ] : \n self . messages . append ( f\"<STR_LIT>\" ) \n self . submitter = FormSubmitter ( \n self . url , \n self . form , \n input_field , \n self . requester , \n self . callback , \n ) \n self . cracker = Cracker ( \n self . submitter , \n self . callback , \n options = self . options , \n ) \n if not self . cracker . has_respond ( ) : \n continue \n self . full_payload_gen = self . cracker . crack ( ) \n if self . full_payload_gen : \n self . messages . append ( \"<STR_LIT>\" ) \n self . success = True \n break \n if not self . success : \n self . messages . append ( \"<STR_LIT>\" ) \n class CrackPathTaskThread ( BaseCrackTaskThread ) : \n def __init__ ( self , url , interval : float , options : Options ) : \n super ( ) . __init__ ( ) \n self . url = url \n self . options = options \n self . submitter : Union [ Submitter , None ] = None \n self . full_payload_gen : Union [ FullPayloadGen , None ] = None \n self . cracker : Union [ Cracker , None ] \n self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) \n def run ( self ) : \n self . submitter = PathSubmitter ( self . url , self . requester , self . callback ) \n self . cracker = Cracker ( self . submitter , self . callback , options = self . options ) \n if not self . cracker . has_respond ( ) : \n self . messages . append ( \"<STR_LIT>\" ) \n return \n self . full_payload_gen = self . cracker . crack ( ) \n if self . full_payload_gen : \n self . messages . append ( \"<STR_LIT>\" ) \n self . success = True \n class ScanTaskThread ( BaseCrackTaskThread ) : \n def __init__ ( self , url , interval : float , options : Options ) : \n super ( ) . __init__ ( ) \n self . url = url \n self . options = options \n self . submitter : Union [ Submitter , None ] = None \n self . full_payload_gen : Union [ FullPayloadGen , None ] = None \n self . cracker : Union [ Cracker , None ] \n self . requester = HTTPRequester ( interval = interval , user_agent = DEFAULT_USER_AGENT ) \n def run ( self ) : \n url_forms = ( \n ( page_url , form ) \n for ( page_url , forms ) in yield_form ( self . requester , self . url ) \n for form in forms \n ) \n for page_url , form in url_forms : \n for input_field in form [ \"<STR_LIT>\" ] : \n self . messages . append ( f\"<STR_LIT>\" ) \n self . submitter = FormSubmitter ( \n page_url , \n form , \n input_field , \n self . requester , \n self . callback , \n ) \n self . cracker = Cracker ( \n self . submitter , \n self . callback , \n options = self . options , \n ) \n if not self . cracker . has_respond ( ) : \n continue \n self . full_payload_gen = self . cracker . crack ( ) \n if self . full_payload_gen : \n self . messages . append ( \"<STR_LIT>\" ) \n self . success = True \n return \n if not self . success : \n self . messages . append ( \"<STR_LIT>\" ) \n class InteractiveTaskThread ( threading . Thread ) : \n def __init__ ( \n self , \n submitter : Submitter , \n full_payload_gen : FullPayloadGen , \n cmd : str , \n ) : \n super ( ) . __init__ ( ) \n self . submitter = submitter \n self . full_payload_gen = full_payload_gen \n self . cmd = cmd \n self . flash_messages = [ ] \n self . messages = [ ] \n self . callback = CallBackLogger ( self . flash_messages , self . messages ) \n self . submitter . callback = self . callback \n self . full_payload_gen . callback = self . callback \n def run ( self ) : \n self . messages . append ( \"<STR_LIT>\" ) \n payload , will_print = self . full_payload_gen . generate ( OS_POPEN_READ , self . cmd ) \n if not payload : \n self . messages . append ( \"<STR_LIT>\" ) \n return \n if not will_print : \n self . messages . append ( \"<STR_LIT>\" ) \n resp = self . submitter . submit ( payload ) \n assert resp is not None \n self . messages . append ( \"<STR_LIT>\" ) \n self . messages . append ( resp . text ) \n def manage_task_thread ( task : threading . Thread ) : \n taskid = uuid . uuid4 ( ) . hex \n task . daemon = True \n task . start ( ) \n tasks [ taskid ] = task \n return taskid \n def parse_options ( request_form ) -> Options : \n options = Options ( ) \n if request_form . get ( \"<STR_LIT>\" , None ) : \n options . detect_mode = DetectMode ( request_form . get ( \"<STR_LIT>\" , None ) ) \n if request_form . get ( \"<STR_LIT>\" , None ) : \n options . environment = TemplateEnvironment ( request_form . get ( \"<STR_LIT>\" , None ) ) \n if request_form . get ( \"<STR_LIT>\" , None ) : \n options . replaced_keyword_strategy = ReplacedKeywordStrategy ( \n request_form . get ( \"<STR_LIT>\" , None ) \n ) \n return options \n @ app . route ( \"<STR_LIT>\" ) \n def index ( ) : \n return render_template ( \"<STR_LIT>\" ) \n @ app . route ( \"<STR_LIT>\" ) \n def crack_path ( ) : \n return render_template ( \"<STR_LIT>\" )", "output": "@ app . route ( \"<STR_LIT>\" ) \n def scan ( ) : \n return render_template ( \"<STR_LIT>\" ) \n @ app . route ( \n \"<STR_LIT>\" , \n methods = [ \"<STR_LIT>\" ] , \n ) \n def create_task ( ) : \n task_type = request . form . get ( \"<STR_LIT>\" , None ) \n if task_type == \"<STR_LIT>\" : \n if request . form [ \"<STR_LIT>\" ] == \"<STR_LIT>\" or request . form [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n + f\"<STR_LIT>\" , \n } \n ) \n taskid = manage_task_thread ( \n CrackTaskThread ( \n url = request . form [ \"<STR_LIT>\" ] , \n form = get_form ( \n action = request . form [ \"<STR_LIT>\" ] or urlparse ( request . form [ \"<STR_LIT>\" ] ) . path , \n method = request . form [ \"<STR_LIT>\" ] , \n inputs = request . form [ \"<STR_LIT>\" ] . split ( \"<STR_LIT>\" ) , \n ) , \n interval = float ( request . form [ \"<STR_LIT>\" ] ) , \n options = parse_options ( request . form ) , \n ) \n ) \n return jsonify ( { \"<STR_LIT>\" : APICODE_OK , \"<STR_LIT>\" : taskid } ) \n if task_type == \"<STR_LIT>\" : \n if request . form [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n return jsonify ( \n { \"<STR_LIT>\" : APICODE_WRONG_INPUT , \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n taskid = manage_task_thread ( \n CrackPathTaskThread ( \n url = request . form [ \"<STR_LIT>\" ] , \n interval = float ( request . form [ \"<STR_LIT>\" ] ) , \n options = parse_options ( request . form ) , \n ) \n ) \n return jsonify ( { \"<STR_LIT>\" : APICODE_OK , \"<STR_LIT>\" : taskid } ) \n if task_type == \"<STR_LIT>\" : \n if request . form [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n return jsonify ( \n { \"<STR_LIT>\" : APICODE_WRONG_INPUT , \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n taskid = manage_task_thread ( \n ScanTaskThread ( \n url = request . form [ \"<STR_LIT>\" ] , \n interval = float ( request . form [ \"<STR_LIT>\" ] ) , \n options = parse_options ( request . form ) , \n ) \n ) \n return jsonify ( { \"<STR_LIT>\" : APICODE_OK , \"<STR_LIT>\" : taskid } ) \n if task_type == \"<STR_LIT>\" : \n cmd , last_task_id = ( \n request . form [ \"<STR_LIT>\" ] , \n request . form [ \"<STR_LIT>\" ] , \n ) \n last_task = tasks . get ( last_task_id , None ) \n if cmd == \"<STR_LIT>\" : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n elif not isinstance ( last_task , BaseCrackTaskThread ) : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } \n ) \n elif not last_task . success : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } \n ) \n assert ( \n last_task . submitter is not None and last_task . full_payload_gen is not None \n ) \n taskid = manage_task_thread ( \n InteractiveTaskThread ( last_task . submitter , last_task . full_payload_gen , cmd ) \n ) \n return jsonify ( { \"<STR_LIT>\" : APICODE_OK , \"<STR_LIT>\" : taskid } ) \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } \n ) \n @ app . route ( \n \"<STR_LIT>\" , \n methods = [ \n \"<STR_LIT>\" , \n ] , \n ) \n def watch_task ( ) : \n if \"<STR_LIT>\" not in request . form : \n return jsonify ( { \"<STR_LIT>\" : APICODE_WRONG_INPUT , \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n if request . form [ \"<STR_LIT>\" ] not in tasks : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_WRONG_INPUT , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n } \n ) \n taskid = request . form [ \"<STR_LIT>\" ] \n task : Union [ CrackTaskThread , CrackPathTaskThread , InteractiveTaskThread ] = tasks [ \n taskid \n ] \n if isinstance ( task , BaseCrackTaskThread ) : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_OK , \n \"<STR_LIT>\" : taskid , \n \"<STR_LIT>\" : not task . is_alive ( ) , \n \"<STR_LIT>\" : task . messages , \n \"<STR_LIT>\" : task . flash_messages , \n \"<STR_LIT>\" : task . success , \n } \n ) \n if isinstance ( task , InteractiveTaskThread ) : \n return jsonify ( \n { \n \"<STR_LIT>\" : APICODE_OK , \n \"<STR_LIT>\" : taskid , \n \"<STR_LIT>\" : not task . is_alive ( ) , \n \"<STR_LIT>\" : task . messages , \n \"<STR_LIT>\" : task . flash_messages , \n } \n ) \n assert False , \"<STR_LIT>\" \n def should_open_browser ( ) -> bool : \n if system ( ) == \"<STR_LIT>\" : \n return True \n return environ . get ( \"<STR_LIT>\" ) is not None \n def browser_open_url_delayed ( url , delay ) : \n def f ( ) : \n time . sleep ( delay ) \n try : \n webbrowser . open ( url ) \n except webbrowser . Error : \n logger . warning ( \"<STR_LIT>\" ) \n t = threading . Thread ( target = f ) \n t . daemon = True \n t . start ( ) \n def main ( host = \"<STR_LIT>\" , port = <NUM_LIT> , open_browser = True ) : \n if open_browser and should_open_browser ( ) : \n browser_open_url_delayed ( f\"<STR_LIT>\" , <NUM_LIT> ) \n app . run ( host = host , port = port ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "import mutagen \n import mutagen . id3 \n import mutagen . easyid3 \n import mutagen . mp3 \n import mutagen . trueaudio \n from . import util \n from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY \n def get_tracknumA ( afile , norm_key ) : \n return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_tracknumA ( afile , norm_key , val ) : \n return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaltracksA ( afile , norm_key ) : \n return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaltracksA ( afile , norm_key , val ) : \n return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_discnumA ( afile , norm_key ) : \n return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_discnumA ( afile , norm_key , val ) : \n return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaldiscsA ( afile , norm_key ) : \n return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaldiscsA ( afile , norm_key , val ) : \n return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_tracknumB ( afile , norm_key ) : \n return util . get_easy_tracknum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_tracknumB ( afile , norm_key , val ) : \n return util . set_easy_tracknum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaltracksB ( afile , norm_key ) : \n return util . get_easy_totaltracks ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaltracksB ( afile , norm_key , val ) : \n return util . set_easy_totaltracks ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_discnumB ( afile , norm_key ) : \n return util . get_easy_discnum ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_discnumB ( afile , norm_key , val ) : \n return util . set_easy_discnum ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_totaldiscsB ( afile , norm_key ) : \n return util . get_easy_totaldiscs ( afile , norm_key , _tag_name = '<STR_LIT>' ) \n def set_totaldiscsB ( afile , norm_key , val ) : \n return util . set_easy_totaldiscs ( afile , norm_key , val , _tag_name = '<STR_LIT>' ) \n def get_pictures ( afile , norm_key ) : \n pics = afile . mfile . tags . getall ( '<STR_LIT>' ) + afile . mfile . tags . getall ( '<STR_LIT>' ) \n artworks = [ ] \n for p in pics : \n artworks . append ( Artwork ( p . data , pic_type = p . type ) ) \n return MetadataItem ( Artwork , None , artworks ) \n def set_pictures ( afile , norm_key , artworks ) : \n if afile . mfile . tags . getall ( '<STR_LIT>' ) : \n kls = mutagen . id3 . APIC \n elif afile . mfile . tags . getall ( '<STR_LIT>' ) : \n kls = mutagen . id3 . PIC \n else : \n if afile . mfile . tags . version [ : <NUM_LIT> ] == ( <NUM_LIT> , <NUM_LIT> ) : \n kls = mutagen . id3 . PIC \n else : \n kls = mutagen . id3 . APIC \n tag = str ( kls . __name__ ) . strip ( '<STR_LIT>' ) \n afile . mfile . tags . delall ( tag ) \n for i , art in enumerate ( artworks . values ) : \n if kls == mutagen . id3 . PIC : \n mime = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } [ art . mime . lower ( ) ] \n else : \n mime = art . mime \n afile . mfile . tags . add ( kls ( data = art . raw , type = art . pic_type , desc = str ( i ) , \n mime = mime ) ) \n _TAG_MAP_ID3_1 = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) ,", "output": "'<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n } \n _TAG_MAP_ID3_2_2 = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumA , \n setter = set_tracknumA , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksA , \n setter = set_totaltracksA , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumA , \n setter = set_discnumA , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsA , \n setter = set_totaldiscsA , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n type = Artwork ) , \n } \n _TAG_MAP_ID3_2_3 = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumB , \n setter = set_tracknumB , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksB , \n setter = set_totaltracksB , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumB , \n setter = set_discnumB , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsB , \n setter = set_totaldiscsB , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n type = Artwork ) , \n } \n _TAG_MAP_ID3_2_4 = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknumB , \n setter = set_tracknumB , \n remover = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracksB , \n setter = set_totaltracksB , \n remover = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnumB , \n setter = set_discnumB , \n remover = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscsB , \n setter = set_totaldiscsB , \n remover = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n remover = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = Artwork ) , \n } \n class Id3File ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . id3 . ID3FileType \n def __init__ ( self , filename , ** kwargs ) : \n mfile = kwargs . get ( '<STR_LIT>' , None ) \n if mfile is None : \n mfile = mutagen . File ( filename ) \n kwargs [ '<STR_LIT>' ] = mfile \n if mfile . tags : \n id3_ver = mfile . tags . version [ : <NUM_LIT> ] \n else : \n id3_ver = ( <NUM_LIT> , <NUM_LIT> ) \n if id3_ver [ <NUM_LIT> ] == <NUM_LIT> : \n self . _TAG_MAP = _TAG_MAP_ID3_2_4 \n elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : \n self . _TAG_MAP = _TAG_MAP_ID3_2_4 \n elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : \n self . _TAG_MAP = _TAG_MAP_ID3_2_4 \n elif id3_ver == ( <NUM_LIT> , <NUM_LIT> ) : \n self . _TAG_MAP = _TAG_MAP_ID3_2_4 \n else : \n raise NotImplementedError ( \"<STR_LIT>\" \n \"<STR_LIT>\" . format ( mfile . tags . version ) ) \n super ( Id3File , self ) . __init__ ( filename , ** kwargs ) \n def _ft_getter ( self , key ) : \n vals = self . mfile . tags . getall ( key ) \n ret = [ ] \n for val in vals : \n if isinstance ( val . text , ( list , tuple ) ) : \n ret += [ str ( t ) for t in val . text ] \n else : \n ret += [ str ( val . text ) ] \n if not ret : \n ret = None \n return ret \n def _ft_setter ( self , key , md_val , appendable = True ) : \n self . mfile . tags . delall ( key ) \n kls = getattr ( mutagen . id3 , key . split ( '<STR_LIT>' ) [ <NUM_LIT> ] ) \n kwargs = { } \n _o = kls ( ) \n if hasattr ( _o , '<STR_LIT>' ) : \n kwargs [ '<STR_LIT>' ] = '<STR_LIT>' \n self . mfile . tags . add ( kls ( text = str ( md_val ) , ** kwargs ) ) \n def _ft_rmtag ( self , key ) : \n self . mfile . tags . delall ( key ) \n class Mp3File ( Id3File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . mp3 . MP3 \n def __init__ ( self , filename , ** kwargs ) : \n super ( Mp3File , self ) . __init__ ( filename , ** kwargs ) \n self . tag_map = self . tag_map . copy ( ) \n self . tag_map . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n } ) \n class EasyMp3File ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . mp3 . EasyMP3 \n class EasyId3File ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . easyid3 . EasyID3FileType \n class TrueAudioFile ( Id3File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . trueaudio . TrueAudio \n class EasyTrueAudioFile ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . trueaudio . EasyTrueAudio"}, {"input": "import os , sys \n import gradio as gr \n import regex as re \n import shutil \n import datetime \n import random \n from core import ( \n run_infer_script , \n run_batch_infer_script , \n ) \n from assets . i18n . i18n import I18nAuto \n from rvc . lib . utils import format_title \n i18n = I18nAuto ( ) \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n model_root = os . path . join ( now_dir , \"<STR_LIT>\" ) \n audio_root = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n model_root_relative = os . path . relpath ( model_root , now_dir ) \n audio_root_relative = os . path . relpath ( audio_root , now_dir ) \n sup_audioext = { \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n } \n names = [ \n os . path . join ( root , file ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for file in files \n if ( \n file . endswith ( ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n and not ( file . startswith ( \"<STR_LIT>\" ) or file . startswith ( \"<STR_LIT>\" ) ) \n ) \n ] \n indexes_list = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in name \n ] \n audio_paths = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( audio_root_relative , topdown = False ) \n for name in files \n if name . endswith ( tuple ( sup_audioext ) ) \n and root == audio_root_relative \n and \"<STR_LIT>\" not in name \n ] \n def output_path_fn ( input_audio_path ) : \n original_name_without_extension = os . path . basename ( input_audio_path ) . rsplit ( \"<STR_LIT>\" , <NUM_LIT> ) [ \n <NUM_LIT> \n ] \n new_name = original_name_without_extension + \"<STR_LIT>\" \n output_path = os . path . join ( os . path . dirname ( input_audio_path ) , new_name ) \n return output_path \n def change_choices ( ) : \n names = [ \n os . path . join ( root , file ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for file in files \n if ( \n file . endswith ( ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n and not ( file . startswith ( \"<STR_LIT>\" ) or file . startswith ( \"<STR_LIT>\" ) ) \n ) \n ] \n indexes_list = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in name \n ] \n audio_paths = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( audio_root_relative , topdown = False ) \n for name in files \n if name . endswith ( tuple ( sup_audioext ) ) \n and root == audio_root_relative \n and \"<STR_LIT>\" not in name \n ] \n return ( \n { \"<STR_LIT>\" : sorted ( names ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( indexes_list ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( audio_paths ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n def get_indexes ( ) : \n indexes_list = [ \n os . path . join ( dirpath , filename ) \n for dirpath , _ , filenames in os . walk ( model_root_relative ) \n for filename in filenames \n if filename . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in filename \n ] \n return indexes_list if indexes_list else \"<STR_LIT>\" \n def save_to_wav ( record_button ) : \n if record_button is None : \n pass \n else : \n path_to_file = record_button \n new_name = datetime . datetime . now ( ) . strftime ( \"<STR_LIT>\" ) + \"<STR_LIT>\" \n target_path = os . path . join ( audio_root_relative , os . path . basename ( new_name ) ) \n shutil . move ( path_to_file , target_path ) \n return target_path , output_path_fn ( target_path ) \n def save_to_wav2 ( upload_audio ) : \n file_path = upload_audio \n formated_name = format_title ( os . path . basename ( file_path ) ) \n target_path = os . path . join ( audio_root_relative , formated_name ) \n if os . path . exists ( target_path ) : \n os . remove ( target_path ) \n shutil . copy ( file_path , target_path ) \n return target_path , output_path_fn ( target_path ) \n def delete_outputs ( ) : \n gr . Info ( f\"<STR_LIT>\" ) \n for root , _ , files in os . walk ( audio_root_relative , topdown = False ) : \n for name in files : \n if name . endswith ( tuple ( sup_audioext ) ) and name . __contains__ ( \"<STR_LIT>\" ) : \n os . remove ( os . path . join ( root , name ) ) \n def match_index ( model_file_value ) : \n if model_file_value : \n model_folder = os . path . dirname ( model_file_value ) \n index_files = get_indexes ( ) \n for index_file in index_files : \n if os . path . dirname ( index_file ) == model_folder : \n return index_file \n return \"<STR_LIT>\" \n def inference_tab ( ) : \n default_weight = random . choice ( names ) if names else None \n with gr . Row ( ) : \n with gr . Row ( ) : \n model_file = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = sorted ( names , key = lambda path : os . path . getsize ( path ) ) , \n interactive = True , \n value = default_weight , \n allow_custom_value = True , \n ) \n index_file = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_indexes ( ) , \n value = match_index ( default_weight ) if default_weight else \"<STR_LIT>\" , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Column ( ) : \n refresh_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n unload_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n unload_button . click ( \n fn = lambda : ( \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) , \n inputs = [ ] , \n outputs = [ model_file , index_file ] , \n ) \n model_file . select ( \n fn = lambda model_file_value : match_index ( model_file_value ) , \n inputs = [ model_file ] , \n outputs = [ index_file ] , \n ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n with gr . Column ( ) : \n upload_audio = gr . Audio ( \n label = i18n ( \"<STR_LIT>\" ) , type = \"<STR_LIT>\" , editable = False \n ) \n with gr . Row ( ) : \n audio = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = sorted ( audio_paths ) , \n value = audio_paths [ <NUM_LIT> ] if audio_paths else \"<STR_LIT>\" , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) , open = False ) : \n with gr . Column ( ) : \n clear_outputs_infer = gr . Button ( \n i18n ( \"<STR_LIT>\" ) \n ) \n output_path = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = ( \n output_path_fn ( audio_paths [ <NUM_LIT> ] ) \n if audio_paths \n else os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n ) , \n interactive = True , \n ) \n export_format = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n split_audio = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n autotune = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n clean_audio = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n clean_strength = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = False , \n value = <NUM_LIT> , \n interactive = True , \n ) \n pitch = gr . Slider ( \n minimum = - <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n filter_radius = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n step = <NUM_LIT> , \n interactive = True , \n ) \n index_rate = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n rms_mix_rate = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n protect = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n hop_length = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = False , \n value = <NUM_LIT> , \n interactive = True , \n ) \n with gr . Column ( ) : \n f0method = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n convert_button1 = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n with gr . Row ( ) : \n vc_output1 = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n ) \n vc_output2 = gr . Audio ( label = i18n ( \"<STR_LIT>\" ) ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n input_folder_batch = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n value = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n interactive = True , \n ) \n output_folder_batch = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n value = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n interactive = True , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) , open = False ) : \n with gr . Column ( ) : \n clear_outputs_batch = gr . Button ( \n i18n ( \"<STR_LIT>\" ) \n ) \n export_format_batch = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n split_audio_batch = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n autotune_batch = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n clean_audio_batch = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n clean_strength_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = False , \n value = <NUM_LIT> , \n interactive = True , \n ) \n pitch_batch = gr . Slider ( \n minimum = - <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n filter_radius_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n step = <NUM_LIT> , \n interactive = True , \n ) \n index_rate_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n rms_mix_rate_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n protect_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n hop_length_batch = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = False , \n value = <NUM_LIT> , \n interactive = True , \n ) \n with gr . Column ( ) : \n f0method_batch = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n convert_button2 = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n with gr . Row ( ) : \n vc_output3 = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n ) \n def toggle_visible ( checkbox ) : \n return { \"<STR_LIT>\" : checkbox , \"<STR_LIT>\" : \"<STR_LIT>\" } \n def toggle_visible_hop_length ( f0method ) : \n if f0method == \"<STR_LIT>\" or f0method == \"<STR_LIT>\" : \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" } \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } \n clean_audio . change ( \n fn = toggle_visible , \n inputs = [ clean_audio ] , \n outputs = [ clean_strength ] , \n ) \n clean_audio_batch . change ( \n fn = toggle_visible , \n inputs = [ clean_audio_batch ] , \n outputs = [ clean_strength_batch ] , \n ) \n f0method . change ( \n fn = toggle_visible_hop_length , \n inputs = [ f0method ] , \n outputs = [ hop_length ] , \n ) \n f0method_batch . change ( \n fn = toggle_visible_hop_length , \n inputs = [ f0method_batch ] , \n outputs = [ hop_length_batch ] , \n ) \n refresh_button . click ( \n fn = change_choices , \n inputs = [ ] , \n outputs = [ model_file , index_file , audio ] , \n ) \n audio . change ( \n fn = output_path_fn , \n inputs = [ audio ] , \n outputs = [ output_path ] , \n ) \n upload_audio . upload ( \n fn = save_to_wav2 , \n inputs = [ upload_audio ] , \n outputs = [ audio , output_path ] , \n ) \n upload_audio . stop_recording ( \n fn = save_to_wav , \n inputs = [ upload_audio ] , \n outputs = [ audio , output_path ] , \n ) \n clear_outputs_infer . click ( \n fn = delete_outputs , \n inputs = [ ] , \n outputs = [ ] , \n ) \n clear_outputs_batch . click ( \n fn = delete_outputs ,", "output": "inputs = [ ] , \n outputs = [ ] , \n ) \n convert_button1 . click ( \n fn = run_infer_script , \n inputs = [ \n pitch , \n filter_radius , \n index_rate , \n rms_mix_rate , \n protect , \n hop_length , \n f0method , \n audio , \n output_path , \n model_file , \n index_file , \n split_audio , \n autotune , \n clean_audio , \n clean_strength , \n export_format , \n ] , \n outputs = [ vc_output1 , vc_output2 ] , \n ) \n convert_button2 . click ( \n fn = run_batch_infer_script , \n inputs = [ \n pitch_batch , \n filter_radius_batch , \n index_rate_batch , \n rms_mix_rate_batch , \n protect_batch , \n hop_length_batch , \n f0method_batch , \n input_folder_batch , \n output_folder_batch , \n model_file , \n index_file , \n split_audio_batch , \n autotune_batch , \n clean_audio_batch , \n clean_strength_batch , \n export_format_batch , \n ] , \n outputs = [ vc_output3 ] , \n )"}, {"input": "import logging . config \n from datetime import datetime \n import svcs", "output": "logging . config . dictConfig ( \n { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : False , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : False , \n } , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } \n ) \n reg = svcs . Registry ( ) \n reg . register_factory ( datetime , datetime . now ) \n reg . register_value ( str , \"<STR_LIT>\" )"}, {"input": "import requests \n from mod import tools \n class MiGuMusicClient : \n BASE_URL = \"<STR_LIT>\" \n header = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n def fetch_lyric ( self , song_id ) : \n url = f'<STR_LIT>' \n res = requests . get ( url , headers = self . header ) \n return res . json ( ) [ \"<STR_LIT>\" ] \n def fetch_id3_by_title ( self , title ) : \n url = self . BASE_URL + f\"<STR_LIT>\" \n res = requests . get ( url , headers = self . header ) \n songs = res . json ( ) [ \"<STR_LIT>\" ]", "output": "results = [ ] \n for song in songs : \n lyrics = self . fetch_lyric ( song [ '<STR_LIT>' ] ) \n results . append ( { \n \"<STR_LIT>\" : song [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : song [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : song [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : lyrics , \n \"<STR_LIT>\" : song [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : tools . calculate_md5 ( \n f\"<STR_LIT>\" ) \n } ) \n return results \n def search ( title = '<STR_LIT>' , artist = '<STR_LIT>' , album = '<STR_LIT>' ) -> list : \n migumusic = MiGuMusicClient ( ) \n result = migumusic . fetch_id3_by_title ( title ) \n return result \n if __name__ == \"<STR_LIT>\" : \n print ( search ( title = \"<STR_LIT>\" ) )"}, {"input": "import math \n import torch \n from torch import nn \n from torch . nn import functional as F \n from . import commons \n from . modules import LayerNorm \n class Encoder ( nn . Module ) : \n def __init__ ( \n self , \n hidden_channels , \n filter_channels , \n n_heads , \n n_layers , \n kernel_size = <NUM_LIT> , \n p_dropout = <NUM_LIT> , \n window_size = <NUM_LIT> , \n ** kwargs \n ) : \n super ( ) . __init__ ( ) \n self . hidden_channels = hidden_channels \n self . filter_channels = filter_channels \n self . n_heads = n_heads \n self . n_layers = n_layers \n self . kernel_size = kernel_size \n self . p_dropout = p_dropout \n self . window_size = window_size \n self . drop = nn . Dropout ( p_dropout ) \n self . attn_layers = nn . ModuleList ( ) \n self . norm_layers_1 = nn . ModuleList ( ) \n self . ffn_layers = nn . ModuleList ( ) \n self . norm_layers_2 = nn . ModuleList ( ) \n for i in range ( self . n_layers ) : \n self . attn_layers . append ( \n MultiHeadAttention ( \n hidden_channels , \n hidden_channels , \n n_heads , \n p_dropout = p_dropout , \n window_size = window_size , \n ) \n ) \n self . norm_layers_1 . append ( LayerNorm ( hidden_channels ) ) \n self . ffn_layers . append ( \n FFN ( \n hidden_channels , \n hidden_channels , \n filter_channels , \n kernel_size , \n p_dropout = p_dropout , \n ) \n ) \n self . norm_layers_2 . append ( LayerNorm ( hidden_channels ) ) \n def forward ( self , x , x_mask ) : \n attn_mask = x_mask . unsqueeze ( <NUM_LIT> ) * x_mask . unsqueeze ( - <NUM_LIT> ) \n x = x * x_mask \n for i in range ( self . n_layers ) : \n y = self . attn_layers [ i ] ( x , x , attn_mask ) \n y = self . drop ( y ) \n x = self . norm_layers_1 [ i ] ( x + y ) \n y = self . ffn_layers [ i ] ( x , x_mask ) \n y = self . drop ( y ) \n x = self . norm_layers_2 [ i ] ( x + y ) \n x = x * x_mask \n return x \n class Decoder ( nn . Module ) : \n def __init__ ( \n self , \n hidden_channels , \n filter_channels , \n n_heads , \n n_layers , \n kernel_size = <NUM_LIT> , \n p_dropout = <NUM_LIT> , \n proximal_bias = False , \n proximal_init = True , \n ** kwargs \n ) : \n super ( ) . __init__ ( ) \n self . hidden_channels = hidden_channels \n self . filter_channels = filter_channels \n self . n_heads = n_heads \n self . n_layers = n_layers \n self . kernel_size = kernel_size \n self . p_dropout = p_dropout \n self . proximal_bias = proximal_bias \n self . proximal_init = proximal_init \n self . drop = nn . Dropout ( p_dropout ) \n self . self_attn_layers = nn . ModuleList ( ) \n self . norm_layers_0 = nn . ModuleList ( ) \n self . encdec_attn_layers = nn . ModuleList ( ) \n self . norm_layers_1 = nn . ModuleList ( ) \n self . ffn_layers = nn . ModuleList ( ) \n self . norm_layers_2 = nn . ModuleList ( ) \n for i in range ( self . n_layers ) : \n self . self_attn_layers . append ( \n MultiHeadAttention ( \n hidden_channels , \n hidden_channels , \n n_heads , \n p_dropout = p_dropout , \n proximal_bias = proximal_bias , \n proximal_init = proximal_init , \n ) \n ) \n self . norm_layers_0 . append ( LayerNorm ( hidden_channels ) ) \n self . encdec_attn_layers . append ( \n MultiHeadAttention ( \n hidden_channels , hidden_channels , n_heads , p_dropout = p_dropout \n ) \n ) \n self . norm_layers_1 . append ( LayerNorm ( hidden_channels ) ) \n self . ffn_layers . append ( \n FFN ( \n hidden_channels , \n hidden_channels , \n filter_channels , \n kernel_size , \n p_dropout = p_dropout , \n causal = True , \n ) \n ) \n self . norm_layers_2 . append ( LayerNorm ( hidden_channels ) ) \n def forward ( self , x , x_mask , h , h_mask ) : \n self_attn_mask = commons . subsequent_mask ( x_mask . size ( <NUM_LIT> ) ) . to ( \n device = x . device , dtype = x . dtype \n ) \n encdec_attn_mask = h_mask . unsqueeze ( <NUM_LIT> ) * x_mask . unsqueeze ( - <NUM_LIT> ) \n x = x * x_mask \n for i in range ( self . n_layers ) : \n y = self . self_attn_layers [ i ] ( x , x , self_attn_mask ) \n y = self . drop ( y ) \n x = self . norm_layers_0 [ i ] ( x + y ) \n y = self . encdec_attn_layers [ i ] ( x , h , encdec_attn_mask ) \n y = self . drop ( y ) \n x = self . norm_layers_1 [ i ] ( x + y ) \n y = self . ffn_layers [ i ] ( x , x_mask ) \n y = self . drop ( y ) \n x = self . norm_layers_2 [ i ] ( x + y ) \n x = x * x_mask \n return x \n class MultiHeadAttention ( nn . Module ) : \n def __init__ ( \n self , \n channels , \n out_channels , \n n_heads , \n p_dropout = <NUM_LIT> , \n window_size = None , \n heads_share = True , \n block_length = None , \n proximal_bias = False , \n proximal_init = False , \n ) : \n super ( ) . __init__ ( ) \n assert channels % n_heads == <NUM_LIT> \n self . channels = channels \n self . out_channels = out_channels \n self . n_heads = n_heads \n self . p_dropout = p_dropout \n self . window_size = window_size \n self . heads_share = heads_share \n self . block_length = block_length \n self . proximal_bias = proximal_bias \n self . proximal_init = proximal_init \n self . attn = None \n self . k_channels = channels // n_heads \n self . conv_q = nn . Conv1d ( channels , channels , <NUM_LIT> ) \n self . conv_k = nn . Conv1d ( channels , channels , <NUM_LIT> ) \n self . conv_v = nn . Conv1d ( channels , channels , <NUM_LIT> ) \n self . conv_o = nn . Conv1d ( channels , out_channels , <NUM_LIT> ) \n self . drop = nn . Dropout ( p_dropout ) \n if window_size is not None : \n n_heads_rel = <NUM_LIT> if heads_share else n_heads \n rel_stddev = self . k_channels ** - <NUM_LIT> \n self . emb_rel_k = nn . Parameter ( \n torch . randn ( n_heads_rel , window_size * <NUM_LIT> + <NUM_LIT> , self . k_channels ) \n * rel_stddev \n ) \n self . emb_rel_v = nn . Parameter ( \n torch . randn ( n_heads_rel , window_size * <NUM_LIT> + <NUM_LIT> , self . k_channels ) \n * rel_stddev \n ) \n nn . init . xavier_uniform_ ( self . conv_q . weight ) \n nn . init . xavier_uniform_ ( self . conv_k . weight ) \n nn . init . xavier_uniform_ ( self . conv_v . weight ) \n if proximal_init : \n with torch . no_grad ( ) : \n self . conv_k . weight . copy_ ( self . conv_q . weight ) \n self . conv_k . bias . copy_ ( self . conv_q . bias ) \n def forward ( self , x , c , attn_mask = None ) : \n q = self . conv_q ( x )", "output": "k = self . conv_k ( c ) \n v = self . conv_v ( c ) \n x , self . attn = self . attention ( q , k , v , mask = attn_mask ) \n x = self . conv_o ( x ) \n return x \n def attention ( self , query , key , value , mask = None ) : \n b , d , t_s , t_t = ( * key . size ( ) , query . size ( <NUM_LIT> ) ) \n query = query . view ( b , self . n_heads , self . k_channels , t_t ) . transpose ( <NUM_LIT> , <NUM_LIT> ) \n key = key . view ( b , self . n_heads , self . k_channels , t_s ) . transpose ( <NUM_LIT> , <NUM_LIT> ) \n value = value . view ( b , self . n_heads , self . k_channels , t_s ) . transpose ( <NUM_LIT> , <NUM_LIT> ) \n scores = torch . matmul ( query / math . sqrt ( self . k_channels ) , key . transpose ( - <NUM_LIT> , - <NUM_LIT> ) ) \n if self . window_size is not None : \n assert ( \n t_s == t_t \n ) , \"<STR_LIT>\" \n key_relative_embeddings = self . _get_relative_embeddings ( self . emb_rel_k , t_s ) \n rel_logits = self . _matmul_with_relative_keys ( \n query / math . sqrt ( self . k_channels ) , key_relative_embeddings \n ) \n scores_local = self . _relative_position_to_absolute_position ( rel_logits ) \n scores = scores + scores_local \n if self . proximal_bias : \n assert t_s == t_t , \"<STR_LIT>\" \n scores = scores + self . _attention_bias_proximal ( t_s ) . to ( \n device = scores . device , dtype = scores . dtype \n ) \n if mask is not None : \n scores = scores . masked_fill ( mask == <NUM_LIT> , - <NUM_LIT> ) \n if self . block_length is not None : \n assert ( \n t_s == t_t \n ) , \"<STR_LIT>\" \n block_mask = ( \n torch . ones_like ( scores ) \n . triu ( - self . block_length ) \n . tril ( self . block_length ) \n ) \n scores = scores . masked_fill ( block_mask == <NUM_LIT> , - <NUM_LIT> ) \n p_attn = F . softmax ( scores , dim = - <NUM_LIT> ) \n p_attn = self . drop ( p_attn ) \n output = torch . matmul ( p_attn , value ) \n if self . window_size is not None : \n relative_weights = self . _absolute_position_to_relative_position ( p_attn ) \n value_relative_embeddings = self . _get_relative_embeddings ( \n self . emb_rel_v , t_s \n ) \n output = output + self . _matmul_with_relative_values ( \n relative_weights , value_relative_embeddings \n ) \n output = output . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) . view ( b , d , t_t ) \n return output , p_attn \n def _matmul_with_relative_values ( self , x , y ) : \n ret = torch . matmul ( x , y . unsqueeze ( <NUM_LIT> ) ) \n return ret \n def _matmul_with_relative_keys ( self , x , y ) : \n ret = torch . matmul ( x , y . unsqueeze ( <NUM_LIT> ) . transpose ( - <NUM_LIT> , - <NUM_LIT> ) ) \n return ret \n def _get_relative_embeddings ( self , relative_embeddings , length ) : \n pad_length = max ( length - ( self . window_size + <NUM_LIT> ) , <NUM_LIT> ) \n slice_start_position = max ( ( self . window_size + <NUM_LIT> ) - length , <NUM_LIT> ) \n slice_end_position = slice_start_position + <NUM_LIT> * length - <NUM_LIT> \n if pad_length > <NUM_LIT> : \n padded_relative_embeddings = F . pad ( \n relative_embeddings , \n commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ pad_length , pad_length ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) , \n ) \n else : \n padded_relative_embeddings = relative_embeddings \n used_relative_embeddings = padded_relative_embeddings [ \n : , slice_start_position : slice_end_position \n ] \n return used_relative_embeddings \n def _relative_position_to_absolute_position ( self , x ) : \n batch , heads , length , _ = x . size ( ) \n x = F . pad ( x , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) \n x_flat = x . view ( [ batch , heads , length * <NUM_LIT> * length ] ) \n x_flat = F . pad ( \n x_flat , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , length - <NUM_LIT> ] ] ) \n ) \n x_final = x_flat . view ( [ batch , heads , length + <NUM_LIT> , <NUM_LIT> * length - <NUM_LIT> ] ) [ \n : , : , : length , length - <NUM_LIT> : \n ] \n return x_final \n def _absolute_position_to_relative_position ( self , x ) : \n batch , heads , length , _ = x . size ( ) \n x = F . pad ( \n x , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , length - <NUM_LIT> ] ] ) \n ) \n x_flat = x . view ( [ batch , heads , length ** <NUM_LIT> + length * ( length - <NUM_LIT> ) ] ) \n x_flat = F . pad ( x_flat , commons . convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ length , <NUM_LIT> ] ] ) ) \n x_final = x_flat . view ( [ batch , heads , length , <NUM_LIT> * length ] ) [ : , : , : , <NUM_LIT> : ] \n return x_final \n def _attention_bias_proximal ( self , length ) : \n r = torch . arange ( length , dtype = torch . float32 ) \n diff = torch . unsqueeze ( r , <NUM_LIT> ) - torch . unsqueeze ( r , <NUM_LIT> ) \n return torch . unsqueeze ( torch . unsqueeze ( - torch . log1p ( torch . abs ( diff ) ) , <NUM_LIT> ) , <NUM_LIT> ) \n class FFN ( nn . Module ) : \n def __init__ ( \n self , \n in_channels , \n out_channels , \n filter_channels , \n kernel_size , \n p_dropout = <NUM_LIT> , \n activation = None , \n causal = False , \n ) : \n super ( ) . __init__ ( ) \n self . in_channels = in_channels \n self . out_channels = out_channels \n self . filter_channels = filter_channels \n self . kernel_size = kernel_size \n self . p_dropout = p_dropout \n self . activation = activation \n self . causal = causal \n if causal : \n self . padding = self . _causal_padding \n else : \n self . padding = self . _same_padding \n self . conv_1 = nn . Conv1d ( in_channels , filter_channels , kernel_size ) \n self . conv_2 = nn . Conv1d ( filter_channels , out_channels , kernel_size ) \n self . drop = nn . Dropout ( p_dropout ) \n def forward ( self , x , x_mask ) : \n x = self . conv_1 ( self . padding ( x * x_mask ) ) \n if self . activation == \"<STR_LIT>\" : \n x = x * torch . sigmoid ( <NUM_LIT> * x ) \n else : \n x = torch . relu ( x ) \n x = self . drop ( x ) \n x = self . conv_2 ( self . padding ( x * x_mask ) ) \n return x * x_mask \n def _causal_padding ( self , x ) : \n if self . kernel_size == <NUM_LIT> : \n return x \n pad_l = self . kernel_size - <NUM_LIT> \n pad_r = <NUM_LIT> \n padding = [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ pad_l , pad_r ] ] \n x = F . pad ( x , commons . convert_pad_shape ( padding ) ) \n return x \n def _same_padding ( self , x ) : \n if self . kernel_size == <NUM_LIT> : \n return x \n pad_l = ( self . kernel_size - <NUM_LIT> ) // <NUM_LIT> \n pad_r = self . kernel_size // <NUM_LIT> \n padding = [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ pad_l , pad_r ] ] \n x = F . pad ( x , commons . convert_pad_shape ( padding ) ) \n return x"}, {"input": "import logging \n import random \n import string \n import traceback \n import time \n import re \n from copy import copy \n from collections import Counter , namedtuple \n from functools import lru_cache \n from typing import Dict , Callable , Tuple , Union , List \n from . const import ( \n DetectMode , \n ReplacedKeywordStrategy , \n DANGEROUS_KEYWORDS , \n WafFunc , \n ) \n from . colorize import colored \n from . submitter import Submitter \n from . options import Options \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Result = namedtuple ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n dangerous_keywords = copy ( DANGEROUS_KEYWORDS ) \n random . shuffle ( dangerous_keywords ) \n render_error_keywords = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n def grouped_payloads ( size = <NUM_LIT> , sep = \"<STR_LIT>\" ) -> List [ str ] : \n return [ \n sep . join ( dangerous_keywords [ i : i + size ] ) \n for i in range ( <NUM_LIT> , len ( dangerous_keywords ) , size ) \n ] \n def removeprefix_string ( text : str , prefix : str ) -> str : \n if text . startswith ( prefix ) : \n return text [ len ( prefix ) : ] \n return text \n def get_next_p ( b : str ) -> List [ int ] : \n answer = [ ] \n for i , c in enumerate ( b ) : \n if i == <NUM_LIT> : \n answer . append ( - <NUM_LIT> ) \n continue \n p = answer [ i - <NUM_LIT> ] \n while p >= <NUM_LIT> and b [ p + <NUM_LIT> ] != c : \n assert answer [ p ] < p \n p = answer [ p ] \n if c == b [ p + <NUM_LIT> ] : \n answer . append ( p + <NUM_LIT> ) \n else : \n answer . append ( p ) \n return answer \n def kmp ( a : str , b : str ) -> Tuple [ int , Union [ int , None ] ] : \n logger . debug ( \"<STR_LIT>\" , len ( a ) , len ( b ) ) \n if b == \"<STR_LIT>\" : \n return <NUM_LIT> , None \n next_p = get_next_p ( b ) \n max_answer , max_answer_pos = <NUM_LIT> , None \n j = - <NUM_LIT> \n for i , c in enumerate ( a ) : \n while j >= <NUM_LIT> and b [ j + <NUM_LIT> ] != c : \n assert next_p [ j ] < j \n j = next_p [ j ] \n if c == b [ j + <NUM_LIT> ] : \n j += <NUM_LIT> \n if j + <NUM_LIT> > max_answer : \n max_answer = j + <NUM_LIT> \n max_answer_pos = i \n if j == len ( b ) - <NUM_LIT> : \n j = - <NUM_LIT> \n logger . debug ( \"<STR_LIT>\" , i , c , j ) \n return max_answer , max_answer_pos \n def find_pieces ( resp_text , payload ) : \n assert len ( resp_text ) < <NUM_LIT> and len ( payload ) < <NUM_LIT> \n logger . debug ( \"<STR_LIT>\" , resp_text [ : <NUM_LIT> ] , payload [ : <NUM_LIT> ] ) \n max_answer , max_answer_pos = kmp ( resp_text , payload ) \n logger . debug ( \"<STR_LIT>\" , max_answer , str ( max_answer_pos ) ) \n if max_answer <= <NUM_LIT> or max_answer_pos is None : \n logger . debug ( \"<STR_LIT>\" ) \n return [ ] \n resp_text_matched = resp_text [ max_answer_pos - max_answer + <NUM_LIT> : max_answer_pos + <NUM_LIT> ] \n resp_text_unmatched , payload_unmatched = ( \n resp_text [ max_answer_pos + <NUM_LIT> : ] , \n payload [ len ( resp_text_matched ) : ] , \n ) \n if payload_unmatched == \"<STR_LIT>\" : \n logger . debug ( \"<STR_LIT>\" ) \n return [ ] \n max_answer_unmatched , max_answer_pos_unmatched = kmp ( \n payload_unmatched , resp_text_unmatched \n ) \n if max_answer_pos_unmatched is None : \n return [ ] \n payload_unmatched_before = payload_unmatched [ \n : max_answer_pos_unmatched - max_answer_unmatched + <NUM_LIT> \n ] \n resp_text_next = removeprefix_string ( resp_text_unmatched , payload_unmatched_before ) \n payload_next = removeprefix_string ( payload_unmatched , payload_unmatched_before ) \n assert len ( resp_text_next ) < len ( resp_text ) and len ( payload_next ) < len ( payload ) \n return [ \n payload_unmatched_before , \n ] + find_pieces ( resp_text_next , payload_next ) \n def combine_waf ( waf_funcs ) : \n def new_waf_func ( s ) : \n return all ( waf ( s ) for waf in waf_funcs ) \n return new_waf_func \n class WafFuncGen : \n def __init__ ( \n self , \n submitter : Submitter , \n callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , \n options : Union [ Options , None ] = None , \n ) : \n self . subm = submitter \n self . callback : Callable [ [ str , Dict ] , None ] = ( \n callback if callback else ( lambda x , y : None ) \n ) \n self . options = options if options else Options ( ) \n def waf_page_hash ( self ) : \n test_keywords = ( \n grouped_payloads ( <NUM_LIT> ) + dangerous_keywords \n if self . options . detect_mode == DetectMode . ACCURATE \n else grouped_payloads ( <NUM_LIT> ) \n ) \n hashes : List [ int ] = [ ] \n for keyword in test_keywords : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( keyword * <NUM_LIT> ) ) , \n ) \n result = self . subm . submit ( keyword * <NUM_LIT> ) \n if result is None : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n colored ( \"<STR_LIT>\" , repr ( keyword * <NUM_LIT> ) ) , \n ) \n continue \n status_code , text = result \n if status_code == <NUM_LIT> : \n continue \n hashes . append ( hash ( text ) ) \n return [ k for k , v in Counter ( hashes ) . items ( ) if v >= <NUM_LIT> ] \n def long_param_hash ( self ) -> List [ int ] : \n logger . info ( \"<STR_LIT>\" ) \n keywords = [ \n \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) * <NUM_LIT> for _ in range ( <NUM_LIT> ) \n ] \n hashes = [ ] \n for keyword in keywords : \n result = self . subm . submit ( keyword ) \n if result is None : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ) \n continue \n status_code , text = result \n if status_code == <NUM_LIT> : \n continue \n hashes . append ( hash ( text ) ) \n hashes_uniq = list ( set ( hashes ) ) \n if len ( hashes_uniq ) <= <NUM_LIT> : \n logger . warning ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n time . sleep ( <NUM_LIT> ) \n return [ k for k , v in Counter ( hashes ) . items ( ) if v >= <NUM_LIT> ] \n def replaced_keyword ( self ) -> List [ str ] : \n extra = \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) \n test_payloads = ( \n dangerous_keywords \n if self . options . detect_mode == DetectMode . ACCURATE \n else grouped_payloads ( <NUM_LIT> , sep = extra ) \n ) \n keywords = [ ] \n for keyword in test_payloads : \n while extra [ <NUM_LIT> ] == keyword [ <NUM_LIT> ] or extra [ - <NUM_LIT> ] == keyword [ - <NUM_LIT> ] : \n extra = \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) \n payload = extra + keyword + extra \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( payload ) ) , \n ) \n result = self . subm . submit ( payload ) \n if result is None : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n colored ( \"<STR_LIT>\" , repr ( payload ) ) , \n ) \n continue \n status_code , text = result \n if status_code == <NUM_LIT> : \n continue \n if len ( text ) > <NUM_LIT> : \n continue \n try : \n payload_replaced_keyword = find_pieces ( text , payload ) \n except Exception : \n traceback . print_exc ( ) \n continue \n if payload_replaced_keyword : \n payload_replaced_keyword = list ( set ( payload_replaced_keyword ) ) \n if len ( payload_replaced_keyword ) > <NUM_LIT> : \n logger . info ( \n \"<STR_LIT>\" , \n len ( payload_replaced_keyword ) , \n ) \n else : \n keywords += payload_replaced_keyword", "output": "if keyword not in text and extra in text : \n keywords . append ( keyword ) \n keywords = list ( set ( keywords ) ) \n if keywords : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n colored ( \"<STR_LIT>\" , repr ( keywords ) ) , \n ) \n return keywords \n def doubletapping ( self , payload : str , keywords : List [ str ] ) : \n if not keywords : \n return payload \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n colored ( \"<STR_LIT>\" , payload ) , \n ) \n exist_keywords = [ w for w in keywords if w in payload ] \n replacement = { \n w : w [ : len ( w ) // <NUM_LIT> ] + w + w [ len ( w ) // <NUM_LIT> : ] \n for w in exist_keywords \n if len ( w ) >= <NUM_LIT> \n } \n for k , v in sorted ( replacement . items ( ) , key = lambda item : len ( item [ <NUM_LIT> ] ) ) : \n payload = payload . replace ( k , v ) \n return payload \n def generate ( self ) -> WafFunc : \n replaced_keyword = self . replaced_keyword ( ) \n waf_hashes = self . waf_page_hash ( ) \n long_param_hashes = self . long_param_hash ( ) \n long_param_hashes = [ h for h in long_param_hashes if h not in waf_hashes ] \n if ( \n self . options . replaced_keyword_strategy \n == ReplacedKeywordStrategy . DOUBLETAPPING \n ) : \n self . subm . add_tamperer ( lambda s : self . doubletapping ( s , replaced_keyword ) ) \n extra_content , extra_passed = ( \n \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) , \n False , \n ) \n @ lru_cache ( <NUM_LIT> ) \n def waf_func ( value ) : \n nonlocal extra_content , extra_passed , replaced_keyword \n payload = extra_content + value \n for _ in range ( <NUM_LIT> ) : \n if ( \n self . options . replaced_keyword_strategy \n == ReplacedKeywordStrategy . AVOID \n and any ( w in payload for w in replaced_keyword ) \n ) : \n logger . debug ( \"<STR_LIT>\" ) \n return False \n result = self . subm . submit ( payload ) \n if result is None : \n logger . debug ( \"<STR_LIT>\" ) \n return False \n if result . status_code == <NUM_LIT> : \n logger . debug ( \"<STR_LIT>\" ) \n return any ( w in result . text for w in render_error_keywords ) \n hash_text = hash ( result . text ) \n if hash_text in long_param_hashes : \n logger . debug ( \"<STR_LIT>\" ) \n return False \n if ( \n re . match ( r\"<STR_LIT>\" , payload ) \n and payload not in result . text \n ) : \n logger . debug ( \n \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , payload ) \n ) \n return False \n if extra_content in result . text : \n logger . debug ( \"<STR_LIT>\" ) \n return True \n replaced_list = find_pieces ( result . text , payload ) \n if replaced_list : \n logger . debug ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( replaced_list ) ) , \n ) \n replaced_keyword += replaced_list \n return ( \n self . options . replaced_keyword_strategy \n == ReplacedKeywordStrategy . IGNORE \n ) \n if self . options . detect_mode == DetectMode . FAST : \n logger . debug ( \"<STR_LIT>\" ) \n return False \n if extra_passed : \n logger . debug ( \"<STR_LIT>\" ) \n return False \n extra_content_result = self . subm . submit ( extra_content ) \n if extra_content_result is None : \n continue \n if ( \n extra_content_result . status_code != <NUM_LIT> \n and hash ( extra_content_result . text ) in waf_hashes \n ) : \n logger . debug ( \"<STR_LIT>\" ) \n extra_content = \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) \n continue \n extra_passed = True \n logger . debug ( \"<STR_LIT>\" ) \n return False \n return False \n return waf_func"}, {"input": "import asyncio \n from contextlib import asynccontextmanager , contextmanager \n def int_factory ( ) : \n return <NUM_LIT> \n def str_gen_factory ( ) : \n yield \"<STR_LIT>\" \n @ contextmanager \n def bool_cm_factory ( ) : \n yield True \n async def async_int_factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n return <NUM_LIT> \n async def async_str_gen_factory ( ) : \n await asyncio . sleep ( <NUM_LIT> )", "output": "yield str ( <NUM_LIT> ) \n await asyncio . sleep ( <NUM_LIT> ) \n @ asynccontextmanager \n async def async_bool_cm_factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n yield True"}, {"input": "import contextlib \n import sys \n from typing import AsyncGenerator , Generator , NewType , Protocol \n import svcs \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n from typing import Annotated \n else : \n from typing_extensions import Annotated \n reg = svcs . Registry ( ) \n con = svcs . Container ( reg ) \n reg . close ( ) \n with contextlib . closing ( reg ) as reg : \n ... \n with reg as reg : \n reg . register_factory ( int , int ) \n async def func ( ) -> None : \n await reg . aclose ( ) \n await con . aclose ( ) \n async with svcs . Registry ( ) as reg2 : \n reg2 . register_factory ( int , int ) \n async with svcs . Container ( reg2 ) as con2 : \n a : int \n b : str \n c : bool \n d : tuple \n e : object \n f : float \n g : list \n h : dict \n i : set \n j : bytes \n a , b , c , d , e , f , g , h , i , j = await con2 . aget ( \n int , str , bool , tuple , object , float , list , dict , set , bytes \n ) \n def gen ( ) -> Generator : \n yield <NUM_LIT> \n async def async_gen ( ) -> AsyncGenerator : \n yield <NUM_LIT> \n @ contextlib . asynccontextmanager \n async def async_cm ( ) -> AsyncGenerator : \n yield <NUM_LIT> \n def factory_with_cleanup ( ) -> Generator [ int , None , None ] : \n yield <NUM_LIT> \n @ contextlib . contextmanager \n def factory_with_cleanup_ctx ( ) -> Generator [ int , None , None ] : \n yield <NUM_LIT> \n def factory_that_takes_container_by_annotation ( foo : svcs . Container ) -> int : \n return <NUM_LIT> \n async def async_ping ( ) -> None : \n pass \n reg . register_value ( int , <NUM_LIT> ) \n reg . register_value ( int , <NUM_LIT> , ping = lambda : None ) \n reg . register_value ( int , <NUM_LIT> , ping = async_ping ) \n reg . register_value ( int , gen ) \n reg . register_factory ( str , str ) \n reg . register_factory ( int , factory_with_cleanup ) \n reg . register_factory ( int , factory_with_cleanup_ctx ) \n reg . register_factory ( int , factory_with_cleanup , ping = async_ping ) \n reg . register_factory ( str , async_gen ) \n reg . register_factory ( str , async_cm ) \n reg . register_value ( str , str , ping = lambda : None ) \n con = svcs . Container ( reg ) \n con . register_local_factory ( int , factory_with_cleanup ) \n con . register_local_value ( int , <NUM_LIT> ) \n o1 : object = con . get ( object ) \n o2 : int = con . get ( int ) \n a : int \n b : str \n c : bool \n d : tuple \n e : object \n f : float \n g : list \n h : dict \n i : set \n j : bytes \n a , b , c , d , e , f , g , h , i , j = con . get ( \n int , str , bool , tuple , object , float , list , dict , set , bytes \n ) \n class P ( Protocol ) :", "output": "def m ( self ) -> None : ... \n p : P = con . get_abstract ( P ) \n con . close ( ) \n with contextlib . closing ( svcs . Container ( reg ) ) as con : \n ... \n with svcs . Container ( reg ) as con : \n i2 : int = con . get ( int ) \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n async def ctx ( ) -> None : \n async with contextlib . aclosing ( svcs . Container ( reg ) ) : \n ... \n async with contextlib . aclosing ( svcs . Registry ( ) ) : \n ... \n S1 = Annotated [ str , \"<STR_LIT>\" ] \n S2 = NewType ( \"<STR_LIT>\" , str ) \n reg . register_value ( S1 , \"<STR_LIT>\" ) \n reg . register_value ( S2 , \"<STR_LIT>\" ) \n s1 : str = con . get ( S1 ) \n s2 : str = con . get ( S2 )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None :", "output": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Boolean ( ) , nullable = True ) ) \n def downgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "import logging , copy , pickle \n from weakref import ref \n from . . returnvalues import ReturnValue \n from . . utils import update_info_dict \n logger = logging . getLogger ( '<STR_LIT>' ) \n class AttributeDict ( dict ) : \n def __getattr__ ( self , value ) : \n keyName = value [ <NUM_LIT> ] . upper ( ) + value [ <NUM_LIT> : ] \n try : \n return self [ keyName ] \n except KeyError : \n raise AttributeError ( \"<STR_LIT>\" % ( \n self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , keyName ) ) \n def get ( self , v , d = None ) : \n try : \n return self [ v ] \n except KeyError : \n return d \n class UnInitializedItchat ( object ) : \n def _raise_error ( self , * args , ** kwargs ) : \n logger . warning ( '<STR_LIT>' ) \n def __getattr__ ( self , value ) : \n return self . _raise_error \n class ContactList ( list ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( ContactList , self ) . __init__ ( * args , ** kwargs ) \n self . __setstate__ ( None ) \n @ property \n def core ( self ) : \n return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat \n @ core . setter \n def core ( self , value ) : \n self . _core = ref ( value ) \n def set_default_value ( self , initFunction = None , contactClass = None ) : \n if hasattr ( initFunction , '<STR_LIT>' ) : \n self . contactInitFn = initFunction \n if hasattr ( contactClass , '<STR_LIT>' ) : \n self . contactClass = contactClass \n def append ( self , value ) : \n contact = self . contactClass ( value ) \n contact . core = self . core \n if self . contactInitFn is not None : \n contact = self . contactInitFn ( self , contact ) or contact \n super ( ContactList , self ) . append ( contact ) \n def __deepcopy__ ( self , memo ) : \n r = self . __class__ ( [ copy . deepcopy ( v ) for v in self ] ) \n r . contactInitFn = self . contactInitFn \n r . contactClass = self . contactClass \n r . core = self . core \n return r", "output": "def __getstate__ ( self ) : \n return <NUM_LIT> \n def __setstate__ ( self , state ) : \n self . contactInitFn = None \n self . contactClass = User \n def __str__ ( self ) : \n return '<STR_LIT>' % '<STR_LIT>' . join ( [ repr ( v ) for v in self ] ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n self . __str__ ( ) ) \n class AbstractUserDict ( AttributeDict ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( AbstractUserDict , self ) . __init__ ( * args , ** kwargs ) \n @ property \n def core ( self ) : \n return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat \n @ core . setter \n def core ( self , value ) : \n self . _core = ref ( value ) \n def update ( self ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def set_alias ( self , alias ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def set_pinned ( self , isPinned = True ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def verify ( self ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def get_head_image ( self , imageDir = None ) : \n return self . core . get_head_img ( self . userName , picDir = imageDir ) \n def delete_member ( self , userName ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def add_member ( self , userName ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send_raw_msg ( self , msgType , content ) : \n return self . core . send_raw_msg ( msgType , content , self . userName ) \n def send_msg ( self , msg = '<STR_LIT>' ) : \n return self . core . send_msg ( msg , self . userName ) \n def send_file ( self , fileDir , mediaId = None ) : \n return self . core . send_file ( fileDir , self . userName , mediaId ) \n def send_image ( self , fileDir , mediaId = None ) : \n return self . core . send_image ( fileDir , self . userName , mediaId ) \n def send_video ( self , fileDir = None , mediaId = None ) : \n return self . core . send_video ( fileDir , self . userName , mediaId ) \n def send ( self , msg , mediaId = None ) : \n return self . core . send ( msg , self . userName , mediaId ) \n def search_member ( self , name = None , userName = None , remarkName = None , nickName = None , \n wechatAccount = None ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def __deepcopy__ ( self , memo ) : \n r = self . __class__ ( ) \n for k , v in self . items ( ) : \n r [ copy . deepcopy ( k ) ] = copy . deepcopy ( v ) \n r . core = self . core \n return r \n def __str__ ( self ) : \n return '<STR_LIT>' % '<STR_LIT>' . join ( \n [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n self . __str__ ( ) ) \n def __getstate__ ( self ) : \n return <NUM_LIT> \n def __setstate__ ( self , state ) : \n pass \n class User ( AbstractUserDict ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( User , self ) . __init__ ( * args , ** kwargs ) \n self . __setstate__ ( None ) \n def update ( self ) : \n r = self . core . update_friend ( self . userName ) \n if r : \n update_info_dict ( self , r ) \n return r \n def set_alias ( self , alias ) : \n return self . core . set_alias ( self . userName , alias ) \n def set_pinned ( self , isPinned = True ) : \n return self . core . set_pinned ( self . userName , isPinned ) \n def verify ( self ) : \n return self . core . add_friend ( ** self . verifyDict ) \n def __deepcopy__ ( self , memo ) : \n r = super ( User , self ) . __deepcopy__ ( memo ) \n r . verifyDict = copy . deepcopy ( self . verifyDict ) \n return r \n def __setstate__ ( self , state ) : \n super ( User , self ) . __setstate__ ( state ) \n self . verifyDict = { } \n self [ '<STR_LIT>' ] = fakeContactList \n class MassivePlatform ( AbstractUserDict ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( MassivePlatform , self ) . __init__ ( * args , ** kwargs ) \n self . __setstate__ ( None ) \n def __setstate__ ( self , state ) : \n super ( MassivePlatform , self ) . __setstate__ ( state ) \n self [ '<STR_LIT>' ] = fakeContactList \n class Chatroom ( AbstractUserDict ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( Chatroom , self ) . __init__ ( * args , ** kwargs ) \n memberList = ContactList ( ) \n userName = self . get ( '<STR_LIT>' , '<STR_LIT>' ) \n refSelf = ref ( self ) \n def init_fn ( parentList , d ) : \n d . chatroom = refSelf ( ) or parentList . core . search_chatrooms ( userName = userName ) \n memberList . set_default_value ( init_fn , ChatroomMember ) \n if '<STR_LIT>' in self : \n for member in self . memberList : \n memberList . append ( member ) \n self [ '<STR_LIT>' ] = memberList \n @ property \n def core ( self ) : \n return getattr ( self , '<STR_LIT>' , lambda : fakeItchat ) ( ) or fakeItchat \n @ core . setter \n def core ( self , value ) : \n self . _core = ref ( value ) \n self . memberList . core = value \n for member in self . memberList : \n member . core = value \n def update ( self , detailedMember = False ) : \n r = self . core . update_chatroom ( self . userName , detailedMember ) \n if r : \n update_info_dict ( self , r ) \n self [ '<STR_LIT>' ] = r [ '<STR_LIT>' ] \n return r \n def set_alias ( self , alias ) : \n return self . core . set_chatroom_name ( self . userName , alias ) \n def set_pinned ( self , isPinned = True ) : \n return self . core . set_pinned ( self . userName , isPinned ) \n def delete_member ( self , userName ) : \n return self . core . delete_member_from_chatroom ( self . userName , userName ) \n def add_member ( self , userName ) : \n return self . core . add_member_into_chatroom ( self . userName , userName ) \n def search_member ( self , name = None , userName = None , remarkName = None , nickName = None , \n wechatAccount = None ) : \n with self . core . storageClass . updateLock : \n if ( name or userName or remarkName or nickName or wechatAccount ) is None : \n return None \n elif userName : \n for m in self . memberList : \n if m . userName == userName : \n return copy . deepcopy ( m ) \n else : \n matchDict = { \n '<STR_LIT>' : remarkName , \n '<STR_LIT>' : nickName , \n '<STR_LIT>' : wechatAccount , } \n for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n if matchDict [ k ] is None : \n del matchDict [ k ] \n if name : \n contact = [ ] \n for m in self . memberList : \n if any ( [ m . get ( k ) == name for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : \n contact . append ( m ) \n else : \n contact = self . memberList [ : ] \n if matchDict : \n friendList = [ ] \n for m in contact : \n if all ( [ m . get ( k ) == v for k , v in matchDict . items ( ) ] ) : \n friendList . append ( m ) \n return copy . deepcopy ( friendList ) \n else : \n return copy . deepcopy ( contact ) \n def __setstate__ ( self , state ) : \n super ( Chatroom , self ) . __setstate__ ( state ) \n if not '<STR_LIT>' in self : \n self [ '<STR_LIT>' ] = fakeContactList \n class ChatroomMember ( AbstractUserDict ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( AbstractUserDict , self ) . __init__ ( * args , ** kwargs ) \n self . __setstate__ ( None ) \n @ property \n def chatroom ( self ) : \n r = getattr ( self , '<STR_LIT>' , lambda : fakeChatroom ) ( ) \n if r is None : \n userName = getattr ( self , '<STR_LIT>' , '<STR_LIT>' ) \n r = self . core . search_chatrooms ( userName = userName ) \n if isinstance ( r , dict ) : \n self . chatroom = r \n return r or fakeChatroom \n @ chatroom . setter \n def chatroom ( self , value ) : \n if isinstance ( value , dict ) and '<STR_LIT>' in value : \n self . _chatroom = ref ( value ) \n self . _chatroomUserName = value [ '<STR_LIT>' ] \n def get_head_image ( self , imageDir = None ) : \n return self . core . get_head_img ( self . userName , self . chatroom . userName , picDir = imageDir ) \n def delete_member ( self , userName ) : \n return self . core . delete_member_from_chatroom ( self . chatroom . userName , self . userName ) \n def send_raw_msg ( self , msgType , content ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send_msg ( self , msg = '<STR_LIT>' ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send_file ( self , fileDir , mediaId = None ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send_image ( self , fileDir , mediaId = None ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send_video ( self , fileDir = None , mediaId = None ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def send ( self , msg , mediaId = None ) : \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : '<STR_LIT>' % self . __class__ . __name__ , } , } ) \n def __setstate__ ( self , state ) : \n super ( ChatroomMember , self ) . __setstate__ ( state ) \n self [ '<STR_LIT>' ] = fakeContactList \n def wrap_user_dict ( d ) : \n userName = d . get ( '<STR_LIT>' ) \n if '<STR_LIT>' in userName : \n r = Chatroom ( d ) \n elif d . get ( '<STR_LIT>' , <NUM_LIT> ) & <NUM_LIT> == <NUM_LIT> : \n r = User ( d ) \n else : \n r = MassivePlatform ( d ) \n return r \n fakeItchat = UnInitializedItchat ( ) \n fakeContactList = ContactList ( ) \n fakeChatroom = Chatroom ( )"}, {"input": "from logging . config import fileConfig \n from alembic import context \n from feedi . models import db \n from sqlalchemy import engine_from_config , pool \n config = context . config \n if config . config_file_name is not None :", "output": "fileConfig ( config . config_file_name ) \n target_metadata = db . Model . metadata \n def run_migrations_offline ( ) -> None : \n url = config . get_main_option ( \"<STR_LIT>\" ) \n context . configure ( \n url = url , \n target_metadata = target_metadata , \n literal_binds = True , \n render_as_batch = True , \n dialect_opts = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n with context . begin_transaction ( ) : \n context . run_migrations ( ) \n def run_migrations_online ( ) -> None : \n connectable = engine_from_config ( \n config . get_section ( config . config_ini_section , { } ) , \n prefix = \"<STR_LIT>\" , \n poolclass = pool . NullPool , \n ) \n with connectable . connect ( ) as connection : \n context . configure ( \n connection = connection , \n target_metadata = target_metadata , \n render_as_batch = True \n ) \n with context . begin_transaction ( ) : \n context . run_migrations ( ) \n if context . is_offline_mode ( ) : \n run_migrations_offline ( ) \n else : \n run_migrations_online ( )"}, {"input": "import logging , traceback , sys , threading \n try : \n import Queue \n except ImportError : \n import queue as Queue \n from . . log import set_logging \n from . . utils import test_connect \n from . . storage import templates \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_register ( core ) : \n core . auto_login = auto_login \n core . configured_reply = configured_reply \n core . msg_register = msg_register \n core . run = run \n async def auto_login ( self , EventScanPayload = None , ScanStatus = None , event_stream = None , \n hotReload = True , statusStorageDir = '<STR_LIT>' , \n enableCmdQR = False , picDir = None , qrCallback = None ,", "output": "loginCallback = None , exitCallback = None ) : \n if not test_connect ( ) : \n logger . info ( \"<STR_LIT>\" ) \n sys . exit ( ) \n self . useHotReload = hotReload \n self . hotReloadDir = statusStorageDir \n if hotReload : \n if await self . load_login_status ( statusStorageDir , \n loginCallback = loginCallback , exitCallback = exitCallback ) : \n return \n await self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , EventScanPayload = EventScanPayload , ScanStatus = ScanStatus , event_stream = event_stream , \n loginCallback = loginCallback , exitCallback = exitCallback ) \n await self . dump_login_status ( statusStorageDir ) \n else : \n await self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , EventScanPayload = EventScanPayload , ScanStatus = ScanStatus , event_stream = event_stream , \n loginCallback = loginCallback , exitCallback = exitCallback ) \n async def configured_reply ( self , event_stream , payload , message_container ) : \n try : \n msg = self . msgList . get ( timeout = <NUM_LIT> ) \n if '<STR_LIT>' in msg . keys ( ) : \n message_container [ msg [ '<STR_LIT>' ] ] = msg \n except Queue . Empty : \n pass \n else : \n if isinstance ( msg [ '<STR_LIT>' ] , templates . User ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n elif isinstance ( msg [ '<STR_LIT>' ] , templates . MassivePlatform ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n elif isinstance ( msg [ '<STR_LIT>' ] , templates . Chatroom ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n if replyFn is None : \n r = None \n else : \n try : \n r = await replyFn ( msg ) \n if r is not None : \n await self . send ( r , msg . get ( '<STR_LIT>' ) ) \n except : \n logger . warning ( traceback . format_exc ( ) ) \n def msg_register ( self , msgType , isFriendChat = False , isGroupChat = False , isMpChat = False ) : \n if not ( isinstance ( msgType , list ) or isinstance ( msgType , tuple ) ) : \n msgType = [ msgType ] \n def _msg_register ( fn ) : \n for _msgType in msgType : \n if isFriendChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if isGroupChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if isMpChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if not any ( ( isFriendChat , isGroupChat , isMpChat ) ) : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n return fn \n return _msg_register \n async def run ( self , debug = False , blockThread = True ) : \n logger . info ( '<STR_LIT>' ) \n if debug : \n set_logging ( loggingLevel = logging . DEBUG ) \n async def reply_fn ( ) : \n try : \n while self . alive : \n await self . configured_reply ( ) \n except KeyboardInterrupt : \n if self . useHotReload : \n await self . dump_login_status ( ) \n self . alive = False \n logger . debug ( '<STR_LIT>' ) \n logger . info ( '<STR_LIT>' ) \n if blockThread : \n await reply_fn ( ) \n else : \n replyThread = threading . Thread ( target = reply_fn ) \n replyThread . setDaemon ( True ) \n replyThread . start ( )"}, {"input": "import random \n import string \n import time \n import json \n from . crypto import crypto", "output": "def generate_cookie_string ( length = <NUM_LIT> ) : \n characters = string . ascii_letters + string . digits \n return '<STR_LIT>' . join ( random . choice ( characters ) for _ in range ( length ) ) \n def set_cookie ( key : str ) -> str : \n now = time . time ( ) \n plain_text = json . dumps ( { '<STR_LIT>' : key , '<STR_LIT>' : now + <NUM_LIT> } ) \n return crypto . encrypt ( plain_text ) \n def cookie_key ( cookie_string : str ) -> str : \n current_time = int ( time . time ( ) ) \n plain_text = crypto . decrypt ( cookie_string ) \n try : \n data = json . loads ( plain_text ) \n except json . JSONDecodeError : \n return '<STR_LIT>' \n if current_time > data . get ( '<STR_LIT>' , <NUM_LIT> ) : \n return '<STR_LIT>' \n return data . get ( '<STR_LIT>' )"}, {"input": "from __future__ import annotations \n import contextlib \n import inspect \n import sys \n from typing import AsyncGenerator , Callable \n import attrs \n from fastapi import Depends , FastAPI , Request \n import svcs \n from svcs . _core import _KEY_REGISTRY \n if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : \n from typing_extensions import Annotated \n else : \n from typing import Annotated \n @ attrs . define \n class lifespan : \n _lifespan : (", "output": "Callable [ \n [ FastAPI , svcs . Registry ] , \n contextlib . AbstractAsyncContextManager [ dict [ str , object ] ] , \n ] \n | Callable [ \n [ FastAPI , svcs . Registry ] , \n contextlib . AbstractAsyncContextManager [ None ] , \n ] \n | Callable [ \n [ FastAPI , svcs . Registry ] , AsyncGenerator [ dict [ str , object ] , None ] \n ] \n | Callable [ [ FastAPI , svcs . Registry ] , AsyncGenerator [ None , None ] ] \n ) \n _state : dict [ str , object ] = attrs . field ( factory = dict ) \n registry : svcs . Registry = attrs . field ( factory = svcs . Registry ) \n @ contextlib . asynccontextmanager \n async def __call__ ( \n self , app : FastAPI \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n cm : Callable [ \n [ FastAPI , svcs . Registry ] , contextlib . AbstractAsyncContextManager \n ] \n if inspect . isasyncgenfunction ( self . _lifespan ) : \n cm = contextlib . asynccontextmanager ( self . _lifespan ) \n else : \n cm = self . _lifespan \n async with self . registry , cm ( app , self . registry ) as state : \n self . _state = state or { } \n self . _state [ _KEY_REGISTRY ] = self . registry \n yield self . _state \n async def container ( request : Request ) -> AsyncGenerator [ svcs . Container , None ] : \n async with svcs . Container ( getattr ( request . state , _KEY_REGISTRY ) ) as cont : \n yield cont \n DepContainer = Annotated [ svcs . Container , Depends ( container ) ]"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>'", "output": "branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' ) \n batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "import os \n import sys \n import tempfile \n from functools import partial \n from contextlib import contextmanager \n from instld . errors import InstallingPackageError , RunningCommandError \n from instld . module . context import Context \n from instld . module . runner import run_python as standard_runner \n from instld . module . lock import lock \n @ contextmanager \n def search_path ( base_dir , logger , runner ) : \n sys_path = os . path . join ( base_dir , '<STR_LIT>' )", "output": "standard_runner ( [ '<STR_LIT>' , '<STR_LIT>' , base_dir ] , logger , True ) \n for maybe_directory in os . listdir ( path = sys_path ) : \n maybe_directory_full = os . path . join ( sys_path , maybe_directory ) \n if maybe_directory . startswith ( '<STR_LIT>' ) and os . path . isdir ( maybe_directory_full ) : \n sys_path = os . path . join ( sys_path , maybe_directory , '<STR_LIT>' ) \n break \n with lock : \n sys . path . insert ( <NUM_LIT> , sys_path ) \n yield sys_path \n with lock : \n del sys . path [ sys . path . index ( sys_path ) ] \n @ contextmanager \n def pip_context ( packages_names , options , logger , runner , catch_output , where ) : \n if where is not None : \n @ contextmanager \n def create_temp_directory ( ) : \n yield where \n else : \n create_temp_directory = tempfile . TemporaryDirectory \n with create_temp_directory ( ) as directory : \n with search_path ( directory , logger , runner ) as where : \n try : \n if '<STR_LIT>' in options or '<STR_LIT>' in options : \n runner ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , f'<STR_LIT>' , * options ] , logger , catch_output ) \n else : \n for package_name in packages_names : \n runner ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , f'<STR_LIT>' , * options , package_name ] , logger , catch_output ) \n except RunningCommandError as e : \n new_error = InstallingPackageError ( f'<STR_LIT>' ) \n new_error . stdout = e . stdout \n new_error . stderr = e . stderr \n raise new_error from e \n yield Context ( where , logger , catch_output , options , partial ( pip_context , logger = logger , runner = runner , catch_output = catch_output , where = directory ) )"}, {"input": "import ipaddress , subprocess , datetime , os , util \n from util import * \n notEnoughParameter = { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } \n good = { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" } \n def ret ( status = True , reason = \"<STR_LIT>\" , data = \"<STR_LIT>\" ) : \n return { \"<STR_LIT>\" : status , \"<STR_LIT>\" : reason , \"<STR_LIT>\" : data } \n def togglePeerAccess ( data , g ) : \n checkUnlock = g . cur . execute ( f\"<STR_LIT>\" ) . fetchone ( ) \n if checkUnlock : \n moveUnlockToLock = g . cur . execute ( \n f\"<STR_LIT>\" ) \n if g . cur . rowcount == <NUM_LIT> : \n print ( g . cur . rowcount ) \n print ( util . deletePeers ( data [ '<STR_LIT>' ] , [ data [ '<STR_LIT>' ] ] , g . cur , g . db ) ) \n else : \n moveLockToUnlock = g . cur . execute ( \n f\"<STR_LIT>\" ) . fetchone ( ) \n try : \n if len ( moveLockToUnlock [ - <NUM_LIT> ] ) == <NUM_LIT> : \n status = subprocess . check_output ( \n f\"<STR_LIT>\" , \n shell = True , stderr = subprocess . STDOUT ) \n else : \n now = str ( datetime . datetime . now ( ) . strftime ( \"<STR_LIT>\" ) ) \n f_name = now + \"<STR_LIT>\" \n f = open ( f_name , \"<STR_LIT>\" ) \n f . write ( moveLockToUnlock [ - <NUM_LIT> ] ) \n f . close ( ) \n subprocess . check_output ( \n f\"<STR_LIT>\" , \n shell = True , stderr = subprocess . STDOUT ) \n os . remove ( f_name ) \n status = subprocess . check_output ( f\"<STR_LIT>\" , shell = True , stderr = subprocess . STDOUT ) \n g . cur . execute ( \n f\"<STR_LIT>\" ) \n if g . cur . rowcount == <NUM_LIT> : \n g . cur . execute ( f\"<STR_LIT>\" ) \n except subprocess . CalledProcessError as exc : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : str ( exc . output . strip ( ) ) } \n return good \n class addConfiguration : \n def AddressCheck ( self , data ) : \n address = data [ '<STR_LIT>' ] \n address = address . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n address = address . split ( '<STR_LIT>' ) \n amount = <NUM_LIT>", "output": "for i in address : \n try : \n ips = ipaddress . ip_network ( i , False ) \n amount += ips . num_addresses \n except ValueError as e : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : str ( e ) } \n if amount >= <NUM_LIT> : \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : f\"<STR_LIT>\" } \n else : \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : f\"<STR_LIT>\" } \n def PortCheck ( self , data , configs ) : \n port = data [ '<STR_LIT>' ] \n if ( not port . isdigit ( ) ) or int ( port ) < <NUM_LIT> or int ( port ) > <NUM_LIT> : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : f\"<STR_LIT>\" } \n for i in configs : \n if i [ '<STR_LIT>' ] == port : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : f\"<STR_LIT>\" } \n return good \n def NameCheck ( self , data , configs ) : \n name = data [ '<STR_LIT>' ] \n name = name . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n for i in configs : \n if name == i [ '<STR_LIT>' ] : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : f\"<STR_LIT>\" } \n illegal_filename = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , '<STR_LIT>' '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n for i in illegal_filename : \n name = name . replace ( i , \"<STR_LIT>\" ) \n if len ( name ) == <NUM_LIT> : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } \n return good \n def addConfiguration ( self , data , configs , WG_CONF_PATH ) : \n output = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n required = [ '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' ] \n for i in required : \n e = data [ i ] \n if len ( e ) != <NUM_LIT> : \n key = i . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n o = f\"<STR_LIT>\" \n output . append ( o ) \n name = data [ '<STR_LIT>' ] \n illegal_filename = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , '<STR_LIT>' '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n for i in illegal_filename : \n name = name . replace ( i , \"<STR_LIT>\" ) \n try : \n newFile = open ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) \n newFile . write ( \"<STR_LIT>\" . join ( output ) ) \n except Exception as e : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : str ( e ) } \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : name } \n def deleteConfiguration ( self , data , config , g , WG_CONF_PATH ) : \n confs = [ ] \n for i in config : \n confs . append ( i [ '<STR_LIT>' ] ) \n print ( confs ) \n if data [ '<STR_LIT>' ] not in confs : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } \n for i in config : \n if i [ '<STR_LIT>' ] == data [ '<STR_LIT>' ] : \n if i [ '<STR_LIT>' ] == \"<STR_LIT>\" : \n try : \n subprocess . check_output ( \"<STR_LIT>\" + data [ '<STR_LIT>' ] , shell = True , stderr = subprocess . STDOUT ) \n except subprocess . CalledProcessError as exc : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str ( exc . output . strip ( ) . decode ( \"<STR_LIT>\" ) ) } \n g . cur . execute ( f'<STR_LIT>' ) \n g . cur . execute ( f'<STR_LIT>' ) \n g . db . commit ( ) \n try : \n os . remove ( f'<STR_LIT>' ) \n except Exception as e : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str ( e ) } \n return good \n class settings : \n def setTheme ( self , theme , config , setConfig ) : \n themes = [ '<STR_LIT>' , '<STR_LIT>' ] \n if theme not in themes : \n return ret ( status = False , reason = \"<STR_LIT>\" ) \n config [ '<STR_LIT>' ] [ '<STR_LIT>' ] = theme \n setConfig ( config ) \n return ret ( )"}, {"input": "import numpy as np \n class Circulo : \n def __init__ ( self , radio ) : \n self . _radio = radio \n @ property \n def radio ( self ) : \n return self . _radio \n @ radio . setter \n def radio ( self , valor ) : \n if valor < <NUM_LIT> : \n raise ValueError ( \"<STR_LIT>\" ) \n self . _radio = valor \n @ property \n def diametro ( self ) : \n return self . _radio * <NUM_LIT> \n @ property \n def area ( self ) : \n return np . pi * self . _radio ** <NUM_LIT> \n mi_circulo = Circulo ( <NUM_LIT> )", "output": "print ( mi_circulo . radio ) \n print ( mi_circulo . diametro ) \n print ( mi_circulo . area ) \n mi_circulo . radio = <NUM_LIT> \n print ( mi_circulo . radio ) \n print ( mi_circulo . diametro ) \n print ( mi_circulo . area )"}, {"input": "import platform \n IS_SUPPORTED_PLATFORM = platform . system ( ) != \"<STR_LIT>\" \n IS_COLORING_ENABLED = False \n def set_enable_coloring ( enable = True ) : \n global IS_COLORING_ENABLED \n IS_COLORING_ENABLED = enable \n def colored ( color , text , bold = False ) : \n if not IS_SUPPORTED_PLATFORM or not IS_COLORING_ENABLED : \n return text \n colors = { \n \"<STR_LIT>\" : \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n format_str = \"<STR_LIT>\" \n if bold : \n format_str = \"<STR_LIT>\" \n if color not in colors : \n color = \"<STR_LIT>\" \n return format_str . format ( int ( bold ) , colors [ color ] , text )"}, {"input": "import asyncio \n import os \n import sys \n sys . path . append ( '<STR_LIT>' ) \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n from main import reliableGPT \n import openai \n openai . api_key = os . getenv ( \"<STR_LIT>\" ) \n openai . api_type = \"<STR_LIT>\" \n openai . api_base = os . getenv ( \"<STR_LIT>\" ) \n openai . api_version = \"<STR_LIT>\" \n print ( f\"<STR_LIT>\" ) \n openai . ChatCompletion . acreate = reliableGPT ( \n openai . ChatCompletion . acreate , _test = True , \n user_email = \"<STR_LIT>\" , azure_fallback_strategy = [ \"<STR_LIT>\" ] , verbose = True ) \n async def create_chat_completion ( ) : \n chat_completion_resp = await openai . ChatCompletion . acreate ( engine = \"<STR_LIT>\" , model = \"<STR_LIT>\" , messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } ] ) \n print ( chat_completion_resp ) \n async def call_create_chat_completion ( ) :", "output": "for _ in range ( <NUM_LIT> ) : \n await create_chat_completion ( ) \n asyncio . run ( call_create_chat_completion ( ) )"}, {"input": "from typing import Dict , List , Optional , Any \n from abc import ABC , abstractmethod \n from functools import cached_property \n import torch . nn as nn \n import torch \n class Modality ( ABC ) : \n @ abstractmethod \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n pass \n @ property \n @ abstractmethod \n def name ( self ) -> str : \n pass \n @ property \n @ abstractmethod \n def token ( self ) -> str : \n pass \n @ property \n @ abstractmethod \n def data_key ( self ) -> str : \n pass \n @ property", "output": "@ abstractmethod \n def token_width ( self ) -> int : \n pass \n @ cached_property \n def token_idx ( self ) -> int : \n hash_ = sum ( ord ( c ) ** i for i , c in enumerate ( self . token ) ) \n return - abs ( hash_ % <NUM_LIT> ) \n @ abstractmethod \n def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Any ] ] : \n pass \n @ abstractmethod \n def forward ( self , encoded_values : List [ Any ] ) -> List [ torch . Tensor ] : \n pass \n def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n return self"}, {"input": "from setuptools import setup , find_namespace_packages \n import sys \n with open ( \"<STR_LIT>\" ) as f : \n reqs = f . read ( ) \n if __name__ == \"<STR_LIT>\" : \n setup ( \n name = \"<STR_LIT>\" , \n version = \"<STR_LIT>\" , \n description = \"<STR_LIT>\" , \n python_requires = \"<STR_LIT>\" , \n packages = find_namespace_packages ( include = [ '<STR_LIT>' ] ) , \n install_requires = reqs . strip ( ) . split ( \"<STR_LIT>\" ) , \n classifiers = [", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n )"}, {"input": "from typing import Optional \n from uuid import uuid4 \n import time \n import json \n from profyle . domain . trace import TraceCreate \n from profyle . domain . trace_repository import TraceRepository \n from profyle . domain . trace import Trace \n class InMemoryTraceRepository ( TraceRepository ) : \n def __init__ ( self ) :", "output": "self . traces : list [ Trace ] = [ ] \n self . selected_trace : int = <NUM_LIT> \n def create_trace_selected_table ( self ) -> None : \n ... \n def create_trace_table ( self ) -> None : \n ... \n def delete_all_traces ( self ) -> int : \n removed = len ( self . traces ) \n self . traces = [ ] \n return removed \n def vacuum ( self ) -> None : \n ... \n def store_trace_selected ( self , trace_id : int ) -> None : \n self . selected_trace = trace_id \n def store_trace ( self , new_trace : TraceCreate ) -> None : \n trace = Trace ( \n id = uuid4 ( ) . int , \n timestamp = str ( time . time ( ) ) , \n data = json . dumps ( new_trace . data ) , \n duration = new_trace . duration , \n name = new_trace . name , \n ) \n self . traces . append ( trace ) \n def get_all_traces ( self ) -> list [ Trace ] : \n return self . traces \n def get_trace_by_id ( self , id : int ) -> Optional [ Trace ] : \n for trace in self . traces : \n if trace . id == id : \n return trace \n return \n def get_trace_selected ( self ) -> Optional [ int ] : \n return self . selected_trace \n def delete_trace_by_id ( self , trace_id : int ) : \n for trace in self . traces : \n if trace . id == trace_id : \n self . traces . remove ( trace ) \n return"}, {"input": "import inspect \n import pytest \n from instld . errors import InstallingPackageError \n from instld . cli . parsing_comments . get_options_from_comments import get_options_from_comments_by_frame \n def test_get_normal_options ( ) : \n options = get_options_from_comments_by_frame ( inspect . currentframe ( ) )", "output": "assert isinstance ( options , dict ) \n assert len ( options ) == <NUM_LIT> \n assert options [ '<STR_LIT>' ] == '<STR_LIT>' \n assert options [ '<STR_LIT>' ] == '<STR_LIT>' \n @ pytest . mark . skip ( \"<STR_LIT>\" ) \n def test_real_options ( ) : \n options = get_options_from_comments_by_frame ( inspect . currentframe ( ) ) \n assert options == { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n } \n def test_get_wrong_options ( ) : \n with pytest . raises ( InstallingPackageError ) : \n get_options_from_comments_by_frame ( inspect . currentframe ( ) ) \n with pytest . raises ( InstallingPackageError ) : \n get_options_from_comments_by_frame ( inspect . currentframe ( ) ) \n with pytest . raises ( InstallingPackageError ) : \n get_options_from_comments_by_frame ( inspect . currentframe ( ) ) \n def test_get_empty_options ( ) : \n options = get_options_from_comments_by_frame ( inspect . currentframe ( ) ) \n assert isinstance ( options , dict ) \n assert len ( options ) == <NUM_LIT>"}, {"input": "while True : \n try : \n file = open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n edad = int ( input ( \"<STR_LIT>\" ) ) \n file . write ( \"<STR_LIT>\" + str ( edad ** <NUM_LIT> ) ) \n print ( \"<STR_LIT>\" )", "output": "break \n except KeyboardInterrupt : \n print ( \"<STR_LIT>\" ) \n break \n except IOError : \n print ( \"<STR_LIT>\" ) \n except ValueError ( \"<STR_LIT>\" ) as error : \n print ( \"<STR_LIT>\" ) \n finally : \n file . close ( ) \n print ( \"<STR_LIT>\" )"}, {"input": "import re \n import subprocess \n import dashboard \n def regex_match ( regex , text ) : \n pattern = re . compile ( regex ) \n return pattern . search ( text ) is not None \n def check_IP ( ip ) : \n ip_patterns = ( \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" \n ) \n for match_pattern in ip_patterns : \n match_result = regex_match ( match_pattern , ip ) \n if match_result : \n result = match_result \n break \n else : \n result = None \n return result \n def clean_IP ( ip ) : \n return ip . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n def clean_IP_with_range ( ip ) : \n return clean_IP ( ip ) . split ( '<STR_LIT>' ) \n def check_IP_with_range ( ip ) : \n ip_patterns = ( \n r\"<STR_LIT>\" , \n r\"<STR_LIT>\" \n ) \n for match_pattern in ip_patterns : \n match_result = regex_match ( match_pattern , ip ) \n if match_result : \n result = match_result \n break \n else : \n result = None \n return result \n def check_Allowed_IPs ( ip ) : \n ip = clean_IP_with_range ( ip ) \n for i in ip : \n if not check_IP_with_range ( i ) : return False \n return True \n def check_DNS ( dns ) : \n dns = dns . replace ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) \n status = True \n for i in dns : \n if not ( check_IP ( i ) or regex_match ( \"<STR_LIT>\" , i ) ) : \n return False \n return True \n def check_remote_endpoint ( address ) :", "output": "return ( check_IP ( address ) or regex_match ( \"<STR_LIT>\" , \n address ) ) \n def deletePeers ( config_name , delete_keys , cur , db ) : \n sql_command = [ ] \n wg_command = [ \"<STR_LIT>\" , \"<STR_LIT>\" , config_name ] \n for delete_key in delete_keys : \n if delete_key not in dashboard . get_conf_peer_key ( config_name ) : \n return \"<STR_LIT>\" \n sql_command . append ( \"<STR_LIT>\" + config_name + \"<STR_LIT>\" + delete_key + \"<STR_LIT>\" ) \n wg_command . append ( \"<STR_LIT>\" ) \n wg_command . append ( delete_key ) \n wg_command . append ( \"<STR_LIT>\" ) \n try : \n print ( \"<STR_LIT>\" ) \n remove_wg = subprocess . check_output ( \"<STR_LIT>\" . join ( wg_command ) , \n shell = True , stderr = subprocess . STDOUT ) \n save_wg = subprocess . check_output ( f\"<STR_LIT>\" , shell = True , stderr = subprocess . STDOUT ) \n cur . executescript ( '<STR_LIT>' . join ( sql_command ) ) \n db . commit ( ) \n except subprocess . CalledProcessError as exc : \n return exc . output . strip ( ) \n return \"<STR_LIT>\" \n def checkJSONAllParameter ( required , data ) : \n if len ( data ) == <NUM_LIT> : \n return False \n for i in required : \n if i not in list ( data . keys ( ) ) or len ( data [ i ] ) == <NUM_LIT> : \n return False \n return True"}, {"input": "import os \n import json \n from long_captions . prepare . remote_language_model import RemoteLanguageModel \n from long_captions . dense_image import DenseCaptionedImage , get_key_for , get_dci_count \n from long_captions . config import DATASET_COMPLETE_PATH \n import threading \n from queue import Queue \n from typing import Tuple , Dict , List \n NUM_THREADS = <NUM_LIT> \n def get_new_summaries ( \n rlm : RemoteLanguageModel , dci : DenseCaptionedImage , max_retry = <NUM_LIT> \n ) -> Tuple [ Dict [ str , List [ str ] ] , bool , bool ] : \n did_nothing = True \n summaries = dci . get_summaries ( ) \n while not isinstance ( summaries [ '<STR_LIT>' ] , list ) : \n try : \n summaries [ '<STR_LIT>' ] = [ summaries [ '<STR_LIT>' ] ] + rlm . get_new_captions_for_dci ( dci ) \n did_nothing = False \n except AssertionError : \n max_retry -= <NUM_LIT> \n if max_retry == <NUM_LIT> : \n return summaries , did_nothing , False \n all_masks = dci . filter_masks_by_size ( ) \n for m in all_masks : \n key = f'<STR_LIT>' \n while not isinstance ( summaries [ key ] , list ) : \n try : \n summaries [ key ] = [ summaries [ key ] ] + rlm . get_new_captions_for_mask ( dci , m ) \n did_nothing = False \n except AssertionError : \n max_retry -= <NUM_LIT> \n if max_retry == <NUM_LIT> : \n return summaries , did_nothing , False \n return summaries , did_nothing , True \n def thread_entry ( idx_pool : Queue , rlm ) : \n while not idx_pool . empty ( ) : \n i = idx_pool . get ( ) \n if int ( i ) % <NUM_LIT> == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n dci = DenseCaptionedImage ( i ) \n key = get_key_for ( i )", "output": "source_path = os . path . join ( DATASET_COMPLETE_PATH , key ) \n if not os . path . exists ( source_path ) : \n continue \n new_summaries , did_nothing , success = get_new_summaries ( rlm , dci ) \n if did_nothing is True : \n print ( \"<STR_LIT>\" , end = '<STR_LIT>' , flush = True ) \n else : \n with open ( source_path , '<STR_LIT>' ) as jsonf : \n orig_data = json . load ( jsonf ) \n orig_data [ '<STR_LIT>' ] = new_summaries \n if success is False : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n with open ( source_path , '<STR_LIT>' ) as jsonf : \n json . dump ( orig_data , jsonf ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n def run_gen_additional_summary ( ) : \n rlm = RemoteLanguageModel ( '<STR_LIT>' ) \n idx_pool = Queue ( ) \n count = get_dci_count ( ) \n for i in range ( count ) : \n idx_pool . put ( i ) \n thread_pool = [ ] \n for _ in range ( NUM_THREADS ) : \n t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) \n t . start ( ) \n thread_pool . append ( t ) \n for thread in thread_pool : \n thread . join ( ) \n if __name__ == '<STR_LIT>' : \n run_gen_additional_summary ( )"}, {"input": "import os \n import torch \n import hashlib \n import datetime \n from collections import OrderedDict \n def replace_keys_in_dict ( d , old_key_part , new_key_part ) : \n if isinstance ( d , OrderedDict ) : \n updated_dict = OrderedDict ( ) \n else : \n updated_dict = { } \n for key , value in d . items ( ) : \n new_key = key . replace ( old_key_part , new_key_part ) \n if isinstance ( value , dict ) : \n value = replace_keys_in_dict ( value , old_key_part , new_key_part ) \n updated_dict [ new_key ] = value \n return updated_dict \n def extract_small_model ( path , name , sr , if_f0 , version , epoch , step ) : \n try : \n ckpt = torch . load ( path , map_location = \"<STR_LIT>\" ) \n pth_file = f\"<STR_LIT>\" \n pth_file_old_version_path = os . path . join ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) \n opt = OrderedDict ( \n weight = { \n key : value . half ( ) for key , value in ckpt . items ( ) if \"<STR_LIT>\" not in key \n } \n ) \n if \"<STR_LIT>\" in ckpt : \n ckpt = ckpt [ \"<STR_LIT>\" ] \n opt = OrderedDict ( ) \n opt [ \"<STR_LIT>\" ] = { } \n for key in ckpt . keys ( ) : \n if \"<STR_LIT>\" in key :", "output": "continue \n opt [ \"<STR_LIT>\" ] [ key ] = ckpt [ key ] . half ( ) \n if sr == \"<STR_LIT>\" : \n opt [ \"<STR_LIT>\" ] = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n elif sr == \"<STR_LIT>\" : \n if version == \"<STR_LIT>\" : \n opt [ \"<STR_LIT>\" ] = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n else : \n opt [ \"<STR_LIT>\" ] = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n elif sr == \"<STR_LIT>\" : \n if version == \"<STR_LIT>\" : \n opt [ \"<STR_LIT>\" ] = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n else : \n opt [ \"<STR_LIT>\" ] = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n opt [ \"<STR_LIT>\" ] = epoch \n opt [ \"<STR_LIT>\" ] = step \n opt [ \"<STR_LIT>\" ] = sr \n opt [ \"<STR_LIT>\" ] = int ( if_f0 ) \n opt [ \"<STR_LIT>\" ] = version \n opt [ \"<STR_LIT>\" ] = datetime . datetime . now ( ) . isoformat ( ) \n hash_input = f\"<STR_LIT>\" \n model_hash = hashlib . sha256 ( hash_input . encode ( ) ) . hexdigest ( ) \n opt [ \"<STR_LIT>\" ] = model_hash \n model = torch . load ( pth_file_old_version_path , map_location = torch . device ( \"<STR_LIT>\" ) ) \n torch . save ( \n replace_keys_in_dict ( \n replace_keys_in_dict ( \n model , \"<STR_LIT>\" , \"<STR_LIT>\" \n ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n pth_file_old_version_path , \n ) \n os . remove ( pth_file_old_version_path ) \n os . rename ( pth_file_old_version_path , pth_file ) \n except Exception as error : \n print ( error )"}, {"input": "import requests \n import concurrent . futures \n import time \n url = \"<STR_LIT>\"", "output": "params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n queries = [ ] \n for i in range ( <NUM_LIT> ) : \n query = f\"<STR_LIT>\" \n queries . append ( query ) \n def make_request ( query ) : \n params [ \"<STR_LIT>\" ] = query \n params [ \"<STR_LIT>\" ] = <NUM_LIT> \n print ( f\"<STR_LIT>\" ) \n response = requests . get ( url , params = params ) \n print ( response ) \n return response . text \n start_time = time . time ( ) \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : \n futures = [ executor . submit ( make_request , query ) for query in queries ] \n concurrent . futures . wait ( futures ) \n results = [ future . result ( ) for future in futures ] \n end_time = time . time ( ) \n print ( f\"<STR_LIT>\" ) \n for result in results : \n print ( result )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n import random \n import openai \n from datasets import Dataset , load_dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n PROMPT = \n QUESTIONS = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n OPENAI_TOOLS = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } , \n } , \n } \n ] \n def _build_convo ( row ) -> List : \n client = openai . Client ( ) \n captions = [ row [ \"<STR_LIT>\" ] ] \n paths = [ row [ \"<STR_LIT>\" ] ] \n captions_text = \"<STR_LIT>\" . join ( [ f\"<STR_LIT>\" for i , cap in enumerate ( captions ) ] )", "output": "prompt = PROMPT . format ( \n captions = captions_text , question = random . choice ( QUESTIONS ) \n ) . strip ( ) \n completion = client . chat . completions . create ( \n model = \"<STR_LIT>\" , \n messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : prompt } ] , \n tools = OPENAI_TOOLS , \n tool_choice = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n ) \n resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) \n if \"<STR_LIT>\" not in resp : \n print ( resp ) \n q = resp [ \"<STR_LIT>\" ] \n a = resp [ \"<STR_LIT>\" ] \n if random . choice ( [ True , False ] ) : \n q = \"<STR_LIT>\" * len ( captions ) + \"<STR_LIT>\" + q \n else : \n q = q + \"<STR_LIT>\" + \"<STR_LIT>\" * len ( captions ) \n example = { \n \"<STR_LIT>\" : paths , \n \"<STR_LIT>\" : [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : q , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : a , \n } , \n ] , \n } \n return example \n def main ( args ) : \n data = load_dataset ( \"<STR_LIT>\" , split = \"<STR_LIT>\" ) \n data_idxs = list ( range ( len ( data ) ) ) \n os . makedirs ( args . cache_folder , exist_ok = True ) \n def gen ( seeds ) : \n r = random . Random ( seeds [ <NUM_LIT> ] + <NUM_LIT> ) \n cache = open ( \n os . path . join ( args . cache_folder , f\"<STR_LIT>\" ) , \"<STR_LIT>\" \n ) \n i = <NUM_LIT> \n while i < len ( seeds ) : \n selected_idxs = r . sample ( data_idxs , k = <NUM_LIT> ) [ <NUM_LIT> ] \n selected_example = data [ selected_idxs ] \n try : \n example = _build_convo ( selected_example ) \n cache . write ( json . dumps ( example ) + \"<STR_LIT>\" ) \n yield example \n i += <NUM_LIT> \n except Exception as e : \n print ( e ) \n continue \n cache . close ( ) \n ds = Dataset . from_generator ( \n gen , \n num_proc = args . num_proc , \n gen_kwargs = { \"<STR_LIT>\" : list ( range ( args . num_examples ) ) } , \n ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import json \n import hmac \n import hashlib \n import base64 \n import time \n import requests \n from urllib . parse import quote_plus \n from common . log import logger \n from config import conf \n from bridge . bridge import Bridge \n from flask import Flask , request , render_template , make_response \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n class DingTalkHandler ( Channel ) : \n def __init__ ( self ) : \n self . dingtalk_key = conf ( ) . get ( '<STR_LIT>' ) \n self . dingtalk_secret = conf ( ) . get ( '<STR_LIT>' ) \n self . dingtalk_token = conf ( ) . get ( '<STR_LIT>' ) \n self . dingtalk_post_token = conf ( ) . get ( '<STR_LIT>' ) \n self . access_token = None \n logger . info ( \"<STR_LIT>\" . format ( self . dingtalk_secret , self . dingtalk_token , self . dingtalk_post_token ) ) \n def notify_dingtalk_webhook ( self , data ) : \n timestamp = round ( time . time ( ) * <NUM_LIT> ) \n secret_enc = bytes ( self . dingtalk_secret , encoding = '<STR_LIT>' ) \n string_to_sign = '<STR_LIT>' . format ( timestamp , self . dingtalk_secret ) \n string_to_sign_enc = bytes ( string_to_sign , encoding = '<STR_LIT>' ) \n hmac_code = hmac . new ( secret_enc , string_to_sign_enc , \n digestmod = hashlib . sha256 ) . digest ( ) \n sign = quote_plus ( base64 . b64encode ( hmac_code ) ) \n notify_url = f\"<STR_LIT>\" \n try : \n logger . info ( \"<STR_LIT>\" . format ( str ( notify_url ) ) ) \n r = requests . post ( notify_url , json = data ) \n reply = r . json ( ) \n logger . info ( \"<STR_LIT>\" . format ( str ( reply ) ) ) \n except Exception as e : \n logger . error ( e ) \n def get_token_internal ( self ) : \n access_token_url = '<STR_LIT>' \n try : \n r = requests . post ( access_token_url , json = { \"<STR_LIT>\" : self . dingtalk_key , \"<STR_LIT>\" : self . dingtalk_secret } ) \n except : \n raise Exception ( \"<STR_LIT>\" ) \n data = json . loads ( r . content ) \n access_token = data [ '<STR_LIT>' ] \n expire_in = data [ '<STR_LIT>' ] \n self . access_token = access_token \n self . expire_at = int ( expire_in ) + time . time ( ) \n return self . access_token \n def get_token ( self ) : \n if self . access_token is None or self . expire_at <= time . time ( ) : \n self . get_token_internal ( ) \n return self . access_token \n def get_post_url ( self , data ) : \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n return f\"<STR_LIT>\" \n else : \n return f\"<STR_LIT>\" \n def build_response ( self , reply , data ) : \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n return self . build_oto_response ( reply , data ) \n else : \n return self . build_group_response ( reply , data ) \n def build_oto_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robotCode = data [ '<STR_LIT>' ] \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : reply \n } ) , \n \"<STR_LIT>\" : robotCode , \n \"<STR_LIT>\" : [ staffid ] \n } \n return resp \n def build_group_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robot_code = data [ '<STR_LIT>' ] \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : reply + \"<STR_LIT>\" + \"<STR_LIT>\" + nick \n } ) , \n \"<STR_LIT>\" : robot_code , \n \"<STR_LIT>\" : conversation_id , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n return resp \n def build_webhook_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robotCode = data [ '<STR_LIT>' ] \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : reply \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n return resp \n def chat ( self , channel , data ) :", "output": "reply = channel . handle ( data ) \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n reply_json = self . build_response ( reply , data ) \n self . notify_dingtalk ( data , reply_json ) \n else : \n reply_json = self . build_webhook_response ( reply , data ) \n self . notify_dingtalk_webhook ( reply_json ) \n def notify_dingtalk ( self , data , reply_json ) : \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : self . get_token ( ) \n } \n notify_url = self . get_post_url ( data ) \n try : \n r = requests . post ( notify_url , json = reply_json , headers = headers ) \n resp = r . json ( ) \n logger . info ( \"<STR_LIT>\" . format ( str ( resp ) ) ) \n except Exception as e : \n logger . error ( e ) \n class DingTalkChannel ( Channel ) : \n def __init__ ( self ) : \n self . host = conf ( ) . get ( '<STR_LIT>' ) \n self . port = conf ( ) . get ( '<STR_LIT>' ) \n logger . info ( \"<STR_LIT>\" ) \n def startup ( self ) : \n http_app . run ( host = self . host , port = self . port ) \n def handle ( self , data ) : \n reply = \"<STR_LIT>\" \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n if str ( prompt ) != <NUM_LIT> : \n conversation_id = data [ '<STR_LIT>' ] \n sender_id = data [ '<STR_LIT>' ] \n context = dict ( ) \n id = sender_id \n context [ '<STR_LIT>' ] = str ( id ) \n reply = self . build_reply_content ( prompt , context ) \n return reply \n def build_reply_content ( self , query , context = None ) : \n return Bridge ( ) . fetch_reply_content ( query , context ) \n dd = DingTalkChannel ( ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n http_app = Flask ( __name__ , ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def chat ( ) : \n handlers = DingTalkHandler ( ) \n logger . info ( \"<STR_LIT>\" . format ( str ( request . headers ) ) ) \n logger . info ( \"<STR_LIT>\" . format ( str ( request . data ) ) ) \n token = request . headers . get ( '<STR_LIT>' ) \n data = json . loads ( request . data ) \n if data : \n content = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if not content : \n return \n code = data [ '<STR_LIT>' ] \n group_name = None \n if '<STR_LIT>' in data : \n group_name = data [ '<STR_LIT>' ] \n handlers . chat ( dd , data ) \n return { '<STR_LIT>' : <NUM_LIT> } \n return { '<STR_LIT>' : <NUM_LIT> }"}, {"input": "from typing import List \n import random \n import argparse \n from datasets import load_dataset \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n from multi_token . modalities . document_gte import ( \n split_text_into_documents , \n ) \n TEMP_TOKEN = \"<STR_LIT>\" \n PRETRAIN_PHRASES = [ \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n f\"<STR_LIT>\" , \n ] \n def _write_convo ( row , max_document_chunks ) -> List : \n docs = split_text_into_documents ( row [ \"<STR_LIT>\" ] ) \n if len ( docs ) > max_document_chunks : \n raise ValueError ( \"<STR_LIT>\" ) \n example = { \n \"<STR_LIT>\" : str ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : docs , \n } \n phrase = random . choice ( PRETRAIN_PHRASES ) \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : phrase . replace ( TEMP_TOKEN , \"<STR_LIT>\" * len ( docs ) ) , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] , \n } , \n ] \n return example \n def main ( args ) : \n wiki_data = load_dataset ( \"<STR_LIT>\" , \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n idxs = list ( range ( len ( wiki_data ) ) ) \n random . shuffle ( idxs ) \n def gen ( ) : \n i = <NUM_LIT> \n for idx in idxs :", "output": "row = wiki_data [ idx ] \n try : \n yield _write_convo ( row , args . max_document_chunks ) \n except ValueError : \n pass \n else : \n i += <NUM_LIT> \n if i >= args . max_examples : \n break \n ds = Dataset . from_generator ( gen ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import pytest \n from instld . common_utils . convert_options import convert_options \n def test_convert_options ( ) : \n assert convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) == [ '<STR_LIT>' , '<STR_LIT>' ]", "output": "assert convert_options ( { '<STR_LIT>' : True } ) == [ '<STR_LIT>' ] \n assert convert_options ( { '<STR_LIT>' : False } ) == [ ] \n assert convert_options ( { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : True } ) == [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n def test_convert_options_wrong ( ) : \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : True } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : '<STR_LIT>' } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : True } ) \n with pytest . raises ( ValueError ) : \n convert_options ( { '<STR_LIT>' : <NUM_LIT> } )"}, {"input": "from legged_gym . envs import AnymalCRoughCfg , AnymalCRoughCfgPPO \n class AnymalCFlatCfg ( AnymalCRoughCfg ) : \n class env ( AnymalCRoughCfg . env ) : \n num_observations = <NUM_LIT> \n class terrain ( AnymalCRoughCfg . terrain ) : \n mesh_type = '<STR_LIT>' \n measure_heights = False \n class asset ( AnymalCRoughCfg . asset ) : \n self_collisions = <NUM_LIT> \n class rewards ( AnymalCRoughCfg . rewards ) : \n max_contact_force = <NUM_LIT> \n class scales ( AnymalCRoughCfg . rewards . scales ) : \n orientation = - <NUM_LIT> \n torques = - <NUM_LIT> \n feet_air_time = <NUM_LIT> \n class commands ( AnymalCRoughCfg . commands ) : \n heading_command = False \n resampling_time = <NUM_LIT>", "output": "class ranges ( AnymalCRoughCfg . commands . ranges ) : \n ang_vel_yaw = [ - <NUM_LIT> , <NUM_LIT> ] \n class domain_rand ( AnymalCRoughCfg . domain_rand ) : \n friction_range = [ <NUM_LIT> , <NUM_LIT> ] \n class AnymalCFlatCfgPPO ( AnymalCRoughCfgPPO ) : \n class policy ( AnymalCRoughCfgPPO . policy ) : \n actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n activation = '<STR_LIT>' \n class algorithm ( AnymalCRoughCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT> \n class runner ( AnymalCRoughCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>' \n load_run = - <NUM_LIT> \n max_iterations = <NUM_LIT>"}, {"input": "import os \n from claude_api import Client \n def get_cookie ( ) : \n cookie = os . getenv ( '<STR_LIT>' ) \n if not cookie : \n raise ValueError ( \"<STR_LIT>\" ) \n return cookie \n def main ( ) : \n cookie = get_cookie ( ) \n claude = Client ( cookie ) \n conversation_id = None \n print ( \"<STR_LIT>\" ) \n while True : \n user_input = input ( \"<STR_LIT>\" ) \n if user_input . lower ( ) == '<STR_LIT>' : \n print ( \"<STR_LIT>\" ) \n break \n if not conversation_id :", "output": "conversation = claude . create_new_chat ( ) \n conversation_id = conversation [ '<STR_LIT>' ] \n response = claude . send_message ( user_input , conversation_id ) \n print ( \"<STR_LIT>\" , response ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "import gradio as gr \n from core import run_model_information_script \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n def model_information_tab ( ) :", "output": "with gr . Column ( ) : \n model_name = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n interactive = True , \n ) \n model_information_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n interactive = False , \n ) \n model_information_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n model_information_button . click ( \n run_model_information_script , \n [ model_name ] , \n model_information_output_info , \n api_name = \"<STR_LIT>\" , \n )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None", "output": "def upgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) \n def downgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "import time \n import threading \n class Benchmark : \n def __init__ ( self , threads = <NUM_LIT> , rounds = <NUM_LIT> ) : \n self . threads = threads \n self . rounds = rounds \n def _work ( self , func , * args , ** kwargs ) : \n for i in range ( self . rounds ) : \n func ( * args , ** kwargs ) \n return \n def run ( self , func , * args , ** kwargs ) :", "output": "threads = [ ] \n for i in range ( self . threads ) : \n t = threading . Thread ( target = self . _work , args = ( func , * args ) , kwargs = kwargs ) \n threads . append ( t ) \n start = time . time ( ) \n for t in threads : \n t . start ( ) \n for t in threads : \n t . join ( ) \n end = time . time ( ) \n all_time = end - start \n avg_time = all_time / ( self . threads * self . rounds ) \n return all_time , avg_time"}, {"input": "import torch \n import torch . nn as nn \n import torch . optim as optim \n from rsl_rl . modules import ActorCriticRMA \n from rsl_rl . storage import RolloutStorage \n import wandb \n from rsl_rl . utils import unpad_trajectories \n class RMS ( object ) : \n def __init__ ( self , device , epsilon = <NUM_LIT> , shape = ( <NUM_LIT> , ) ) : \n self . M = torch . zeros ( shape , device = device ) \n self . S = torch . ones ( shape , device = device ) \n self . n = epsilon \n def __call__ ( self , x ) :", "output": "bs = x . size ( <NUM_LIT> ) \n delta = torch . mean ( x , dim = <NUM_LIT> ) - self . M \n new_M = self . M + delta * bs / ( self . n + bs ) \n new_S = ( self . S * self . n + torch . var ( x , dim = <NUM_LIT> ) * bs + ( delta ** <NUM_LIT> ) * self . n * bs / ( self . n + bs ) ) / ( self . n + bs ) \n self . M = new_M \n self . S = new_S \n self . n += bs \n return self . M , self . S \n class PPO : \n actor_critic : ActorCriticRMA \n def __init__ ( self , \n actor_critic , \n estimator , \n estimator_paras , \n depth_encoder , \n depth_encoder_paras , \n depth_actor , \n num_learning_epochs = <NUM_LIT> , \n num_mini_batches = <NUM_LIT> , \n clip_param = <NUM_LIT> , \n gamma = <NUM_LIT> , \n lam = <NUM_LIT> , \n value_loss_coef = <NUM_LIT> , \n entropy_coef = <NUM_LIT> , \n learning_rate = <NUM_LIT> , \n max_grad_norm = <NUM_LIT> , \n use_clipped_value_loss = True , \n schedule = \"<STR_LIT>\" , \n desired_kl = <NUM_LIT> , \n device = '<STR_LIT>' , \n dagger_update_freq = <NUM_LIT> , \n priv_reg_coef_schedual = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n ** kwargs \n ) : \n self . device = device \n self . desired_kl = desired_kl \n self . schedule = schedule \n self . learning_rate = learning_rate \n self . actor_critic = actor_critic \n self . actor_critic . to ( self . device ) \n self . storage = None \n self . optimizer = optim . Adam ( self . actor_critic . parameters ( ) , lr = learning_rate ) \n self . transition = RolloutStorage . Transition ( ) \n self . clip_param = clip_param \n self . num_learning_epochs = num_learning_epochs \n self . num_mini_batches = num_mini_batches \n self . value_loss_coef = value_loss_coef \n self . entropy_coef = entropy_coef \n self . gamma = gamma \n self . lam = lam \n self . max_grad_norm = max_grad_norm \n self . use_clipped_value_loss = use_clipped_value_loss \n self . hist_encoder_optimizer = optim . Adam ( self . actor_critic . actor . history_encoder . parameters ( ) , lr = learning_rate ) \n self . priv_reg_coef_schedual = priv_reg_coef_schedual \n self . counter = <NUM_LIT> \n self . estimator = estimator \n self . priv_states_dim = estimator_paras [ \"<STR_LIT>\" ] \n self . num_prop = estimator_paras [ \"<STR_LIT>\" ] \n self . num_scan = estimator_paras [ \"<STR_LIT>\" ] \n self . estimator_optimizer = optim . Adam ( self . estimator . parameters ( ) , lr = estimator_paras [ \"<STR_LIT>\" ] ) \n self . train_with_estimated_states = estimator_paras [ \"<STR_LIT>\" ] \n self . if_depth = depth_encoder != None \n if self . if_depth : \n self . depth_encoder = depth_encoder \n self . depth_encoder_optimizer = optim . Adam ( self . depth_encoder . parameters ( ) , lr = depth_encoder_paras [ \"<STR_LIT>\" ] ) \n self . depth_encoder_paras = depth_encoder_paras \n self . depth_actor = depth_actor \n self . depth_actor_optimizer = optim . Adam ( [ * self . depth_actor . parameters ( ) , * self . depth_encoder . parameters ( ) ] , lr = depth_encoder_paras [ \"<STR_LIT>\" ] ) \n def init_storage ( self , num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , action_shape ) : \n self . storage = RolloutStorage ( num_envs , num_transitions_per_env , actor_obs_shape , critic_obs_shape , action_shape , self . device ) \n def test_mode ( self ) : \n self . actor_critic . test ( ) \n def train_mode ( self ) : \n self . actor_critic . train ( ) \n def act ( self , obs , critic_obs , info , hist_encoding = False ) : \n if self . actor_critic . is_recurrent : \n self . transition . hidden_states = self . actor_critic . get_hidden_states ( ) \n if self . train_with_estimated_states : \n obs_est = obs . clone ( ) \n priv_states_estimated = self . estimator ( obs_est [ : , : self . num_prop ] ) \n obs_est [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . priv_states_dim ] = priv_states_estimated \n self . transition . actions = self . actor_critic . act ( obs_est , hist_encoding ) . detach ( ) \n else : \n self . transition . actions = self . actor_critic . act ( obs , hist_encoding ) . detach ( ) \n self . transition . values = self . actor_critic . evaluate ( critic_obs ) . detach ( ) \n self . transition . actions_log_prob = self . actor_critic . get_actions_log_prob ( self . transition . actions ) . detach ( ) \n self . transition . action_mean = self . actor_critic . action_mean . detach ( ) \n self . transition . action_sigma = self . actor_critic . action_std . detach ( ) \n self . transition . observations = obs \n self . transition . critic_observations = critic_obs \n return self . transition . actions \n def process_env_step ( self , rewards , dones , infos ) : \n rewards_total = rewards . clone ( ) \n self . transition . rewards = rewards_total . clone ( ) \n self . transition . dones = dones \n if '<STR_LIT>' in infos : \n self . transition . rewards += self . gamma * torch . squeeze ( self . transition . values * infos [ '<STR_LIT>' ] . unsqueeze ( <NUM_LIT> ) . to ( self . device ) , <NUM_LIT> ) \n self . storage . add_transitions ( self . transition ) \n self . transition . clear ( ) \n self . actor_critic . reset ( dones ) \n return rewards_total \n def compute_returns ( self , last_critic_obs ) : \n last_values = self . actor_critic . evaluate ( last_critic_obs ) . detach ( ) \n self . storage . compute_returns ( last_values , self . gamma , self . lam ) \n def update ( self ) : \n mean_value_loss = <NUM_LIT> \n mean_surrogate_loss = <NUM_LIT> \n mean_estimator_loss = <NUM_LIT> \n mean_discriminator_loss = <NUM_LIT> \n mean_discriminator_acc = <NUM_LIT> \n mean_priv_reg_loss = <NUM_LIT> \n if self . actor_critic . is_recurrent : \n generator = self . storage . reccurent_mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) \n else : \n generator = self . storage . mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) \n for obs_batch , critic_obs_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , hid_states_batch , masks_batch in generator : \n self . actor_critic . act ( obs_batch , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) \n actions_log_prob_batch = self . actor_critic . get_actions_log_prob ( actions_batch ) \n value_batch = self . actor_critic . evaluate ( critic_obs_batch , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) \n mu_batch = self . actor_critic . action_mean \n sigma_batch = self . actor_critic . action_std \n entropy_batch = self . actor_critic . entropy \n priv_latent_batch = self . actor_critic . actor . infer_priv_latent ( obs_batch ) \n with torch . inference_mode ( ) : \n hist_latent_batch = self . actor_critic . actor . infer_hist_latent ( obs_batch ) \n priv_reg_loss = ( priv_latent_batch - hist_latent_batch . detach ( ) ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n priv_reg_stage = min ( max ( ( self . counter - self . priv_reg_coef_schedual [ <NUM_LIT> ] ) , <NUM_LIT> ) / self . priv_reg_coef_schedual [ <NUM_LIT> ] , <NUM_LIT> ) \n priv_reg_coef = priv_reg_stage * ( self . priv_reg_coef_schedual [ <NUM_LIT> ] - self . priv_reg_coef_schedual [ <NUM_LIT> ] ) + self . priv_reg_coef_schedual [ <NUM_LIT> ] \n priv_states_predicted = self . estimator ( obs_batch [ : , : self . num_prop ] ) \n estimator_loss = ( priv_states_predicted - obs_batch [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . priv_states_dim ] ) . pow ( <NUM_LIT> ) . mean ( ) \n self . estimator_optimizer . zero_grad ( ) \n estimator_loss . backward ( ) \n nn . utils . clip_grad_norm_ ( self . estimator . parameters ( ) , self . max_grad_norm ) \n self . estimator_optimizer . step ( ) \n if self . desired_kl != None and self . schedule == '<STR_LIT>' : \n with torch . inference_mode ( ) : \n kl = torch . sum ( \n torch . log ( sigma_batch / old_sigma_batch + <NUM_LIT> ) + ( torch . square ( old_sigma_batch ) + torch . square ( old_mu_batch - mu_batch ) ) / ( <NUM_LIT> * torch . square ( sigma_batch ) ) - <NUM_LIT> , axis = - <NUM_LIT> ) \n kl_mean = torch . mean ( kl ) \n if kl_mean > self . desired_kl * <NUM_LIT> : \n self . learning_rate = max ( <NUM_LIT> , self . learning_rate / <NUM_LIT> ) \n elif kl_mean < self . desired_kl / <NUM_LIT> and kl_mean > <NUM_LIT> : \n self . learning_rate = min ( <NUM_LIT> , self . learning_rate * <NUM_LIT> ) \n for param_group in self . optimizer . param_groups : \n param_group [ '<STR_LIT>' ] = self . learning_rate \n ratio = torch . exp ( actions_log_prob_batch - torch . squeeze ( old_actions_log_prob_batch ) ) \n surrogate = - torch . squeeze ( advantages_batch ) * ratio \n surrogate_clipped = - torch . squeeze ( advantages_batch ) * torch . clamp ( ratio , <NUM_LIT> - self . clip_param , \n <NUM_LIT> + self . clip_param ) \n surrogate_loss = torch . max ( surrogate , surrogate_clipped ) . mean ( ) \n if self . use_clipped_value_loss : \n value_clipped = target_values_batch + ( value_batch - target_values_batch ) . clamp ( - self . clip_param , \n self . clip_param ) \n value_losses = ( value_batch - returns_batch ) . pow ( <NUM_LIT> ) \n value_losses_clipped = ( value_clipped - returns_batch ) . pow ( <NUM_LIT> ) \n value_loss = torch . max ( value_losses , value_losses_clipped ) . mean ( ) \n else : \n value_loss = ( returns_batch - value_batch ) . pow ( <NUM_LIT> ) . mean ( ) \n loss = surrogate_loss + self . value_loss_coef * value_loss - self . entropy_coef * entropy_batch . mean ( ) + priv_reg_coef * priv_reg_loss \n self . optimizer . zero_grad ( ) \n loss . backward ( ) \n nn . utils . clip_grad_norm_ ( self . actor_critic . parameters ( ) , self . max_grad_norm ) \n self . optimizer . step ( ) \n mean_value_loss += value_loss . item ( ) \n mean_surrogate_loss += surrogate_loss . item ( ) \n mean_estimator_loss += estimator_loss . item ( ) \n mean_priv_reg_loss += priv_reg_loss . item ( ) \n mean_discriminator_loss += <NUM_LIT> \n mean_discriminator_acc += <NUM_LIT> \n num_updates = self . num_learning_epochs * self . num_mini_batches \n mean_value_loss /= num_updates \n mean_surrogate_loss /= num_updates \n mean_estimator_loss /= num_updates \n mean_priv_reg_loss /= num_updates \n mean_discriminator_loss /= num_updates \n mean_discriminator_acc /= num_updates \n self . storage . clear ( ) \n self . update_counter ( ) \n return mean_value_loss , mean_surrogate_loss , mean_estimator_loss , mean_discriminator_loss , mean_discriminator_acc , mean_priv_reg_loss , priv_reg_coef \n def update_dagger ( self ) : \n mean_hist_latent_loss = <NUM_LIT> \n if self . actor_critic . is_recurrent : \n generator = self . storage . reccurent_mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) \n else : \n generator = self . storage . mini_batch_generator ( self . num_mini_batches , self . num_learning_epochs ) \n for obs_batch , critic_obs_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , hid_states_batch , masks_batch in generator : \n with torch . inference_mode ( ) : \n self . actor_critic . act ( obs_batch , hist_encoding = True , masks = masks_batch , hidden_states = hid_states_batch [ <NUM_LIT> ] ) \n with torch . inference_mode ( ) : \n priv_latent_batch = self . actor_critic . actor . infer_priv_latent ( obs_batch ) \n hist_latent_batch = self . actor_critic . actor . infer_hist_latent ( obs_batch ) \n hist_latent_loss = ( priv_latent_batch . detach ( ) - hist_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n self . hist_encoder_optimizer . zero_grad ( ) \n hist_latent_loss . backward ( ) \n nn . utils . clip_grad_norm_ ( self . actor_critic . actor . history_encoder . parameters ( ) , self . max_grad_norm ) \n self . hist_encoder_optimizer . step ( ) \n mean_hist_latent_loss += hist_latent_loss . item ( ) \n num_updates = self . num_learning_epochs * self . num_mini_batches \n mean_hist_latent_loss /= num_updates \n self . storage . clear ( ) \n self . update_counter ( ) \n return mean_hist_latent_loss \n def update_depth_encoder ( self , depth_latent_batch , scandots_latent_batch ) : \n if self . if_depth : \n depth_encoder_loss = ( scandots_latent_batch . detach ( ) - depth_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n self . depth_encoder_optimizer . zero_grad ( ) \n depth_encoder_loss . backward ( ) \n nn . utils . clip_grad_norm_ ( self . depth_encoder . parameters ( ) , self . max_grad_norm ) \n self . depth_encoder_optimizer . step ( ) \n return depth_encoder_loss . item ( ) \n def update_depth_actor ( self , actions_student_batch , actions_teacher_batch , yaw_student_batch , yaw_teacher_batch ) : \n if self . if_depth : \n depth_actor_loss = ( actions_teacher_batch . detach ( ) - actions_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n yaw_loss = ( yaw_teacher_batch . detach ( ) - yaw_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n loss = depth_actor_loss + yaw_loss \n self . depth_actor_optimizer . zero_grad ( ) \n loss . backward ( ) \n nn . utils . clip_grad_norm_ ( self . depth_actor . parameters ( ) , self . max_grad_norm ) \n self . depth_actor_optimizer . step ( ) \n return depth_actor_loss . item ( ) , yaw_loss . item ( ) \n def update_depth_both ( self , depth_latent_batch , scandots_latent_batch , actions_student_batch , actions_teacher_batch ) : \n if self . if_depth : \n depth_encoder_loss = ( scandots_latent_batch . detach ( ) - depth_latent_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n depth_actor_loss = ( actions_teacher_batch . detach ( ) - actions_student_batch ) . norm ( p = <NUM_LIT> , dim = <NUM_LIT> ) . mean ( ) \n depth_loss = depth_encoder_loss + depth_actor_loss \n self . depth_actor_optimizer . zero_grad ( ) \n depth_loss . backward ( ) \n nn . utils . clip_grad_norm_ ( [ * self . depth_actor . parameters ( ) , * self . depth_encoder . parameters ( ) ] , self . max_grad_norm ) \n self . depth_actor_optimizer . step ( ) \n return depth_encoder_loss . item ( ) , depth_actor_loss . item ( ) \n def update_counter ( self ) : \n self . counter += <NUM_LIT> \n def compute_apt_reward ( self , source , target ) : \n b1 , b2 = source . size ( <NUM_LIT> ) , target . size ( <NUM_LIT> ) \n sim_matrix = torch . norm ( source [ : , None , : ] . view ( b1 , <NUM_LIT> , - <NUM_LIT> ) - target [ None , : , : ] . view ( <NUM_LIT> , b2 , - <NUM_LIT> ) , dim = - <NUM_LIT> , p = <NUM_LIT> ) \n reward , _ = sim_matrix . topk ( self . knn_k , dim = <NUM_LIT> , largest = False , sorted = True ) \n if not self . knn_avg : \n reward = reward [ : , - <NUM_LIT> ] \n reward = reward . reshape ( - <NUM_LIT> , <NUM_LIT> ) \n if self . rms : \n moving_mean , moving_std = self . disc_state_rms ( reward ) \n reward = reward / moving_std \n reward = torch . clamp ( reward - self . knn_clip , <NUM_LIT> ) \n else : \n reward = reward . reshape ( - <NUM_LIT> , <NUM_LIT> ) \n if self . rms : \n moving_mean , moving_std = self . disc_state_rms ( reward ) \n reward = reward / moving_std \n reward = torch . clamp ( reward - self . knn_clip , <NUM_LIT> ) \n reward = reward . reshape ( ( b1 , self . knn_k ) ) \n reward = reward . mean ( dim = <NUM_LIT> ) \n reward = torch . log ( reward + <NUM_LIT> ) \n return reward"}, {"input": "from typing import Optional \n from starlette . types import ASGIApp , Scope , Receive , Send \n from profyle . application . profyle import profyle \n from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository", "output": "class ProfyleMiddleware : \n def __init__ ( \n self , \n app : ASGIApp , \n enabled : bool = True , \n pattern : Optional [ str ] = None , \n max_stack_depth : int = - <NUM_LIT> , \n min_duration : int = <NUM_LIT> , \n trace_repo : SQLiteTraceRepository = SQLiteTraceRepository ( ) \n ) : \n self . app = app \n self . enabled = enabled \n self . pattern = pattern \n self . max_stack_depth = max_stack_depth \n self . min_duration = min_duration \n self . trace_repo = trace_repo \n async def __call__ ( self , scope : Scope , receive : Receive , send : Send ) -> None : \n if self . enabled and scope [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n method = scope . get ( '<STR_LIT>' , '<STR_LIT>' ) . upper ( ) \n path = scope . get ( '<STR_LIT>' , b'<STR_LIT>' ) . decode ( '<STR_LIT>' ) \n with profyle ( \n name = f\"<STR_LIT>\" , \n pattern = self . pattern , \n repo = self . trace_repo , \n max_stack_depth = self . max_stack_depth , \n min_duration = self . min_duration \n ) : \n await self . app ( scope , receive , send ) \n return \n await self . app ( scope , receive , send )"}, {"input": "from . import * \n from flask import request , redirect , jsonify , render_template_string , make_response \n from mod . auth import webui , cookie \n from mod . auth . authentication import require_auth \n from mod . args import GlobalArgs \n args = GlobalArgs ( ) \n @ app . route ( '<STR_LIT>' ) \n def login_check ( ) : \n if require_auth ( request = request ) < <NUM_LIT> and args . auth : \n return render_template_string ( webui . html_login ( ) ) \n return redirect ( '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def login_api ( ) : \n data = request . get_json ( )", "output": "if '<STR_LIT>' in data : \n pwd = data [ '<STR_LIT>' ] \n if args . valid ( pwd ) : \n logger . info ( \"<STR_LIT>\" ) \n response = make_response ( jsonify ( success = True ) ) \n response . set_cookie ( '<STR_LIT>' , cookie . set_cookie ( pwd ) ) \n return response \n return jsonify ( success = False )"}, {"input": "class Spiderman : \n def __init__ ( self ) : \n print ( \"<STR_LIT>\" ) \n def lanzar_telara\u00f1a ( self ) : \n print ( \"<STR_LIT>\" ) \n def activar_sentido_aracnido ( self , peligro ) : \n if peligro : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n class MilesMorales ( Spiderman ) : \n def __init__ ( self ) : \n print ( \"<STR_LIT>\" ) \n def lanzar_telara\u00f1a ( self ) : \n print ( \"<STR_LIT>\" ) \n return super ( ) . lanzar_telara\u00f1a ( ) \n def lanzar_rayitos ( self ) : \n print ( \"<STR_LIT>\" ) \n class SpiderGwen ( Spiderman ) : \n def __init__ ( self ) : \n print ( \"<STR_LIT>\" ) \n def lanzar_telara\u00f1a_rosa ( self ) : \n print ( \"<STR_LIT>\" ) \n return super ( ) . lanzar_telara\u00f1a ( ) \n class Spidercito ( MilesMorales , SpiderGwen ) : \n def __init__ ( self ) : \n print ( \"<STR_LIT>\" ) \n super ( ) . lanzar_rayitos ( )", "output": "miles = MilesMorales ( ) \n miles . lanzar_telara\u00f1a ( ) \n gwen = SpiderGwen ( ) \n gwen . lanzar_telara\u00f1a_rosa ( ) \n spidercito = Spidercito ( ) \n spidercito . lanzar_telara\u00f1a_rosa ( ) \n spidercito . activar_sentido_aracnido ( peligro = True ) \n print ( isinstance ( spidercito , Spiderman ) )"}, {"input": "import os \n import sys \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n import requests \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from assets . flask . server import start_flask , load_config_flask , save_config \n i18n = I18nAuto ( ) \n def flask_server_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n flask_checkbox = gr . Checkbox ( \n label = i18n ( \n \"<STR_LIT>\" \n ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n value = load_config_flask ( ) , \n ) \n flask_checkbox . change ( \n fn = toggle , \n inputs = [ flask_checkbox ] , \n outputs = [ ] ,", "output": ") \n def toggle ( checkbox ) : \n save_config ( bool ( checkbox ) ) \n if load_config_flask ( ) == True : \n start_flask ( ) \n else : \n try : \n requests . post ( \"<STR_LIT>\" ) \n except requests . exceptions . ConnectionError : \n pass"}, {"input": "import hashlib \n import re \n def calculate_md5 ( string : str , base = \"<STR_LIT>\" ) :", "output": "md5_hash = hashlib . md5 ( ) \n md5_hash . update ( string . encode ( '<STR_LIT>' ) ) \n if base == \"<STR_LIT>\" : \n md5_hex = md5_hash . hexdigest ( ) \n return md5_hex \n elif base == \"<STR_LIT>\" : \n md5_dec = int ( md5_hash . hexdigest ( ) , <NUM_LIT> ) \n return md5_dec \n elif base == \"<STR_LIT>\" : \n md5_bin = format ( int ( md5_hash . hexdigest ( ) , <NUM_LIT> ) , '<STR_LIT>' ) \n return md5_bin \n elif base == \"<STR_LIT>\" : \n md5_bytes = md5_hash . digest ( ) \n return md5_bytes . hex ( ) \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n def merge_dictionaries ( dict_a : dict , dict_b : dict ) -> dict : \n merged_dict = { } \n if type ( dict_a ) is not dict : \n return dict_b \n for key in set ( dict_a . keys ( ) ) | set ( dict_b . keys ( ) ) : \n value_a = dict_a . get ( key ) \n value_b = dict_b . get ( key ) \n if value_a and value_b : \n merged_dict [ key ] = value_a \n elif not value_a : \n merged_dict [ key ] = value_b \n elif not value_b : \n merged_dict [ key ] = value_a \n else : \n merged_dict [ key ] = value_a \n return merged_dict \n def standard_lrc ( lrc_text : str ) -> str : \n if not lrc_text or type ( lrc_text ) is not str : \n return lrc_text \n elif '<STR_LIT>' in lrc_text and '<STR_LIT>' in lrc_text : \n lrc_text = lrc_text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n pattern = re . compile ( r'<STR_LIT>' ) \n matches = pattern . findall ( lrc_text ) \n for match_s in matches : \n replacement = '<STR_LIT>' + '<STR_LIT>' . join ( match_s . split ( '<STR_LIT>' ) ) + '<STR_LIT>' \n lrc_text = lrc_text . replace ( f'<STR_LIT>' , replacement ) \n pattern = r\"<STR_LIT>\" \n return re . sub ( pattern , lambda match : \"<STR_LIT>\" + match . group ( <NUM_LIT> ) + \"<STR_LIT>\" , lrc_text ) \n else : \n return lrc_text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" )"}, {"input": "from long_captions . dense_image import DenseCaptionedImage , get_key_for \n from long_captions . prepare . remote_language_model import RemoteLanguageModel \n from long_captions . utils import get_clip_token_length , truncate_long_captions \n from long_captions . config import OUTPUT_SUMMARY_PATH \n import os \n import json \n import threading \n from queue import Queue \n from typing import Dict , Any , Optional , Tuple \n IDX_TARGET = <NUM_LIT> \n DEBUG = False \n SKIP_EXISTING = False \n NUM_THREADS = <NUM_LIT>", "output": "def gen_summaries_for ( idx , rlm , max_fails = <NUM_LIT> , existing = None ) -> Tuple [ Optional [ Dict [ str , Any ] ] , bool , bool ] : \n failures = <NUM_LIT> \n dci = DenseCaptionedImage ( idx ) \n if existing is not None : \n summaries = existing \n else : \n summaries = { \n '<STR_LIT>' : None , \n } \n created_any = False \n while summaries [ '<STR_LIT>' ] is None or get_clip_token_length ( summaries [ '<STR_LIT>' ] ) > <NUM_LIT> : \n try : \n base_caption = dci . get_formatted_complete_description ( ) [ <NUM_LIT> ] [ '<STR_LIT>' ] \n base_caption = truncate_long_captions ( base_caption ) \n summary_caption = rlm . get_summary ( base_caption ) \n assert get_clip_token_length ( summary_caption ) < <NUM_LIT> , \"<STR_LIT>\" \n summary_caption_reduced = rlm . reduce_length ( summary_caption ) \n summaries [ '<STR_LIT>' ] = summary_caption_reduced \n created_any = True \n except AssertionError as e : \n if DEBUG : \n import traceback \n traceback . print_exc ( ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n failures += <NUM_LIT> \n if failures >= max_fails : \n return summaries , created_any , False \n all_masks = dci . filter_masks_by_size ( ) \n for m in all_masks : \n entry = dci . get_caption_with_subcaptions ( m ) [ <NUM_LIT> ] \n key = entry [ '<STR_LIT>' ] \n caption = entry [ '<STR_LIT>' ] \n if key in summaries and get_clip_token_length ( summaries [ key ] ) <= <NUM_LIT> : \n continue \n mask_sum_caption = None \n short = get_clip_token_length ( caption ) <= <NUM_LIT> \n while mask_sum_caption is None : \n try : \n caption = truncate_long_captions ( caption ) \n mask_sum_caption_long = rlm . get_summary ( caption , short = short ) \n assert get_clip_token_length ( mask_sum_caption_long ) < <NUM_LIT> , \"<STR_LIT>\" \n mask_sum_caption = rlm . reduce_length ( mask_sum_caption_long ) \n summaries [ key ] = mask_sum_caption \n created_any = True \n except AssertionError as e : \n if DEBUG : \n import traceback \n traceback . print_exc ( ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n failures += <NUM_LIT> \n if failures >= max_fails : \n return summaries , created_any , False \n if DEBUG : \n print ( summaries ) \n return summaries , created_any , True \n def thread_entry ( idx_pool : Queue , rlm ) : \n while not idx_pool . empty ( ) : \n i , existing = idx_pool . get ( ) \n key = get_key_for ( i ) \n if int ( i ) % <NUM_LIT> == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n target_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) \n summaries , created_any , success = gen_summaries_for ( i , rlm , existing = existing ) \n if created_any is False : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n else : \n if success is False : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n with open ( target_path , '<STR_LIT>' ) as jsonf : \n json . dump ( summaries , jsonf ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n def run_gen_summary ( ) : \n rlm = RemoteLanguageModel ( '<STR_LIT>' ) \n idx_pool = Queue ( ) \n for i in range ( IDX_TARGET ) : \n key = get_key_for ( i ) \n target_path = os . path . join ( OUTPUT_SUMMARY_PATH , key ) \n existing = None \n if os . path . exists ( target_path ) : \n if SKIP_EXISTING : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n else : \n with open ( target_path ) as jsonf : \n existing = json . load ( jsonf ) \n idx_pool . put ( ( i , existing ) ) \n thread_pool = [ ] \n for _ in range ( NUM_THREADS ) : \n t = threading . Thread ( target = thread_entry , args = ( idx_pool , rlm ) ) \n t . start ( ) \n thread_pool . append ( t ) \n for thread in thread_pool : \n thread . join ( ) \n if __name__ == '<STR_LIT>' : \n run_gen_summary ( )"}, {"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor \n import pyworld \n import numpy as np \n class DioF0Predictor ( F0Predictor ) : \n def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : \n self . hop_length = hop_length \n self . f0_min = f0_min", "output": "self . f0_max = f0_max \n self . sampling_rate = sampling_rate \n def interpolate_f0 ( self , f0 ) : \n data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) \n vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) \n vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> \n vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> \n ip_data = data \n frame_number = data . size \n last_value = <NUM_LIT> \n for i in range ( frame_number ) : \n if data [ i ] <= <NUM_LIT> : \n j = i + <NUM_LIT> \n for j in range ( i + <NUM_LIT> , frame_number ) : \n if data [ j ] > <NUM_LIT> : \n break \n if j < frame_number - <NUM_LIT> : \n if last_value > <NUM_LIT> : \n step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) \n for k in range ( i , j ) : \n ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) \n else : \n for k in range ( i , j ) : \n ip_data [ k ] = data [ j ] \n else : \n for k in range ( i , frame_number ) : \n ip_data [ k ] = last_value \n else : \n ip_data [ i ] = data [ i ] \n last_value = data [ i ] \n return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] \n def resize_f0 ( self , x , target_len ) : \n source = np . array ( x ) \n source [ source < <NUM_LIT> ] = np . nan \n target = np . interp ( \n np . arange ( <NUM_LIT> , len ( source ) * target_len , len ( source ) ) / target_len , \n np . arange ( <NUM_LIT> , len ( source ) ) , \n source , \n ) \n res = np . nan_to_num ( target ) \n return res \n def compute_f0 ( self , wav , p_len = None ) : \n if p_len is None : \n p_len = wav . shape [ <NUM_LIT> ] // self . hop_length \n f0 , t = pyworld . dio ( \n wav . astype ( np . double ) , \n fs = self . sampling_rate , \n f0_floor = self . f0_min , \n f0_ceil = self . f0_max , \n frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , \n ) \n f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) \n for index , pitch in enumerate ( f0 ) : \n f0 [ index ] = round ( pitch , <NUM_LIT> ) \n return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) [ <NUM_LIT> ] \n def compute_f0_uv ( self , wav , p_len = None ) : \n if p_len is None : \n p_len = wav . shape [ <NUM_LIT> ] // self . hop_length \n f0 , t = pyworld . dio ( \n wav . astype ( np . double ) , \n fs = self . sampling_rate , \n f0_floor = self . f0_min , \n f0_ceil = self . f0_max , \n frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , \n ) \n f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) \n for index , pitch in enumerate ( f0 ) : \n f0 [ index ] = round ( pitch , <NUM_LIT> ) \n return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) )"}, {"input": "import os \n import os \n from urllib . request import urlretrieve \n from tqdm import tqdm \n import json as js \n def image_url_download ( url_file , to_folder ) : \n count = <NUM_LIT> \n if not os . path . exists ( to_folder ) : \n os . mkdir ( to_folder )", "output": "contents = js . load ( open ( url_file , '<STR_LIT>' ) ) \n for img in tqdm ( contents ) : \n if not os . path . exists ( os . path . join ( to_folder , img ) ) : \n try : \n urlretrieve ( contents [ img ] , os . path . join ( to_folder , img ) ) \n except : \n count += <NUM_LIT> \n print ( count ) \n if __name__ == '<STR_LIT>' : \n import sys \n if len ( sys . argv ) != <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n image_url_download ( sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] )"}, {"input": "import os \n import torch \n def change_info ( path , info , name ) :", "output": "try : \n ckpt = torch . load ( path , map_location = \"<STR_LIT>\" ) \n ckpt [ \"<STR_LIT>\" ] = info \n if name == \"<STR_LIT>\" : \n name = os . path . basename ( path ) \n torch . save ( ckpt , f\"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n except Exception as error : \n print ( error )"}, {"input": "import os \n import importlib . util \n from plugins . event import EventAction , EventContext , Event \n from plugins . plugin_registry import PluginRegistry \n from common import functions , log", "output": "@ functions . singleton \n class PluginManager : \n def __init__ ( self , plugins_dir = \"<STR_LIT>\" ) : \n self . plugins_dir = plugins_dir \n self . plugin_registry = PluginRegistry ( ) \n self . load_plugins ( ) \n def load_plugins ( self ) : \n for plugin_name in self . find_plugin_names ( ) : \n if os . path . exists ( f\"<STR_LIT>\" ) : \n try : \n plugin_module = self . load_plugin_module ( plugin_name ) \n self . plugin_registry . register_from_module ( plugin_module ) \n except Exception as e : \n log . warn ( \"<STR_LIT>\" % ( plugin_name ) ) \n def find_plugin_names ( self ) : \n plugin_names = [ ] \n for entry in os . scandir ( self . plugins_dir ) : \n if entry . is_dir ( ) : \n plugin_names . append ( entry . name ) \n return plugin_names \n def load_plugin_module ( self , plugin_name ) : \n spec = importlib . util . spec_from_file_location ( \n plugin_name , os . path . join ( self . plugins_dir , plugin_name , f\"<STR_LIT>\" ) \n ) \n module = importlib . util . module_from_spec ( spec ) \n spec . loader . exec_module ( module ) \n return module \n def emit_event ( self , e_context : EventContext , * args , ** kwargs ) : \n for plugin in self . plugin_registry . list_plugins ( ) : \n if plugin . enabled and e_context . action == EventAction . CONTINUE : \n if ( e_context . event in plugin . handlers ) : \n plugin . handlers [ e_context . event ] ( e_context , * args , ** kwargs ) \n return e_context"}, {"input": "from flask import Flask , request , render_template , render_template_string , send_from_directory \n import re \n import os \n import logging \n app = Flask ( __name__ ) \n app . logger . disabled = True \n log = logging . getLogger ( '<STR_LIT>' ) \n log . disabled = True \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def index ( ) : \n return str ( '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def secr3t ( ) : \n name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) \n template = \n bl = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for i in bl : \n if i in name : \n return str ( '<STR_LIT>' ) \n two_bracket_pattern = r\"<STR_LIT>\" \n two_bracket_match = re . search ( two_bracket_pattern , name ) \n bracket_comma_bracket_pattern = r\"<STR_LIT>\" \n bracket_comma_bracket_match = re . search ( bracket_comma_bracket_pattern , name ) \n bracket_bracket_line_pattern = r\"<STR_LIT>\" \n bracket_bracket_line_match = re . search ( bracket_bracket_line_pattern , name ) \n comma_bracket_bracket_line_pattern = r\"<STR_LIT>\" \n comma_bracket_bracket_line_match = re . search ( comma_bracket_bracket_line_pattern , name ) \n pattern_mo = r\"<STR_LIT>\" \n matche_mo = re . search ( pattern_mo , name ) \n if two_bracket_match : \n if bracket_comma_bracket_match . group ( <NUM_LIT> ) : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n elif comma_bracket_bracket_line_match : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n elif bracket_bracket_line_match : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n else : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n if matche_mo : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n a = render_template_string ( template % name ) \n print ( \"<STR_LIT>\" + name )", "output": "if \"<STR_LIT>\" in a : \n return a + str ( '<STR_LIT>' ) \n return a \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def robots ( ) : \n return send_from_directory ( os . path . join ( app . root_path , '<STR_LIT>' ) , \n '<STR_LIT>' , mimetype = '<STR_LIT>' ) \n if __name__ == '<STR_LIT>' : \n app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = False )"}, {"input": "from time import time \n import numpy as np \n import os \n from isaacgym . torch_utils import * \n from isaacgym import gymtorch , gymapi , gymutil \n import torch \n from typing import Tuple , Dict \n from legged_gym . envs import LeggedRobot", "output": "class Cassie ( LeggedRobot ) : \n def _reward_no_fly ( self ) : \n contacts = self . contact_forces [ : , self . feet_indices , <NUM_LIT> ] > <NUM_LIT> \n single_contact = torch . sum ( <NUM_LIT> * contacts , dim = <NUM_LIT> ) == <NUM_LIT> \n return <NUM_LIT> * single_contact"}, {"input": "import time \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n from common . log import logger \n from config import conf \n from wechatpy . enterprise . crypto import WeChatCrypto \n from wechatpy . enterprise import WeChatClient \n from wechatpy . exceptions import InvalidSignatureException \n from wechatpy . enterprise . exceptions import InvalidCorpIdException \n from wechatpy . enterprise import parse_message \n from flask import Flask , request , abort \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n app = Flask ( __name__ ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def handler_msg ( ) : \n return WechatEnterpriseChannel ( ) . handle ( ) \n _conf = conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n class WechatEnterpriseChannel ( Channel ) :", "output": "def __init__ ( self ) : \n self . CorpId = _conf . get ( '<STR_LIT>' ) \n self . Secret = _conf . get ( '<STR_LIT>' ) \n self . AppId = _conf . get ( '<STR_LIT>' ) \n self . TOKEN = _conf . get ( '<STR_LIT>' ) \n self . EncodingAESKey = _conf . get ( '<STR_LIT>' ) \n self . crypto = WeChatCrypto ( self . TOKEN , self . EncodingAESKey , self . CorpId ) \n self . client = WeChatClient ( self . CorpId , self . Secret , self . AppId ) \n def startup ( self ) : \n app . run ( host = '<STR_LIT>' , port = _conf . get ( '<STR_LIT>' ) ) \n def send ( self , msg , receiver ) : \n n = <NUM_LIT> \n if len ( msg ) < n : \n logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) \n self . client . message . send_text ( self . AppId , receiver , msg ) \n return \n chunks = [ msg [ i : i + n ] for i in range ( <NUM_LIT> , len ( msg ) , n ) ] \n total = len ( chunks ) \n for i , chunk in enumerate ( chunks ) : \n logger . info ( '<STR_LIT>' . format ( msg , chunk , i + <NUM_LIT> , total ) ) \n self . client . message . send_text ( self . AppId , receiver , chunk ) \n time . sleep ( <NUM_LIT> ) \n def _do_send ( self , query , reply_user_id ) : \n try : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = reply_user_id \n reply_text = super ( ) . build_reply_content ( query , context ) \n if reply_text : \n self . send ( reply_text , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def handle ( self ) : \n query_params = request . args \n signature = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n timestamp = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n nonce = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n if request . method == '<STR_LIT>' : \n echostr = query_params . get ( '<STR_LIT>' , '<STR_LIT>' ) \n try : \n echostr = self . crypto . check_signature ( signature , timestamp , nonce , echostr ) \n except InvalidSignatureException : \n abort ( <NUM_LIT> ) \n print ( echostr ) \n return echostr \n elif request . method == '<STR_LIT>' : \n try : \n message = self . crypto . decrypt_message ( \n request . data , \n signature , \n timestamp , \n nonce \n ) \n except ( InvalidSignatureException , InvalidCorpIdException ) : \n abort ( <NUM_LIT> ) \n msg = parse_message ( message ) \n if msg . type == '<STR_LIT>' : \n thread_pool . submit ( self . _do_send , msg . content , msg . source ) \n else : \n reply = '<STR_LIT>' \n return '<STR_LIT>'"}, {"input": "import os , sys \n import gradio as gr \n import regex as re \n import json \n import random \n from core import ( \n run_tts_script , \n ) \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n model_root = os . path . join ( now_dir , \"<STR_LIT>\" ) \n model_root_relative = os . path . relpath ( model_root , now_dir ) \n names = [ \n os . path . join ( root , file ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for file in files \n if ( \n file . endswith ( ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n and not ( file . startswith ( \"<STR_LIT>\" ) or file . startswith ( \"<STR_LIT>\" ) ) \n ) \n ] \n indexes_list = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in name \n ] \n def change_choices ( ) : \n names = [ \n os . path . join ( root , file ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for file in files \n if ( \n file . endswith ( ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n and not ( file . startswith ( \"<STR_LIT>\" ) or file . startswith ( \"<STR_LIT>\" ) ) \n ) \n ] \n indexes_list = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( model_root_relative , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in name \n ] \n return ( \n { \"<STR_LIT>\" : sorted ( names ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( indexes_list ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n def get_indexes ( ) : \n indexes_list = [ \n os . path . join ( dirpath , filename ) \n for dirpath , _ , filenames in os . walk ( model_root_relative ) \n for filename in filenames \n if filename . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in filename \n ]", "output": "return indexes_list if indexes_list else \"<STR_LIT>\" \n def process_input ( file_path ) : \n with open ( file_path , \"<STR_LIT>\" ) as file : \n file_contents = file . read ( ) \n gr . Info ( f\"<STR_LIT>\" ) \n return file_contents , None \n def match_index ( model_file_value ) : \n if model_file_value : \n model_folder = os . path . dirname ( model_file_value ) \n index_files = get_indexes ( ) \n for index_file in index_files : \n if os . path . dirname ( index_file ) == model_folder : \n return index_file \n return \"<STR_LIT>\" \n def tts_tab ( ) : \n default_weight = random . choice ( names ) if names else \"<STR_LIT>\" \n with gr . Row ( ) : \n with gr . Row ( ) : \n model_file = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = sorted ( names , key = lambda path : os . path . getsize ( path ) ) , \n interactive = True , \n value = default_weight , \n allow_custom_value = True , \n ) \n best_default_index_path = match_index ( model_file . value ) \n index_file = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_indexes ( ) , \n value = best_default_index_path , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Column ( ) : \n refresh_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n unload_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n unload_button . click ( \n fn = lambda : ( \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) , \n inputs = [ ] , \n outputs = [ model_file , index_file ] , \n ) \n model_file . select ( \n fn = lambda model_file_value : match_index ( model_file_value ) , \n inputs = [ model_file ] , \n outputs = [ index_file ] , \n ) \n json_path = os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n with open ( json_path , \"<STR_LIT>\" ) as file : \n tts_voices_data = json . load ( file ) \n short_names = [ voice . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) for voice in tts_voices_data ] \n tts_voice = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = short_names , \n interactive = True , \n value = None , \n ) \n tts_text = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n lines = <NUM_LIT> , \n ) \n txt_file = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , \n type = \"<STR_LIT>\" , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) , open = False ) : \n with gr . Column ( ) : \n output_tts_path = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n value = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n interactive = True , \n ) \n output_rvc_path = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n value = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n interactive = True , \n ) \n export_format = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n split_audio = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n autotune = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = False , \n interactive = True , \n ) \n clean_audio = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = True , \n interactive = True , \n ) \n clean_strength = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n value = <NUM_LIT> , \n interactive = True , \n ) \n pitch = gr . Slider ( \n minimum = - <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n filter_radius = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n step = <NUM_LIT> , \n interactive = True , \n ) \n index_rate = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n rms_mix_rate = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n protect = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n hop_length = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = <NUM_LIT> , \n interactive = True , \n ) \n with gr . Column ( ) : \n f0method = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n convert_button1 = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n with gr . Row ( ) : \n vc_output1 = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n ) \n vc_output2 = gr . Audio ( label = i18n ( \"<STR_LIT>\" ) ) \n def toggle_visible ( checkbox ) : \n return { \"<STR_LIT>\" : checkbox , \"<STR_LIT>\" : \"<STR_LIT>\" } \n clean_audio . change ( \n fn = toggle_visible , \n inputs = [ clean_audio ] , \n outputs = [ clean_strength ] , \n ) \n refresh_button . click ( \n fn = change_choices , \n inputs = [ ] , \n outputs = [ model_file , index_file ] , \n ) \n txt_file . upload ( \n fn = process_input , \n inputs = [ txt_file ] , \n outputs = [ tts_text , txt_file ] , \n ) \n convert_button1 . click ( \n fn = run_tts_script , \n inputs = [ \n tts_text , \n tts_voice , \n pitch , \n filter_radius , \n index_rate , \n rms_mix_rate , \n protect , \n hop_length , \n f0method , \n output_tts_path , \n output_rvc_path , \n model_file , \n index_file , \n split_audio , \n autotune , \n clean_audio , \n clean_strength , \n export_format , \n ] , \n outputs = [ vc_output1 , vc_output2 ] , \n )"}, {"input": "SQLALCHEMY_DATABASE_URI = \"<STR_LIT>\" \n ENTRY_PAGE_SIZE = <NUM_LIT> \n SYNC_FEEDS_CRON_MINUTES = '<STR_LIT>' \n DELETE_OLD_CRON_HOURS = '<STR_LIT>' \n SKIP_RECENTLY_UPDATED_MINUTES = <NUM_LIT> \n CONTENT_PREFETCH_MINUTES = '<STR_LIT>' \n RSS_SKIP_OLDER_THAN_DAYS = <NUM_LIT>", "output": "DELETE_AFTER_DAYS = <NUM_LIT> \n RSS_MINIMUM_ENTRY_AMOUNT = <NUM_LIT> \n MASTODON_FETCH_LIMIT = <NUM_LIT> \n HUEY_POOL_SIZE = <NUM_LIT> \n DEFAULT_AUTH_USER = '<STR_LIT>'"}, {"input": "import matplotlib . pyplot as plt \n import numpy as np \n from collections import defaultdict \n from multiprocessing import Process , Value \n class Logger : \n def __init__ ( self , dt ) : \n self . state_log = defaultdict ( list ) \n self . rew_log = defaultdict ( list ) \n self . dt = dt \n self . num_episodes = <NUM_LIT> \n self . plot_process = None \n def log_state ( self , key , value ) : \n self . state_log [ key ] . append ( value ) \n def log_states ( self , dict ) : \n for key , value in dict . items ( ) : \n self . log_state ( key , value ) \n def log_rewards ( self , dict , num_episodes ) : \n for key , value in dict . items ( ) : \n if '<STR_LIT>' in key : \n self . rew_log [ key ] . append ( value . item ( ) * num_episodes ) \n self . num_episodes += num_episodes \n def reset ( self ) : \n self . state_log . clear ( ) \n self . rew_log . clear ( ) \n def plot_states ( self ) : \n self . plot_process = Process ( target = self . _plot ) \n self . plot_process . start ( ) \n def _plot ( self ) : \n nb_rows = <NUM_LIT> \n nb_cols = <NUM_LIT> \n fig , axs = plt . subplots ( nb_rows , nb_cols ) \n for key , value in self . state_log . items ( ) : \n time = np . linspace ( <NUM_LIT> , len ( value ) * self . dt , len ( value ) ) \n break \n log = self . state_log \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( )", "output": "a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] : \n forces = np . array ( log [ \"<STR_LIT>\" ] ) \n for i in range ( forces . shape [ <NUM_LIT> ] ) : \n a . plot ( time , forces [ : , i ] , label = f'<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] != [ ] and log [ \"<STR_LIT>\" ] != [ ] : a . plot ( log [ \"<STR_LIT>\" ] , log [ \"<STR_LIT>\" ] , '<STR_LIT>' , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n a = axs [ <NUM_LIT> , <NUM_LIT> ] \n if log [ \"<STR_LIT>\" ] != [ ] : a . plot ( time , log [ \"<STR_LIT>\" ] , label = '<STR_LIT>' ) \n a . set ( xlabel = '<STR_LIT>' , ylabel = '<STR_LIT>' , title = '<STR_LIT>' ) \n a . legend ( ) \n plt . show ( ) \n def print_rewards ( self ) : \n print ( \"<STR_LIT>\" ) \n for key , values in self . rew_log . items ( ) : \n mean = np . sum ( np . array ( values ) ) / self . num_episodes \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n def __del__ ( self ) : \n if self . plot_process is not None : \n self . plot_process . kill ( )"}, {"input": "from densely_captioned_images . repro . eval . run_full_evals import CLIPEvalConfig , CLIPEvalJob \n from densely_captioned_images . repro . config import MODEL_PATH , DENSE_CAPS_DIR \n import submitit \n import os \n DO_FULL = False \n def main ( ) : \n with open ( os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) ) as f : \n MODEL_FILES = [ \n os . path . join ( MODEL_PATH , m . strip ( ) ) \n for m in f . readlines ( ) \n ] \n print ( \"<STR_LIT>\" , len ( MODEL_FILES ) ) \n eval_sweep = [ ] \n if not DO_FULL : \n for shots in [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] : \n eval_sweep += [ \n CLIPEvalConfig ( \n run_aro = False , \n run_vlc = False , \n run_dense_cap = False , \n run_winoground = False , \n run_elevater = shots ,", "output": "lora_weight_location = model_path , \n model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n ) for model_path in MODEL_FILES \n ] \n else : \n for dataset in range ( <NUM_LIT> ) : \n eval_sweep += [ \n CLIPEvalConfig ( \n run_aro = False , \n run_vlc = False , \n run_dense_cap = False , \n run_winoground = False , \n run_elevater = - <NUM_LIT> , \n elevater_dataset = dataset , \n lora_weight_location = model_path , \n model_name = model_path . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n ) for model_path in MODEL_FILES \n ] \n log_base_folder = os . path . join ( MODEL_PATH , '<STR_LIT>' , f\"<STR_LIT>\" ) \n os . makedirs ( log_base_folder , exist_ok = True ) \n log_folder = f\"<STR_LIT>\" \n executor = submitit . AutoExecutor ( folder = log_folder ) \n executor . update_parameters ( \n slurm_partition = '<STR_LIT>' , \n nodes = <NUM_LIT> , \n timeout_min = <NUM_LIT> * <NUM_LIT> * <NUM_LIT> , \n tasks_per_node = <NUM_LIT> , \n gpus_per_node = <NUM_LIT> , \n cpus_per_task = <NUM_LIT> , \n slurm_mem = '<STR_LIT>' , \n ) \n job_array = [ ] \n for sweep_args in eval_sweep : \n job = executor . submit ( CLIPEvalJob ( ) , sweep_args ) \n job_array . append ( job ) \n print ( f\"<STR_LIT>\" ) \n for job in job_array : \n _ = job . result ( ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "import os , sys \n import signal \n from flask import Flask , request , redirect \n now_dir = os . getcwd ( )", "output": "sys . path . append ( now_dir ) \n from core import run_download_script \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" ] ) \n def download ( url ) : \n file_path = run_download_script ( url ) \n if file_path == \"<STR_LIT>\" : \n if \"<STR_LIT>\" in request . headers . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) : \n return redirect ( \"<STR_LIT>\" , code = <NUM_LIT> ) \n else : \n return \"<STR_LIT>\" \n else : \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" ] ) \n def shutdown ( ) : \n print ( \"<STR_LIT>\" ) \n os . kill ( os . getpid ( ) , signal . SIGTERM ) \n if __name__ == \"<STR_LIT>\" : \n app . run ( host = \"<STR_LIT>\" , port = <NUM_LIT> )"}, {"input": "import os , sys \n import json \n import requests \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def load_local_version ( ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n return config [ \"<STR_LIT>\" ] \n def obtain_tag_name ( ) : \n url = \"<STR_LIT>\" \n try : \n response = requests . get ( url ) \n response . raise_for_status ( ) \n data = response . json ( ) \n tag_name = data [ \"<STR_LIT>\" ] \n return tag_name \n except requests . exceptions . RequestException as e : \n print ( f\"<STR_LIT>\" ) \n return None", "output": "def compare_version ( ) : \n local_version = load_local_version ( ) \n online_version = obtain_tag_name ( ) \n elements_online_version = list ( map ( int , online_version . split ( \"<STR_LIT>\" ) ) ) \n elements_local_version = list ( map ( int , local_version . split ( \"<STR_LIT>\" ) ) ) \n for online , local in zip ( elements_online_version , elements_local_version ) : \n if local < online : \n return f\"<STR_LIT>\" \n return f\"<STR_LIT>\""}, {"input": "from fastapi import FastAPI , BackgroundTasks , File , UploadFile \n from claude_api import Client \n import os \n app = FastAPI ( )", "output": "def get_cookie ( ) : \n cookie = os . getenv ( '<STR_LIT>' ) \n if not cookie : \n raise ValueError ( \"<STR_LIT>\" ) \n return cookie \n @ app . post ( \"<STR_LIT>\" ) \n async def create_chat ( prompt : str , background_tasks : BackgroundTasks ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n conversation = client . create_new_chat ( ) \n conversation_id = conversation [ '<STR_LIT>' ] \n background_tasks . add_task ( client . send_message , prompt , conversation_id ) \n return { \"<STR_LIT>\" : conversation_id } \n @ app . get ( \"<STR_LIT>\" ) \n async def get_chat_history ( conversation_id ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n history = client . chat_conversation_history ( conversation_id ) \n return history \n @ app . post ( \"<STR_LIT>\" ) \n async def send_message ( conversation_id : str , prompt : str ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n response = client . send_message ( prompt , conversation_id ) \n return { \"<STR_LIT>\" : response } \n @ app . post ( \"<STR_LIT>\" ) \n async def reset_conversations ( ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n result = client . reset_all ( ) \n return { \"<STR_LIT>\" : result } \n @ app . post ( \"<STR_LIT>\" ) \n async def rename_conversation ( conversation_id : str , title : str ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n result = client . rename_chat ( title , conversation_id ) \n return { \"<STR_LIT>\" : result } \n @ app . post ( \"<STR_LIT>\" ) \n async def upload_attachment ( file : UploadFile ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n file_path = save_upload_file ( file ) \n response = client . upload_attachment ( file_path ) \n return { \"<STR_LIT>\" : response } \n def save_upload_file ( uploaded_file ) : \n file_path = f\"<STR_LIT>\" \n with open ( file_path , \"<STR_LIT>\" ) as buffer : \n buffer . write ( uploaded_file . file . read ( ) ) \n return file_path \n @ app . get ( \"<STR_LIT>\" ) \n async def list_all_conversations ( ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n conversations = client . list_all_conversations ( ) \n return { \"<STR_LIT>\" : conversations } \n @ app . get ( \"<STR_LIT>\" ) \n async def chat_conversation_history ( conversation_id ) : \n cookie = get_cookie ( ) \n client = Client ( cookie ) \n history = client . chat_conversation_history ( conversation_id ) \n return { \"<STR_LIT>\" : history }"}, {"input": "import os \n import sys \n import tqdm \n import torch \n import torch . nn . functional as F \n import fairseq \n import soundfile as sf \n import numpy as np \n import logging \n logging . getLogger ( \"<STR_LIT>\" ) . setLevel ( logging . WARNING ) \n device = sys . argv [ <NUM_LIT> ] \n n_parts = int ( sys . argv [ <NUM_LIT> ] ) \n i_part = int ( sys . argv [ <NUM_LIT> ] ) \n if len ( sys . argv ) == <NUM_LIT> : \n exp_dir , version , is_half = sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] , bool ( sys . argv [ <NUM_LIT> ] ) \n else : \n i_gpu , exp_dir = sys . argv [ <NUM_LIT> ] , sys . argv [ <NUM_LIT> ] \n os . environ [ \"<STR_LIT>\" ] = str ( i_gpu ) \n version , is_half = sys . argv [ <NUM_LIT> ] , bool ( sys . argv [ <NUM_LIT> ] ) \n def forward_dml ( ctx , x , scale ) : \n ctx . scale = scale \n res = x . clone ( ) . detach ( ) \n return res \n fairseq . modules . grad_multiply . GradMultiply . forward = forward_dml \n model_path = \"<STR_LIT>\" \n wav_path = f\"<STR_LIT>\" \n out_path = f\"<STR_LIT>\" if version == \"<STR_LIT>\" else f\"<STR_LIT>\" \n os . makedirs ( out_path , exist_ok = True ) \n def read_wave ( wav_path , normalize = False ) : \n wav , sr = sf . read ( wav_path ) \n assert sr == <NUM_LIT> \n feats = torch . from_numpy ( wav ) \n feats = feats . half ( ) if is_half else feats . float ( ) \n feats = feats . mean ( - <NUM_LIT> ) if feats . dim ( ) == <NUM_LIT> else feats \n feats = feats . view ( <NUM_LIT> , - <NUM_LIT> ) \n if normalize : \n with torch . no_grad ( ) : \n feats = F . layer_norm ( feats , feats . shape ) \n return feats \n print ( \"<STR_LIT>\" ) \n models , saved_cfg , task = fairseq . checkpoint_utils . load_model_ensemble_and_task ( \n [ model_path ] , \n suffix = \"<STR_LIT>\" , \n ) \n model = models [ <NUM_LIT> ] \n model = model . to ( device ) \n if device not in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] : \n model = model . half ( ) \n model . eval ( ) \n todo = sorted ( os . listdir ( wav_path ) ) [ i_part : : n_parts ] \n n = max ( <NUM_LIT> , len ( todo ) // <NUM_LIT> ) \n if len ( todo ) == <NUM_LIT> : \n print ( \n \"<STR_LIT>\" \n ) \n else : \n print ( f\"<STR_LIT>\" ) \n with tqdm . tqdm ( total = len ( todo ) ) as pbar : \n for idx , file in enumerate ( todo ) : \n try : \n if file . endswith ( \"<STR_LIT>\" ) : \n wav_file_path = os . path . join ( wav_path , file ) \n out_file_path = os . path . join ( out_path , file . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n if os . path . exists ( out_file_path ) : \n continue \n feats = read_wave ( wav_file_path , normalize = saved_cfg . task . normalize ) \n padding_mask = torch . BoolTensor ( feats . shape ) . fill_ ( False ) \n inputs = { \n \"<STR_LIT>\" : feats . to ( device ) , \n \"<STR_LIT>\" : padding_mask . to ( device ) , \n \"<STR_LIT>\" : <NUM_LIT> if version == \"<STR_LIT>\" else <NUM_LIT> , \n }", "output": "with torch . no_grad ( ) : \n logits = model . extract_features ( ** inputs ) \n feats = ( \n model . final_proj ( logits [ <NUM_LIT> ] ) \n if version == \"<STR_LIT>\" \n else logits [ <NUM_LIT> ] \n ) \n feats = feats . squeeze ( <NUM_LIT> ) . float ( ) . cpu ( ) . numpy ( ) \n if np . isnan ( feats ) . sum ( ) == <NUM_LIT> : \n np . save ( out_file_path , feats , allow_pickle = False ) \n else : \n print ( f\"<STR_LIT>\" ) \n pbar . set_description ( f\"<STR_LIT>\" ) \n except Exception as error : \n print ( error ) \n pbar . update ( <NUM_LIT> ) \n print ( \"<STR_LIT>\" )"}, {"input": "", "output": "frutas = { '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' } \n print ( frutas ) \n palabra1 = set ( \"<STR_LIT>\" ) \n print ( palabra1 ) \n lista_frutas = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n lista_frutas = list ( set ( lista_frutas ) ) \n lista_frutas . sort ( ) \n print ( lista_frutas ) \n print ( '<STR_LIT>' in frutas ) \n print ( '<STR_LIT>' in frutas ) \n palabra1 = set ( \"<STR_LIT>\" ) \n palabra2 = set ( '<STR_LIT>' ) \n print ( palabra1 ) \n print ( palabra2 ) \n print ( palabra1 - palabra2 ) \n print ( palabra2 - palabra1 ) \n print ( palabra1 | palabra2 ) \n print ( palabra1 & palabra2 ) \n print ( palabra1 ^ palabra2 ) \n a = { elemento for elemento in \"<STR_LIT>\" if elemento not in \"<STR_LIT>\" } \n print ( a )"}, {"input": "from flask import Flask , request , render_template , render_template_string , send_from_directory \n import re \n import os \n import logging \n app = Flask ( __name__ ) \n app . logger . disabled = True \n log = logging . getLogger ( '<STR_LIT>' ) \n log . disabled = True", "output": "@ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def index ( ) : \n return str ( '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def secr3t ( ) : \n name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) \n template = \n bl = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for i in bl : \n if i in name : \n return str ( '<STR_LIT>' ) \n two_bracket_pattern = r\"<STR_LIT>\" \n two_bracket_match = re . search ( two_bracket_pattern , name ) \n bracket_comma_bracket_pattern = r\"<STR_LIT>\" \n bracket_comma_bracket_match = re . search ( bracket_comma_bracket_pattern , name ) \n bracket_bracket_line_pattern = r\"<STR_LIT>\" \n bracket_bracket_line_match = re . search ( bracket_bracket_line_pattern , name ) \n comma_bracket_bracket_line_pattern = r\"<STR_LIT>\" \n comma_bracket_bracket_line_match = re . search ( comma_bracket_bracket_line_pattern , name ) \n pattern_mo = r\"<STR_LIT>\" \n matche_mo = re . search ( pattern_mo , name ) \n if two_bracket_match : \n if bracket_comma_bracket_match . group ( <NUM_LIT> ) : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n elif comma_bracket_bracket_line_match : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n elif bracket_bracket_line_match : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n else : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n if matche_mo : \n print ( \"<STR_LIT>\" + name ) \n return str ( '<STR_LIT>' ) \n a = render_template_string ( template % name ) \n print ( \"<STR_LIT>\" + name ) \n if \"<STR_LIT>\" in a : \n return a + str ( '<STR_LIT>' ) \n return a \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def robots ( ) : \n return send_from_directory ( os . path . join ( app . root_path , '<STR_LIT>' ) , \n '<STR_LIT>' , mimetype = '<STR_LIT>' ) \n if __name__ == '<STR_LIT>' : \n app . run ( host = '<STR_LIT>' , port = <NUM_LIT> , debug = False )"}, {"input": "from api import * \n import re \n from flask import request , abort \n @ app . before_request \n def check ( ) : \n path = request . path \n if waf ( path ) : \n logger . warning ( f\"<STR_LIT>\" ) \n abort ( <NUM_LIT> ) \n def waf ( req : str ) : \n NN_RULES = \n for re_str in NN_RULES . split ( \"<STR_LIT>\" ) : \n if re . search ( re_str , req ) : \n logger . warning ( f\"<STR_LIT>\" ) \n return True \n return False \n def test ( ) : \n DATAS = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for data in DATAS : \n if not waf ( data ) : \n print ( f\"<STR_LIT>\" ) \n if __name__ == \"<STR_LIT>\" : \n test ( )"}, {"input": "from copy import deepcopy \n import os \n from datetime import datetime \n from typing import Tuple \n import torch \n import numpy as np \n from rsl_rl . env import VecEnv \n from rsl_rl . runners import OnPolicyRunner \n from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR \n from . helpers import get_args , update_cfg_from_args , class_to_dict , get_load_path , set_seed , parse_sim_params \n from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class TaskRegistry ( ) : \n def __init__ ( self ) : \n self . task_classes = { } \n self . env_cfgs = { } \n self . train_cfgs = { } \n def register ( self , name : str , task_class : VecEnv , env_cfg : LeggedRobotCfg , train_cfg : LeggedRobotCfgPPO ) : \n self . task_classes [ name ] = task_class \n self . env_cfgs [ name ] = env_cfg \n self . train_cfgs [ name ] = train_cfg \n def get_task_class ( self , name : str ) -> VecEnv : \n return self . task_classes [ name ] \n def get_cfgs ( self , name ) -> Tuple [ LeggedRobotCfg , LeggedRobotCfgPPO ] : \n train_cfg = self . train_cfgs [ name ] \n env_cfg = self . env_cfgs [ name ] \n env_cfg . seed = train_cfg . seed \n return env_cfg , train_cfg \n def make_env ( self , name , args = None , env_cfg = None ) -> Tuple [ VecEnv , LeggedRobotCfg ] : \n if args is None : \n args = get_args ( ) \n if name in self . task_classes : \n task_class = self . get_task_class ( name ) \n else : \n raise ValueError ( f\"<STR_LIT>\" ) \n if env_cfg is None : \n env_cfg , _ = self . get_cfgs ( name ) \n env_cfg , _ = update_cfg_from_args ( env_cfg , None , args ) \n set_seed ( env_cfg . seed ) \n sim_params = { \"<STR_LIT>\" : class_to_dict ( env_cfg . sim ) } \n sim_params = parse_sim_params ( args , sim_params ) \n env = task_class ( cfg = env_cfg , \n sim_params = sim_params , \n physics_engine = args . physics_engine , \n sim_device = args . sim_device , \n headless = args . headless ) \n return env , env_cfg \n def make_alg_runner ( self , env , name = None , args = None , train_cfg = None , init_wandb = True , log_root = \"<STR_LIT>\" , ** kwargs ) -> Tuple [ OnPolicyRunner , LeggedRobotCfgPPO ] : \n if args is None : \n args = get_args ( ) \n if train_cfg is None : \n if name is None : \n raise ValueError ( \"<STR_LIT>\" ) \n _ , train_cfg = self . get_cfgs ( name ) \n else : \n if name is not None : \n print ( f\"<STR_LIT>\" ) \n _ , train_cfg = update_cfg_from_args ( None , train_cfg , args ) \n if log_root == \"<STR_LIT>\" : \n log_root = os . path . join ( LEGGED_GYM_ROOT_DIR , '<STR_LIT>' , train_cfg . runner . experiment_name ) \n log_dir = os . path . join ( log_root , datetime . now ( ) . strftime ( '<STR_LIT>' ) + '<STR_LIT>' + train_cfg . runner . run_name ) \n elif log_root is None : \n log_dir = None \n else : \n log_dir = log_root \n train_cfg_dict = class_to_dict ( train_cfg ) \n runner = OnPolicyRunner ( env , \n train_cfg_dict , \n log_dir , \n init_wandb = init_wandb , \n device = args . rl_device , ** kwargs )", "output": "resume = train_cfg . runner . resume \n if args . resumeid : \n log_root = LEGGED_GYM_ROOT_DIR + f\"<STR_LIT>\" + args . resumeid \n resume = True \n if resume : \n print ( log_root ) \n print ( train_cfg . runner . load_run ) \n resume_path = get_load_path ( log_root , load_run = train_cfg . runner . load_run , checkpoint = train_cfg . runner . checkpoint ) \n runner . load ( resume_path ) \n if not train_cfg . policy . continue_from_last_std : \n runner . alg . actor_critic . reset_std ( train_cfg . policy . init_noise_std , <NUM_LIT> , device = runner . device ) \n if \"<STR_LIT>\" in kwargs : \n return runner , train_cfg , os . path . dirname ( resume_path ) \n else : \n return runner , train_cfg \n task_registry = TaskRegistry ( )"}, {"input": "import requests \n from config import conf \n class SensitiveWord : \n def __init__ ( self ) : \n try : \n self . config = conf ( ) \n except Exception as e : \n print ( e ) \n self . url = \"<STR_LIT>\" \n self . access_token = self . get_access_token ( ) \n def get_access_token ( self ) : \n if self . config is not None and \"<STR_LIT>\" in self . config and \"<STR_LIT>\" in self . config [ \"<STR_LIT>\" ] and self . config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] : \n url = \"<STR_LIT>\" \n params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : self . config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : self . config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n } \n response = requests . post ( url , params = params ) \n response_json = response . json ( ) \n access_token = response_json . get ( \"<STR_LIT>\" ) \n if not access_token : \n raise ValueError ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n return access_token \n def process_text ( self , text ) : \n if self . config is not None and \"<STR_LIT>\" in self . config and \"<STR_LIT>\" in self . config [ \"<STR_LIT>\" ] and self . config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] : \n url = \"<STR_LIT>\" \n access_token = self . get_access_token ( ) \n headers = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n params = { \n \"<STR_LIT>\" : text . encode ( \"<STR_LIT>\" ) ,", "output": "\"<STR_LIT>\" : access_token \n } \n response = requests . post ( url , data = params , headers = headers ) \n if response . status_code != <NUM_LIT> : \n raise ValueError ( f\"<STR_LIT>\" ) \n conclusion_type = response . json ( ) . get ( \"<STR_LIT>\" ) \n print ( response . json ( ) ) \n if conclusion_type in [ <NUM_LIT> , None ] : \n return False \n else : \n return True \n else : \n return False"}, {"input": "import mutagen . aiff \n from . file import TAG_MAP_ENTRY \n from . id3 import Id3File \n class AiffFile ( Id3File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . aiff . AIFF \n def __init__ ( self , filename , ** kwargs ) : \n super ( AiffFile , self ) . __init__ ( filename , ** kwargs ) \n self . tag_map = self . tag_map . copy ( ) \n self . tag_map . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' ,", "output": "type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = int ) , \n } )"}, {"input": "from flask import Flask , request \n from jinja2 import Template \n import re \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" )", "output": "def index ( ) : \n name = request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) \n if not re . findall ( r\"<STR_LIT>\" , name ) : \n t = Template ( \"<STR_LIT>\" + name ) \n return t . render ( ) \n else : \n t = Template ( \"<STR_LIT>\" ) \n return t . render ( ) \n if __name__ == \"<STR_LIT>\" : \n app . run ( host = \"<STR_LIT>\" , port = <NUM_LIT> )"}, {"input": "import random \n def metodo_quicksort ( lista ) : \n if len ( lista ) <= <NUM_LIT> : \n return lista", "output": "else : \n pivote = lista [ len ( lista ) // <NUM_LIT> ] \n menores = [ x for x in lista if x < pivote ] \n iguales = [ x for x in lista if x == pivote ] \n mayores = [ x for x in lista if x > pivote ] \n return metodo_quicksort ( menores ) + iguales + metodo_quicksort ( mayores ) \n lista_aleatoria = [ random . randint ( <NUM_LIT> , <NUM_LIT> ) for _ in range ( <NUM_LIT> ) ] \n print ( lista_aleatoria ) \n print ( metodo_quicksort ( lista_aleatoria ) )"}, {"input": "def count_traceback_lenth ( traceback_object ) : \n result = <NUM_LIT> \n while traceback_object is not None : \n result += <NUM_LIT> \n traceback_object = traceback_object . tb_next \n return result \n def cut_base_of_traceback ( traceback_object , base_size ) : \n index = <NUM_LIT>", "output": "while traceback_object is not None : \n if index == base_size : \n return traceback_object \n index += <NUM_LIT> \n traceback_object = traceback_object . tb_next \n def cut_importlib_bug ( traceback_object ) : \n try : \n while traceback_object is not None : \n if not ( traceback_object . tb_frame . f_code . co_filename == '<STR_LIT>' or traceback_object . tb_frame . f_code . co_filename == '<STR_LIT>' ) : \n return traceback_object \n traceback_object = traceback_object . tb_next \n except AttributeError : \n return traceback_object"}, {"input": "from pypresence import Presence \n import datetime as dt \n import time \n class RichPresenceManager : \n def __init__ ( self ) : \n self . client_id = \"<STR_LIT>\" \n self . rpc = None \n self . running = False \n def start_presence ( self ) : \n if not self . running : \n self . running = True \n self . rpc = Presence ( self . client_id ) \n try : \n self . rpc . connect ( ) \n self . update_presence ( ) \n except KeyboardInterrupt as error : \n print ( error ) \n self . rpc = None \n self . running = False \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n self . rpc = None \n self . running = False \n def update_presence ( self ) : \n if self . rpc : \n self . rpc . update ( \n state = \"<STR_LIT>\" , \n details = \"<STR_LIT>\" , \n buttons = [ \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ] , \n large_image = \"<STR_LIT>\" ,", "output": "large_text = \"<STR_LIT>\" , \n start = dt . datetime . now ( ) . timestamp ( ) , \n ) \n def stop_presence ( self ) : \n self . running = False \n if self . rpc : \n self . rpc . close ( ) \n self . rpc = None \n RPCManager = RichPresenceManager ( )"}, {"input": "import os , sys \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from core import run_model_information_script \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n import gradio as gr \n def processing ( ) : \n with gr . Accordion ( label = i18n ( \"<STR_LIT>\" ) ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n model_view_model_path = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n interactive = True , \n placeholder = i18n ( \"<STR_LIT>\" ) ,", "output": ") \n model_view_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n ) \n model_view_button = gr . Button ( i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\" ) \n model_view_button . click ( \n run_model_information_script , \n [ model_view_model_path ] , \n model_view_output_info , \n api_name = \"<STR_LIT>\" , \n )"}, {"input": "def busqueda_binaria ( lista , x ) : \n low = <NUM_LIT>", "output": "high = len ( lista ) - <NUM_LIT> \n mid = <NUM_LIT> \n while low <= high : \n mid = ( high + low ) // <NUM_LIT> \n if lista [ mid ] < x : \n low = mid + <NUM_LIT> \n elif lista [ mid ] > x : \n high = mid - <NUM_LIT> \n else : \n return mid \n return - <NUM_LIT> \n lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n x = <NUM_LIT> \n resultado = busqueda_binaria ( lista , x ) \n if resultado != - <NUM_LIT> : \n print ( \"<STR_LIT>\" , resultado ) \n else : \n print ( \"<STR_LIT>\" )"}, {"input": "class Motor : \n def iniciar ( self ) : \n motor_iniciado = True \n print ( \"<STR_LIT>\" ) \n return motor_iniciado \n class CajaVelocidades : \n def cambiar_velocidad ( self , subir = False ) : \n if subir : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n class Carro : \n def __init__ ( self ) : \n self . motor_iniciado = False \n self . motor = Motor ( ) \n self . velocidades = CajaVelocidades ( ) \n def iniciar_motor ( self ) : \n self . motor_iniciado = self . motor . iniciar ( ) \n def cambiar_marcha ( self , subir = False ) :", "output": "if self . motor_iniciado : \n self . velocidades . cambiar_velocidad ( subir ) \n else : \n print ( \"<STR_LIT>\" ) \n mi_coche = Carro ( ) \n mi_coche . iniciar_motor ( ) \n mi_coche . cambiar_marcha ( True )"}, {"input": "import asyncio \n import json \n from channel . http import auth \n from flask import Flask , request , render_template , make_response \n from datetime import timedelta \n from common import const \n from common import functions \n from config import channel_conf \n from config import channel_conf_val \n from channel . channel import Channel \n from flask_socketio import SocketIO \n from common import log \n from plugins . plugin_manager import * \n http_app = Flask ( __name__ , ) \n socketio = SocketIO ( http_app , close_timeout = <NUM_LIT> ) \n http_app . jinja_env . auto_reload = True \n http_app . config [ '<STR_LIT>' ] = True \n http_app . config [ '<STR_LIT>' ] = timedelta ( seconds = <NUM_LIT> ) \n async def return_stream ( data ) : \n async for final , response in HttpChannel ( ) . handle_stream ( data = data ) : \n try : \n if ( final ) : \n socketio . server . emit ( \n '<STR_LIT>' , { '<STR_LIT>' : response , '<STR_LIT>' : final } , request . sid , namespace = \"<STR_LIT>\" ) \n disconnect ( ) \n else : \n socketio . server . emit ( \n '<STR_LIT>' , { '<STR_LIT>' : response , '<STR_LIT>' : final } , request . sid , namespace = \"<STR_LIT>\" ) \n except Exception as e : \n disconnect ( ) \n log . warn ( \"<STR_LIT>\" , e ) \n break \n @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) \n def stream ( data ) : \n if ( auth . identify ( request ) == False ) : \n client_sid = request . sid \n socketio . server . disconnect ( client_sid ) \n return \n data = json . loads ( data [ \"<STR_LIT>\" ] ) \n if ( data ) : \n img_match_prefix = functions . check_prefix ( \n data [ \"<STR_LIT>\" ] , channel_conf_val ( const . HTTP , '<STR_LIT>' ) ) \n if img_match_prefix : \n reply_text = HttpChannel ( ) . handle ( data = data ) \n socketio . emit ( \n '<STR_LIT>' , { '<STR_LIT>' : reply_text } , namespace = '<STR_LIT>' ) \n disconnect ( ) \n return \n asyncio . run ( return_stream ( data ) ) \n @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) \n def connect ( ) : \n log . info ( '<STR_LIT>' ) \n socketio . emit ( '<STR_LIT>' , { '<STR_LIT>' : \"<STR_LIT>\" } , namespace = '<STR_LIT>' ) \n @ socketio . on ( '<STR_LIT>' , namespace = '<STR_LIT>' ) \n def disconnect ( ) : \n log . info ( '<STR_LIT>' ) \n socketio . server . disconnect ( request . sid , namespace = \"<STR_LIT>\" ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def chat ( ) : \n if ( auth . identify ( request ) == False ) : \n return \n data = json . loads ( request . data ) \n if data : \n msg = data [ '<STR_LIT>' ] \n if not msg : \n return \n reply_text = HttpChannel ( ) . handle ( data = data ) \n return { '<STR_LIT>' : reply_text } \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def index ( ) : \n if ( auth . identify ( request ) == False ) : \n return login ( ) \n return render_template ( '<STR_LIT>' ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def login ( ) : \n response = make_response ( \"<STR_LIT>\" , <NUM_LIT> ) \n response . headers . add_header ( '<STR_LIT>' , '<STR_LIT>' ) \n response . headers . add_header ( '<STR_LIT>' , '<STR_LIT>' ) \n if ( auth . identify ( request ) == True ) : \n return response \n else : \n if request . method == \"<STR_LIT>\" : \n token = auth . authenticate ( request . form [ '<STR_LIT>' ] ) \n if ( token != False ) : \n response . set_cookie ( key = '<STR_LIT>' , value = token ) \n return response", "output": "else : \n return render_template ( '<STR_LIT>' ) \n response . headers . set ( '<STR_LIT>' , '<STR_LIT>' ) \n return response \n class HttpChannel ( Channel ) : \n def startup ( self ) : \n http_app . run ( host = '<STR_LIT>' , port = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) ) \n def handle ( self , data ) : \n context = dict ( ) \n query = data [ \"<STR_LIT>\" ] \n id = data [ \"<STR_LIT>\" ] \n context [ '<STR_LIT>' ] = str ( id ) \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { \n '<STR_LIT>' : self , '<STR_LIT>' : query , \"<STR_LIT>\" : context } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if not e_context . is_pass ( ) : \n reply = super ( ) . build_reply_content ( e_context [ \"<STR_LIT>\" ] , e_context [ \"<STR_LIT>\" ] ) \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { \n '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , \"<STR_LIT>\" : context } ) ) \n reply = e_context [ '<STR_LIT>' ] \n return reply \n async def handle_stream ( self , data ) : \n context = dict ( ) \n id = data [ \"<STR_LIT>\" ] \n context [ '<STR_LIT>' ] = str ( id ) \n context [ '<STR_LIT>' ] = True \n context [ '<STR_LIT>' ] = data [ \"<STR_LIT>\" ] \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { \n '<STR_LIT>' : self , '<STR_LIT>' : data [ \"<STR_LIT>\" ] , '<STR_LIT>' : data [ \"<STR_LIT>\" ] , \"<STR_LIT>\" : context } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if not e_context . is_pass ( ) : \n async for final , reply in super ( ) . build_reply_stream ( data [ \"<STR_LIT>\" ] , context ) : \n yield final , reply \n else : \n yield True , reply"}, {"input": "import logging , traceback , sys , threading \n try : \n import Queue \n except ImportError : \n import queue as Queue \n from . . log import set_logging \n from . . utils import test_connect \n from . . storage import templates \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_register ( core ) : \n core . auto_login = auto_login \n core . configured_reply = configured_reply \n core . msg_register = msg_register \n core . run = run \n def auto_login ( self , hotReload = False , statusStorageDir = '<STR_LIT>' , \n enableCmdQR = False , picDir = None , qrCallback = None , \n loginCallback = None , exitCallback = None ) : \n if not test_connect ( ) : \n logger . info ( \"<STR_LIT>\" ) \n sys . exit ( ) \n self . useHotReload = hotReload \n self . hotReloadDir = statusStorageDir \n if hotReload : \n rval = self . load_login_status ( statusStorageDir , \n loginCallback = loginCallback , exitCallback = exitCallback ) \n if rval : \n return \n logger . error ( '<STR_LIT>' . format ( rval ) ) \n self . logout ( ) \n self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , \n loginCallback = loginCallback , exitCallback = exitCallback ) \n self . dump_login_status ( statusStorageDir ) \n else : \n self . login ( enableCmdQR = enableCmdQR , picDir = picDir , qrCallback = qrCallback , \n loginCallback = loginCallback , exitCallback = exitCallback ) \n def configured_reply ( self ) : \n try : \n msg = self . msgList . get ( timeout = <NUM_LIT> ) \n except Queue . Empty :", "output": "pass \n else : \n if isinstance ( msg [ '<STR_LIT>' ] , templates . User ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n elif isinstance ( msg [ '<STR_LIT>' ] , templates . MassivePlatform ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n elif isinstance ( msg [ '<STR_LIT>' ] , templates . Chatroom ) : \n replyFn = self . functionDict [ '<STR_LIT>' ] . get ( msg [ '<STR_LIT>' ] ) \n if replyFn is None : \n r = None \n else : \n try : \n r = replyFn ( msg ) \n if r is not None : \n self . send ( r , msg . get ( '<STR_LIT>' ) ) \n except : \n logger . warning ( traceback . format_exc ( ) ) \n def msg_register ( self , msgType , isFriendChat = False , isGroupChat = False , isMpChat = False ) : \n if not ( isinstance ( msgType , list ) or isinstance ( msgType , tuple ) ) : \n msgType = [ msgType ] \n def _msg_register ( fn ) : \n for _msgType in msgType : \n if isFriendChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if isGroupChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if isMpChat : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n if not any ( ( isFriendChat , isGroupChat , isMpChat ) ) : \n self . functionDict [ '<STR_LIT>' ] [ _msgType ] = fn \n return fn \n return _msg_register \n def run ( self , debug = False , blockThread = True ) : \n logger . info ( '<STR_LIT>' ) \n if debug : \n set_logging ( loggingLevel = logging . DEBUG ) \n def reply_fn ( ) : \n try : \n while self . alive : \n self . configured_reply ( ) \n except KeyboardInterrupt : \n if self . useHotReload : \n self . dump_login_status ( ) \n self . alive = False \n logger . debug ( '<STR_LIT>' ) \n logger . info ( '<STR_LIT>' ) \n if blockThread : \n reply_fn ( ) \n else : \n replyThread = threading . Thread ( target = reply_fn ) \n replyThread . setDaemon ( True ) \n replyThread . start ( )"}, {"input": "from typing import List , Dict \n import logging \n import torch \n def _find_all_linear_names ( model ) -> List [ str ] : \n cls = torch . nn . Linear \n lora_module_names = set ( ) \n for name , module in model . named_modules ( ) : \n if isinstance ( module , cls ) : \n names = name . split ( \"<STR_LIT>\" ) \n lora_module_names . add ( names [ <NUM_LIT> ] if len ( names ) == <NUM_LIT> else names [ - <NUM_LIT> ] ) \n if \"<STR_LIT>\" in lora_module_names : \n lora_module_names . remove ( \"<STR_LIT>\" ) \n return list ( lora_module_names ) \n def maybe_zero_3 ( param , ignore_status = False , name = None ) : \n from deepspeed import zero \n from deepspeed . runtime . zero . partition_parameters import ZeroParamStatus \n if hasattr ( param , \"<STR_LIT>\" ) : \n if param . ds_status == ZeroParamStatus . NOT_AVAILABLE : \n if not ignore_status : \n logging . warning ( \n f\"<STR_LIT>\" \n ) \n with zero . GatheredParameters ( [ param ] ) : \n param = param . data . detach ( ) . cpu ( ) . clone ( ) \n else : \n param = param . detach ( ) . cpu ( ) . clone ( ) \n return param \n def get_peft_state ( named_params , bias ) -> Dict : \n if bias == \"<STR_LIT>\" : \n to_return = { k : t for k , t in named_params if \"<STR_LIT>\" in k } \n elif bias == \"<STR_LIT>\" : \n to_return = { k : t for k , t in named_params if \"<STR_LIT>\" in k or \"<STR_LIT>\" in k } \n elif bias == \"<STR_LIT>\" : \n to_return = { } \n maybe_lora_bias = { } \n lora_bias_names = set ( ) \n for k , t in named_params : \n if \"<STR_LIT>\" in k : \n to_return [ k ] = t \n bias_name = k . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] + \"<STR_LIT>\" \n lora_bias_names . add ( bias_name ) \n elif \"<STR_LIT>\" in k : \n maybe_lora_bias [ k ] = t \n for k , t in maybe_lora_bias : \n if bias_name in lora_bias_names : \n to_return [ bias_name ] = t \n else : \n raise NotImplementedError ( ) \n to_return = { k : maybe_zero_3 ( v , ignore_status = True ) for k , v in to_return . items ( ) } \n return to_return \n def get_peft_state_non_lora ( named_params ) -> Dict : \n to_return = { \n k : t \n for k , t in named_params \n if \"<STR_LIT>\" not in k and ( t . requires_grad or \"<STR_LIT>\" in k ) \n } \n to_return = { \n k : maybe_zero_3 ( v , ignore_status = True ) . cpu ( ) for k , v in to_return . items ( ) \n } \n return to_return \n def make_model_lora ( model , training_args : \"<STR_LIT>\" ) : \n from peft import LoraConfig , get_peft_model \n lora_config = LoraConfig ( \n r = training_args . lora_r , \n lora_alpha = training_args . lora_alpha , \n target_modules = _find_all_linear_names ( model ) , \n lora_dropout = training_args . lora_dropout , \n bias = training_args . lora_bias , \n task_type = \"<STR_LIT>\" , \n ) \n if training_args . bits == <NUM_LIT> : \n if training_args . bf16 : \n model . to ( torch . bfloat16 ) \n if training_args . fp16 : \n model . to ( torch . float16 ) \n model = get_peft_model ( model , lora_config )", "output": "return model \n def fix_tokenizer ( tokenizer ) : \n if tokenizer . pad_token is None : \n tokenizer . pad_token = tokenizer . unk_token \n if tokenizer . mask_token is None : \n tokenizer . mask_token = tokenizer . unk_token \n if tokenizer . cls_token is None : \n tokenizer . cls_token = tokenizer . unk_token \n if tokenizer . sep_token is None : \n tokenizer . sep_token = tokenizer . unk_token"}, {"input": "import os \n import sys \n import base64 \n import pathlib \n import tempfile \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n import assets . themes . loadThemes as loadThemes \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n i18n = I18nAuto ( )", "output": "def theme_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n themes_select = gr . Dropdown ( \n loadThemes . get_list ( ) , \n value = loadThemes . read_json ( ) , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n visible = True , \n ) \n themes_select . change ( \n fn = loadThemes . select_theme , \n inputs = themes_select , \n outputs = [ ] , \n )"}, {"input": "from instld . cli . traceback_cutting . traceback_utils import count_traceback_lenth , cut_base_of_traceback \n def test_count_traceback_lenth_empty ( ) : \n assert count_traceback_lenth ( None ) == <NUM_LIT> \n def test_count_traceback_lenth_not_empty ( ) : \n def function_3 ( ) : \n raise ValueError \n def function_2 ( ) : \n function_3 ( ) \n def function_1 ( ) : \n function_2 ( ) \n try : \n function_1 ( ) \n except ValueError as e : \n assert count_traceback_lenth ( e . __traceback__ ) == <NUM_LIT> \n def test_cut_base_of_traceback ( ) : \n def function_3 ( ) : \n raise ValueError \n def function_2 ( ) : \n function_3 ( ) \n def function_1 ( ) : \n function_2 ( ) \n try : \n function_1 ( ) \n except ValueError as e : \n assert count_traceback_lenth ( e . __traceback__ ) == <NUM_LIT> \n cutted_traceback = cut_base_of_traceback ( e . __traceback__ , <NUM_LIT> ) \n assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT>", "output": "assert cutted_traceback is e . __traceback__ \n cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) \n assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> \n cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) \n assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> \n cutted_traceback = cut_base_of_traceback ( cutted_traceback , <NUM_LIT> ) \n assert count_traceback_lenth ( cutted_traceback ) == <NUM_LIT> \n assert cutted_traceback is None"}, {"input": "import torch \n import numpy as np \n from rsl_rl . utils import split_and_pad_trajectories \n class RolloutStorage : \n class Transition : \n def __init__ ( self ) : \n self . observations = None \n self . critic_observations = None \n self . actions = None \n self . rewards = None \n self . dones = None \n self . values = None \n self . actions_log_prob = None \n self . action_mean = None \n self . action_sigma = None \n self . hidden_states = None \n def clear ( self ) : \n self . __init__ ( ) \n def __init__ ( self , num_envs , num_transitions_per_env , obs_shape , privileged_obs_shape , actions_shape , device = '<STR_LIT>' ) : \n self . device = device \n self . obs_shape = obs_shape \n self . privileged_obs_shape = privileged_obs_shape \n self . actions_shape = actions_shape \n self . observations = torch . zeros ( num_transitions_per_env , num_envs , * obs_shape , device = self . device ) \n if privileged_obs_shape [ <NUM_LIT> ] is not None : \n self . privileged_observations = torch . zeros ( num_transitions_per_env , num_envs , * privileged_obs_shape , device = self . device ) \n else : \n self . privileged_observations = None \n self . rewards = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) \n self . actions = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) \n self . dones = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) . byte ( ) \n self . actions_log_prob = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) \n self . values = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) \n self . returns = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) \n self . advantages = torch . zeros ( num_transitions_per_env , num_envs , <NUM_LIT> , device = self . device ) \n self . mu = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) \n self . sigma = torch . zeros ( num_transitions_per_env , num_envs , * actions_shape , device = self . device ) \n self . num_transitions_per_env = num_transitions_per_env \n self . num_envs = num_envs \n self . saved_hidden_states_a = None \n self . saved_hidden_states_c = None \n self . step = <NUM_LIT> \n def add_transitions ( self , transition : Transition ) : \n if self . step >= self . num_transitions_per_env : \n raise AssertionError ( \"<STR_LIT>\" ) \n self . observations [ self . step ] . copy_ ( transition . observations ) \n if self . privileged_observations is not None : self . privileged_observations [ self . step ] . copy_ ( transition . critic_observations ) \n self . actions [ self . step ] . copy_ ( transition . actions ) \n self . rewards [ self . step ] . copy_ ( transition . rewards . view ( - <NUM_LIT> , <NUM_LIT> ) ) \n self . dones [ self . step ] . copy_ ( transition . dones . view ( - <NUM_LIT> , <NUM_LIT> ) ) \n self . values [ self . step ] . copy_ ( transition . values ) \n self . actions_log_prob [ self . step ] . copy_ ( transition . actions_log_prob . view ( - <NUM_LIT> , <NUM_LIT> ) ) \n self . mu [ self . step ] . copy_ ( transition . action_mean ) \n self . sigma [ self . step ] . copy_ ( transition . action_sigma ) \n self . _save_hidden_states ( transition . hidden_states ) \n self . step += <NUM_LIT> \n def _save_hidden_states ( self , hidden_states ) : \n if hidden_states is None or hidden_states == ( None , None ) : \n return \n hid_a = hidden_states [ <NUM_LIT> ] if isinstance ( hidden_states [ <NUM_LIT> ] , tuple ) else ( hidden_states [ <NUM_LIT> ] , ) \n hid_c = hidden_states [ <NUM_LIT> ] if isinstance ( hidden_states [ <NUM_LIT> ] , tuple ) else ( hidden_states [ <NUM_LIT> ] , ) \n if self . saved_hidden_states_a is None : \n self . saved_hidden_states_a = [ torch . zeros ( self . observations . shape [ <NUM_LIT> ] , * hid_a [ i ] . shape , device = self . device ) for i in range ( len ( hid_a ) ) ] \n self . saved_hidden_states_c = [ torch . zeros ( self . observations . shape [ <NUM_LIT> ] , * hid_c [ i ] . shape , device = self . device ) for i in range ( len ( hid_c ) ) ]", "output": "for i in range ( len ( hid_a ) ) : \n self . saved_hidden_states_a [ i ] [ self . step ] . copy_ ( hid_a [ i ] ) \n self . saved_hidden_states_c [ i ] [ self . step ] . copy_ ( hid_c [ i ] ) \n def clear ( self ) : \n self . step = <NUM_LIT> \n def compute_returns ( self , last_values , gamma , lam ) : \n advantage = <NUM_LIT> \n for step in reversed ( range ( self . num_transitions_per_env ) ) : \n if step == self . num_transitions_per_env - <NUM_LIT> : \n next_values = last_values \n else : \n next_values = self . values [ step + <NUM_LIT> ] \n next_is_not_terminal = <NUM_LIT> - self . dones [ step ] . float ( ) \n delta = self . rewards [ step ] + next_is_not_terminal * gamma * next_values - self . values [ step ] \n advantage = delta + next_is_not_terminal * gamma * lam * advantage \n self . returns [ step ] = advantage + self . values [ step ] \n self . advantages = self . returns - self . values \n self . advantages = ( self . advantages - self . advantages . mean ( ) ) / ( self . advantages . std ( ) + <NUM_LIT> ) \n def get_statistics ( self ) : \n done = self . dones \n done [ - <NUM_LIT> ] = <NUM_LIT> \n flat_dones = done . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) . reshape ( - <NUM_LIT> , <NUM_LIT> ) \n done_indices = torch . cat ( ( flat_dones . new_tensor ( [ - <NUM_LIT> ] , dtype = torch . int64 ) , flat_dones . nonzero ( as_tuple = False ) [ : , <NUM_LIT> ] ) ) \n trajectory_lengths = ( done_indices [ <NUM_LIT> : ] - done_indices [ : - <NUM_LIT> ] ) \n return trajectory_lengths . float ( ) . mean ( ) , self . rewards . mean ( ) \n def mini_batch_generator ( self , num_mini_batches , num_epochs = <NUM_LIT> ) : \n batch_size = self . num_envs * self . num_transitions_per_env \n mini_batch_size = batch_size // num_mini_batches \n indices = torch . randperm ( num_mini_batches * mini_batch_size , requires_grad = False , device = self . device ) \n observations = self . observations . flatten ( <NUM_LIT> , <NUM_LIT> ) \n if self . privileged_observations is not None : \n critic_observations = self . privileged_observations . flatten ( <NUM_LIT> , <NUM_LIT> ) \n else : \n critic_observations = observations \n actions = self . actions . flatten ( <NUM_LIT> , <NUM_LIT> ) \n values = self . values . flatten ( <NUM_LIT> , <NUM_LIT> ) \n returns = self . returns . flatten ( <NUM_LIT> , <NUM_LIT> ) \n old_actions_log_prob = self . actions_log_prob . flatten ( <NUM_LIT> , <NUM_LIT> ) \n advantages = self . advantages . flatten ( <NUM_LIT> , <NUM_LIT> ) \n old_mu = self . mu . flatten ( <NUM_LIT> , <NUM_LIT> ) \n old_sigma = self . sigma . flatten ( <NUM_LIT> , <NUM_LIT> ) \n for epoch in range ( num_epochs ) : \n for i in range ( num_mini_batches ) : \n start = i * mini_batch_size \n end = ( i + <NUM_LIT> ) * mini_batch_size \n batch_idx = indices [ start : end ] \n obs_batch = observations [ batch_idx ] \n critic_observations_batch = critic_observations [ batch_idx ] \n actions_batch = actions [ batch_idx ] \n target_values_batch = values [ batch_idx ] \n returns_batch = returns [ batch_idx ] \n old_actions_log_prob_batch = old_actions_log_prob [ batch_idx ] \n advantages_batch = advantages [ batch_idx ] \n old_mu_batch = old_mu [ batch_idx ] \n old_sigma_batch = old_sigma [ batch_idx ] \n yield obs_batch , critic_observations_batch , actions_batch , target_values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , ( None , None ) , None \n def reccurent_mini_batch_generator ( self , num_mini_batches , num_epochs = <NUM_LIT> ) : \n padded_obs_trajectories , trajectory_masks = split_and_pad_trajectories ( self . observations , self . dones ) \n if self . privileged_observations is not None : \n padded_critic_obs_trajectories , _ = split_and_pad_trajectories ( self . privileged_observations , self . dones ) \n else : \n padded_critic_obs_trajectories = padded_obs_trajectories \n mini_batch_size = self . num_envs // num_mini_batches \n for ep in range ( num_epochs ) : \n first_traj = <NUM_LIT> \n for i in range ( num_mini_batches ) : \n start = i * mini_batch_size \n stop = ( i + <NUM_LIT> ) * mini_batch_size \n dones = self . dones . squeeze ( - <NUM_LIT> ) \n last_was_done = torch . zeros_like ( dones , dtype = torch . bool ) \n last_was_done [ <NUM_LIT> : ] = dones [ : - <NUM_LIT> ] \n last_was_done [ <NUM_LIT> ] = True \n trajectories_batch_size = torch . sum ( last_was_done [ : , start : stop ] ) \n last_traj = first_traj + trajectories_batch_size \n masks_batch = trajectory_masks [ : , first_traj : last_traj ] \n obs_batch = padded_obs_trajectories [ : , first_traj : last_traj ] \n critic_obs_batch = padded_critic_obs_trajectories [ : , first_traj : last_traj ] \n actions_batch = self . actions [ : , start : stop ] \n old_mu_batch = self . mu [ : , start : stop ] \n old_sigma_batch = self . sigma [ : , start : stop ] \n returns_batch = self . returns [ : , start : stop ] \n advantages_batch = self . advantages [ : , start : stop ] \n values_batch = self . values [ : , start : stop ] \n old_actions_log_prob_batch = self . actions_log_prob [ : , start : stop ] \n last_was_done = last_was_done . permute ( <NUM_LIT> , <NUM_LIT> ) \n hid_a_batch = [ saved_hidden_states . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) [ last_was_done ] [ first_traj : last_traj ] . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) \n for saved_hidden_states in self . saved_hidden_states_a ] \n hid_c_batch = [ saved_hidden_states . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) [ last_was_done ] [ first_traj : last_traj ] . transpose ( <NUM_LIT> , <NUM_LIT> ) . contiguous ( ) \n for saved_hidden_states in self . saved_hidden_states_c ] \n hid_a_batch = hid_a_batch [ <NUM_LIT> ] if len ( hid_a_batch ) == <NUM_LIT> else hid_a_batch \n hid_c_batch = hid_c_batch [ <NUM_LIT> ] if len ( hid_c_batch ) == <NUM_LIT> else hid_a_batch \n yield obs_batch , critic_obs_batch , actions_batch , values_batch , advantages_batch , returns_batch , old_actions_log_prob_batch , old_mu_batch , old_sigma_batch , ( hid_a_batch , hid_c_batch ) , masks_batch \n first_traj = last_traj"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . INTEGER ( ) , \n nullable = False ) \n batch_op . drop_index ( '<STR_LIT>' ) \n batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] , unique = False ) \n batch_op . create_unique_constraint ( '<STR_LIT>' , [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) \n batch_op . drop_index ( '<STR_LIT>' )", "output": "batch_op . create_index ( '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . INTEGER ( ) , \n nullable = True )"}, {"input": "import datetime \n import resend \n import time \n resend . api_key = \"<STR_LIT>\" \n from termcolor import colored \n class Alerting : \n def __init__ ( self , \n alert_threshold = <NUM_LIT> , \n send_notifications = True , \n user_emails = set ( ) ) : \n self . unhandled_errors = { \n } \n now = datetime . datetime . now ( ) \n self . current_time_block = now . hour \n self . set_cooldown = True \n self . cooldown_start_time = time . time ( ) \n self . send_notifications = send_notifications \n self . user_emails = user_emails \n def start_cooldown ( self ) : \n self . set_cooldown = True \n self . cooldown_start_time = time . time ( ) \n def send_alert ( self , error_type , most_recent_error ) : \n print ( colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n params = { \n \"<STR_LIT>\" : \n \"<STR_LIT>\" , \n \"<STR_LIT>\" : \n . format ( error_type ) , \n \"<STR_LIT>\" : \n . format ( error_type , most_recent_error ) \n } \n for email in self . user_emails : \n params [ \"<STR_LIT>\" ] = email \n email = resend . Emails . send ( params ) \n return \n def add_emails ( self , user_email ) : \n if type ( user_email ) == list : \n for email in user_email : \n self . user_emails . add ( email ) \n else : \n self . user_emails . add ( user_email ) \n return \n def add_error ( self , openai_error = None , error_description = None , error_type = None ) : \n if openai_error != None : \n openai_error = openai_error . error \n if \"<STR_LIT>\" in openai_error : \n error_type = openai_error [ '<STR_LIT>' ] \n elif error_description and error_type : \n error_type = error_type", "output": "openai_error = error_description \n now = datetime . datetime . now ( ) \n curr_time = now . hour \n if curr_time == self . current_time_block : \n if error_type in self . unhandled_errors : \n self . unhandled_errors [ error_type ] += <NUM_LIT> \n else : \n self . unhandled_errors [ error_type ] = <NUM_LIT> \n if self . unhandled_errors [ error_type ] >= <NUM_LIT> : \n self . unhandled_errors [ error_type ] = <NUM_LIT> \n if self . set_cooldown and time . time ( ) - self . cooldown_start_time > <NUM_LIT> : \n self . send_alert ( error_type , openai_error ) \n self . set_cooldown ( ) \n elif self . set_cooldown == False : \n self . send_alert ( error_type , openai_error ) \n self . start_cooldown ( ) \n else : \n self . unhandled_errors [ error_type ] = <NUM_LIT> \n self . current_time_block = curr_time \n return"}, {"input": "import inspect \n from plugins . plugin import Plugin \n from common . log import logger \n from common import functions \n @ functions . singleton \n class PluginRegistry : \n def __init__ ( self ) : \n self . plugins = [ ] \n def register ( self , name : str , desire_priority : int = <NUM_LIT> , ** kwargs ) : \n def wrapper ( plugin_cls ) : \n plugin_cls . name = name \n plugin_cls . priority = desire_priority \n plugin_cls . desc = kwargs . get ( '<STR_LIT>' )", "output": "plugin_cls . author = kwargs . get ( '<STR_LIT>' ) \n plugin_cls . version = kwargs . get ( '<STR_LIT>' ) or \"<STR_LIT>\" \n plugin_cls . namecn = kwargs . get ( '<STR_LIT>' ) or name \n plugin_cls . hidden = kwargs . get ( '<STR_LIT>' ) or False \n plugin_cls . enabled = kwargs . get ( '<STR_LIT>' ) or True \n logger . info ( f\"<STR_LIT>\" ) \n return plugin_cls \n return wrapper \n def register_from_module ( self , module ) : \n plugins = [ ] \n for name , obj in inspect . getmembers ( module ) : \n if inspect . isclass ( obj ) and issubclass ( obj , Plugin ) and obj != Plugin : \n plugin_name = getattr ( obj , \"<STR_LIT>\" , None ) \n if plugin_name : \n plugin = obj ( ) \n plugin . name = plugin_name \n plugin . priority = getattr ( obj , \"<STR_LIT>\" , <NUM_LIT> ) \n plugin . desc = getattr ( obj , \"<STR_LIT>\" , None ) \n plugin . author = getattr ( obj , \"<STR_LIT>\" , None ) \n plugin . version = getattr ( obj , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n plugin . namecn = getattr ( obj , \"<STR_LIT>\" , plugin_name ) \n plugin . hidden = getattr ( obj , \"<STR_LIT>\" , False ) \n plugin . enabled = getattr ( obj , \"<STR_LIT>\" , True ) \n self . plugins . append ( plugin ) \n self . plugins . sort ( key = lambda x : x . priority , reverse = True ) \n def get_plugin ( self , name ) : \n plugin = next ( ( p for p in self . plugins if p . name . upper ( ) == name . upper ( ) ) , None ) \n return plugin \n def list_plugins ( self ) : \n return [ plugin for plugin in self . plugins ]"}, {"input": "from legged_gym import LEGGED_GYM_ROOT_DIR \n import os \n import code \n import isaacgym \n from legged_gym . envs import * \n from legged_gym . utils import get_args , export_policy_as_jit , task_registry , Logger \n from isaacgym import gymtorch , gymapi , gymutil \n import numpy as np \n import torch \n import cv2 \n from collections import deque \n import statistics \n import faulthandler \n from copy import deepcopy \n import matplotlib . pyplot as plt \n from time import time , sleep \n from legged_gym . utils import webviewer \n from tqdm import tqdm \n def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = \"<STR_LIT>\" ) : \n if checkpoint == - <NUM_LIT> : \n models = [ file for file in os . listdir ( root ) if model_name_include in file ] \n models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) \n model = models [ - <NUM_LIT> ] \n checkpoint = model . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n return model , checkpoint \n def play ( args ) : \n if args . web : \n web_viewer = webviewer . WebViewer ( ) \n faulthandler . enable ( ) \n exptid = args . exptid \n log_pth = \"<STR_LIT>\" . format ( args . proj_name ) + args . exptid \n env_cfg , train_cfg = task_registry . get_cfgs ( name = args . task ) \n if args . nodelay : \n env_cfg . domain_rand . action_delay_view = <NUM_LIT> \n env_cfg . env . num_envs = <NUM_LIT> \n env_cfg . env . episode_length_s = <NUM_LIT> \n env_cfg . commands . resampling_time = <NUM_LIT> \n env_cfg . terrain . num_rows = <NUM_LIT> \n env_cfg . terrain . num_cols = <NUM_LIT> \n env_cfg . terrain . height = [ <NUM_LIT> , <NUM_LIT> ] \n env_cfg . terrain . terrain_dict = { \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> } \n env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) \n env_cfg . terrain . curriculum = False", "output": "env_cfg . terrain . max_difficulty = False \n env_cfg . depth . angle = [ <NUM_LIT> , <NUM_LIT> ] \n env_cfg . noise . add_noise = True \n env_cfg . domain_rand . randomize_friction = True \n env_cfg . domain_rand . push_robots = True \n env_cfg . domain_rand . push_interval_s = <NUM_LIT> \n env_cfg . domain_rand . randomize_base_mass = False \n env_cfg . domain_rand . randomize_base_com = False \n depth_latent_buffer = [ ] \n env : LeggedRobot \n env , _ = task_registry . make_env ( name = args . task , args = args , env_cfg = env_cfg ) \n obs = env . get_observations ( ) \n if args . web : \n web_viewer . setup ( env ) \n train_cfg . runner . resume = True \n ppo_runner , train_cfg , log_pth = task_registry . make_alg_runner ( log_root = log_pth , env = env , name = args . task , args = args , train_cfg = train_cfg , return_log_dir = True ) \n policy = ppo_runner . get_inference_policy ( device = env . device ) \n if env . cfg . depth . use_camera : \n depth_encoder = ppo_runner . get_depth_encoder_inference_policy ( device = env . device ) \n total_steps = <NUM_LIT> \n rewbuffer = deque ( maxlen = total_steps ) \n lenbuffer = deque ( maxlen = total_steps ) \n num_waypoints_buffer = deque ( maxlen = total_steps ) \n time_to_fall_buffer = deque ( maxlen = total_steps ) \n edge_violation_buffer = deque ( maxlen = total_steps ) \n cur_reward_sum = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) \n cur_episode_length = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) \n cur_edge_violation = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) \n cur_time_from_start = torch . zeros ( env . num_envs , dtype = torch . float , device = env . device ) \n actions = torch . zeros ( env . num_envs , <NUM_LIT> , device = env . device , requires_grad = False ) \n infos = { } \n infos [ \"<STR_LIT>\" ] = env . depth_buffer . clone ( ) . to ( ppo_runner . device ) [ : , - <NUM_LIT> ] if ppo_runner . if_depth else None \n for i in tqdm ( range ( <NUM_LIT> ) ) : \n if env . cfg . depth . use_camera : \n if infos [ \"<STR_LIT>\" ] is not None : \n obs_student = obs [ : , : env . cfg . env . n_proprio ] \n obs_student [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> \n with torch . no_grad ( ) : \n depth_latent_and_yaw = depth_encoder ( infos [ \"<STR_LIT>\" ] , obs_student ) \n depth_latent = depth_latent_and_yaw [ : , : - <NUM_LIT> ] \n yaw = depth_latent_and_yaw [ : , - <NUM_LIT> : ] \n obs [ : , <NUM_LIT> : <NUM_LIT> ] = <NUM_LIT> * yaw \n else : \n depth_latent = None \n if hasattr ( ppo_runner . alg , \"<STR_LIT>\" ) : \n with torch . no_grad ( ) : \n actions = ppo_runner . alg . depth_actor ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) \n else : \n actions = policy ( obs . detach ( ) , hist_encoding = True , scandots_latent = depth_latent ) \n cur_goal_idx = env . cur_goal_idx . clone ( ) \n obs , _ , rews , dones , infos = env . step ( actions . detach ( ) ) \n if args . web : \n web_viewer . render ( fetch_results = True , \n step_graphics = True , \n render_all_camera_sensors = True , \n wait_for_page_load = True ) \n id = env . lookat_id \n edge_violation_buffer . extend ( env . feet_at_edge . sum ( dim = <NUM_LIT> ) . float ( ) . cpu ( ) . numpy ( ) . tolist ( ) ) \n cur_reward_sum += rews \n cur_episode_length += <NUM_LIT> \n cur_time_from_start += <NUM_LIT> \n new_ids = ( dones > <NUM_LIT> ) . nonzero ( as_tuple = False ) \n killed_ids = ( ( dones > <NUM_LIT> ) & ( ~ infos [ \"<STR_LIT>\" ] ) ) . nonzero ( as_tuple = False ) \n rewbuffer . extend ( cur_reward_sum [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) \n lenbuffer . extend ( cur_episode_length [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) \n num_waypoints_buffer . extend ( cur_goal_idx [ new_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) \n time_to_fall_buffer . extend ( cur_time_from_start [ killed_ids ] [ : , <NUM_LIT> ] . cpu ( ) . numpy ( ) . tolist ( ) ) \n cur_reward_sum [ new_ids ] = <NUM_LIT> \n cur_episode_length [ new_ids ] = <NUM_LIT> \n cur_edge_violation [ new_ids ] = <NUM_LIT> \n cur_time_from_start [ killed_ids ] = <NUM_LIT> \n rew_mean = statistics . mean ( rewbuffer ) \n rew_std = statistics . stdev ( rewbuffer ) \n len_mean = statistics . mean ( lenbuffer ) \n len_std = statistics . stdev ( lenbuffer ) \n num_waypoints_mean = np . mean ( np . array ( num_waypoints_buffer ) . astype ( float ) / <NUM_LIT> ) \n num_waypoints_std = np . std ( np . array ( num_waypoints_buffer ) . astype ( float ) / <NUM_LIT> ) \n edge_violation_mean = np . mean ( edge_violation_buffer ) \n edge_violation_std = np . std ( edge_violation_buffer ) \n print ( \"<STR_LIT>\" . format ( rew_mean , rew_std ) ) \n print ( \"<STR_LIT>\" . format ( len_mean , len_std ) ) \n print ( \"<STR_LIT>\" . format ( num_waypoints_mean , num_waypoints_std ) ) \n print ( \"<STR_LIT>\" . format ( edge_violation_mean , edge_violation_std ) ) \n if __name__ == '<STR_LIT>' : \n EXPORT_POLICY = False \n RECORD_FRAMES = False \n MOVE_CAMERA = False \n args = get_args ( ) \n play ( args )"}, {"input": "import torch . nn as nn \n import torch , numpy as np \n import torch . nn . functional as F \n from librosa . filters import mel \n class BiGRU ( nn . Module ) : \n def __init__ ( self , input_features , hidden_features , num_layers ) : \n super ( BiGRU , self ) . __init__ ( ) \n self . gru = nn . GRU ( \n input_features , \n hidden_features , \n num_layers = num_layers , \n batch_first = True , \n bidirectional = True , \n ) \n def forward ( self , x ) : \n return self . gru ( x ) [ <NUM_LIT> ] \n class ConvBlockRes ( nn . Module ) : \n def __init__ ( self , in_channels , out_channels , momentum = <NUM_LIT> ) : \n super ( ConvBlockRes , self ) . __init__ ( ) \n self . conv = nn . Sequential ( \n nn . Conv2d ( \n in_channels = in_channels , \n out_channels = out_channels , \n kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , \n stride = ( <NUM_LIT> , <NUM_LIT> ) , \n padding = ( <NUM_LIT> , <NUM_LIT> ) , \n bias = False , \n ) , \n nn . BatchNorm2d ( out_channels , momentum = momentum ) , \n nn . ReLU ( ) , \n nn . Conv2d ( \n in_channels = out_channels , \n out_channels = out_channels , \n kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , \n stride = ( <NUM_LIT> , <NUM_LIT> ) , \n padding = ( <NUM_LIT> , <NUM_LIT> ) , \n bias = False , \n ) , \n nn . BatchNorm2d ( out_channels , momentum = momentum ) , \n nn . ReLU ( ) , \n ) \n if in_channels != out_channels : \n self . shortcut = nn . Conv2d ( in_channels , out_channels , ( <NUM_LIT> , <NUM_LIT> ) ) \n self . is_shortcut = True \n else : \n self . is_shortcut = False \n def forward ( self , x ) : \n if self . is_shortcut : \n return self . conv ( x ) + self . shortcut ( x ) \n else : \n return self . conv ( x ) + x \n class Encoder ( nn . Module ) : \n def __init__ ( \n self , \n in_channels , \n in_size , \n n_encoders , \n kernel_size , \n n_blocks , \n out_channels = <NUM_LIT> , \n momentum = <NUM_LIT> , \n ) : \n super ( Encoder , self ) . __init__ ( ) \n self . n_encoders = n_encoders \n self . bn = nn . BatchNorm2d ( in_channels , momentum = momentum ) \n self . layers = nn . ModuleList ( ) \n self . latent_channels = [ ] \n for i in range ( self . n_encoders ) : \n self . layers . append ( \n ResEncoderBlock ( \n in_channels , out_channels , kernel_size , n_blocks , momentum = momentum \n ) \n ) \n self . latent_channels . append ( [ out_channels , in_size ] ) \n in_channels = out_channels \n out_channels *= <NUM_LIT> \n in_size //= <NUM_LIT> \n self . out_size = in_size \n self . out_channel = out_channels \n def forward ( self , x ) : \n concat_tensors = [ ] \n x = self . bn ( x ) \n for i in range ( self . n_encoders ) : \n _ , x = self . layers [ i ] ( x ) \n concat_tensors . append ( _ ) \n return x , concat_tensors \n class ResEncoderBlock ( nn . Module ) : \n def __init__ ( \n self , in_channels , out_channels , kernel_size , n_blocks = <NUM_LIT> , momentum = <NUM_LIT> \n ) : \n super ( ResEncoderBlock , self ) . __init__ ( ) \n self . n_blocks = n_blocks \n self . conv = nn . ModuleList ( ) \n self . conv . append ( ConvBlockRes ( in_channels , out_channels , momentum ) ) \n for i in range ( n_blocks - <NUM_LIT> ) : \n self . conv . append ( ConvBlockRes ( out_channels , out_channels , momentum ) ) \n self . kernel_size = kernel_size \n if self . kernel_size is not None : \n self . pool = nn . AvgPool2d ( kernel_size = kernel_size ) \n def forward ( self , x ) : \n for i in range ( self . n_blocks ) : \n x = self . conv [ i ] ( x ) \n if self . kernel_size is not None : \n return x , self . pool ( x ) \n else : \n return x \n class Intermediate ( nn . Module ) : \n def __init__ ( self , in_channels , out_channels , n_inters , n_blocks , momentum = <NUM_LIT> ) : \n super ( Intermediate , self ) . __init__ ( ) \n self . n_inters = n_inters \n self . layers = nn . ModuleList ( ) \n self . layers . append ( \n ResEncoderBlock ( in_channels , out_channels , None , n_blocks , momentum ) \n ) \n for i in range ( self . n_inters - <NUM_LIT> ) : \n self . layers . append ( \n ResEncoderBlock ( out_channels , out_channels , None , n_blocks , momentum ) \n ) \n def forward ( self , x ) : \n for i in range ( self . n_inters ) : \n x = self . layers [ i ] ( x ) \n return x \n class ResDecoderBlock ( nn . Module ) : \n def __init__ ( self , in_channels , out_channels , stride , n_blocks = <NUM_LIT> , momentum = <NUM_LIT> ) : \n super ( ResDecoderBlock , self ) . __init__ ( ) \n out_padding = ( <NUM_LIT> , <NUM_LIT> ) if stride == ( <NUM_LIT> , <NUM_LIT> ) else ( <NUM_LIT> , <NUM_LIT> ) \n self . n_blocks = n_blocks \n self . conv1 = nn . Sequential ( \n nn . ConvTranspose2d ( \n in_channels = in_channels , \n out_channels = out_channels , \n kernel_size = ( <NUM_LIT> , <NUM_LIT> ) , \n stride = stride , \n padding = ( <NUM_LIT> , <NUM_LIT> ) , \n output_padding = out_padding , \n bias = False , \n ) , \n nn . BatchNorm2d ( out_channels , momentum = momentum ) , \n nn . ReLU ( ) , \n ) \n self . conv2 = nn . ModuleList ( ) \n self . conv2 . append ( ConvBlockRes ( out_channels * <NUM_LIT> , out_channels , momentum ) ) \n for i in range ( n_blocks - <NUM_LIT> ) : \n self . conv2 . append ( ConvBlockRes ( out_channels , out_channels , momentum ) ) \n def forward ( self , x , concat_tensor ) : \n x = self . conv1 ( x ) \n x = torch . cat ( ( x , concat_tensor ) , dim = <NUM_LIT> ) \n for i in range ( self . n_blocks ) : \n x = self . conv2 [ i ] ( x ) \n return x \n class Decoder ( nn . Module ) : \n def __init__ ( self , in_channels , n_decoders , stride , n_blocks , momentum = <NUM_LIT> ) : \n super ( Decoder , self ) . __init__ ( ) \n self . layers = nn . ModuleList ( ) \n self . n_decoders = n_decoders \n for i in range ( self . n_decoders ) : \n out_channels = in_channels // <NUM_LIT> \n self . layers . append ( \n ResDecoderBlock ( in_channels , out_channels , stride , n_blocks , momentum ) \n ) \n in_channels = out_channels \n def forward ( self , x , concat_tensors ) : \n for i in range ( self . n_decoders ) : \n x = self . layers [ i ] ( x , concat_tensors [ - <NUM_LIT> - i ] ) \n return x \n class DeepUnet ( nn . Module ) : \n def __init__ ( \n self , \n kernel_size , \n n_blocks , \n en_de_layers = <NUM_LIT> , \n inter_layers = <NUM_LIT> , \n in_channels = <NUM_LIT> , \n en_out_channels = <NUM_LIT> , \n ) : \n super ( DeepUnet , self ) . __init__ ( ) \n self . encoder = Encoder ( \n in_channels , <NUM_LIT> , en_de_layers , kernel_size , n_blocks , en_out_channels \n ) \n self . intermediate = Intermediate ( \n self . encoder . out_channel // <NUM_LIT> , \n self . encoder . out_channel , \n inter_layers , \n n_blocks , \n ) \n self . decoder = Decoder ( \n self . encoder . out_channel , en_de_layers , kernel_size , n_blocks \n ) \n def forward ( self , x ) : \n x , concat_tensors = self . encoder ( x ) \n x = self . intermediate ( x ) \n x = self . decoder ( x , concat_tensors ) \n return x \n class E2E ( nn . Module ) : \n def __init__ ( \n self , \n n_blocks , \n n_gru , \n kernel_size , \n en_de_layers = <NUM_LIT> , \n inter_layers = <NUM_LIT> , \n in_channels = <NUM_LIT> , \n en_out_channels = <NUM_LIT> , \n ) : \n super ( E2E , self ) . __init__ ( ) \n self . unet = DeepUnet ( \n kernel_size , \n n_blocks , \n en_de_layers , \n inter_layers , \n in_channels , \n en_out_channels , \n ) \n self . cnn = nn . Conv2d ( en_out_channels , <NUM_LIT> , ( <NUM_LIT> , <NUM_LIT> ) , padding = ( <NUM_LIT> , <NUM_LIT> ) ) \n if n_gru : \n self . fc = nn . Sequential ( \n BiGRU ( <NUM_LIT> * <NUM_LIT> , <NUM_LIT> , n_gru ) , \n nn . Linear ( <NUM_LIT> , <NUM_LIT> ) , \n nn . Dropout ( <NUM_LIT> ) , \n nn . Sigmoid ( ) , \n ) \n def forward ( self , mel ) : \n mel = mel . transpose ( - <NUM_LIT> , - <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) \n x = self . cnn ( self . unet ( mel ) ) . transpose ( <NUM_LIT> , <NUM_LIT> ) . flatten ( - <NUM_LIT> ) \n x = self . fc ( x ) \n return x \n class MelSpectrogram ( torch . nn . Module ) : \n def __init__ ( \n self , \n is_half , \n n_mel_channels , \n sampling_rate , \n win_length , \n hop_length , \n n_fft = None , \n mel_fmin = <NUM_LIT> , \n mel_fmax = None , \n clamp = <NUM_LIT> , \n ) : \n super ( ) . __init__ ( ) \n n_fft = win_length if n_fft is None else n_fft \n self . hann_window = { } \n mel_basis = mel ( \n sr = sampling_rate , \n n_fft = n_fft , \n n_mels = n_mel_channels , \n fmin = mel_fmin , \n fmax = mel_fmax , \n htk = True , \n ) \n mel_basis = torch . from_numpy ( mel_basis ) . float ( ) \n self . register_buffer ( \"<STR_LIT>\" , mel_basis ) \n self . n_fft = win_length if n_fft is None else n_fft \n self . hop_length = hop_length \n self . win_length = win_length \n self . sampling_rate = sampling_rate \n self . n_mel_channels = n_mel_channels \n self . clamp = clamp \n self . is_half = is_half \n def forward ( self , audio , keyshift = <NUM_LIT> , speed = <NUM_LIT> , center = True ) : \n factor = <NUM_LIT> ** ( keyshift / <NUM_LIT> ) \n n_fft_new = int ( np . round ( self . n_fft * factor ) ) \n win_length_new = int ( np . round ( self . win_length * factor ) ) \n hop_length_new = int ( np . round ( self . hop_length * speed ) ) \n keyshift_key = str ( keyshift ) + \"<STR_LIT>\" + str ( audio . device ) \n if keyshift_key not in self . hann_window : \n self . hann_window [ keyshift_key ] = torch . hann_window ( win_length_new ) . to ( \n audio . device \n ) \n fft = torch . stft ( \n audio , \n n_fft = n_fft_new , \n hop_length = hop_length_new , \n win_length = win_length_new , \n window = self . hann_window [ keyshift_key ] , \n center = center , \n return_complex = True , \n ) \n magnitude = torch . sqrt ( fft . real . pow ( <NUM_LIT> ) + fft . imag . pow ( <NUM_LIT> ) ) \n if keyshift != <NUM_LIT> : \n size = self . n_fft // <NUM_LIT> + <NUM_LIT> \n resize = magnitude . size ( <NUM_LIT> ) \n if resize < size : \n magnitude = F . pad ( magnitude , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , size - resize ) ) \n magnitude = magnitude [ : , : size , : ] * self . win_length / win_length_new \n mel_output = torch . matmul ( self . mel_basis , magnitude ) \n if self . is_half == True : \n mel_output = mel_output . half ( ) \n log_mel_spec = torch . log ( torch . clamp ( mel_output , min = self . clamp ) ) \n return log_mel_spec \n class RMVPE : \n def __init__ ( self , model_path , is_half , device = None ) : \n self . resample_kernel = { } \n model = E2E ( <NUM_LIT> , <NUM_LIT> , ( <NUM_LIT> , <NUM_LIT> ) ) \n ckpt = torch . load ( model_path , map_location = \"<STR_LIT>\" ) \n model . load_state_dict ( ckpt ) \n model . eval ( ) \n if is_half == True : \n model = model . half ( ) \n self . model = model \n self . resample_kernel = { } \n self . is_half = is_half \n if device is None : \n device = \"<STR_LIT>\" if torch . cuda . is_available ( ) else \"<STR_LIT>\" \n self . device = device \n self . mel_extractor = MelSpectrogram ( \n is_half , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , None , <NUM_LIT> , <NUM_LIT> \n ) . to ( device ) \n self . model = self . model . to ( device ) \n cents_mapping = <NUM_LIT> * np . arange ( <NUM_LIT> ) + <NUM_LIT> \n self . cents_mapping = np . pad ( cents_mapping , ( <NUM_LIT> , <NUM_LIT> ) ) \n def mel2hidden ( self , mel ) : \n with torch . no_grad ( ) : \n n_frames = mel . shape [ - <NUM_LIT> ] \n mel = F . pad ( \n mel , ( <NUM_LIT> , <NUM_LIT> * ( ( n_frames - <NUM_LIT> ) // <NUM_LIT> + <NUM_LIT> ) - n_frames ) , mode = \"<STR_LIT>\" \n ) \n hidden = self . model ( mel ) \n return hidden [ : , : n_frames ] \n def decode ( self , hidden , thred = <NUM_LIT> ) : \n cents_pred = self . to_local_average_cents ( hidden , thred = thred ) \n f0 = <NUM_LIT> * ( <NUM_LIT> ** ( cents_pred / <NUM_LIT> ) ) \n f0 [ f0 == <NUM_LIT> ] = <NUM_LIT> \n return f0 \n def infer_from_audio ( self , audio , thred = <NUM_LIT> ) : \n audio = torch . from_numpy ( audio ) . float ( ) . to ( self . device ) . unsqueeze ( <NUM_LIT> ) \n mel = self . mel_extractor ( audio , center = True ) \n hidden = self . mel2hidden ( mel ) \n hidden = hidden . squeeze ( <NUM_LIT> ) . cpu ( ) . numpy ( ) \n if self . is_half == True :", "output": "hidden = hidden . astype ( \"<STR_LIT>\" ) \n f0 = self . decode ( hidden , thred = thred ) \n return f0 \n def to_local_average_cents ( self , salience , thred = <NUM_LIT> ) : \n center = np . argmax ( salience , axis = <NUM_LIT> ) \n salience = np . pad ( salience , ( ( <NUM_LIT> , <NUM_LIT> ) , ( <NUM_LIT> , <NUM_LIT> ) ) ) \n center += <NUM_LIT> \n todo_salience = [ ] \n todo_cents_mapping = [ ] \n starts = center - <NUM_LIT> \n ends = center + <NUM_LIT> \n for idx in range ( salience . shape [ <NUM_LIT> ] ) : \n todo_salience . append ( salience [ : , starts [ idx ] : ends [ idx ] ] [ idx ] ) \n todo_cents_mapping . append ( self . cents_mapping [ starts [ idx ] : ends [ idx ] ] ) \n todo_salience = np . array ( todo_salience ) \n todo_cents_mapping = np . array ( todo_cents_mapping ) \n product_sum = np . sum ( todo_salience * todo_cents_mapping , <NUM_LIT> ) \n weight_sum = np . sum ( todo_salience , <NUM_LIT> ) \n devided = product_sum / weight_sum \n maxx = np . max ( salience , axis = <NUM_LIT> ) \n devided [ maxx <= thred ] = <NUM_LIT> \n return devided"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO", "output": "class A1ParkourCfg ( LeggedRobotCfg ) : \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class control ( LeggedRobotCfg . control ) : \n control_type = '<STR_LIT>' \n stiffness = { '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset ( LeggedRobotCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n terminate_after_contacts_on = [ \"<STR_LIT>\" ] \n self_collisions = <NUM_LIT> \n class rewards ( LeggedRobotCfg . rewards ) : \n soft_dof_pos_limit = <NUM_LIT> \n base_height_target = <NUM_LIT> \n class A1ParkourCfgPPO ( LeggedRobotCfgPPO ) : \n class algorithm ( LeggedRobotCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT> \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>'"}, {"input": "import sys \n sys . path . append ( '<STR_LIT>' )", "output": "import os \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n import openai \n from Model import Model \n openai . api_key = os . getenv ( '<STR_LIT>' ) \n obj = Model ( openai . ChatCompletion . create ) \n create_completion = obj . get_original_completion ( ) \n print ( create_completion ( model = \"<STR_LIT>\" , prompt = \"<STR_LIT>\" ) )"}, {"input": "from common import const \n def create_bot ( model_type ) : \n if model_type == const . OPEN_AI : \n from model . openai . open_ai_model import OpenAIModel \n return OpenAIModel ( ) \n elif model_type == const . CHATGPT : \n from model . openai . chatgpt_model import ChatGPTModel \n return ChatGPTModel ( ) \n elif model_type == const . BAIDU : \n from model . baidu . yiyan_model import YiyanModel \n return YiyanModel ( )", "output": "elif model_type == const . BING : \n from model . bing . new_bing_model import BingModel \n return BingModel ( ) \n elif model_type == const . BARD : \n from model . google . bard_model import BardModel \n return BardModel ( ) \n raise RuntimeError"}, {"input": "from __future__ import annotations \n import os \n from typing import AsyncGenerator \n from fastapi import FastAPI \n import svcs \n config = { \"<STR_LIT>\" : os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) } \n class Database : \n @ classmethod \n async def connect ( cls , db_url : str ) -> Database : \n return Database ( )", "output": "async def get_user ( self , user_id : int ) -> dict [ str , str ] : \n return { } \n @ svcs . fastapi . lifespan \n async def lifespan ( \n app : FastAPI , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n async def connect_to_db ( ) -> Database : \n return await Database . connect ( config [ \"<STR_LIT>\" ] ) \n registry . register_factory ( Database , connect_to_db ) \n yield { \"<STR_LIT>\" : \"<STR_LIT>\" } \n app = FastAPI ( lifespan = lifespan ) \n @ app . get ( \"<STR_LIT>\" ) \n async def get_user ( user_id : int , services : svcs . fastapi . DepContainer ) -> dict : \n db = await services . aget ( Database ) \n try : \n return { \"<STR_LIT>\" : await db . get_user ( user_id ) } \n except Exception as e : \n return { \"<STR_LIT>\" : e . args [ <NUM_LIT> ] }"}, {"input": "from unittest . mock import Mock \n import pytest \n import svcs \n from tests . helpers import nop \n from tests . ifaces import AnotherService , Interface , Service \n try : \n import flask \n from svcs . flask import teardown \n except ImportError : \n pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _app ( ) : \n return flask . Flask ( \"<STR_LIT>\" ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _clean_app_ctx ( registry , app ) : \n svcs . flask . init_app ( app , registry = registry ) \n with app . app_context ( ) as ctx : \n yield ctx \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _container ( clean_app_ctx ) : \n return svcs . flask . svcs_from ( ) \n @ pytest . mark . usefixtures ( \"<STR_LIT>\" ) \n class TestFlask : \n def test_register_value_multiple ( self , registry ) : \n registry . register_value ( Service , <NUM_LIT> ) \n registry . register_value ( AnotherService , <NUM_LIT> ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == svcs . flask . get ( Service , AnotherService ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == svcs . flask . get ( Service , AnotherService ) \n def test_cleanup_added ( self , registry , app ) : \n cleanup1 = Mock ( ) \n cleanup2 = Mock ( ) \n def factory1 ( ) : \n yield Service ( ) \n cleanup1 ( ) \n def factory2 ( ) : \n yield AnotherService ( ) \n cleanup2 ( ) \n registry . register_factory ( Service , factory1 ) \n svcs . flask . register_factory ( app , AnotherService , factory2 ) \n svc1 = svcs . flask . get ( Service ) \n svc2 = svcs . flask . get ( AnotherService ) \n assert isinstance ( svc1 , Service ) \n assert isinstance ( svc2 , AnotherService ) \n assert <NUM_LIT> == len ( flask . g . svcs_container . _on_close ) \n teardown ( None ) \n cleanup1 . assert_called_once_with ( ) \n cleanup2 . assert_called_once_with ( ) \n def test_overwrite_value ( self , registry , app ) : \n registry . register_value ( Interface , Service ( ) , ping = nop ) \n assert isinstance ( svcs . flask . get ( Interface ) , Interface ) \n container = svcs . flask . svcs_from ( ) \n assert container . _instantiated \n svcs . flask . overwrite_value ( Interface , AnotherService ( ) ) \n assert not container . _instantiated \n assert isinstance ( svcs . flask . get ( Interface ) , AnotherService ) \n assert [ ] == svcs . flask . get_pings ( ) \n def test_overwrite_factory ( self , app ) : \n svcs . flask . register_value ( app , Interface , Service ( ) , ping = nop ) \n assert isinstance ( svcs . flask . get ( Interface ) , Interface ) \n container = svcs . flask . svcs_from ( ) \n assert container . _instantiated \n svcs . flask . overwrite_factory ( Interface , AnotherService ) \n assert not container . _instantiated \n assert isinstance ( svcs . flask . get ( Interface ) , AnotherService ) \n assert [ ] == svcs . flask . get_pings ( ) \n def test_cache ( self , app ) : \n svcs . flask . register_factory ( app , Interface , Service ) \n assert svcs . flask . get ( Interface ) is svcs . flask . get ( Interface ) \n def test_not_found ( self ) : \n with pytest . raises ( svcs . exceptions . ServiceNotFoundError ) : \n svcs . flask . get ( Interface ) \n def test_get_pingeable ( self , app ) : \n svcs . flask . register_factory ( app , Service , Service ) \n svcs . flask . register_factory ( \n app , AnotherService , AnotherService , ping = nop \n ) \n assert [ AnotherService ] == [ \n ping . _svc_type for ping in svcs . flask . get_pings ( ) \n ]", "output": "@ pytest . mark . asyncio ( ) \n async def test_teardown_warns_on_async_on_close ( self , container ) : \n async def factory ( ) : \n yield Service ( ) \n container . registry . register_factory ( Service , factory ) \n await container . aget ( Service ) \n with pytest . warns ( UserWarning ) as wi : \n teardown ( None ) \n w = wi . pop ( ) \n assert <NUM_LIT> == len ( wi . list ) \n assert ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" == w . message . args [ <NUM_LIT> ] \n ) \n def test_register_factory_get_abstract ( self , registry , container ) : \n registry . register_factory ( Interface , Service ) \n svc = container . get_abstract ( Interface ) \n assert isinstance ( svc , Interface ) \n assert svc is svcs . flask . get_abstract ( Interface ) \n def test_svcs_from ( self , container ) : \n assert ( \n container \n is svcs . flask . svcs_from ( ) \n is flask . g . svcs_container \n is svcs . flask . svcs_from ( ) \n ) \n def test_local_proxy_container ( self , container ) : \n assert ( \n container \n == flask . g . svcs_container \n == svcs . flask . svcs_from ( ) \n == svcs . flask . container \n ) \n def test_local_proxy_registry ( self , registry , app ) : \n assert ( \n registry \n == svcs . flask . get_registry ( flask . current_app ) \n == svcs . flask . get_registry ( app ) \n == svcs . flask . get_registry ( ) \n ) \n class TestNonContextHelpers : \n def test_get_registry ( self , registry , app ) : \n svcs . flask . init_app ( app , registry = registry ) \n assert registry is svcs . flask . get_registry ( app ) \n def test_register_factory_helper ( self , registry , app ) : \n svcs . flask . init_app ( app , registry = registry ) \n svcs . flask . register_factory ( app , Interface , Service ) \n assert Interface in registry . _services \n def test_register_value_helper ( self , registry , app ) : \n svcs . flask . init_app ( app , registry = registry ) \n svcs . flask . register_value ( app , Interface , <NUM_LIT> ) \n assert Interface in registry . _services \n class TestInitApp : \n def test_implicit_registry ( self ) : \n app = flask . Flask ( \"<STR_LIT>\" ) \n svcs . flask . init_app ( app ) \n assert isinstance ( app . extensions [ \"<STR_LIT>\" ] , svcs . Registry ) \n def test_explicit_registry ( self ) : \n registry = svcs . Registry ( ) \n app = flask . Flask ( \"<STR_LIT>\" ) \n svcs . flask . init_app ( app , registry = registry ) \n assert registry is app . extensions [ \"<STR_LIT>\" ] \n class TestCloseRegistry : \n def test_nop ( self ) : \n app = flask . Flask ( \"<STR_LIT>\" ) \n svcs . flask . close_registry ( app ) \n def test_closes ( self , app ) : \n close = Mock ( ) \n svcs . flask . init_app ( app ) \n svcs . flask . register_factory ( \n app , Interface , Service , on_registry_close = close \n ) \n svcs . flask . close_registry ( app ) \n assert close . called"}, {"input": "try : \n import mutagen . wave \n from component . music_tag . id3 import Id3File \n class WaveId3File ( Id3File ) : \n tag_format = \"<STR_LIT>\"", "output": "mutagen_kls = mutagen . wave . WAVE \n def __init__ ( self , filename , ** kwargs ) : \n super ( WaveId3File , self ) . __init__ ( filename , ** kwargs ) \n except ImportError : \n pass"}, {"input": "import os \n import socket \n import subprocess \n import time \n import requests \n import sys \n import json \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n env_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n host = \"<STR_LIT>\" \n port = <NUM_LIT> \n sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM )", "output": "sock . settimeout ( <NUM_LIT> ) \n def start_flask ( ) : \n try : \n sock . connect ( ( host , port ) ) \n print ( \n f\"<STR_LIT>\" \n ) \n print ( \"<STR_LIT>\" ) \n sock . close ( ) \n requests . post ( \"<STR_LIT>\" ) \n time . sleep ( <NUM_LIT> ) \n script_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n try : \n subprocess . Popen ( \n [ env_path , script_path ] , creationflags = subprocess . CREATE_NEW_CONSOLE \n ) \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n print ( e ) \n except Exception as e : \n sock . close ( ) \n script_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n try : \n subprocess . Popen ( \n [ env_path , script_path ] , creationflags = subprocess . CREATE_NEW_CONSOLE \n ) \n except Exception as e : \n print ( \"<STR_LIT>\" ) \n print ( e ) \n def load_config_flask ( ) : \n with open ( config_file , \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n return config [ \"<STR_LIT>\" ] \n def save_config ( value ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n config [ \"<STR_LIT>\" ] = value \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( config , file , indent = <NUM_LIT> )"}, {"input": "def funcion ( ) : \n raise ExceptionGroup ( \"<STR_LIT>\" , [ OSError ( <NUM_LIT> ) , SystemError ( <NUM_LIT> ) , \n ExceptionGroup ( \"<STR_LIT>\" , [ OSError ( <NUM_LIT> ) , RecursionError ( <NUM_LIT> ) ] ) ] ) \n try : \n funcion ( ) \n except * OSError as err : \n print ( \"<STR_LIT>\" ) \n except * SystemError as err : \n print ( \"<STR_LIT>\" ) \n except * RecursionError as err : \n print ( \"<STR_LIT>\" ) \n try : \n raise TypeError ( \"<STR_LIT>\" ) \n except Exception as error : \n error . add_note ( \"<STR_LIT>\" ) \n error . add_note ( \"<STR_LIT>\" ) \n error . add_note ( \"<STR_LIT>\" ) \n print ( error ) \n def funcion1 ( ) : \n raise OSError ( \"<STR_LIT>\" ) \n problemas = [ ] \n for i in range ( <NUM_LIT> ) : \n try : \n funcion1 ( )", "output": "except Exception as error : \n error . add_note ( f\"<STR_LIT>\" ) \n problemas . append ( error ) \n raise ExceptionGroup ( \"<STR_LIT>\" , problemas )"}, {"input": "import time \n import sys \n from segment_anything import sam_model_registry \n from segment_anything . automatic_mask_generator import SamAutomaticMaskGenerator \n from . mask_creation_utils import get_groups_simple , refine_groups_simple , FinalGrouping , FinalGroup , get_points_from_canny_greedy \n from . efficient_mask import EfficientMask \n from PIL import Image \n import numpy as np \n import os \n import base64 \n from io import BytesIO \n import cv2 \n import json \n from typing import TypedDict , List \n LOW = <NUM_LIT> \n HIGH = <NUM_LIT> \n SETEV_MODEL_ROOT = '<STR_LIT>' \n ANNOTATE_ROOT = os . path . dirname ( os . path . dirname ( __file__ ) ) \n SOURCE_DIR = os . path . join ( ANNOTATE_ROOT , \"<STR_LIT>\" ) \n OUT_DIR = os . path . join ( ANNOTATE_ROOT , \"<STR_LIT>\" ) \n class SAMResult ( TypedDict ) : \n segmentation : np . ndarray \n bbox : List [ float ] \n area : int \n predicted_iou : float \n point_coords : List [ List [ float ] ] \n stability_score : float \n crop_box : List [ float ] \n def fold_group_tree ( g : FinalGrouping ) : \n def fold_group ( subg : FinalGroup ) : \n outer_mask = subg [ '<STR_LIT>' ] \n mask_img = Image . fromarray ( np . uint8 ( outer_mask . mask * <NUM_LIT> ) ) \n mask_img = mask_img . convert ( '<STR_LIT>' ) \n maskbuf = BytesIO ( ) \n mask_img . save ( maskbuf , format = '<STR_LIT>' , bits = <NUM_LIT> , optimize = True ) \n mask_bytes = maskbuf . getvalue ( ) \n as_base64 = base64 . b64encode ( mask_bytes ) \n as_str = as_base64 . decode ( '<STR_LIT>' ) \n ( t , l ) , ( b , r ) = subg [ '<STR_LIT>' ] . get_tlbr ( ) \n return { \n '<STR_LIT>' : as_str , \n '<STR_LIT>' : int ( outer_mask . get_size ( ) ) , \n '<STR_LIT>' : ( ( int ( t ) , int ( l ) ) , ( int ( b ) , int ( r ) ) ) , \n '<STR_LIT>' : { \n idx : fold_group ( subsubg ) for ( idx , subsubg ) in subg [ '<STR_LIT>' ] . items ( ) \n } \n } \n return { \n idx : fold_group ( subg ) for ( idx , subg ) in g . items ( ) \n } \n def group_outputs ( outputs : List [ SAMResult ] ) -> FinalGrouping : \n as_efficient_masks : List [ EfficientMask ] = [ \n EfficientMask ( \n res [ '<STR_LIT>' ] , \n res [ '<STR_LIT>' ] * ( res [ '<STR_LIT>' ] ** <NUM_LIT> ) , \n size = res [ '<STR_LIT>' ] , \n ) for res in outputs \n ] \n in_order = sorted ( as_efficient_masks , key = lambda x : x . get_size ( ) , reverse = True )", "output": "return get_groups_simple ( in_order ) \n def main ( ) : \n all_images = os . listdir ( SOURCE_DIR ) \n target_images = all_images [ LOW : HIGH ] \n sam_checkpoint = SETEV_MODEL_ROOT \n model_type = \"<STR_LIT>\" \n device = \"<STR_LIT>\" \n sam = sam_model_registry [ model_type ] ( checkpoint = sam_checkpoint ) \n sam . to ( device = device ) \n generator = SamAutomaticMaskGenerator ( \n sam , \n points_per_side = <NUM_LIT> , \n points_per_batch = <NUM_LIT> , \n pred_iou_thresh = <NUM_LIT> , \n stability_score_thresh = <NUM_LIT> , \n stability_score_offset = <NUM_LIT> , \n box_nms_thresh = <NUM_LIT> , \n min_mask_region_area = <NUM_LIT> , \n output_mode = \"<STR_LIT>\" , \n ) \n first_start = time . time ( ) \n for idx , img in enumerate ( target_images ) : \n try : \n start_time = time . time ( ) \n path = os . path . join ( SOURCE_DIR , img ) \n img_array = cv2 . imread ( path ) \n img_array = cv2 . cvtColor ( img_array , cv2 . COLOR_BGR2RGB ) \n canny_points = get_points_from_canny_greedy ( img_array , distance_threshold = <NUM_LIT> , jitter_amount = <NUM_LIT> , num_extra = <NUM_LIT> ) \n if len ( canny_points ) == <NUM_LIT> : \n canny_results = [ ] \n print ( f\"<STR_LIT>\" ) \n else : \n points_for_sam = np . array ( [ \n [ pt [ <NUM_LIT> ] / img_array . shape [ <NUM_LIT> ] , pt [ <NUM_LIT> ] / img_array . shape [ <NUM_LIT> ] ] for pt in canny_points \n ] ) \n canny_generator = SamAutomaticMaskGenerator ( \n sam , \n points_per_side = None , \n point_grids = points_for_sam , \n points_per_batch = <NUM_LIT> , \n pred_iou_thresh = <NUM_LIT> , \n stability_score_thresh = <NUM_LIT> , \n stability_score_offset = <NUM_LIT> , \n box_nms_thresh = <NUM_LIT> , \n min_mask_region_area = <NUM_LIT> , \n output_mode = \"<STR_LIT>\" , \n ) \n canny_results = canny_generator . generate ( img_array ) \n print ( f\"<STR_LIT>\" ) \n result = generator . generate ( img_array ) \n print ( f\"<STR_LIT>\" ) \n result += canny_results \n grouped = group_outputs ( result ) \n refined = refine_groups_simple ( grouped ) \n folded = fold_group_tree ( refined ) \n with open ( os . path . join ( OUT_DIR , img + \"<STR_LIT>\" ) , '<STR_LIT>' ) as json_outf : \n json . dump ( folded , json_outf ) \n print ( f\"<STR_LIT>\" ) \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n main ( )"}, {"input": "from concurrent . futures import ThreadPoolExecutor \n import io \n import requests \n import telebot \n from common . log import logger \n from channel . channel import Channel \n from config import conf \n bot = telebot . TeleBot ( token = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ bot . message_handler ( commands = [ '<STR_LIT>' ] ) \n def send_welcome ( message ) : \n bot . send_message ( message . chat . id , \"<STR_LIT>\" , parse_mode = \"<STR_LIT>\" ) \n @ bot . message_handler ( content_types = [ '<STR_LIT>' ] ) \n def send_welcome ( msg ) : \n TelegramChannel ( ) . handle ( msg ) \n class TelegramChannel ( Channel ) : \n def __init__ ( self ) : \n pass \n def startup ( self ) :", "output": "logger . info ( \"<STR_LIT>\" ) \n bot . infinity_polling ( ) \n def handle ( self , msg ) : \n logger . debug ( \"<STR_LIT>\" + msg . text ) \n thread_pool . submit ( self . _dosend , msg . text , msg ) \n def _dosend ( self , query , msg ) : \n context = dict ( ) \n context [ '<STR_LIT>' ] = str ( msg . chat . id ) \n reply_text = super ( ) . build_reply_content ( query , context ) \n logger . info ( '<STR_LIT>' . format ( reply_text ) ) \n bot . reply_to ( msg , reply_text )"}, {"input": "import json \n import hmac \n import hashlib \n import base64 \n import time \n import requests \n from urllib . parse import quote_plus \n from common import log \n from flask import Flask , request , render_template , make_response \n from common import const \n from common import functions \n from config import channel_conf \n from config import channel_conf_val \n from channel . channel import Channel \n from urllib import request as url_request \n from channel . feishu . store import MemoryStore \n class FeiShuChannel ( Channel ) : \n def __init__ ( self ) : \n self . app_id = channel_conf ( \n const . FEISHU ) . get ( '<STR_LIT>' ) \n self . app_secret = channel_conf ( \n const . FEISHU ) . get ( '<STR_LIT>' ) \n self . verification_token = channel_conf ( \n const . FEISHU ) . get ( '<STR_LIT>' ) \n log . info ( \"<STR_LIT>\" . format ( \n self . app_id , self . app_secret , self . verification_token ) ) \n self . memory_store = MemoryStore ( ) \n def startup ( self ) : \n http_app . run ( host = '<STR_LIT>' , port = channel_conf ( \n const . FEISHU ) . get ( '<STR_LIT>' ) ) \n def get_tenant_access_token ( self ) : \n url = \"<STR_LIT>\" \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n req_body = { \n \"<STR_LIT>\" : self . app_id , \n \"<STR_LIT>\" : self . app_secret \n } \n data = bytes ( json . dumps ( req_body ) , encoding = '<STR_LIT>' ) \n req = url_request . Request ( url = url , data = data , \n headers = headers , method = '<STR_LIT>' ) \n try : \n response = url_request . urlopen ( req ) \n except Exception as e : \n print ( e . read ( ) . decode ( ) ) \n return \"<STR_LIT>\" \n rsp_body = response . read ( ) . decode ( '<STR_LIT>' ) \n rsp_dict = json . loads ( rsp_body ) \n code = rsp_dict . get ( \"<STR_LIT>\" , - <NUM_LIT> ) \n if code != <NUM_LIT> : \n print ( \"<STR_LIT>\" , code ) \n return \"<STR_LIT>\" \n return rsp_dict . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def notify_feishu ( self , token , receive_type , receive_id , at_id , answer ) : \n log . info ( \"<STR_LIT>\" , \n receive_type , receive_id ) \n url = \"<STR_LIT>\" \n params = { \"<STR_LIT>\" : receive_type } \n text = answer . lstrip ( ) \n log . info ( \"<STR_LIT>\" , text ) \n msgContent = { \n \"<STR_LIT>\" : text , \n } \n req = { \n \"<STR_LIT>\" : receive_id , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( msgContent ) , \n } \n payload = json . dumps ( req ) \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + token , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n response = requests . request ( \n \"<STR_LIT>\" , url , params = params , headers = headers , data = payload \n ) \n log . info ( \"<STR_LIT>\" , response . content ) \n def handle ( self , message ) : \n event = message [ \"<STR_LIT>\" ] \n msg = event [ \"<STR_LIT>\" ] \n messageId = msg [ \"<STR_LIT>\" ] \n chat_type = msg [ \"<STR_LIT>\" ] \n sender_id = event [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n prompt = json . loads ( msg [ \"<STR_LIT>\" ] ) [ \"<STR_LIT>\" ] \n prompt = prompt . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n r , v = self . memory_store . get ( messageId ) \n if v : \n return { '<STR_LIT>' : <NUM_LIT> } \n self . memory_store . set ( messageId , True ) \n message_type = msg [ \"<STR_LIT>\" ] \n if message_type != \"<STR_LIT>\" : \n return { '<STR_LIT>' : <NUM_LIT> } \n if chat_type == \"<STR_LIT>\" : \n mentions = msg [ \"<STR_LIT>\" ] \n if not mentions : \n return { '<STR_LIT>' : <NUM_LIT> } \n receive_type = \"<STR_LIT>\" \n receive_id = msg . get ( \"<STR_LIT>\" ) \n at_id = sender_id \n elif chat_type == \"<STR_LIT>\" : \n receive_type = \"<STR_LIT>\" \n receive_id = sender_id \n at_id = None \n access_token = self . get_tenant_access_token ( ) \n if access_token == \"<STR_LIT>\" : \n log . error ( \"<STR_LIT>\" ) \n return { '<STR_LIT>' : <NUM_LIT> } \n context = dict ( ) \n img_match_prefix = functions . check_prefix ( \n prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) \n if img_match_prefix : \n prompt = prompt . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n context [ '<STR_LIT>' ] = str ( sender_id ) \n reply = super ( ) . build_reply_content ( prompt , context ) \n if img_match_prefix : \n if not isinstance ( reply , list ) : \n return { '<STR_LIT>' : <NUM_LIT> }", "output": "images = \"<STR_LIT>\" \n for url in reply : \n images += f\"<STR_LIT>\" \n reply = images \n self . notify_feishu ( access_token , receive_type , \n receive_id , at_id , reply ) \n return { '<STR_LIT>' : <NUM_LIT> } \n def handle_request_url_verify ( self , post_obj ) : \n challenge = post_obj . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n return { '<STR_LIT>' : challenge } \n feishu = FeiShuChannel ( ) \n http_app = Flask ( __name__ , ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def chat ( ) : \n log . info ( \"<STR_LIT>\" . format ( str ( request . data ) ) ) \n obj = json . loads ( request . data ) \n if not obj : \n return { '<STR_LIT>' : <NUM_LIT> } \n headers = obj . get ( \"<STR_LIT>\" ) \n if not headers : \n return { '<STR_LIT>' : <NUM_LIT> } \n token = headers . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if token != feishu . verification_token : \n log . error ( \"<STR_LIT>\" , token ) \n return { '<STR_LIT>' : <NUM_LIT> } \n t = obj . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if \"<STR_LIT>\" == t : \n return feishu . handle_request_url_verify ( obj ) \n elif headers . get ( \"<STR_LIT>\" , None ) == \"<STR_LIT>\" : \n return feishu . handle ( obj ) \n return { '<STR_LIT>' : <NUM_LIT> }"}, {"input": "def convert_options ( options ) : \n result = [ ] \n def add_to_buffer ( key , * value , is_option = True ) : \n if len ( value ) == <NUM_LIT> : \n value = value [ <NUM_LIT> ] \n else : \n value = None \n string_variants = ( \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ) \n bool_variants = ( \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n '<STR_LIT>' , \n ) \n if key in string_variants : \n if not isinstance ( value , str ) : \n raise ValueError ( f'<STR_LIT>' ) \n elif key in bool_variants : \n if value is not None : \n raise ValueError ( f'<STR_LIT>' ) \n else : \n raise ValueError ( \n f'<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' \n ) \n if key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n raise ValueError ( f'<STR_LIT>' ) \n result . append ( key ) \n if value is not None :", "output": "result . append ( value ) \n for name , value in options . items ( ) : \n name = name . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n if isinstance ( value , str ) : \n if len ( name ) == <NUM_LIT> : \n add_to_buffer ( f'<STR_LIT>' , value ) \n else : \n add_to_buffer ( f'<STR_LIT>' , value ) \n elif isinstance ( value , bool ) : \n if value == True : \n if len ( name ) == <NUM_LIT> : \n add_to_buffer ( f'<STR_LIT>' ) \n else : \n add_to_buffer ( f'<STR_LIT>' ) \n else : \n raise ValueError ( '<STR_LIT>' ) \n return result"}, {"input": "import parse \n import os \n FORMAT_STRING = \n def extract_numbers ( fpath ) : \n with open ( fpath ) as f : \n dat = f . read ( ) \n parsed = parse . parse ( FORMAT_STRING , dat ) \n mn = parsed [ <NUM_LIT> ] \n vgr = parsed [ <NUM_LIT> ] \n vga = parsed [ <NUM_LIT> ] \n coco = parsed [ <NUM_LIT> ] \n flickr = parsed [ <NUM_LIT> ] \n vlco = parsed [ <NUM_LIT> ] \n vlca = parsed [ <NUM_LIT> ] \n vlcr = parsed [ <NUM_LIT> ] \n wgt = parsed [ <NUM_LIT> ] \n wgi = parsed [ <NUM_LIT> ] \n wgg = parsed [ <NUM_LIT> ] \n dcas = parsed [ <NUM_LIT> ] \n dcasn = parsed [ <NUM_LIT> ] \n dcp5 = parsed [ <NUM_LIT> ] \n dcp5n = parsed [ <NUM_LIT> ] \n dcbn = parsed [ <NUM_LIT> ] \n dch = parsed [ <NUM_LIT> ] \n dchn = parsed [ <NUM_LIT> ] \n print ( f\"<STR_LIT>\" ) \n def print_all_scores ( ) : \n evals_dir = input ( \"<STR_LIT>\" ) \n for subdir in os . listdir ( evals_dir ) : \n outpath = os . path . join ( evals_dir , subdir ) \n outfile = os . path . join ( outpath , [ n for n in os . listdir ( outpath ) if \"<STR_LIT>\" in n ] [ <NUM_LIT> ] )", "output": "extract_numbers ( outfile ) \n if __name__ == '<STR_LIT>' : \n print_all_scores ( )"}, {"input": "from typing import Any \n import threading \n from threading import active_count \n import requests \n import traceback \n from flask import Flask , request \n class reliableCache : \n def __init__ ( self , query_arg = None , customer_instance_arg = None , user_email = None , max_threads = <NUM_LIT> ) -> None : \n self . max_threads = max_threads \n self . verbose = True \n self . query_arg = query_arg", "output": "self . customer_instance_arg = customer_instance_arg \n self . user_email = user_email \n self . cache_wrapper_threads = { } \n self . hot_cache = { } \n pass \n def print_verbose ( self , print_statement ) : \n if self . verbose : \n print ( \"<STR_LIT>\" + str ( print_statement ) ) \n def add_cache ( self , user_email , instance_id , input_prompt , response ) : \n try : \n url = \"<STR_LIT>\" \n querystring = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : instance_id , \n \"<STR_LIT>\" : user_email , \n \"<STR_LIT>\" : input_prompt , \n \"<STR_LIT>\" : response \n } \n response = requests . post ( url , params = querystring ) \n except : \n pass \n def try_cache_request ( self , user_email , instance_id , query = None ) : \n try : \n if ( user_email , instance_id , query ) in self . hot_cache : \n result = self . hot_cache [ ( user_email , instance_id , query ) ] \n return result \n else : \n url = \"<STR_LIT>\" \n querystring = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : instance_id , \n \"<STR_LIT>\" : user_email , \n \"<STR_LIT>\" : query , \n } \n response = requests . get ( url , params = querystring ) \n extracted_result = response . json ( ) [ \"<STR_LIT>\" ] \n self . hot_cache [ ( user_email , instance_id , query ) ] = extracted_result \n return extracted_result \n except : \n pass \n self . print_verbose ( f\"<STR_LIT>\" ) \n return None \n def cache_wrapper ( self , func ) : \n def wrapper ( * args , ** kwargs ) : \n query = request . args . get ( self . query_arg ) \n instance_id = request . args . get ( self . customer_instance_arg ) \n curr_thread_id = threading . get_ident ( ) \n self . cache_wrapper_threads [ curr_thread_id ] = True \n try : \n self . print_verbose ( f\"<STR_LIT>\" ) \n thread_utilization = self . get_wrapper_thread_utilization ( ) \n self . print_verbose ( f\"<STR_LIT>\" ) \n if thread_utilization > <NUM_LIT> : \n result = self . try_cache_request ( user_email = self . user_email , instance_id = instance_id , query = query ) \n if result != None : \n self . cache_wrapper_threads [ curr_thread_id ] = False \n return result \n result = func ( * args , ** kwargs ) \n thread = threading . Thread ( target = self . add_cache , args = ( self . user_email , instance_id , query , result ) ) \n thread . start ( ) \n pass \n except : \n traceback . print_exc ( ) \n pass \n finally : \n self . cache_wrapper_threads [ curr_thread_id ] = False \n return result \n return wrapper \n def get_wrapper_thread_utilization ( self ) : \n self . print_verbose ( f\"<STR_LIT>\" ) \n active_cache_threads = <NUM_LIT> \n for value in self . cache_wrapper_threads . values ( ) : \n if value == True : \n active_cache_threads += <NUM_LIT> \n self . print_verbose ( f\"<STR_LIT>\" ) \n return active_cache_threads / self . max_threads"}, {"input": "import sys \n from typing import AsyncGenerator , Generator \n import pytest \n from . fake_factories import ( \n async_int_factory , \n async_str_gen_factory , \n int_factory , \n str_gen_factory , \n ) \n from . helpers import nop \n def test_nop ( ) : \n assert None is nop ( ) \n assert None is nop ( <NUM_LIT> ) \n assert None is nop ( <NUM_LIT> , x = <NUM_LIT> ) \n def test_int_factory ( ) : \n assert isinstance ( int_factory ( ) , int ) \n def test_str_cleanup_factory ( ) : \n gen = str_gen_factory ( ) \n assert isinstance ( gen , Generator ) \n assert isinstance ( next ( gen ) , str ) \n with pytest . raises ( StopIteration ) : \n next ( gen ) \n @ pytest . mark . asyncio ( ) \n async def test_async_int_factory ( ) :", "output": "assert isinstance ( await async_int_factory ( ) , int ) \n @ pytest . mark . asyncio ( ) \n async def test_async_str_cleanup_factory ( ) : \n gen = async_str_gen_factory ( ) \n assert isinstance ( gen , AsyncGenerator ) \n assert isinstance ( await anext ( gen ) , str ) \n with pytest . raises ( StopAsyncIteration ) : \n await anext ( gen ) \n if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : \n def anext ( gen : AsyncGenerator ) : \n return gen . __anext__ ( )"}, {"input": "import os \n import subprocess \n platforms = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ]", "output": "base_image = '<STR_LIT>' \n root_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n container_root_dir = '<STR_LIT>' \n command_in_docker = \"<STR_LIT>\" \n for platform in platforms : \n command = f\"<STR_LIT>\" \n subprocess . run ( command , shell = True )"}, {"input": "import os , platform \n VERSION = '<STR_LIT>' \n ASYNC_COMPONENTS = os . environ . get ( '<STR_LIT>' , False ) \n BASE_URL = '<STR_LIT>'", "output": "OS = platform . system ( ) \n DIR = os . getcwd ( ) \n DEFAULT_QR = '<STR_LIT>' \n TIMEOUT = ( <NUM_LIT> , <NUM_LIT> ) \n USER_AGENT = '<STR_LIT>' \n UOS_PATCH_CLIENT_VERSION = '<STR_LIT>' \n UOS_PATCH_EXTSPAM = '<STR_LIT>'"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class A1RoughCfg ( LeggedRobotCfg ) : \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class init_state_slope ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class control ( LeggedRobotCfg . control ) : \n control_type = '<STR_LIT>' \n stiffness = { '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset ( LeggedRobotCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n terminate_after_contacts_on = [ \"<STR_LIT>\" ] \n self_collisions = <NUM_LIT>", "output": "class rewards ( LeggedRobotCfg . rewards ) : \n soft_dof_pos_limit = <NUM_LIT> \n base_height_target = <NUM_LIT> \n class A1RoughCfgPPO ( LeggedRobotCfgPPO ) : \n class algorithm ( LeggedRobotCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT> \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>'"}, {"input": "from datetime import datetime \n import pytest \n from app . models . horoscope import Horoscope \n from app . repositories . horoscope_repository import HoroscopeRepository \n from app . services . horoscope_service import HoroscopeService \n def test_get_horoscope_info_with_valid_sign_and_date_and_lang ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository )", "output": "sign = \"<STR_LIT>\" \n date = \"<STR_LIT>\" \n lang = \"<STR_LIT>\" \n result = service . get_horoscope_info ( sign , date , lang ) \n assert isinstance ( result , Horoscope ) \n assert result . sign == sign \n assert result . date == date \n def test_get_horoscope_info_with_valid_sign_without_date_and_lang ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n result = service . get_horoscope_info ( sign ) \n assert isinstance ( result , Horoscope ) \n assert result . sign == sign \n assert result . date == datetime . now ( ) . strftime ( \"<STR_LIT>\" ) \n def test_get_horoscope_info_with_invalid_sign ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) as excinfo : \n service . get_horoscope_info ( sign ) \n assert \"<STR_LIT>\" in str ( excinfo . value ) \n def test_get_horoscope_info_with_invalid_date ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n date = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) as excinfo : \n service . get_horoscope_info ( sign , date ) \n assert \"<STR_LIT>\" in str ( excinfo . value ) \n def test_get_horoscope_info_with_future_date ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n date = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) as excinfo : \n service . get_horoscope_info ( sign , date ) \n assert \"<STR_LIT>\" in str ( excinfo . value ) \n def test_get_horoscope_info_with_invalid_lang ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n lang = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) as excinfo : \n service . get_horoscope_info ( sign , lang = lang ) \n assert \"<STR_LIT>\" in str ( excinfo . value ) \n def test_get_horoscope_info_with_invalid_lang ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n lang = \"<STR_LIT>\" \n with pytest . raises ( ValueError ) as excinfo : \n service . get_horoscope_info ( sign , lang = lang ) \n assert \"<STR_LIT>\" in str ( excinfo . value ) \n def test_get_horoscope_info_with_none_date ( ) : \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n sign = \"<STR_LIT>\" \n result = service . get_horoscope_info ( sign , date = None ) \n assert isinstance ( result , Horoscope ) \n assert result . sign == sign \n assert result . date == datetime . now ( ) . strftime ( \"<STR_LIT>\" )"}, {"input": "import gradio as gr \n import tabs . extra . processing . processing as processing \n import tabs . extra . analyzer . analyzer as analyzer \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n def extra_tab ( ) : \n gr . Markdown ( \n value = i18n ( \n \"<STR_LIT>\" \n ) \n ) \n with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) : \n processing . processing ( ) \n with gr . TabItem ( i18n ( \"<STR_LIT>\" ) ) :", "output": "analyzer . analyzer ( )"}, {"input": "import sys \n sys . path . append ( \"<STR_LIT>\" ) \n from fenjing . form import get_form \n from fenjing . requester import HTTPRequester \n import unittest \n import fenjing \n from typing import Union \n from fenjing . const import TemplateEnvironment , ReplacedKeywordStrategy \n from fenjing . cracker import Cracker \n from fenjing . options import Options \n from fenjing . submitter import FormSubmitter , PathSubmitter , Submitter , HTTPResponse \n from fenjing import const , waf_func_gen \n import logging \n import os \n VULUNSERVER_ADDR = os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n SLEEP_INTERVAL = float ( os . environ . get ( \"<STR_LIT>\" , <NUM_LIT> ) ) \n class WrappedSubmitter ( Submitter ) : \n def __init__ ( self , subm , blacklist ) : \n super ( ) . __init__ ( ) \n self . subm = subm \n self . blacklist = blacklist \n def submit_raw ( self , raw_payload ) : \n if any ( w in raw_payload for w in self . blacklist ) : \n return HTTPResponse ( status_code = <NUM_LIT> , text = \"<STR_LIT>\" ) \n return self . subm . submit ( raw_payload ) \n class TestBase ( unittest . TestCase ) : \n def setup_local_waf ( self , blacklist ) : \n self . local_blacklist = blacklist \n self . subm = WrappedSubmitter ( \n FormSubmitter ( \n url = VULUNSERVER_ADDR , \n form = get_form ( action = \"<STR_LIT>\" , inputs = [ \"<STR_LIT>\" ] , method = \"<STR_LIT>\" ) , \n target_field = \"<STR_LIT>\" , \n requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , \n ) , \n self . local_blacklist , \n ) \n def setup_remote_waf ( self , remote_uri ) : \n self . local_blacklist = None \n self . subm = FormSubmitter ( \n url = VULUNSERVER_ADDR , \n form = get_form ( action = remote_uri , inputs = [ \"<STR_LIT>\" ] , method = \"<STR_LIT>\" ) , \n target_field = \"<STR_LIT>\" , \n requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , \n ) \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . cracker_other_opts = { } \n self . setup_local_waf ( [ \"<STR_LIT>\" ] ) \n def test_waf ( self ) : \n cracker = Cracker ( self . subm , ** self . cracker_other_opts ) \n full_payload_gen = cracker . crack ( ) \n assert full_payload_gen is not None , self . __class__ . __name__ \n payload , will_print = full_payload_gen . generate ( \n const . OS_POPEN_READ , \n \"<STR_LIT>\" + self . __class__ . __name__ , \n ) \n assert (", "output": "payload is not None \n ) , self . __class__ . __name__ \n self . assertTrue ( will_print ) \n if self . local_blacklist : \n for w in self . local_blacklist : \n self . assertNotIn ( w , payload ) \n resp = self . subm . submit ( payload ) \n assert resp is not None \n self . assertIn ( '<STR_LIT>' , resp . text , resp . text ) \n class TestEasy ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestStringOct ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestStringHex ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestStringUnicodeHex ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestStringLower1 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestIntegerAdd ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestIntegerSub ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestPath ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . local_blacklist = None \n self . subm = PathSubmitter ( \n url = VULUNSERVER_ADDR + \"<STR_LIT>\" , \n requester = HTTPRequester ( interval = SLEEP_INTERVAL ) , \n ) \n class TestHard1 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard2 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard3 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard4 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard5 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard6 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestHard7 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_local_waf ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n ) \n class TestStaticWAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n class TestStaticWAF2 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n class TestDynamicWAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . blacklist = None \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n class TestWeirdWAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . blacklist = None \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n class TestReversedWAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . blacklist = None \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . subm . add_tamperer ( lambda x : x [ : : - <NUM_LIT> ] ) \n class TestLengthLimit1WAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . blacklist = None \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n def test_waf ( self ) : \n cracker = Cracker ( self . subm , ** self . cracker_other_opts ) \n full_payload_gen = cracker . crack ( ) \n assert full_payload_gen is not None , self . __class__ . __name__ \n payload , will_print = full_payload_gen . generate ( \n const . OS_POPEN_READ , \n \"<STR_LIT>\" + self . __class__ . __name__ , \n ) \n assert ( \n payload is not None \n ) , self . __class__ . __name__ \n self . assertTrue ( will_print ) \n if self . local_blacklist : \n for w in self . local_blacklist : \n self . assertNotIn ( w , payload ) \n resp = self . subm . submit ( payload ) \n assert resp is not None \n self . assertIn ( '<STR_LIT>' , resp . text , resp . text ) \n class TestLengthLimit2WAF ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . blacklist = None \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n def test_waf ( self ) : \n cracker = Cracker ( self . subm , options = Options ( environment = TemplateEnvironment . FLASK ) ) \n result = cracker . crack_eval_args ( ) \n assert result is not None \n subm , will_print = result \n payload = \"<STR_LIT>\" \n self . assertTrue ( will_print ) \n resp = subm . submit ( payload ) \n assert resp is not None \n self . assertIn ( \"<STR_LIT>\" , resp . text ) \n class TestReplacedWAFAvoid ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . cracker_other_opts = { \"<STR_LIT>\" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . AVOID ) } \n class TestReplacedWAFAvoid2 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . cracker_other_opts = { \"<STR_LIT>\" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . AVOID ) } \n class TestReplacedWAFDoubleTapping ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . cracker_other_opts = { \"<STR_LIT>\" : Options ( replaced_keyword_strategy = ReplacedKeywordStrategy . DOUBLETAPPING ) } \n class TestJinjaEnv ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . cracker_other_opts = { \"<STR_LIT>\" : Options ( environment = TemplateEnvironment . JINJA2 ) } \n class TestFix500 ( TestBase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . setup_remote_waf ( \"<STR_LIT>\" ) \n self . cracker_other_opts = { \"<STR_LIT>\" : Options ( environment = TemplateEnvironment . FLASK ) }"}, {"input": "import os \n from . import * \n from flask import request , render_template_string \n from mod import tag \n from mod . auth import webui \n from mod . auth . authentication import require_auth \n from mod . dev . debugger import debugger \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' , '<STR_LIT>' ] ) \n def setTag ( ) : \n match require_auth ( request = request , permission = '<STR_LIT>' ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n music_data = request . json \n audio_path = music_data . get ( \"<STR_LIT>\" ) \n if not audio_path : \n return \"<STR_LIT>\" , <NUM_LIT> \n debugger . log ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) \n if not os . path . exists ( audio_path ) : \n debugger . log ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) \n return \"<STR_LIT>\" , <NUM_LIT> \n supported_tags = { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ( str , bool , type ( None ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } ,", "output": "\"<STR_LIT>\" : { \"<STR_LIT>\" : ( str , bool , type ( None ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ( str , bool , type ( None ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ( int , bool , type ( None ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ( str , bool , type ( None ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } \n } \n music_data [ \"<STR_LIT>\" ] = music_data . get ( \"<STR_LIT>\" ) or music_data . get ( \"<STR_LIT>\" ) \n tags_to_set = { } \n for key , value in music_data . items ( ) : \n if key in supported_tags and isinstance ( value , supported_tags [ key ] [ \"<STR_LIT>\" ] ) : \n tags_to_set [ key ] = value \n try : \n tag . write ( tags = tags_to_set , file = audio_path ) \n except TypeError as e : \n return { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : str ( e ) } , <NUM_LIT> \n except FileNotFoundError as e : \n return { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : str ( e ) } , <NUM_LIT> \n except Exception as e : \n return { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : str ( e ) } , <NUM_LIT> \n return { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : f\"<STR_LIT>\" } , <NUM_LIT>"}, {"input": "import numpy as np \n import matplotlib . pyplot as plt \n import librosa . display \n import librosa \n def calculate_features ( y , sr ) : \n stft = np . abs ( librosa . stft ( y ) ) \n duration = librosa . get_duration ( y = y , sr = sr )", "output": "cent = librosa . feature . spectral_centroid ( S = stft , sr = sr ) [ <NUM_LIT> ] \n bw = librosa . feature . spectral_bandwidth ( S = stft , sr = sr ) [ <NUM_LIT> ] \n rolloff = librosa . feature . spectral_rolloff ( S = stft , sr = sr ) [ <NUM_LIT> ] \n return stft , duration , cent , bw , rolloff \n def plot_title ( title ) : \n plt . suptitle ( title , fontsize = <NUM_LIT> , fontweight = \"<STR_LIT>\" ) \n def plot_spectrogram ( y , sr , stft , duration , cmap = \"<STR_LIT>\" ) : \n plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n plt . imshow ( \n librosa . amplitude_to_db ( stft , ref = np . max ) , \n origin = \"<STR_LIT>\" , \n extent = [ <NUM_LIT> , duration , <NUM_LIT> , sr / <NUM_LIT> ] , \n aspect = \"<STR_LIT>\" , \n cmap = cmap , \n ) \n plt . colorbar ( format = \"<STR_LIT>\" ) \n plt . xlabel ( \"<STR_LIT>\" ) \n plt . ylabel ( \"<STR_LIT>\" ) \n plt . title ( \"<STR_LIT>\" ) \n def plot_waveform ( y , sr , duration ) : \n plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n librosa . display . waveshow ( y , sr = sr ) \n plt . xlabel ( \"<STR_LIT>\" ) \n plt . ylabel ( \"<STR_LIT>\" ) \n plt . title ( \"<STR_LIT>\" ) \n def plot_features ( times , cent , bw , rolloff , duration ) : \n plt . subplot ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n plt . plot ( times , cent , label = \"<STR_LIT>\" , color = \"<STR_LIT>\" ) \n plt . plot ( times , bw , label = \"<STR_LIT>\" , color = \"<STR_LIT>\" ) \n plt . plot ( times , rolloff , label = \"<STR_LIT>\" , color = \"<STR_LIT>\" ) \n plt . xlabel ( \"<STR_LIT>\" ) \n plt . title ( \"<STR_LIT>\" ) \n plt . legend ( ) \n def analyze_audio ( audio_file , save_plot_path = \"<STR_LIT>\" ) : \n y , sr = librosa . load ( audio_file ) \n stft , duration , cent , bw , rolloff = calculate_features ( y , sr ) \n plt . figure ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) \n plot_title ( \"<STR_LIT>\" + \"<STR_LIT>\" + audio_file . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] ) \n plot_spectrogram ( y , sr , stft , duration ) \n plot_waveform ( y , sr , duration ) \n plot_features ( librosa . times_like ( cent ) , cent , bw , rolloff , duration ) \n plt . tight_layout ( ) \n if save_plot_path : \n plt . savefig ( save_plot_path , bbox_inches = \"<STR_LIT>\" , dpi = <NUM_LIT> ) \n plt . close ( ) \n audio_info = \n return audio_info , save_plot_path"}, {"input": "import json \n import os \n import importlib \n import gradio as gr \n now_dir = os . getcwd ( ) \n folder = os . path . dirname ( os . path . abspath ( __file__ ) ) \n folder = os . path . dirname ( folder )", "output": "folder = os . path . dirname ( folder ) \n folder = os . path . join ( folder , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n import sys \n sys . path . append ( folder ) \n def get_class ( filename ) : \n with open ( filename , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n for line_number , line in enumerate ( file , start = <NUM_LIT> ) : \n if \"<STR_LIT>\" in line : \n found = line . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] . strip ( ) \n return found \n break \n return None \n def get_list ( ) : \n themes_from_files = [ \n os . path . splitext ( name ) [ <NUM_LIT> ] \n for root , _ , files in os . walk ( folder , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and root == folder \n ] \n json_file_path = os . path . join ( folder , \"<STR_LIT>\" ) \n try : \n with open ( json_file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n themes_from_url = [ item [ \"<STR_LIT>\" ] for item in json . load ( json_file ) ] \n except FileNotFoundError : \n themes_from_url = [ ] \n combined_themes = set ( themes_from_files + themes_from_url ) \n return list ( combined_themes ) \n def select_theme ( name ) : \n selected_file = name + \"<STR_LIT>\" \n full_path = os . path . join ( folder , selected_file ) \n if not os . path . exists ( full_path ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n config_data = json . load ( json_file ) \n config_data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = None \n config_data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = name \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n json . dump ( config_data , json_file , indent = <NUM_LIT> ) \n print ( f\"<STR_LIT>\" ) \n gr . Info ( f\"<STR_LIT>\" ) \n return \n class_found = get_class ( full_path ) \n if class_found : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n config_data = json . load ( json_file ) \n config_data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = selected_file \n config_data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = class_found \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n json . dump ( config_data , json_file , indent = <NUM_LIT> ) \n print ( f\"<STR_LIT>\" ) \n gr . Info ( f\"<STR_LIT>\" ) \n else : \n print ( f\"<STR_LIT>\" ) \n def read_json ( ) : \n try : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n data = json . load ( json_file ) \n selected_file = data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n class_name = data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n if selected_file is not None and class_name : \n return class_name \n elif selected_file == None and class_name : \n return class_name \n else : \n return \"<STR_LIT>\" \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n def load_json ( ) : \n try : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as json_file : \n data = json . load ( json_file ) \n selected_file = data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n class_name = data [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n if selected_file is not None and class_name : \n module = importlib . import_module ( selected_file [ : - <NUM_LIT> ] ) \n obtained_class = getattr ( module , class_name ) \n instance = obtained_class ( ) \n print ( f\"<STR_LIT>\" ) \n return instance \n elif selected_file == None and class_name : \n return class_name \n else : \n print ( \"<STR_LIT>\" ) \n return None \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n return None"}, {"input": "from pymongo import MongoClient \n client = MongoClient ( '<STR_LIT>' , <NUM_LIT> ) \n db = client [ '<STR_LIT>' ] \n collection = db [ '<STR_LIT>' ] \n usuarios = [ \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } }", "output": "] \n collection . insert_many ( usuarios ) \n users_36_years = collection . find ( { \"<STR_LIT>\" : <NUM_LIT> } ) \n for user in users_36_years : \n print ( user ) \n pipeline = [ \n { \"<STR_LIT>\" : { \"<STR_LIT>\" : None , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } } \n ] \n promedio_edad = collection . aggregate ( pipeline ) \n for resultado in promedio_edad : \n print ( resultado )"}, {"input": "import sys \n sys . path . append ( \"<STR_LIT>\" ) \n import unittest \n import fenjing \n from fenjing . payload_gen import PayloadGenerator , expression_gens \n from fenjing import const \n import logging \n from jinja2 import Template , TemplateError \n fenjing . payload_gen . logger . setLevel ( logging . ERROR ) \n def get_payload_gen ( blacklist , context ) : \n def waf_func ( x ) : \n return all ( word not in x for word in blacklist ) \n return PayloadGenerator ( waf_func , context ) \n class PayloadGenTestsTargetRules ( unittest . TestCase ) : \n def setUp ( self ) : \n super ( ) . setUp ( ) \n self . context = { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } \n self . context_payload = \"<STR_LIT>\" \n self . payload_gen = get_payload_gen ( [ ] , self . context ) \n def target_test ( self , target ) : \n gen_type = target [ <NUM_LIT> ] \n for gen_func in expression_gens [ gen_type ] : \n try : \n target_list = gen_func ( self . context , * target [ <NUM_LIT> : ] ) \n except Exception as e : \n raise RuntimeError ( f\"<STR_LIT>\" ) from e \n result = self . payload_gen . generate_by_list ( target_list ) \n if not result : \n continue \n try : \n str_result , _ , _ = result \n Template ( self . context_payload + f\"<STR_LIT>\" ) . render ( ) \n except TemplateError as e : \n raise RuntimeError ( f\"<STR_LIT>\" ) from e \n def test_targets ( self ) : \n targets = [ \n ( const . VARIABLE_OF , \"<STR_LIT>\" ) , \n ( const . ZERO , ) , \n ( const . INTEGER , <NUM_LIT> ) , \n ( const . INTEGER , <NUM_LIT> ) , \n ( const . PLUS , ( const . INTEGER , <NUM_LIT> ) , ( const . INTEGER , <NUM_LIT> ) ) , \n ( const . MULTIPLY , ( const . INTEGER , <NUM_LIT> ) , ( const . INTEGER , <NUM_LIT> ) ) , \n ( const . MULTIPLY , ( const . STRING , \"<STR_LIT>\" ) , ( const . INTEGER , <NUM_LIT> ) ) , \n ( const . STRING_PERCENT , ) , \n ( const . STRING_LOWERC , ) , \n ( const . STRING_PERCENT_LOWER_C , ) , \n ( const . STRING_MANY_PERCENT_LOWER_C , <NUM_LIT> ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ( const . MODULE_OS ) , \n ( const . OS_POPEN_READ , \"<STR_LIT>\" ) , \n ] \n for target in targets : \n logging . info ( \"<STR_LIT>\" , repr ( target ) ) \n self . target_test ( target ) \n class PayloadGenTestCaseSimple ( unittest . TestCase ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . payload_gen = get_payload_gen ( \n [ \n \"<STR_LIT>\" , \n ] , \n { } , \n ) \n def test_string ( self ) : \n strings = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for string in strings : \n self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) ) \n def test_os_popen_read ( self ) : \n self . assertIsNotNone ( \n self . payload_gen . generate ( const . OS_POPEN_READ , \"<STR_LIT>\" ) \n ) \n class PayloadGenTestCaseNoNumber ( unittest . TestCase ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . payload_gen = get_payload_gen ( \n [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> ,", "output": "} , \n ) \n def test_integers ( self ) : \n for num in range ( <NUM_LIT> , <NUM_LIT> ) : \n self . assertIsNotNone ( self . payload_gen . generate ( const . INTEGER , num ) ) \n def test_string ( self ) : \n strings = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for string in strings : \n self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) ) \n def test_os_popen_read ( self ) : \n self . assertIsNotNone ( \n self . payload_gen . generate ( const . OS_POPEN_READ , \"<STR_LIT>\" ) \n ) \n class PayloadGenTestCaseHard ( unittest . TestCase ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . payload_gen = get_payload_gen ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n } , \n ) \n def test_targets ( self ) : \n targets = [ \n ( const . STRING_PERCENT , ) , \n ( const . STRING_LOWERC , ) , \n ( const . INTEGER , <NUM_LIT> ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ( const . STRING_PERCENT_LOWER_C , ) , \n ( const . STRING_MANY_PERCENT_LOWER_C , <NUM_LIT> ) , \n ( const . STRING , \"<STR_LIT>\" ) , \n ] \n for target in targets : \n result = self . payload_gen . generate ( target [ <NUM_LIT> ] , * target [ <NUM_LIT> : ] ) \n self . assertIsNotNone ( result , repr ( target ) ) \n def test_string ( self ) : \n strings = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for string in strings : \n self . assertIsNotNone ( self . payload_gen . generate ( const . STRING , string ) , string ) \n def test_os_popen_read ( self ) : \n self . assertIsNotNone ( \n self . payload_gen . generate ( const . OS_POPEN_READ , \"<STR_LIT>\" ) \n )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>'", "output": "down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . String ( ) , \n nullable = False ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n existing_type = sa . String ( ) , \n nullable = True )"}, {"input": "tu_dinero = int ( input ( '<STR_LIT>' ) ) \n edad = int ( input ( \"<STR_LIT>\" ) ) \n cover = <NUM_LIT> \n costo_vip = <NUM_LIT> \n print ( \"<STR_LIT>\" , edad ) \n if edad >= <NUM_LIT> and tu_dinero >= cover : \n tu_dinero = tu_dinero - cover \n print ( \"<STR_LIT>\" , cover ) \n print ( \"<STR_LIT>\" )", "output": "print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , tu_dinero ) \n vip = int ( input ( \"<STR_LIT>\" ) ) \n if vip == <NUM_LIT> and tu_dinero >= costo_vip : \n tu_dinero = tu_dinero - costo_vip \n print ( \"<STR_LIT>\" , costo_vip ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , tu_dinero ) \n elif vip == <NUM_LIT> and tu_dinero < costo_vip : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , tu_dinero ) \n elif vip == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , tu_dinero ) \n elif edad < <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" )"}, {"input": "from __future__ import print_function \n import argparse \n from argparse import RawTextHelpFormatter \n import csv \n import fnmatch \n import os \n import sys \n from . . import music_tag \n _audio_pattern = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n _default_tags = ( '<STR_LIT>' \n '<STR_LIT>' \n '<STR_LIT>' ) \n def _expand_files ( files ) : \n ret = [ ] \n for f in files : \n if os . path . isdir ( f ) : \n for root , dirs , files in os . walk ( f ) : \n for pattern in _audio_pattern : \n for filename in fnmatch . filter ( files , pattern ) : \n ret . append ( os . path . join ( root , filename ) ) \n else : \n ret . append ( f ) \n return ret \n def _main ( ) : \n parser = argparse . ArgumentParser ( prog = '<STR_LIT>' , \n description = __doc__ , \n formatter_class = RawTextHelpFormatter ) \n action_group = parser . add_mutually_exclusive_group ( required = True ) \n action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n version = '<STR_LIT>' + music_tag . __version__ ) \n action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = [ ] , \n help = '<STR_LIT>' ) \n action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n action_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = _default_tags , \n help = '<STR_LIT>' )", "output": "parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , '<STR_LIT>' , action = '<STR_LIT>' , default = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , nargs = '<STR_LIT>' ) \n args = parser . parse_args ( ) \n if args . print : \n print ( ) \n fnames = _expand_files ( args . files ) \n tags = [ t . strip ( ) for t in args . tags . split ( '<STR_LIT>' ) ] \n for fname in fnames : \n f = music_tag . load_file ( fname ) \n print ( f . info ( tags = tags , show_empty = True , resolve = args . resolve ) ) \n print ( ) \n if args . set : \n set_key_vals = [ s . split ( '<STR_LIT>' ) for s in args . set ] \n set_key_vals = [ ( kv [ <NUM_LIT> ] , '<STR_LIT>' . join ( kv [ <NUM_LIT> : ] ) ) for kv in set_key_vals ] \n fnames = _expand_files ( args . files ) \n for fname in fnames : \n mt_f = music_tag . load_file ( fname ) \n for kv in set_key_vals : \n key , val = kv [ <NUM_LIT> ] , kv [ <NUM_LIT> ] \n if val : \n mt_f [ key ] = val \n else : \n del mt_f [ key ] \n mt_f . save ( ) \n if args . to_csv : \n fnames = _expand_files ( args . files ) \n tags = [ t . strip ( ) for t in args . tags . split ( '<STR_LIT>' ) ] \n with open ( args . to_csv , '<STR_LIT>' , newline = '<STR_LIT>' ) as fout : \n csvwriter = csv . writer ( fout , delimiter = '<STR_LIT>' , quotechar = '<STR_LIT>' , \n dialect = args . csv_dialect , \n quoting = csv . QUOTE_MINIMAL ) \n csvwriter . writerow ( tags + [ '<STR_LIT>' ] ) \n for fname in fnames : \n mt_f = music_tag . load_file ( fname ) \n if args . resolve : \n row = [ mt_f . resolve ( k ) for k in tags ] + [ fname ] \n else : \n row = [ mt_f [ k ] for k in tags ] + [ fname ] \n csvwriter . writerow ( row ) \n if args . from_csv : \n pth0 = '<STR_LIT>' \n if args . files and os . path . isdir ( args . files [ <NUM_LIT> ] ) : \n pth0 = args . files [ <NUM_LIT> ] \n with open ( args . from_csv , newline = '<STR_LIT>' ) as fin : \n csvreader = csv . reader ( fin , delimiter = '<STR_LIT>' , quotechar = '<STR_LIT>' , \n dialect = args . csv_dialect ) \n tags = [ ] \n for row in csvreader : \n if not tags : \n tags = row [ : - <NUM_LIT> ] \n else : \n fname = row [ - <NUM_LIT> ] \n if pth0 : \n fname = os . path . join ( pth0 , fname ) \n if os . path . isfile ( fname ) : \n print ( '<STR_LIT>' , fname ) \n else : \n if args . ignore_missing : \n print ( '<STR_LIT>' , fname , '<STR_LIT>' ) \n continue \n else : \n print ( '<STR_LIT>' , fname , '<STR_LIT>' ) \n return <NUM_LIT> \n mt_f = music_tag . load_file ( fname ) \n for key , val in zip ( tags , row [ : - <NUM_LIT> ] ) : \n if val : \n mt_f [ key ] = val \n else : \n del mt_f [ key ] \n mt_f . save ( ) \n return <NUM_LIT> \n if __name__ == \"<STR_LIT>\" : \n sys . exit ( _main ( ) )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>'", "output": "branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) )"}, {"input": "from __future__ import annotations \n from aiohttp . web import Request , Response , json_response \n import svcs \n async def healthy_view ( request : Request ) -> Response : \n ok : list [ str ] = [ ]", "output": "failing : list [ dict [ str , str ] ] = [ ] \n code = <NUM_LIT> \n for svc in svcs . aiohttp . get_pings ( request ) : \n try : \n await svc . aping ( ) \n ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } ) \n code = <NUM_LIT> \n return json_response ( { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } , status = code )"}, {"input": "import sys \n import asyncio \n import edge_tts \n async def main ( ) : \n text = sys . argv [ <NUM_LIT> ] \n voice = sys . argv [ <NUM_LIT> ]", "output": "output_file = sys . argv [ <NUM_LIT> ] \n await edge_tts . Communicate ( text , voice ) . save ( output_file ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == \"<STR_LIT>\" : \n asyncio . run ( main ( ) )"}, {"input": "from pydub . silence import detect_nonsilent \n from pydub import AudioSegment \n import numpy as np \n import re \n import os \n from rvc . lib . utils import format_title \n def process_audio ( file_path ) : \n try : \n song = AudioSegment . from_file ( file_path ) \n silence_thresh = - <NUM_LIT> \n min_silence_len = <NUM_LIT> \n nonsilent_parts = detect_nonsilent ( \n song , min_silence_len = min_silence_len , silence_thresh = silence_thresh \n ) \n file_dir = os . path . dirname ( file_path ) \n file_name = os . path . basename ( file_path ) . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n file_name = format_title ( file_name ) \n new_dir_path = os . path . join ( file_dir , file_name ) \n os . makedirs ( new_dir_path , exist_ok = True ) \n timestamps_file = os . path . join ( file_dir , f\"<STR_LIT>\" ) \n if os . path . isfile ( timestamps_file ) : \n os . remove ( timestamps_file ) \n segment_count = <NUM_LIT> \n for i , ( start_i , end_i ) in enumerate ( nonsilent_parts ) : \n chunk = song [ start_i : end_i ] \n chunk_file_path = os . path . join ( new_dir_path , f\"<STR_LIT>\" ) \n chunk . export ( chunk_file_path , format = \"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n segment_count += <NUM_LIT> \n with open ( timestamps_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f :", "output": "f . write ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n return \"<STR_LIT>\" , new_dir_path \n except Exception as e : \n print ( f\"<STR_LIT>\" ) \n return \"<STR_LIT>\" , None \n def merge_audio ( timestamps_file ) : \n try : \n prefix = os . path . basename ( timestamps_file ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n timestamps_dir = os . path . dirname ( timestamps_file ) \n with open ( timestamps_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : \n lines = f . readlines ( ) \n audio_segments = [ ] \n last_end_time = <NUM_LIT> \n print ( f\"<STR_LIT>\" ) \n for line in lines : \n match = re . search ( r\"<STR_LIT>\" , line ) \n if match : \n filename , start_time = match . groups ( ) \n start_time = int ( start_time ) \n chunk_file = os . path . join ( timestamps_dir , prefix , filename ) \n silence_duration = max ( start_time - last_end_time , <NUM_LIT> ) \n silence = AudioSegment . silent ( duration = silence_duration ) \n audio_segments . append ( silence ) \n audio = AudioSegment . from_wav ( chunk_file ) \n audio_segments . append ( audio ) \n last_end_time = start_time + len ( audio ) \n print ( f\"<STR_LIT>\" ) \n merged_audio = sum ( audio_segments ) \n merged_audio_np = np . array ( merged_audio . get_array_of_samples ( ) ) \n return merged_audio . frame_rate , merged_audio_np \n except Exception as e : \n print ( f\"<STR_LIT>\" )"}, {"input": "from typing import List , Dict \n from abc import ABC , abstractmethod \n from torch . nn . functional import conv1d \n import torch \n import logging \n from multi_token . modalities . base_modality import Modality \n class LMMMetaModel : \n def __init__ ( self , config ) : \n super ( LMMMetaModel , self ) . __init__ ( config ) \n def _load_projector_weights ( self , weights : Dict ) : \n weights = { \n ( k [ <NUM_LIT> : ] if k . startswith ( \"<STR_LIT>\" ) else k ) : v \n for k , v in weights . items ( ) \n } \n logging . info ( f\"<STR_LIT>\" ) \n load_result = self . load_state_dict ( weights , strict = False ) \n assert ( \n len ( load_result . unexpected_keys ) == <NUM_LIT> \n ) , \"<STR_LIT>\" \n def initialize_pretrained_modules ( self , modalities : List [ Modality ] , weights : Dict ) : \n for m in modalities : \n projector = m . build_projector ( self . config . hidden_size ) \n setattr ( self , m . name + \"<STR_LIT>\" , projector ) \n self . _load_projector_weights ( weights ) \n def initialize_modules ( self , modalities : List [ Modality ] , weights : Dict ) : \n names = [ m . name for m in modalities ] \n self . config . modalities = names \n for m in modalities : \n projector = m . build_projector ( self . config . hidden_size ) \n setattr ( self , m . name + \"<STR_LIT>\" , projector ) \n self . _load_projector_weights ( weights ) \n class LMMMetaForCausalLM ( ABC ) : \n @ abstractmethod \n def get_model ( self ) -> \"<STR_LIT>\" : \n pass \n def prepare_inputs_labels_for_multimodal ( \n self , input_ids , attention_mask , past_key_values , labels , ** kwargs \n ) : \n model = self . get_model ( ) \n batch_size , seq_len = input_ids . shape \n inputs_embeds = torch . zeros ( \n ( batch_size , seq_len , self . config . hidden_size ) , \n dtype = self . dtype , \n device = self . device , \n ) \n projected_tensors = [ ] \n if past_key_values is None : \n for m in self . modalities : \n m_vals = m . forward ( kwargs . get ( m . name ) ) \n mp_vals = [ ] \n proj = getattr ( model , m . name + \"<STR_LIT>\" ) \n for m_val in m_vals : \n mp_vals . append ( proj ( m_val ) ) \n assert all ( \n mp_val . shape [ <NUM_LIT> : ] == ( m . token_width , self . config . hidden_size ) \n for mp_val in mp_vals \n ) , ( \n \"<STR_LIT>\" \n + str ( [ mp_val . shape [ <NUM_LIT> : ] for mp_val in mp_vals ] ) \n + \"<STR_LIT>\" \n + str ( ( m . token_width , self . config . hidden_size ) ) \n ) \n projected_tensors . append ( mp_vals ) \n indices = None \n for i , input_ids_sample in enumerate ( input_ids ) : \n is_text_mask = input_ids_sample >= <NUM_LIT> \n inputs_embeds [ i , is_text_mask ] = model . embed_tokens ( \n input_ids_sample [ is_text_mask ] \n ) \n if is_text_mask . sum ( ) == seq_len : \n continue \n assert ( \n past_key_values is None", "output": ") , \"<STR_LIT>\" \n for mi , m in enumerate ( self . modalities ) : \n m_mask = ( input_ids_sample == m . token_idx ) . float ( ) \n m_kernel = torch . tensor ( \n [ - <NUM_LIT> ] * m . token_width , dtype = m_mask . dtype , device = m_mask . device \n ) \n m_conv = conv1d ( \n m_mask . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) , \n m_kernel . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) , \n ) \n indices = ( m_conv [ <NUM_LIT> , <NUM_LIT> ] == - m . token_width ) . nonzero ( as_tuple = True ) [ <NUM_LIT> ] \n last_covered_idx = - <NUM_LIT> \n k = <NUM_LIT> \n for possible_token_idx in indices : \n if possible_token_idx <= last_covered_idx : \n continue \n batch_modality_tensor = projected_tensors [ mi ] [ i ] [ k ] \n inputs_embeds [ \n i , possible_token_idx : possible_token_idx + m . token_width \n ] = batch_modality_tensor \n last_covered_idx = possible_token_idx + m . token_width - <NUM_LIT> \n k += <NUM_LIT> \n return None , attention_mask , past_key_values , inputs_embeds , labels"}, {"input": "import json \n from densely_captioned_images . dataset . utils import get_clip_token_length \n from densely_captioned_images . repro . config import COCO_TRAIN2017_ANNOTATION_PATH , COCO_VALID2017_ANNOTATION_PATH \n def get_clip_token_lengths_for_coco_split ( split = '<STR_LIT>' ) : \n if split == '<STR_LIT>' : \n annotation_dir = COCO_TRAIN2017_ANNOTATION_PATH \n elif split == '<STR_LIT>' : \n annotation_dir = COCO_VALID2017_ANNOTATION_PATH \n else : \n raise NotImplementedError ( '<STR_LIT>' ) \n with open ( annotation_dir ) as coco_fp : \n coco_annotations = json . load ( coco_fp ) \n caption_count = <NUM_LIT>", "output": "token_count = <NUM_LIT> \n for annotation in coco_annotations [ '<STR_LIT>' ] : \n caption = annotation [ '<STR_LIT>' ] \n caption_count += <NUM_LIT> \n token_count += get_clip_token_length ( caption ) \n return token_count , caption_count , ( token_count / caption_count ) , len ( coco_annotations [ '<STR_LIT>' ] ) \n def get_clip_token_lengths_for_coco ( ) : \n toks , caps , prop , imgs = get_clip_token_lengths_for_coco_split ( '<STR_LIT>' ) \n print ( f\"<STR_LIT>\" ) \n toks , caps , prop , imgs = get_clip_token_lengths_for_coco_split ( '<STR_LIT>' ) \n print ( f\"<STR_LIT>\" ) \n if __name__ == '<STR_LIT>' : \n get_clip_token_lengths_for_coco ( )"}, {"input": "import json \n import random \n import requests \n import re \n class BardBot : \n BARD_URL = \"<STR_LIT>\" \n BARD_CHAT_URL = ( \n \"<STR_LIT>\" \n ) \n HEADERS = {", "output": "\"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def __init__ ( self , session_id : str ) : \n self . _reqid = random . randrange ( <NUM_LIT> , <NUM_LIT> ) \n self . conversation_id = \"<STR_LIT>\" \n self . response_id = \"<STR_LIT>\" \n self . choice_id = \"<STR_LIT>\" \n self . session = requests . Session ( ) \n self . session . headers = self . HEADERS \n self . session . cookies . set ( \"<STR_LIT>\" , session_id ) \n self . SNlM0e = self . __get_snlm0e ( ) \n def __get_snlm0e ( self ) -> str : \n resp = self . session . get ( url = self . BARD_URL , timeout = <NUM_LIT> ) \n if resp . status_code != <NUM_LIT> : \n raise Exception ( \"<STR_LIT>\" ) \n try : \n SNlM0e = re . search ( r\"<STR_LIT>\" , resp . text ) . group ( <NUM_LIT> ) \n return SNlM0e \n except Exception as e : \n raise Exception ( f\"<STR_LIT>\" ) \n def ask ( self , message : str ) -> dict [ str , str ] : \n params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : str ( self . _reqid ) , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n message_struct = [ [ message ] , None , [ self . conversation_id , self . response_id , self . choice_id ] ] \n data = { \"<STR_LIT>\" : json . dumps ( [ None , json . dumps ( message_struct ) ] ) , \"<STR_LIT>\" : self . SNlM0e } \n try : \n resp = self . session . post ( self . BARD_CHAT_URL , params = params , data = data ) \n content = json . loads ( resp . content . splitlines ( ) [ <NUM_LIT> ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] \n if not ( content := json . loads ( resp . content . splitlines ( ) [ <NUM_LIT> ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] ) : \n return { \"<STR_LIT>\" : f\"<STR_LIT>\" } \n json_data = json . loads ( content ) \n results = { \n \"<STR_LIT>\" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , \n \"<STR_LIT>\" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , \n \"<STR_LIT>\" : json_data [ <NUM_LIT> ] [ <NUM_LIT> ] , \n \"<STR_LIT>\" : json_data [ <NUM_LIT> ] , \n \"<STR_LIT>\" : [ { \"<STR_LIT>\" : i [ <NUM_LIT> ] , \"<STR_LIT>\" : i [ <NUM_LIT> ] } for i in json_data [ <NUM_LIT> ] ] , \n } \n self . conversation_id = results [ '<STR_LIT>' ] \n self . response_id = results [ '<STR_LIT>' ] \n self . choice_id = results [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n self . _reqid += <NUM_LIT> \n return results \n except Exception as e : \n raise Exception ( f\"<STR_LIT>\" )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) \n batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) )", "output": "batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "import sys \n import os \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai \n from main import reliableGPT \n import concurrent . futures \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , verbose = True ) \n print ( openai . ChatCompletion . create ) \n good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) \n def test_single_call_bad_key ( ) : \n openai . api_key = \"<STR_LIT>\" \n model = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n temperature = <NUM_LIT> \n error_count = <NUM_LIT> \n failure_count = <NUM_LIT> \n try : \n print ( \"<STR_LIT>\" ) \n response = openai . ChatCompletion . create ( model = model , \n messages = messages , \n temperature = temperature ) \n print ( \"<STR_LIT>\" , response ) \n if response and \"<STR_LIT>\" in response : \n error_count += <NUM_LIT> \n if response == \"<STR_LIT>\" : \n failure_count += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n error_count += <NUM_LIT>", "output": "print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n if error_count == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n def test_embedding_bad_key ( ) : \n openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n user_token = '<STR_LIT>' , \n send_notification = True ) \n openai . api_key = \"<STR_LIT>\" \n def get_embedding ( text , model = \"<STR_LIT>\" ) : \n text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n return openai . Embedding . create ( input = [ text ] , \n model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n result = get_embedding ( \"<STR_LIT>\" ) \n print ( result ) \n def test_embedding_bad_key_fail ( ) : \n openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n send_notification = True ) \n openai . api_key = \"<STR_LIT>\" \n def get_embedding ( text , model = \"<STR_LIT>\" ) : \n text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n return openai . Embedding . create ( input = [ text ] , \n model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n result = get_embedding ( \"<STR_LIT>\" ) \n print ( result ) \n def test_bad_open_ai_call ( ) : \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n test_bad_open_ai_call ( ) \n def test_bad_open_ai_call_with_q ( ) : \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n fallback_strategy = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n queue_requests = True ) \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n def test_multiple_calls ( ) : \n model = \"<STR_LIT>\" \n openai . api_key = good_open_ai_api_key \n messages = [ { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" * <NUM_LIT> \n } , { \n \"<STR_LIT>\" : \n \"<STR_LIT>\" , \n \"<STR_LIT>\" : \n \"<STR_LIT>\" \n } , { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } ] \n temperature = <NUM_LIT> \n error_count = <NUM_LIT> \n failure_count = <NUM_LIT> \n def call_reliable_openai ( ) : \n nonlocal error_count , failure_count \n try : \n print ( \"<STR_LIT>\" ) \n response = openai . ChatCompletion . create ( model = model , \n messages = messages , \n temperature = temperature ) \n print ( response ) \n if response and \"<STR_LIT>\" in response : \n error_count += <NUM_LIT> \n if response == \"<STR_LIT>\" : \n failure_count += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n error_count += <NUM_LIT> \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : \n future_calls = [ executor . submit ( call_reliable_openai ) for _ in range ( <NUM_LIT> ) ] \n concurrent . futures . wait ( future_calls ) \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n if error_count == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" )"}, {"input": "import requests \n import logging \n import threading \n logger = logging . getLogger ( __name__ ) \n def get_version ( ) : \n api = \"<STR_LIT>\" \n data = requests . get ( api ) . json ( ) \n version = data [ '<STR_LIT>' ] [ <NUM_LIT> ] \n return version \n def version_upper ( latest : str , app_version : str ) -> bool : \n v1 = tuple ( map ( int , latest . split ( '<STR_LIT>' ) ) ) \n v2 = tuple ( map ( int , app_version . split ( '<STR_LIT>' ) ) ) \n for i in range ( <NUM_LIT> ) : \n if v1 [ i ] > v2 [ i ] : \n return True \n elif v1 [ i ] < v2 [ i ] : \n return False \n def check_update ( version ) : \n logger . info ( \"<STR_LIT>\" ) \n try :", "output": "latest_version = get_version ( ) \n if version_upper ( latest_version , version ) : \n logger . info ( f\"<STR_LIT>\" ) \n return latest_version \n else : \n logger . info ( \"<STR_LIT>\" ) \n return False \n except : \n logger . warning ( \"<STR_LIT>\" ) \n return False \n def run ( version ) : \n t = threading . Thread ( target = check_update , args = ( version , ) ) \n t . start ( ) \n if __name__ == '<STR_LIT>' : \n run ( \"<STR_LIT>\" ) \n input ( \"<STR_LIT>\" )"}, {"input": "from posixpath import relpath \n from torch . nn . modules . activation import ReLU \n from torch . nn . modules . pooling import MaxPool2d \n from . base_config import BaseConfig \n import torch . nn as nn \n class LeggedRobotCfg ( BaseConfig ) : \n class play : \n load_student_config = False \n mask_priv_obs = False \n class env : \n num_envs = <NUM_LIT> \n n_scan = <NUM_LIT> \n n_priv = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n n_priv_latent = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n n_proprio = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n history_len = <NUM_LIT> \n num_observations = n_proprio + n_scan + history_len * n_proprio + n_priv_latent + n_priv \n num_privileged_obs = None \n num_actions = <NUM_LIT> \n env_spacing = <NUM_LIT> \n send_timeouts = True \n episode_length_s = <NUM_LIT> \n obs_type = \"<STR_LIT>\" \n history_encoding = True \n reorder_dofs = True \n include_foot_contacts = True \n randomize_start_pos = False \n randomize_start_vel = False \n randomize_start_yaw = False \n rand_yaw_range = <NUM_LIT> \n randomize_start_y = False \n rand_y_range = <NUM_LIT> \n randomize_start_pitch = False \n rand_pitch_range = <NUM_LIT> \n contact_buf_len = <NUM_LIT> \n next_goal_threshold = <NUM_LIT> \n reach_goal_delay = <NUM_LIT> \n num_future_goal_obs = <NUM_LIT> \n class depth : \n use_camera = False \n camera_num_envs = <NUM_LIT> \n camera_terrain_num_rows = <NUM_LIT> \n camera_terrain_num_cols = <NUM_LIT> \n position = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n angle = [ - <NUM_LIT> , <NUM_LIT> ] \n update_interval = <NUM_LIT> \n original = ( <NUM_LIT> , <NUM_LIT> ) \n resized = ( <NUM_LIT> , <NUM_LIT> ) \n horizontal_fov = <NUM_LIT> \n buffer_len = <NUM_LIT> \n near_clip = <NUM_LIT> \n far_clip = <NUM_LIT> \n dis_noise = <NUM_LIT> \n scale = <NUM_LIT> \n invert = True \n class normalization : \n class obs_scales : \n lin_vel = <NUM_LIT> \n ang_vel = <NUM_LIT> \n dof_pos = <NUM_LIT> \n dof_vel = <NUM_LIT> \n height_measurements = <NUM_LIT> \n clip_observations = <NUM_LIT> \n clip_actions = <NUM_LIT> \n class noise : \n add_noise = False \n noise_level = <NUM_LIT> \n quantize_height = True \n class noise_scales : \n rotation = <NUM_LIT> \n dof_pos = <NUM_LIT> \n dof_vel = <NUM_LIT> \n lin_vel = <NUM_LIT> \n ang_vel = <NUM_LIT> \n gravity = <NUM_LIT> \n height_measurements = <NUM_LIT> \n class terrain : \n mesh_type = '<STR_LIT>' \n hf2mesh_method = \"<STR_LIT>\" \n max_error = <NUM_LIT> \n max_error_camera = <NUM_LIT> \n y_range = [ - <NUM_LIT> , <NUM_LIT> ] \n edge_width_thresh = <NUM_LIT> \n horizontal_scale = <NUM_LIT> \n horizontal_scale_camera = <NUM_LIT> \n vertical_scale = <NUM_LIT> \n border_size = <NUM_LIT> \n height = [ <NUM_LIT> , <NUM_LIT> ] \n simplify_grid = False \n gap_size = [ <NUM_LIT> , <NUM_LIT> ] \n stepping_stone_distance = [ <NUM_LIT> , <NUM_LIT> ] \n downsampled_scale = <NUM_LIT> \n curriculum = True \n all_vertical = False \n no_flat = True \n static_friction = <NUM_LIT> \n dynamic_friction = <NUM_LIT> \n restitution = <NUM_LIT> \n measure_heights = True \n measured_points_x = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n measured_points_y = [ - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n measure_horizontal_noise = <NUM_LIT> \n selected = False \n terrain_kwargs = None \n max_init_terrain_level = <NUM_LIT> \n terrain_length = <NUM_LIT> \n terrain_width = <NUM_LIT> \n num_rows = <NUM_LIT> \n num_cols = <NUM_LIT> \n terrain_dict = { \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , } \n terrain_proportions = list ( terrain_dict . values ( ) ) \n slope_treshold = <NUM_LIT> \n origin_zero_z = True \n num_goals = <NUM_LIT> \n class commands : \n curriculum = False \n max_curriculum = <NUM_LIT> \n num_commands = <NUM_LIT> \n resampling_time = <NUM_LIT> \n heading_command = True \n lin_vel_clip = <NUM_LIT> \n ang_vel_clip = <NUM_LIT> \n class ranges : \n lin_vel_x = [ <NUM_LIT> , <NUM_LIT> ] \n lin_vel_y = [ <NUM_LIT> , <NUM_LIT> ] \n ang_vel_yaw = [ <NUM_LIT> , <NUM_LIT> ] \n heading = [ <NUM_LIT> , <NUM_LIT> ] \n class max_ranges : \n lin_vel_x = [ <NUM_LIT> , <NUM_LIT> ] \n lin_vel_y = [ - <NUM_LIT> , <NUM_LIT> ] \n ang_vel_yaw = [ - <NUM_LIT> , <NUM_LIT> ] \n heading = [ - <NUM_LIT> , <NUM_LIT> ] \n class crclm_incremnt : \n lin_vel_x = <NUM_LIT> \n lin_vel_y = <NUM_LIT> \n ang_vel_yaw = <NUM_LIT> \n heading = <NUM_LIT> \n waypoint_delta = <NUM_LIT> \n class init_state : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n rot = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n lin_vel = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n ang_vel = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> } \n class control : \n control_type = '<STR_LIT>' \n stiffness = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset : \n file = \"<STR_LIT>\" \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ ] \n terminate_after_contacts_on = [ ] \n disable_gravity = False \n collapse_fixed_joints = True \n fix_base_link = False \n default_dof_drive_mode = <NUM_LIT> \n self_collisions = <NUM_LIT> \n replace_cylinder_with_capsule = True \n flip_visual_attachments = True \n density = <NUM_LIT> \n angular_damping = <NUM_LIT> \n linear_damping = <NUM_LIT> \n max_angular_velocity = <NUM_LIT> \n max_linear_velocity = <NUM_LIT> \n armature = <NUM_LIT> \n thickness = <NUM_LIT> \n class domain_rand : \n randomize_friction = True \n friction_range = [ <NUM_LIT> , <NUM_LIT> ] \n randomize_base_mass = True \n added_mass_range = [ <NUM_LIT> , <NUM_LIT> ] \n randomize_base_com = True \n added_com_range = [ - <NUM_LIT> , <NUM_LIT> ] \n push_robots = True \n push_interval_s = <NUM_LIT> \n max_push_vel_xy = <NUM_LIT>", "output": "randomize_motor = True \n motor_strength_range = [ <NUM_LIT> , <NUM_LIT> ] \n delay_update_global_steps = <NUM_LIT> * <NUM_LIT> \n action_delay = False \n action_curr_step = [ <NUM_LIT> , <NUM_LIT> ] \n action_curr_step_scratch = [ <NUM_LIT> , <NUM_LIT> ] \n action_delay_view = <NUM_LIT> \n action_buf_len = <NUM_LIT> \n class rewards : \n class scales : \n tracking_goal_vel = <NUM_LIT> \n tracking_yaw = <NUM_LIT> \n lin_vel_z = - <NUM_LIT> \n ang_vel_xy = - <NUM_LIT> \n orientation = - <NUM_LIT> \n dof_acc = - <NUM_LIT> \n collision = - <NUM_LIT> \n action_rate = - <NUM_LIT> \n delta_torques = - <NUM_LIT> \n torques = - <NUM_LIT> \n hip_pos = - <NUM_LIT> \n dof_error = - <NUM_LIT> \n feet_stumble = - <NUM_LIT> \n feet_edge = - <NUM_LIT> \n only_positive_rewards = True \n tracking_sigma = <NUM_LIT> \n soft_dof_pos_limit = <NUM_LIT> \n soft_dof_vel_limit = <NUM_LIT> \n soft_torque_limit = <NUM_LIT> \n base_height_target = <NUM_LIT> \n max_contact_force = <NUM_LIT> \n class viewer : \n ref_env = <NUM_LIT> \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n lookat = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n class sim : \n dt = <NUM_LIT> \n substeps = <NUM_LIT> \n gravity = [ <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ] \n up_axis = <NUM_LIT> \n class physx : \n num_threads = <NUM_LIT> \n solver_type = <NUM_LIT> \n num_position_iterations = <NUM_LIT> \n num_velocity_iterations = <NUM_LIT> \n contact_offset = <NUM_LIT> \n rest_offset = <NUM_LIT> \n bounce_threshold_velocity = <NUM_LIT> \n max_depenetration_velocity = <NUM_LIT> \n max_gpu_contact_pairs = <NUM_LIT> ** <NUM_LIT> \n default_buffer_size_multiplier = <NUM_LIT> \n contact_collection = <NUM_LIT> \n class LeggedRobotCfgPPO ( BaseConfig ) : \n seed = <NUM_LIT> \n runner_class_name = '<STR_LIT>' \n class policy : \n init_noise_std = <NUM_LIT> \n continue_from_last_std = True \n scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n critic_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n priv_encoder_dims = [ <NUM_LIT> , <NUM_LIT> ] \n activation = '<STR_LIT>' \n rnn_type = '<STR_LIT>' \n rnn_hidden_size = <NUM_LIT> \n rnn_num_layers = <NUM_LIT> \n tanh_encoder_output = False \n class algorithm : \n value_loss_coef = <NUM_LIT> \n use_clipped_value_loss = True \n clip_param = <NUM_LIT> \n entropy_coef = <NUM_LIT> \n num_learning_epochs = <NUM_LIT> \n num_mini_batches = <NUM_LIT> \n learning_rate = <NUM_LIT> \n schedule = '<STR_LIT>' \n gamma = <NUM_LIT> \n lam = <NUM_LIT> \n desired_kl = <NUM_LIT> \n max_grad_norm = <NUM_LIT> \n dagger_update_freq = <NUM_LIT> \n priv_reg_coef_schedual = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n priv_reg_coef_schedual_resume = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n class depth_encoder : \n if_depth = LeggedRobotCfg . depth . use_camera \n depth_shape = LeggedRobotCfg . depth . resized \n buffer_len = LeggedRobotCfg . depth . buffer_len \n hidden_dims = <NUM_LIT> \n learning_rate = <NUM_LIT> \n num_steps_per_env = LeggedRobotCfg . depth . update_interval * <NUM_LIT> \n class estimator : \n train_with_estimated_states = True \n learning_rate = <NUM_LIT> \n hidden_dims = [ <NUM_LIT> , <NUM_LIT> ] \n priv_states_dim = LeggedRobotCfg . env . n_priv \n num_prop = LeggedRobotCfg . env . n_proprio \n num_scan = LeggedRobotCfg . env . n_scan \n class runner : \n policy_class_name = '<STR_LIT>' \n algorithm_class_name = '<STR_LIT>' \n num_steps_per_env = <NUM_LIT> \n max_iterations = <NUM_LIT> \n save_interval = <NUM_LIT> \n experiment_name = '<STR_LIT>' \n run_name = '<STR_LIT>' \n resume = False \n load_run = - <NUM_LIT> \n checkpoint = - <NUM_LIT> \n resume_path = None"}, {"input": "import json \n import os \n from common . log import logger \n config = { }", "output": "def load_config ( ) : \n global config \n config_path = \"<STR_LIT>\" \n if not os . path . exists ( config_path ) : \n raise Exception ( '<STR_LIT>' ) \n config_str = read_file ( config_path ) \n config = json . loads ( config_str ) \n logger . info ( \"<STR_LIT>\" . format ( config ) ) \n def get_root ( ) : \n return os . path . dirname ( os . path . abspath ( __file__ ) ) \n def read_file ( path ) : \n with open ( path , mode = '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : \n return f . read ( ) \n def conf ( ) : \n return config"}, {"input": "from densely_captioned_images . repro . eval . ARO . dataset_zoo import VG_Relation , VG_Attribution , COCO_Order , Flickr30k_Order \n from densely_captioned_images . repro . eval . clip_aro_wrap import AROtoHFCLIPWrap \n from densely_captioned_images . repro . config import ARO_DIR , COCO_DIR \n from transformers import CLIPProcessor , CLIPModel \n from torch . utils . data import DataLoader \n import pandas as pd \n import nltk \n try : \n nltk . data . find ( '<STR_LIT>' ) \n except LookupError : \n nltk . download ( '<STR_LIT>' ) \n def run_aro_evals ( model : CLIPModel , processor : CLIPProcessor ) : \n vgr_dataset = VG_Relation ( image_preprocess = processor . image_processor , download = True , root_dir = ARO_DIR ) \n vga_dataset = VG_Attribution ( image_preprocess = processor . image_processor , download = True , root_dir = ARO_DIR ) \n coco_order_dataset = COCO_Order ( image_preprocess = processor . image_processor , download = True , root_dir = COCO_DIR ) \n flickr_order_dataset = Flickr30k_Order ( image_preprocess = processor . image_processor , split = '<STR_LIT>' , root_dir = ARO_DIR ) \n vgr_loader = DataLoader ( vgr_dataset , batch_size = <NUM_LIT> , shuffle = False ) \n vga_loader = DataLoader ( vga_dataset , batch_size = <NUM_LIT> , shuffle = False ) \n coco_loader = DataLoader ( coco_order_dataset , batch_size = <NUM_LIT> , shuffle = False ) \n flickr_loader = DataLoader ( flickr_order_dataset , batch_size = <NUM_LIT> , shuffle = False ) \n aro_wrap = AROtoHFCLIPWrap ( model , processor ) \n vgr_scores = aro_wrap . get_retrieval_scores_batched ( vgr_loader ) \n vgr_records = vgr_dataset . evaluate_scores ( vgr_scores ) \n symmetric = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n df = pd . DataFrame ( vgr_records ) \n df = df [ ~ df . Relation . isin ( symmetric ) ] \n vgr_metric = df . Accuracy . mean ( ) \n print ( f\"<STR_LIT>\" ) \n vga_scores = aro_wrap . get_retrieval_scores_batched ( vga_loader ) \n vga_records = vga_dataset . evaluate_scores ( vga_scores ) \n df = pd . DataFrame ( vga_records ) \n vga_metric = df . Accuracy . mean ( ) \n print ( f\"<STR_LIT>\" ) \n coco_scores = aro_wrap . get_retrieval_scores_batched ( coco_loader ) \n coco_records = coco_order_dataset . evaluate_scores ( coco_scores ) \n df = pd . DataFrame ( coco_records ) \n coco_metric = df [ '<STR_LIT>' ] . mean ( ) \n print ( f\"<STR_LIT>\" ) \n flickr_scores = aro_wrap . get_retrieval_scores_batched ( flickr_loader ) \n flickr_records = flickr_order_dataset . evaluate_scores ( flickr_scores ) \n df = pd . DataFrame ( flickr_records ) \n flickr_metric = df [ '<STR_LIT>' ] . mean ( ) \n print ( f\"<STR_LIT>\" ) \n return { \n '<STR_LIT>' : vgr_metric , \n '<STR_LIT>' : vga_metric ,", "output": "'<STR_LIT>' : coco_metric , \n '<STR_LIT>' : flickr_metric , \n } \n def run_aro_on_lora ( lora_weight_path ) : \n from peft import PeftModel \n processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n loaded = PeftModel . from_pretrained ( base_clip_model , lora_weight_path ) \n loaded = loaded . merge_and_unload ( ) \n run_aro_evals ( loaded , processor ) \n if __name__ == '<STR_LIT>' : \n from transformers import CLIPProcessor , CLIPModel \n clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n run_aro_evals ( clip_model , clip_processor )"}, {"input": "from __future__ import annotations \n from contextlib import asynccontextmanager \n from typing import AsyncGenerator , Generator , Protocol \n from starlette . applications import Starlette \n from starlette . middleware import Middleware \n from starlette . requests import Request \n import svcs \n def factory_with_cleanup ( ) -> Generator [ int , None , None ] : \n yield <NUM_LIT> \n @ svcs . starlette . lifespan \n async def lifespan ( \n app : Starlette , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n yield { } \n reg : svcs . Registry = lifespan . registry \n @ svcs . starlette . lifespan \n async def lifespan2 ( \n app : Starlette , registry : svcs . Registry \n ) -> AsyncGenerator [ None , None ] : \n yield \n @ svcs . starlette . lifespan \n @ asynccontextmanager \n async def lifespan3 ( \n app : Starlette , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n yield { }", "output": "@ svcs . starlette . lifespan \n @ asynccontextmanager \n async def lifespan4 ( \n app : Starlette , registry : svcs . Registry \n ) -> AsyncGenerator [ None , None ] : \n yield \n reg = svcs . Registry ( ) \n app = Starlette ( \n lifespan = lifespan , middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] \n ) \n a : int \n b : str \n c : bool \n d : tuple \n e : object \n f : float \n g : list \n h : dict \n i : set \n j : bytes \n request = Request ( { } ) \n class P ( Protocol ) : \n def m ( self ) -> None : ... \n async def func ( ) -> None : \n a , b , c , d , e , f , g , h , i , j = await svcs . starlette . aget ( \n request , int , str , bool , tuple , object , float , list , dict , set , bytes \n ) \n p : P = await svcs . starlette . aget_abstract ( request , P ) \n con : svcs . Container = svcs . starlette . svcs_from ( request )"}, {"input": "TERMINAL = \"<STR_LIT>\"", "output": "WECHAT = \"<STR_LIT>\" \n WECHAT_MP = \"<STR_LIT>\" \n WECHAT_MP_SERVICE = \"<STR_LIT>\" \n WECHAT_COM = \"<STR_LIT>\" \n QQ = \"<STR_LIT>\" \n GMAIL = \"<STR_LIT>\" \n TELEGRAM = \"<STR_LIT>\" \n SLACK = \"<STR_LIT>\" \n HTTP = \"<STR_LIT>\" \n DINGTALK = \"<STR_LIT>\" \n FEISHU = \"<STR_LIT>\" \n DISCORD = \"<STR_LIT>\" \n OPEN_AI = \"<STR_LIT>\" \n CHATGPT = \"<STR_LIT>\" \n BAIDU = \"<STR_LIT>\" \n BING = \"<STR_LIT>\" \n BARD = \"<STR_LIT>\""}, {"input": "import torch \n import os \n import json \n import random \n from tqdm import tqdm \n from typing import Dict , List , Any , Callable , Optional , Tuple \n from PIL import Image \n from densely_captioned_images . dataset . utils import get_clip_processor , get_clip_token_length \n from densely_captioned_images . repro . config import COCO_TRAIN2017_DATAPATH , COCO_VALID2017_DATAPATH , COCO_TRAIN2017_ANNOTATION_PATH , COCO_VALID2017_ANNOTATION_PATH \n from densely_captioned_images . dataset . spacy_negs import get_spacy_negative \n def get_dataset_source ( split = '<STR_LIT>' , count = <NUM_LIT> , use_antonyms = False ) : \n if split == '<STR_LIT>' : \n source_dir = COCO_TRAIN2017_DATAPATH \n annotation_dir = COCO_TRAIN2017_ANNOTATION_PATH \n elif split == '<STR_LIT>' : \n source_dir = COCO_VALID2017_DATAPATH \n annotation_dir = COCO_VALID2017_ANNOTATION_PATH \n else : \n raise NotImplementedError ( '<STR_LIT>' ) \n with open ( annotation_dir ) as coco_fp : \n coco_annotations = json . load ( coco_fp ) \n coco_by_img_id = { v [ '<STR_LIT>' ] : v for v in coco_annotations [ '<STR_LIT>' ] } \n for v in coco_by_img_id . values ( ) : \n v [ '<STR_LIT>' ] = [ ] \n for captions in coco_annotations [ '<STR_LIT>' ] :", "output": "coco_by_img_id [ captions [ '<STR_LIT>' ] ] [ '<STR_LIT>' ] . append ( captions [ '<STR_LIT>' ] ) \n res = [ ] \n count_so_far = <NUM_LIT> \n skipped = <NUM_LIT> \n for n in tqdm ( coco_by_img_id . values ( ) ) : \n all_good = True \n for caption in n [ '<STR_LIT>' ] : \n toks = get_clip_token_length ( caption ) \n if toks > <NUM_LIT> : \n all_good = False \n break \n if not all_good : \n skipped += <NUM_LIT> \n continue \n image_path = os . path . join ( source_dir , n [ '<STR_LIT>' ] ) \n res . append ( { \n \"<STR_LIT>\" : image_path , \n \"<STR_LIT>\" : n [ '<STR_LIT>' ] [ <NUM_LIT> ] , \n \"<STR_LIT>\" : n [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : get_spacy_negative ( n [ '<STR_LIT>' ] [ <NUM_LIT> ] , use_antonyms = use_antonyms ) , \n } ) \n count_so_far += <NUM_LIT> \n if count_so_far == count : \n break \n return res \n class COCODataset ( torch . utils . data . Dataset ) : \n def __init__ ( self , dataset_source , caption_bag_size = <NUM_LIT> ) : \n self . data_list = dataset_source \n self . processor = get_clip_processor ( ) \n assert caption_bag_size <= <NUM_LIT> , \"<STR_LIT>\" \n self . caption_bag_size = caption_bag_size \n def __getitem__ ( self , idx ) : \n item = self . data_list [ idx ] \n image = Image . open ( item [ '<STR_LIT>' ] ) \n inputs = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) \n negatives = self . processor ( text = [ item [ '<STR_LIT>' ] ] , images = [ image ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) \n res = { \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n '<STR_LIT>' : negatives [ '<STR_LIT>' ] , \n '<STR_LIT>' : negatives [ '<STR_LIT>' ] , \n '<STR_LIT>' : inputs [ '<STR_LIT>' ] , \n } \n if item . get ( '<STR_LIT>' ) is not None and self . caption_bag_size > <NUM_LIT> : \n use_captions = random . sample ( item [ '<STR_LIT>' ] , self . caption_bag_size ) \n bag_inputs = self . processor ( text = use_captions , images = [ image ] , return_tensors = \"<STR_LIT>\" , padding = \"<STR_LIT>\" ) \n res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] \n res [ '<STR_LIT>' ] = bag_inputs [ '<STR_LIT>' ] \n return res \n def __len__ ( self ) : \n return len ( self . data_list )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>'", "output": "branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True , server_default = None ) ) \n def downgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' ) \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "class Animal : \n def __init__ ( self , nombre ) : \n self . nombre = nombre \n class Perro ( Animal ) : \n especie = \"<STR_LIT>\" \n def __init__ ( self , nombre , raza ) : \n super ( ) . __init__ ( nombre )", "output": "if not isinstance ( nombre , str ) : \n raise ValueError ( \"<STR_LIT>\" ) \n if not isinstance ( raza , str ) : \n raise ValueError ( \"<STR_LIT>\" ) \n self . __raza = raza \n def get_raza ( self ) : \n return self . __raza \n def set_raza ( self , nombre ) : \n self . __raza = nombre \n def ladrar ( self ) : \n print ( f\"<STR_LIT>\" ) \n puppy = Perro ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n manchas = Perro ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( puppy . nombre ) \n print ( puppy . get_raza ( ) ) \n puppy . set_raza ( \"<STR_LIT>\" ) \n print ( puppy . get_raza ( ) ) \n print ( manchas . get_raza ( ) ) \n print ( Perro . especie ) \n print ( puppy . especie ) \n manchas . ladrar ( )"}, {"input": "from typing import List \n import random \n import argparse \n import json \n import os \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n TYPES = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n REPLACEMENTS = { \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n } \n TEMP_TOKEN = \"<STR_LIT>\" \n def _convert_convo ( convo ) -> List : \n type_idx = TYPES . index ( random . choice ( TYPES ) ) \n msgs = [ ] \n for m in convo : \n content = m [ \"<STR_LIT>\" ] . replace ( \"<STR_LIT>\" , TEMP_TOKEN ) \n for k , v in REPLACEMENTS . items ( ) : \n content = content . replace ( k , v [ type_idx ] ) \n content = content . replace ( TEMP_TOKEN , \"<STR_LIT>\" ) \n msgs . append ( \n { \n \"<STR_LIT>\" : { \"<STR_LIT>\" : ROLE_ASSISTANT , \"<STR_LIT>\" : ROLE_USER } [ m [ \"<STR_LIT>\" ] ] , \n \"<STR_LIT>\" : content , \n } \n ) \n return msgs \n def main ( args ) : \n rows = [ ] \n for json_fn in args . llava_json : \n with open ( json_fn ) as f : \n rows . extend ( json . load ( f ) ) \n def gen ( rows ) : \n for row in rows : \n img_path = row [ \"<STR_LIT>\" ] \n fn = os . path . join ( args . image_folder , img_path ) \n if not os . path . exists ( fn ) : \n print ( \"<STR_LIT>\" , fn ) \n continue \n yield { \n \"<STR_LIT>\" : str ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : [ fn ] ,", "output": "\"<STR_LIT>\" : _convert_convo ( row [ \"<STR_LIT>\" ] ) , \n } \n ds = Dataset . from_generator ( gen , gen_kwargs = { \"<STR_LIT>\" : rows } , num_proc = args . num_proc ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , action = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import mutagen . dsf \n from . file import TAG_MAP_ENTRY \n from . id3 import Id3File", "output": "class DsfFile ( Id3File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . dsf . DSF \n def __init__ ( self , filename , ** kwargs ) : \n super ( DsfFile , self ) . __init__ ( filename , ** kwargs ) \n self . tag_map = self . tag_map . copy ( ) \n self . tag_map . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n } )"}, {"input": "import openai \n import copy \n class Model : \n def __init__ ( self , model_function ) : \n self . model_function = model_function \n self . original_openai_chat = openai . ChatCompletion . create \n self . original_openai_completion = openai . Completion . create", "output": "self . original_openai_embeddings = openai . Embedding . create \n self . backup_model = None \n def get_model_function ( self ) : \n return self . model_function \n def get_original_chat ( self ) : \n return self . original_openai_chat \n def get_original_completion ( self ) : \n return self . original_openai_completion \n def get_original_embeddings ( self ) : \n return self . original_openai_embeddings \n def get_original_api_base ( self ) : \n return self . original_api_base \n def get_original_api_version ( self ) : \n return self . original_api_version \n def set_openai_model ( self , model , api_key ) : \n self . backup_model = model . __dict__ . copy ( ) \n model . api_type = \"<STR_LIT>\" \n model . api_base = \"<STR_LIT>\" \n model . api_version = None \n model . api_key = api_key \n return model \n def reset_model ( self ) : \n return self . backup_model"}, {"input": "import requests \n import concurrent . futures \n import time \n url = \"<STR_LIT>\" \n queries = [ ] \n for i in range ( <NUM_LIT> ) : \n query = f\"<STR_LIT>\" \n queries . append ( query ) \n def make_request ( query ) : \n print ( f\"<STR_LIT>\" ) \n response = requests . get ( url ) \n print ( response ) \n return response . text \n start_time = time . time ( ) \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor :", "output": "futures = [ executor . submit ( make_request , query ) for query in queries ] \n concurrent . futures . wait ( futures ) \n results = [ future . result ( ) for future in futures ] \n end_time = time . time ( ) \n print ( f\"<STR_LIT>\" )"}, {"input": "dato = \"<STR_LIT>\" \n print ( dato + dato ) \n dato = <NUM_LIT> \n print ( dato + dato ) \n dato = <NUM_LIT> \n print ( dato + dato ) \n dato = False \n print ( dato + dato ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n cinco = \"<STR_LIT>\" \n numero = <NUM_LIT> \n print ( float ( cinco ) + numero ) \n nombre = \"<STR_LIT>\" \n edad = <NUM_LIT> \n print ( '<STR_LIT>' . format ( nombre , edad ) )", "output": "print ( f'<STR_LIT>' ) \n print ( \"<STR_LIT>\" + nombre + \"<STR_LIT>\" + str ( edad ) + \"<STR_LIT>\" ) \n print ( __name__ )"}, {"input": "from . import * \n import os \n from flask import request , abort , redirect , send_from_directory , render_template_string \n from mod . auth import webui \n from mod . auth . authentication import require_auth \n @ app . route ( '<STR_LIT>' ) \n def redirect_to_welcome ( ) : \n return redirect ( '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' ) \n def favicon ( ) : \n return send_from_directory ( src_path , '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' ) \n def return_index ( ) : \n return send_from_directory ( src_path , '<STR_LIT>' ) \n @ app . route ( '<STR_LIT>' ) \n def serve_file ( filename ) : \n FORBIDDEN_EXTENSIONS = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ) \n _paths = filename . split ( '<STR_LIT>' ) \n for _path in _paths : \n if _path . startswith ( '<STR_LIT>' ) : \n abort ( <NUM_LIT> ) \n if filename . lower ( ) . endswith ( FORBIDDEN_EXTENSIONS ) : \n abort ( <NUM_LIT> ) \n try : \n return send_from_directory ( src_path , filename ) \n except FileNotFoundError : \n abort ( <NUM_LIT> )", "output": "@ app . route ( '<STR_LIT>' ) \n @ v1_bp . route ( '<STR_LIT>' ) \n def file_viewer ( filename ) : \n match require_auth ( request = request ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n ALLOWED_EXTENSIONS = ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , ) \n if filename . lower ( ) . endswith ( ALLOWED_EXTENSIONS ) : \n try : \n return send_from_directory ( os . path . dirname ( filename ) , os . path . basename ( filename ) ) \n except FileNotFoundError : \n abort ( <NUM_LIT> )"}, {"input": "import pytest \n import os \n from flask import Flask \n from fastapi import APIRouter , FastAPI \n from fastapi . testclient import TestClient \n @ pytest . fixture \n def fastapi_app ( ) : \n app = FastAPI ( ) \n router = APIRouter ( ) \n @ router . post ( '<STR_LIT>' ) \n async def test_post ( ) : \n return { '<STR_LIT>' : '<STR_LIT>' } \n @ router . get ( '<STR_LIT>' ) \n async def test_get ( demo : bool = False ) : \n return { '<STR_LIT>' : demo } \n @ router . patch ( '<STR_LIT>' ) \n async def test_patch ( ) : \n return { '<STR_LIT>' : '<STR_LIT>' } \n @ router . put ( '<STR_LIT>' ) \n async def test_put ( ) :", "output": "return { '<STR_LIT>' : '<STR_LIT>' } \n app . include_router ( router ) \n yield app \n @ pytest . fixture \n def flask_app ( ) : \n app = Flask ( '<STR_LIT>' , root_path = os . path . dirname ( __file__ ) ) \n app . config . update ( \n TESTING = True , \n SECRET_KEY = '<STR_LIT>' , \n ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def test_post ( ) : \n return '<STR_LIT>' \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def test_get ( ) : \n return '<STR_LIT>' \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def test_patch ( ) : \n return '<STR_LIT>' \n yield app \n @ pytest . fixture \n def flask_client ( flask_app ) : \n yield flask_app . test_client ( ) \n @ pytest . fixture ( ) \n def fastapi_client ( fastapi_app ) : \n yield TestClient ( fastapi_app )"}, {"input": "import gradio as gr \n import os \n import sys \n now_dir = os . getcwd ( ) \n pid_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" )", "output": "def restart_applio ( ) : \n if os . name != \"<STR_LIT>\" : \n os . system ( \"<STR_LIT>\" ) \n else : \n os . system ( \"<STR_LIT>\" ) \n try : \n with open ( pid_file_path , \"<STR_LIT>\" ) as pid_file : \n pids = [ int ( pid ) for pid in pid_file . readlines ( ) ] \n for pid in pids : \n os . kill ( pid , <NUM_LIT> ) \n os . remove ( pid_file_path ) \n except : \n pass \n python = sys . executable \n os . execl ( python , python , * sys . argv ) \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n def restart_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n restart_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n restart_button . click ( \n fn = restart_applio , \n inputs = [ ] , \n outputs = [ ] , \n )"}, {"input": "import time \n from threading import Lock \n class Store ( object ) : \n def get ( self , key ) : \n return False , '<STR_LIT>' \n def set ( self , key , value , expire ) : \n pass \n class ExpireValue ( object ) : \n def __init__ ( self , value , expireTime ) : \n self . value = value \n self . expireTime = expireTime \n class MemoryStore ( Store ) : \n def __init__ ( self ) : \n self . data = { } \n self . mutex = Lock ( ) \n def get ( self , key ) : \n self . mutex . acquire ( ) \n try : \n val = self . data . get ( key ) \n if val is None : \n return False , \"<STR_LIT>\"", "output": "else : \n if val . expireTime == - <NUM_LIT> : \n return True , val . value \n elif val . expireTime < int ( time . time ( ) ) : \n self . data . pop ( key ) \n return False , \"<STR_LIT>\" \n else : \n return True , val . value \n finally : \n self . mutex . release ( ) \n def set ( self , key , value , expire = None ) : \n self . mutex . acquire ( ) \n try : \n self . data [ key ] = ExpireValue ( \n value , expire == None and - <NUM_LIT> or int ( time . time ( ) ) + expire ) \n finally : \n self . mutex . release ( )"}, {"input": "import sys \n sys . path . append ( \"<STR_LIT>\" ) \n import time \n import threading \n import unittest \n import os \n import requests \n from fenjing import webui , const \n WEBUI_URL = \"<STR_LIT>\" \n VULUNSERVER_URL = os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n t = threading . Thread ( target = webui . main , kwargs = { \"<STR_LIT>\" : False } ) \n t . daemon = True \n t . start ( ) \n time . sleep ( <NUM_LIT> ) \n class TestWebui ( unittest . TestCase ) : \n def __init__ ( self , * args , ** kwargs ) : \n super ( ) . __init__ ( * args , ** kwargs ) \n def test_index ( self ) : \n resp = requests . get ( WEBUI_URL ) \n self . assertEqual ( resp . status_code , <NUM_LIT> ) \n self . assertIn ( \"<STR_LIT>\" , resp . text ) \n def wait_for_task ( self , task_id , task_type , max_time = <NUM_LIT> ) : \n start_time = time . perf_counter ( ) \n while True : \n time . sleep ( <NUM_LIT> ) \n resp = requests . post ( \n WEBUI_URL + \"<STR_LIT>\" , \n data = { \n \"<STR_LIT>\" : task_id , \n } , \n ) \n resp_data = resp . json ( )", "output": "self . assertEqual ( resp_data [ \"<STR_LIT>\" ] , const . APICODE_OK ) \n if resp_data [ \"<STR_LIT>\" ] : \n if task_type == \"<STR_LIT>\" : \n self . assertTrue ( resp_data [ \"<STR_LIT>\" ] ) \n break \n self . assertLessEqual ( time . perf_counter ( ) - start_time , max_time ) \n def general_task_test ( self , request_data ) : \n resp = requests . post ( WEBUI_URL + \"<STR_LIT>\" , data = request_data ) \n resp_data = resp . json ( ) \n self . assertEqual ( resp_data [ \"<STR_LIT>\" ] , const . APICODE_OK ) \n task_id = resp_data [ \"<STR_LIT>\" ] \n self . wait_for_task ( task_id , \"<STR_LIT>\" ) \n resp = requests . post ( \n WEBUI_URL + \"<STR_LIT>\" , \n data = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : task_id , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n ) \n resp_data = resp . json ( ) \n self . assertEqual ( resp_data [ \"<STR_LIT>\" ] , const . APICODE_OK ) \n task_id = resp_data [ \"<STR_LIT>\" ] \n self . wait_for_task ( task_id , \"<STR_LIT>\" ) \n resp = requests . post ( \n WEBUI_URL + \"<STR_LIT>\" , \n data = { \n \"<STR_LIT>\" : task_id , \n } , \n ) \n resp_data = resp . json ( ) \n self . assertEqual ( resp_data [ \"<STR_LIT>\" ] , const . APICODE_OK ) \n messages = resp_data [ \"<STR_LIT>\" ] \n is_cmd_executed = any ( \"<STR_LIT>\" in msg for msg in messages ) \n self . assertTrue ( is_cmd_executed ) \n def test_crack ( self ) : \n self . general_task_test ( \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : VULUNSERVER_URL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_scan ( self ) : \n self . general_task_test ( \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : VULUNSERVER_URL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_crack_path ( self ) : \n self . general_task_test ( \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : VULUNSERVER_URL + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n )"}, {"input": "from model import model_factory \n import config \n from plugins . event import Event , EventContext \n from plugins . plugin_manager import PluginManager \n class Bridge ( object ) : \n def __init__ ( self ) : \n pass \n def fetch_reply_content ( self , query , context ) : \n econtext = PluginManager ( ) . emit_event ( EventContext ( \n Event . ON_BRIDGE_HANDLE_CONTEXT , { '<STR_LIT>' : query , '<STR_LIT>' : context } ) ) \n type = econtext [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) or config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n query = econtext . econtext . get ( \"<STR_LIT>\" , None ) \n reply = econtext . econtext . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not econtext . is_pass ( ) and query : \n return model_factory . create_bot ( type ) . reply ( query , context ) \n else : \n return reply", "output": "async def fetch_reply_stream ( self , query , context ) : \n econtext = PluginManager ( ) . emit_event ( EventContext ( \n Event . ON_BRIDGE_HANDLE_CONTEXT , { '<STR_LIT>' : query , '<STR_LIT>' : context } ) ) \n type = econtext [ '<STR_LIT>' ] . get ( '<STR_LIT>' ) or config . conf ( ) . get ( \"<STR_LIT>\" ) . get ( \"<STR_LIT>\" ) \n query = econtext . econtext . get ( \"<STR_LIT>\" , None ) \n reply = econtext . econtext . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n bot = model_factory . create_bot ( type ) \n if not econtext . is_pass ( ) and query : \n async for final , response in bot . reply_text_stream ( query , context ) : \n yield final , response \n else : \n yield True , reply"}, {"input": "from __future__ import annotations \n from starlette . requests import Request \n from starlette . responses import JSONResponse \n import svcs \n async def healthy ( request : Request ) -> JSONResponse : \n ok : list [ str ] = [ ] \n failing : list [ dict [ str , str ] ] = [ ] \n code = <NUM_LIT> \n for svc in svcs . starlette . get_pings ( request ) : \n try :", "output": "await svc . aping ( ) \n ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } ) \n code = <NUM_LIT> \n return JSONResponse ( \n content = { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } , status_code = code \n )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None", "output": "depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True , server_default = \"<STR_LIT>\" ) ) \n def downgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "import urllib \n import flask \n import flask_login \n from flask import current_app as app \n from flask_login import current_user , login_required \n import feedi . models as models \n from feedi . models import db \n def init ( ) : \n login_manager = flask_login . LoginManager ( ) \n login_manager . login_view = '<STR_LIT>' \n login_manager . init_app ( app ) \n @ login_manager . user_loader \n def load_user ( user_id ) : \n return db . session . get ( models . User , int ( user_id ) ) \n @ app . get ( \"<STR_LIT>\" ) \n def login ( ) : \n default_email = app . config . get ( '<STR_LIT>' ) \n if default_email : \n app . logger . debug ( \"<STR_LIT>\" , default_email ) \n user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = default_email ) ) \n flask_login . login_user ( user , remember = True ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) \n return flask . render_template ( '<STR_LIT>' ) \n @ app . post ( '<STR_LIT>' ) \n def login_post ( ) : \n email = flask . request . form . get ( '<STR_LIT>' ) \n password = flask . request . form . get ( '<STR_LIT>' ) \n if not email or not password : \n return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) \n user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) \n if not user or not user . check_password ( password ) : \n return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) \n flask_login . login_user ( user , remember = True ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def kindle_add ( ) : \n verifier , url = models . KindleDevice . signin_url ( ) \n return flask . render_template ( '<STR_LIT>' , signin_url = url , verifier = verifier ) \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def kindle_add_submit ( ) : \n verifier = flask . request . form . get ( '<STR_LIT>' ) \n redirect_url = flask . request . form . get ( '<STR_LIT>' ) \n models . KindleDevice . add_from_url ( current_user . id , verifier , redirect_url ) \n db . session . commit ( ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' ) ) \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def mastodon_oauth ( ) : \n \"<STR_LIT>\" \n return flask . render_template ( '<STR_LIT>' ) \n @ app . post ( \"<STR_LIT>\" ) \n @ login_required \n def mastodon_oauth_submit ( ) : \n base_url = flask . request . form . get ( '<STR_LIT>' ) \n if not base_url : \n return flask . render_template ( '<STR_LIT>' , error_msg = \"<STR_LIT>\" ) \n url_parts = urllib . parse . urlparse ( base_url ) \n base_url = f'<STR_LIT>'", "output": "app . logger . info ( '<STR_LIT>' , base_url ) \n masto_app = models . MastodonApp . get_or_create ( base_url ) \n return flask . redirect ( masto_app . auth_redirect_url ( ) ) \n @ app . get ( \"<STR_LIT>\" ) \n @ login_required \n def mastodon_oauth_callback ( ) : \n code = flask . request . args . get ( '<STR_LIT>' ) \n base_url = flask . request . args . get ( '<STR_LIT>' ) \n if not code or not base_url : \n app . logger . error ( \"<STR_LIT>\" ) \n flask . abort ( <NUM_LIT> ) \n masto_app = db . session . scalar ( db . select ( models . MastodonApp ) . filter_by ( api_base_url = base_url ) ) \n if not masto_app : \n app . logger . error ( \"<STR_LIT>\" , base_url ) \n flask . abort ( <NUM_LIT> ) \n app . logger . info ( \"<STR_LIT>\" , current_user . id , base_url ) \n account = masto_app . create_account ( current_user . id , code ) \n app . logger . info ( \"<STR_LIT>\" ) \n return flask . redirect ( flask . url_for ( '<STR_LIT>' , masto_acct = account . id ) )"}, {"input": "from app . utils . signs import SIGNS , get_id_from_sign \n from app . models . horoscope import Horoscope \n from datetime import datetime \n class HoroscopeService : \n def __init__ ( self , horoscope_repository ) : \n self . horoscope_repository = horoscope_repository", "output": "def get_horoscope_info ( self , sign , date = None , lang = None ) : \n if date : \n current_date = datetime . now ( ) . date ( ) \n if datetime . strptime ( date , \"<STR_LIT>\" ) . date ( ) > current_date : \n raise ValueError ( \"<STR_LIT>\" ) \n id , sing_extra_info = get_id_from_sign ( sign ) \n content = self . horoscope_repository . get_horoscope_info ( id , date , lang ) \n if not date : \n date = datetime . now ( ) . strftime ( \"<STR_LIT>\" ) \n return Horoscope ( \n sign = sing_extra_info [ '<STR_LIT>' ] , \n horoscope = content , \n date = date , \n icon = sing_extra_info [ '<STR_LIT>' ] , \n id = id \n )"}, {"input": "import shutil \n import logging \n import sys \n import os \n from flask import Flask , Blueprint , request \n from flask_caching import Cache \n app = Flask ( __name__ ) \n logger = logging . getLogger ( __name__ ) \n v1_bp = Blueprint ( '<STR_LIT>' , __name__ , url_prefix = '<STR_LIT>' ) \n v1_bp . config = app . config . copy ( ) \n cache_dir = '<STR_LIT>'", "output": "try : \n shutil . rmtree ( cache_dir ) \n except FileNotFoundError : \n pass \n cache = Cache ( app , config = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : cache_dir \n } ) \n def make_cache_key ( * args , ** kwargs ) : \n path = request . path \n args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) \n return path + args \n def get_base_path ( ) : \n if getattr ( sys , '<STR_LIT>' , False ) : \n return sys . _MEIPASS \n else : \n return os . getcwd ( ) \n src_path = os . path . join ( get_base_path ( ) , '<STR_LIT>' ) \n __all__ = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ]"}, {"input": "import inspect \n import pytest \n from instld . errors import InstallingPackageError \n from instld . cli . parsing_comments . get_comment_string import get_comment_string_by_frame , get_comment_substring_from_string \n def test_get_comment_started_with_instld ( ) : \n comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) \n assert comment == '<STR_LIT>' \n def test_get_comment_not_started_with_instld ( ) : \n comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) \n assert comment is None \n def test_get_comment_without_comment ( ) : \n comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) \n assert comment is None \n def test_get_comment_wrong ( ) : \n with pytest . raises ( InstallingPackageError ) : \n comment = get_comment_string_by_frame ( inspect . currentframe ( ) ) \n def test_get_comment_substring_from_string ( ) :", "output": "assert get_comment_substring_from_string ( '<STR_LIT>' ) is None \n assert get_comment_substring_from_string ( '<STR_LIT>' ) == '<STR_LIT>' \n with pytest . raises ( InstallingPackageError ) : \n assert get_comment_substring_from_string ( '<STR_LIT>' )"}, {"input": "from flask import Flask \n from app . controllers . horoscope_ctr_get import horoscope_blueprint_get \n from app . controllers . horoscope_ctr_home import horoscope_blueprint_home \n from app . controllers . horoscope_ctr_post import horoscope_blueprint_post \n from app . utils . config_docs import config_docs \n def create_app ( ) :", "output": "app = Flask ( __name__ , static_folder = '<STR_LIT>' , static_url_path = '<STR_LIT>' ) \n app . register_blueprint ( horoscope_blueprint_post ) \n app . register_blueprint ( horoscope_blueprint_get ) \n app . register_blueprint ( horoscope_blueprint_home ) \n config_docs ( app ) \n return app"}, {"input": "def create_bot ( bot_type ) : \n if bot_type == '<STR_LIT>' : \n from bot . baidu . baidu_unit_bot import BaiduUnitBot \n return BaiduUnitBot ( )", "output": "elif bot_type == '<STR_LIT>' : \n from bot . chatgpt . chat_gpt_bot import ChatGPTBot \n return ChatGPTBot ( ) \n elif bot_type == '<STR_LIT>' : \n from bot . openai . open_ai_bot import OpenAIBot \n return OpenAIBot ( ) \n elif bot_type == '<STR_LIT>' : \n from bot . claude . claude_ai_bot import ClaudeAiBot \n return ClaudeAiBot ( ) \n raise RuntimeError"}, {"input": "import werobot \n import time \n from config import conf \n from common . log import logger \n from channel . channel import Channel \n from bridge . bridge import Bridge \n from concurrent . futures import ThreadPoolExecutor \n import config \n import os \n config . load_config ( ) \n robot = werobot . WeRoBot ( token = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n cache = { } \n @ robot . text \n def hello_world ( msg ) : \n with open ( '<STR_LIT>' , '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : \n sensitive_words = [ line . strip ( ) for line in f . readlines ( ) ] \n found = False \n for word in sensitive_words : \n if word != '<STR_LIT>' and word in msg . content : \n found = True \n break \n if found : \n return \"<STR_LIT>\" \n else : \n logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) \n key = msg . content + '<STR_LIT>' + msg . source \n if cache . get ( key ) : \n cache . get ( key ) [ '<STR_LIT>' ] += <NUM_LIT> \n return WechatSubsribeAccount ( ) . handle ( msg ) \n class WechatSubsribeAccount ( Channel ) : \n def __init__ ( self ) : \n self . host = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) \n self . port = conf ( ) . get ( '<STR_LIT>' ) . get ( '<STR_LIT>' ) \n logger . info ( \"<STR_LIT>\" . format ( \n self . host , self . port ) ) \n def startup ( self ) : \n logger . info ( '<STR_LIT>' ) \n robot . config [ '<STR_LIT>' ] = self . host \n robot . config [ '<STR_LIT>' ] = self . port \n robot . run ( ) \n def handle ( self , msg , count = <NUM_LIT> ) : \n if msg . content == \"<STR_LIT>\" : \n return self . get_un_send_content ( msg . source ) \n context = dict ( ) \n context [ '<STR_LIT>' ] = msg . source \n key = msg . content + '<STR_LIT>' + msg . source \n res = cache . get ( key ) \n if not res : \n cache [ key ] = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } \n thread_pool . submit ( self . _do_send , msg . content , context ) \n res = cache . get ( key ) \n logger . info ( \"<STR_LIT>\" . format ( count , res ) ) \n if res . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n res [ '<STR_LIT>' ] = \"<STR_LIT>\" \n cache . pop ( key ) \n return res . get ( \"<STR_LIT>\" ) \n if cache . get ( key ) [ '<STR_LIT>' ] == <NUM_LIT> and count >= <NUM_LIT> : \n logger . info ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n if count <= <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n if count == <NUM_LIT> : \n return None \n return self . handle ( msg , count + <NUM_LIT> ) \n def _do_send ( self , query , context ) : \n key = query + '<STR_LIT>' + context [ '<STR_LIT>' ] \n reply_text = self . build_reply_content ( query , context ) \n logger . info ( '<STR_LIT>' . format ( reply_text ) ) \n cache [ key ] [ '<STR_LIT>' ] = \"<STR_LIT>\" \n cache [ key ] [ '<STR_LIT>' ] = reply_text \n def get_un_send_content ( self , from_user_id ) : \n for key in cache : \n if from_user_id in key : \n value = cache [ key ] \n if value . get ( '<STR_LIT>' ) == \"<STR_LIT>\" : \n cache . pop ( key ) \n return value . get ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n return \"<STR_LIT>\" \n def build_reply_content ( self , query , context = None ) :", "output": "return Bridge ( ) . fetch_reply_content ( query , context )"}, {"input": "from typing import List \n import argparse \n import re \n import glob \n import json \n from datasets import load_dataset \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n from multi_token . modalities . document_gte import ( \n split_text_into_documents , \n ) \n TEMP_TOKEN = \"<STR_LIT>\" \n LONG_ALPACA_REGEXES = [ \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) , \n lambda m : f\"<STR_LIT>\" , \n ) , \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) , \n lambda m : f\"<STR_LIT>\" , \n ) , \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) , \n lambda m : f\"<STR_LIT>\" , \n ) , \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) , \n lambda m : f\"<STR_LIT>\" , \n ) , \n ] \n LONG_DATA_REGEXES = [ \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) . strip ( ) , \n lambda m : f\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) . strip ( ) , \n ) , \n ( \n r\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) . strip ( ) , \n lambda m : f\"<STR_LIT>\" , \n lambda m : m . group ( <NUM_LIT> ) . strip ( ) , \n ) , \n ] \n def _write_long_alpaca_convo ( row , max_document_chunks ) -> List : \n doc_text = None \n prompt = None \n for regex , get_doc , get_prompt in LONG_ALPACA_REGEXES : \n match = re . match ( regex , row [ \"<STR_LIT>\" ] ) \n if match : \n doc_text = get_doc ( match ) \n prompt = get_prompt ( match ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n break \n if doc_text is None and row [ \"<STR_LIT>\" ] : \n doc_text = row [ \"<STR_LIT>\" ] \n prompt = row [ \"<STR_LIT>\" ] + f\"<STR_LIT>\" \n if doc_text is None : \n raise ValueError ( \"<STR_LIT>\" ) \n docs = split_text_into_documents ( doc_text ) \n if len ( docs ) > max_document_chunks : \n raise ValueError ( \"<STR_LIT>\" ) \n example = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + str ( hash ( row [ \"<STR_LIT>\" ] ) ) , \n \"<STR_LIT>\" : docs , \n } \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : prompt . replace ( TEMP_TOKEN , \"<STR_LIT>\" * len ( docs ) ) , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n } , \n ] \n return example \n def _write_long_data_collections_convo ( row , max_document_chunks ) -> List : \n doc_text = None \n prompt = None \n answer = None \n for regex , get_doc , get_prompt , get_answer in LONG_DATA_REGEXES : \n match = re . match ( regex , row [ \"<STR_LIT>\" ] ) \n if match : \n doc_text = get_doc ( match ) \n prompt = get_prompt ( match ) \n answer = get_answer ( match ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n break", "output": "if not doc_text or not prompt or not answer : \n raise ValueError ( \"<STR_LIT>\" ) \n docs = split_text_into_documents ( doc_text ) \n if len ( docs ) > max_document_chunks : \n raise ValueError ( \"<STR_LIT>\" ) \n example = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + str ( hash ( row [ \"<STR_LIT>\" ] ) ) , \n \"<STR_LIT>\" : docs , \n } \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : prompt . replace ( TEMP_TOKEN , \"<STR_LIT>\" * len ( docs ) ) , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : answer , \n } , \n ] \n return example \n def main ( args ) : \n long_alpaca = load_dataset ( args . long_alpaca_path , \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n def gen ( ) : \n for row in long_alpaca : \n try : \n yield _write_long_alpaca_convo ( row , args . max_document_chunks ) \n except ValueError : \n continue \n for long_collection_fn in glob . iglob ( args . long_collections_glob ) : \n with open ( long_collection_fn ) as f : \n for line in f : \n row = json . loads ( line ) \n try : \n yield _write_long_data_collections_convo ( \n row , args . max_document_chunks \n ) \n except ValueError : \n continue \n ds = Dataset . from_generator ( gen ) \n ds = ds . shuffle ( seed = <NUM_LIT> ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" ) \n parser . add_argument ( \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from . base import ma \n from app . models import Channel \n from . channel_allowlist import ChannelAllowListSchema \n from . message import MessageSchema \n class ChannelSchema ( ma . SQLAlchemySchema ) :", "output": "class Meta : \n model = Channel \n ordered = True \n name = ma . auto_field ( ) \n allowed_users = ma . Nested ( ChannelAllowListSchema , many = True ) \n messages = ma . Nested ( MessageSchema , many = True )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = None \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None :", "output": "op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . String ( ) , nullable = True ) ) \n def downgrade ( ) -> None : \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "import torch \n import json \n import os \n version_config_list = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n def singleton_variable ( func ) : \n def wrapper ( * args , ** kwargs ) : \n if not wrapper . instance : \n wrapper . instance = func ( * args , ** kwargs ) \n return wrapper . instance \n wrapper . instance = None \n return wrapper \n @ singleton_variable \n class Config : \n def __init__ ( self ) : \n self . device = \"<STR_LIT>\" \n self . is_half = True \n self . use_jit = False \n self . n_cpu = <NUM_LIT> \n self . gpu_name = None \n self . json_config = self . load_config_json ( ) \n self . gpu_mem = None \n self . instead = \"<STR_LIT>\" \n self . x_pad , self . x_query , self . x_center , self . x_max = self . device_config ( ) \n @ staticmethod \n def load_config_json ( ) -> dict : \n d = { } \n for config_file in version_config_list : \n with open ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n d [ config_file ] = json . load ( f ) \n return d \n @ staticmethod \n def has_mps ( ) -> bool : \n if not torch . backends . mps . is_available ( ) : \n return False \n try : \n torch . zeros ( <NUM_LIT> ) . to ( torch . device ( \"<STR_LIT>\" ) ) \n return True \n except Exception : \n return False \n @ staticmethod \n def has_xpu ( ) -> bool : \n if hasattr ( torch , \"<STR_LIT>\" ) and torch . xpu . is_available ( ) : \n return True \n else : \n return False \n def use_fp32_config ( self ) : \n print ( \n f\"<STR_LIT>\" \n ) \n for config_file in version_config_list : \n self . json_config [ config_file ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = False \n with open ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n strr = f . read ( ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n with open ( f\"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n f . write ( strr ) \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n strr = f . read ( ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n f . write ( strr ) \n def device_config ( self ) -> tuple : \n if torch . cuda . is_available ( ) : \n if self . has_xpu ( ) : \n self . device = self . instead = \"<STR_LIT>\" \n self . is_half = True \n i_device = int ( self . device . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] ) \n self . gpu_name = torch . cuda . get_device_name ( i_device ) \n if ( \n ( \"<STR_LIT>\" in self . gpu_name and \"<STR_LIT>\" not in self . gpu_name . upper ( ) ) \n or \"<STR_LIT>\" in self . gpu_name . upper ( ) \n or \"<STR_LIT>\" in self . gpu_name . upper ( ) \n or \"<STR_LIT>\" in self . gpu_name \n or \"<STR_LIT>\" in self . gpu_name \n or \"<STR_LIT>\" in self . gpu_name \n ) : \n self . is_half = False \n self . use_fp32_config ( ) \n self . gpu_mem = int ( \n torch . cuda . get_device_properties ( i_device ) . total_memory \n / <NUM_LIT> \n / <NUM_LIT> \n / <NUM_LIT> \n + <NUM_LIT> \n ) \n if self . gpu_mem <= <NUM_LIT> : \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n strr = f . read ( ) . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n with open ( \"<STR_LIT>\" , \"<STR_LIT>\" ) as f : \n f . write ( strr ) \n elif self . has_mps ( ) : \n print ( \"<STR_LIT>\" ) \n self . device = self . instead = \"<STR_LIT>\" \n self . is_half = False \n self . use_fp32_config ( ) \n else : \n print ( \"<STR_LIT>\" ) \n self . device = self . instead = \"<STR_LIT>\" \n self . is_half = False \n self . use_fp32_config ( ) \n if self . n_cpu == <NUM_LIT> : \n self . n_cpu = os . cpu_count ( ) \n if self . is_half : \n x_pad = <NUM_LIT> \n x_query = <NUM_LIT> \n x_center = <NUM_LIT> \n x_max = <NUM_LIT> \n else : \n x_pad = <NUM_LIT> \n x_query = <NUM_LIT>", "output": "x_center = <NUM_LIT> \n x_max = <NUM_LIT> \n if self . gpu_mem is not None and self . gpu_mem <= <NUM_LIT> : \n x_pad = <NUM_LIT> \n x_query = <NUM_LIT> \n x_center = <NUM_LIT> \n x_max = <NUM_LIT> \n return x_pad , x_query , x_center , x_max \n def max_vram_gpu ( gpu ) : \n if torch . cuda . is_available ( ) : \n gpu_properties = torch . cuda . get_device_properties ( gpu ) \n total_memory_gb = round ( gpu_properties . total_memory / <NUM_LIT> / <NUM_LIT> / <NUM_LIT> ) \n return total_memory_gb \n else : \n return \"<STR_LIT>\" \n def get_gpu_info ( ) : \n ngpu = torch . cuda . device_count ( ) \n gpu_infos = [ ] \n if torch . cuda . is_available ( ) or ngpu != <NUM_LIT> : \n for i in range ( ngpu ) : \n gpu_name = torch . cuda . get_device_name ( i ) \n mem = int ( \n torch . cuda . get_device_properties ( i ) . total_memory / <NUM_LIT> / <NUM_LIT> / <NUM_LIT> \n + <NUM_LIT> \n ) \n gpu_infos . append ( \"<STR_LIT>\" % ( i , gpu_name , mem ) ) \n if len ( gpu_infos ) > <NUM_LIT> : \n gpu_info = \"<STR_LIT>\" . join ( gpu_infos ) \n else : \n gpu_info = \"<STR_LIT>\" \n return gpu_info"}, {"input": "import logging \n try : \n import Queue as queue \n except ImportError : \n import queue \n from . templates import AttributeDict \n logger = logging . getLogger ( '<STR_LIT>' ) \n class Queue ( queue . Queue ) : \n def put ( self , message ) : \n queue . Queue . put ( self , Message ( message ) ) \n class Message ( AttributeDict ) : \n def download ( self , fileName ) : \n if hasattr ( self . text , '<STR_LIT>' ) : \n return self . text ( fileName ) \n else : \n return b'<STR_LIT>' \n def __getitem__ ( self , value ) : \n if value in ( '<STR_LIT>' , '<STR_LIT>' ) :", "output": "v = value [ <NUM_LIT> ] . upper ( ) + value [ <NUM_LIT> : ] \n logger . debug ( '<STR_LIT>' % ( value , v ) ) \n value = v \n return super ( Message , self ) . __getitem__ ( value ) \n def __str__ ( self ) : \n return '<STR_LIT>' % '<STR_LIT>' . join ( \n [ '<STR_LIT>' % ( repr ( k ) , repr ( v ) ) for k , v in self . items ( ) ] ) \n def __repr__ ( self ) : \n return '<STR_LIT>' % ( self . __class__ . __name__ . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , \n self . __str__ ( ) )"}, {"input": "import asyncio \n from contextlib import asynccontextmanager \n import pytest \n import svcs \n from tests . fake_factories import async_bool_cm_factory , async_int_factory \n from tests . helpers import CloseMe \n try : \n from starlette . applications import Starlette \n from starlette . middleware import Middleware \n from starlette . responses import JSONResponse \n from starlette . routing import Route \n from starlette . testclient import TestClient \n except ImportError : \n pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) \n @ pytest . mark . asyncio ( ) \n @ pytest . mark . parametrize ( \"<STR_LIT>\" , [ True , False ] ) \n @ pytest . mark . parametrize ( \"<STR_LIT>\" , [ True , False ] ) \n async def test_integration ( yield_something , cm ) : \n close_me_registry = CloseMe ( ) \n close_me_container = CloseMe ( ) \n async def factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n yield <NUM_LIT> \n await asyncio . sleep ( <NUM_LIT> ) \n await close_me_container . aclose ( ) \n async def close_registry ( ) :", "output": "await close_me_registry . aclose ( ) \n if yield_something : \n async def lifespan ( app : Starlette , registry : svcs . Registry ) : \n registry . register_factory ( \n int , factory , on_registry_close = close_registry \n ) \n yield { \"<STR_LIT>\" : \"<STR_LIT>\" } \n else : \n async def lifespan ( app : Starlette , registry : svcs . Registry ) : \n registry . register_factory ( \n int , factory , on_registry_close = close_registry \n ) \n yield \n if cm : \n lifespan = asynccontextmanager ( lifespan ) \n async def view ( request ) : \n val = await svcs . starlette . aget ( request , int ) \n assert ( \n val \n == await svcs . starlette . aget_abstract ( request , int ) \n == await svcs . starlette . svcs_from ( request ) . aget ( int ) \n ) \n return JSONResponse ( { \"<STR_LIT>\" : val } ) \n app = Starlette ( \n lifespan = svcs . starlette . lifespan ( lifespan ) , \n middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , \n routes = [ Route ( \"<STR_LIT>\" , view ) ] , \n ) \n with TestClient ( app ) as client : \n assert { \"<STR_LIT>\" : <NUM_LIT> } == client . get ( \"<STR_LIT>\" ) . json ( ) \n assert close_me_container . is_aclosed \n assert close_me_registry . is_aclosed \n async def healthy ( request ) : \n ok = [ ] \n failing = [ ] \n code = <NUM_LIT> \n for svc in svcs . starlette . get_pings ( request ) : \n try : \n await svc . aping ( ) \n ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } ) \n code = <NUM_LIT> \n return JSONResponse ( \n content = { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } , status_code = code \n ) \n @ pytest . mark . asyncio ( ) \n async def test_get_pings ( registry , container ) : \n async def aping ( _ ) : ... \n async def aboom ( _ ) : \n raise ValueError ( \"<STR_LIT>\" ) \n @ svcs . starlette . lifespan \n async def lifespan ( app : Starlette , registry : svcs . Registry ) : \n registry . register_factory ( int , async_int_factory , ping = aping ) \n registry . register_factory ( bool , async_bool_cm_factory , ping = aboom ) \n yield { \"<STR_LIT>\" : \"<STR_LIT>\" } \n app = Starlette ( \n lifespan = lifespan , \n middleware = [ Middleware ( svcs . starlette . SVCSMiddleware ) ] , \n routes = [ Route ( \"<STR_LIT>\" , healthy ) ] , \n ) \n with TestClient ( app ) as client : \n assert { \n \"<STR_LIT>\" : [ \n { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ] , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n } == client . get ( \"<STR_LIT>\" ) . json ( )"}, {"input": "import sys \n sys . path . append ( '<STR_LIT>' ) \n from dotenv import load_dotenv \n load_dotenv ( ) \n import openai \n from reliablegpt import reliableGPT \n import os \n import time \n good_open_ai_api_key = os . getenv ( '<STR_LIT>' ) \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n user_token = \"<STR_LIT>\" , \n queue_requests = True , \n fallback_strategy = [ \"<STR_LIT>\" ] ) \n def test_single_call_bad_key ( ) : \n openai . api_key = \"<STR_LIT>\" \n model = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n ] \n temperature = <NUM_LIT> \n error_count = <NUM_LIT> \n failure_count = <NUM_LIT> \n try : \n print ( \"<STR_LIT>\" ) \n response = openai . ChatCompletion . create ( model = model , \n messages = messages , \n temperature = temperature ) \n if response and \"<STR_LIT>\" in response : \n error_count += <NUM_LIT> \n if response == \"<STR_LIT>\" : \n failure_count += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n error_count += <NUM_LIT> \n print ( f\"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n if error_count == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n print ( test_single_call_bad_key ( ) ) \n def test_embedding_bad_key ( ) : \n openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n user_token = '<STR_LIT>' , \n send_notification = True ) \n openai . api_key = \"<STR_LIT>\" \n def get_embedding ( text , model = \"<STR_LIT>\" ) : \n text = text . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n return openai . Embedding . create ( input = [ text ] , \n model = model ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n result = get_embedding ( \"<STR_LIT>\" ) \n test_embedding_bad_key ( ) \n list_questions = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ] \n def simple_openai_call ( prompt ) : \n print ( f\"<STR_LIT>\" ) \n model = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : prompt \n } , \n ] \n result = openai . ChatCompletion . create ( model = model , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n simple_openai_call ( \"<STR_LIT>\" ) \n def test_regular_q ( ) : \n openai . api_key = good_open_ai_api_key", "output": "results = { } \n start_time = time . time ( ) \n for question in list_questions [ : <NUM_LIT> ] : \n print ( \"<STR_LIT>\" ) \n print ( question ) \n result = simple_openai_call ( question ) \n print ( \"<STR_LIT>\" ) \n print ( result ) \n results [ question ] = result \n print ( \"<STR_LIT>\" ) \n print ( results ) \n print ( len ( results ) ) \n end_time = time . time ( ) \n print ( \"<STR_LIT>\" , end_time - start_time ) \n test_regular_q ( )"}, {"input": "TEXT = '<STR_LIT>' \n MAP = '<STR_LIT>' \n CARD = '<STR_LIT>'", "output": "NOTE = '<STR_LIT>' \n SHARING = '<STR_LIT>' \n PICTURE = '<STR_LIT>' \n RECORDING = VOICE = '<STR_LIT>' \n ATTACHMENT = '<STR_LIT>' \n VIDEO = '<STR_LIT>' \n FRIENDS = '<STR_LIT>' \n SYSTEM = '<STR_LIT>' \n INCOME_MSG = [ TEXT , MAP , CARD , NOTE , SHARING , PICTURE , \n RECORDING , VOICE , ATTACHMENT , VIDEO , FRIENDS , SYSTEM ]"}, {"input": "from setuptools import setup , find_packages \n with open ( \"<STR_LIT>\" ) as f : \n required = f . read ( ) . splitlines ( ) \n setup ( \n name = \"<STR_LIT>\" , \n version = \"<STR_LIT>\" , \n description = \"<STR_LIT>\" , \n url = \"<STR_LIT>\" ,", "output": "author = \"<STR_LIT>\" , \n license = \"<STR_LIT>\" , \n packages = find_packages ( ) , \n include_package_data = True , \n install_requires = required , \n )"}, {"input": "import mutagen . asf \n from mutagen . id3 import PictureType \n from . import util \n from . file import AudioFile , TAG_MAP_ENTRY , Artwork , MetadataItem \n pic_type2tag = { \n PictureType . COVER_FRONT : '<STR_LIT>' , \n PictureType . COVER_BACK : '<STR_LIT>' , \n } \n pic_tag2type = { } \n for key , val in pic_type2tag . items ( ) : \n pic_tag2type [ val ] = key \n del key , val \n def get_pictures ( afile , norm_key ) : \n artworks = [ ] \n if \"<STR_LIT>\" in afile . mfile . tags : \n p = afile . mfile . tags [ \"<STR_LIT>\" ] [ <NUM_LIT> ] . value \n if not isinstance ( p , bytes ) : \n p = eval ( p ) \n try : \n artwork = Artwork ( p ) \n except OSError : \n artwork = Artwork ( p . split ( b'<STR_LIT>' , <NUM_LIT> ) [ <NUM_LIT> ] ) \n artworks . append ( artwork ) \n return MetadataItem ( Artwork , None , artworks ) \n def set_pictures ( afile , norm_key , artworks ) : \n for art in artworks . values : \n pic_tag = \"<STR_LIT>\" \n raw = ( pic_tag + '<STR_LIT>' ) . encode ( '<STR_LIT>' ) + b'<STR_LIT>' + art . raw \n afile . mfile . tags [ pic_tag ] = raw \n class AsfFile ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . asf . ASF \n _TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) ,", "output": "'<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int , sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n remover = list ( pic_tag2type . keys ( ) ) , \n type = Artwork ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n } \n def __init__ ( self , filename , ** kwargs ) : \n super ( AsfFile , self ) . __init__ ( filename , ** kwargs )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None", "output": "def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = True ) ) \n batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ \n '<STR_LIT>' ] , [ '<STR_LIT>' ] ) \n batch_op . drop_column ( '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = True ) ) \n batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) \n batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "from flask import Blueprint , jsonify , request \n from app . repositories . horoscope_repository import HoroscopeRepository \n from app . services . horoscope_service import HoroscopeService \n from app . utils . status_code import Status \n from app . utils . signs import SIGNS \n horoscope_blueprint_get = Blueprint ( '<STR_LIT>' , __name__ ) \n @ horoscope_blueprint_get . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def get_horoscope ( sign = None ) : \n date = request . args . get ( '<STR_LIT>' , None ) \n lang = request . args . get ( '<STR_LIT>' , None ) \n horoscope_repository = HoroscopeRepository ( ) \n service = HoroscopeService ( horoscope_repository ) \n try :", "output": "horoscope = service . get_horoscope_info ( sign , date , lang ) \n url = f\"<STR_LIT>\" \n horoscope . icon = horoscope . icon . format ( path = f\"<STR_LIT>\" ) \n return jsonify ( horoscope . __dict__ ) , Status . HTTP_OK \n except ValueError as e : \n return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_BAD_REQUEST \n except Exception as e : \n return jsonify ( { '<STR_LIT>' : e . args [ <NUM_LIT> ] } ) , Status . HTTP_INTERNAL_SERVER_ERROR \n @ horoscope_blueprint_get . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def get_horoscope_list ( ) : \n url = f\"<STR_LIT>\" \n list_sign = list ( map ( lambda sign : { ** sign , \"<STR_LIT>\" : sign [ \"<STR_LIT>\" ] . format ( path = f\"<STR_LIT>\" ) } , SIGNS . values ( ) ) ) \n return jsonify ( list_sign ) , Status . HTTP_OK"}, {"input": "def esta_es_una_funcion ( ) : \n return \"<STR_LIT>\" \n class Mapping : \n def __init__ ( self , objeto_iterable ) : \n self . lista = [ ] \n self . __actualizar ( objeto_iterable ) \n def actualizar ( self , obbjeto_iterable ) : \n for item in obbjeto_iterable : \n self . lista . append ( item ) \n __actualizar = actualizar \n class MappingSubClass ( Mapping ) : \n def actualizar ( self , keys , values ) : \n for item in zip ( keys , values ) : \n self . lista . append ( item ) \n instancia = MappingSubClass ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n instancia . actualizar ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) \n print ( instancia . lista ) \n numeros = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n colores = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n autos = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n combinados = zip ( numeros , colores , autos )", "output": "for elemento in combinados : \n print ( elemento )"}, {"input": "from typing import List \n import argparse \n import json \n import os \n import random \n import openai \n from datasets import Dataset , load_dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n PROMPT = \n PRETRAIN_PHRASES = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n OPENAI_TOOLS = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } , \n } , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n } , \n } , \n } \n ] \n def _build_convo ( row ) -> List : \n client = openai . Client ( ) \n captions = [ row [ \"<STR_LIT>\" ] ] \n sounds = [ row [ \"<STR_LIT>\" ] ] \n captions_text = \"<STR_LIT>\" . join ( [ f'<STR_LIT>' for i , cap in enumerate ( captions ) ] ) \n prompt = PROMPT . format ( captions = captions_text ) . strip ( ) \n completion = client . chat . completions . create ( \n model = \"<STR_LIT>\" , \n messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : prompt } ] , \n tools = OPENAI_TOOLS , \n tool_choice = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : { \"<STR_LIT>\" : \"<STR_LIT>\" } } , \n ) \n resp = json . loads ( completion . choices [ <NUM_LIT> ] . message . tool_calls [ <NUM_LIT> ] . function . arguments ) \n caption = resp [ \"<STR_LIT>\" ] \n q = random . choice ( PRETRAIN_PHRASES ) \n example = { \n \"<STR_LIT>\" : sounds , \n \"<STR_LIT>\" : [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : q , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : caption , \n } , \n ] , \n } \n return example \n def main ( args ) : \n data = load_dataset ( \"<STR_LIT>\" , split = \"<STR_LIT>\" ) \n os . makedirs ( args . cache_folder , exist_ok = True ) \n def gen ( seeds ) : \n cache = open ( \n os . path . join ( args . cache_folder , f\"<STR_LIT>\" ) , \"<STR_LIT>\" \n ) \n for s in seeds : \n selected_row = data [ s ] \n try : \n example = _build_convo ( selected_row ) \n cache . write ( json . dumps ( example ) + \"<STR_LIT>\" ) \n yield example \n except Exception as e : \n print ( e ) \n continue \n cache . close ( ) \n idxs = list ( range ( len ( data ) ) ) \n random . shuffle ( idxs ) \n ds = Dataset . from_generator ( \n gen , \n num_proc = args . num_proc , \n gen_kwargs = { \"<STR_LIT>\" : idxs } , \n ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "from abc import ABC , abstractmethod \n import torch \n from typing import Tuple , Union \n class VecEnv ( ABC ) : \n num_envs : int \n num_obs : int \n num_privileged_obs : int \n num_actions : int \n max_episode_length : int \n privileged_obs_buf : torch . Tensor \n obs_buf : torch . Tensor \n rew_buf : torch . Tensor \n reset_buf : torch . Tensor \n episode_length_buf : torch . Tensor \n extras : dict \n device : torch . device \n @ abstractmethod \n def step ( self , actions : torch . Tensor ) -> Tuple [ torch . Tensor , Union [ torch . Tensor , None ] , torch . Tensor , torch . Tensor , dict ] : \n pass \n @ abstractmethod", "output": "def reset ( self , env_ids : Union [ list , torch . Tensor ] ) : \n pass \n @ abstractmethod \n def get_observations ( self ) -> torch . Tensor : \n pass \n @ abstractmethod \n def get_privileged_observations ( self ) -> Union [ torch . Tensor , None ] : \n pass"}, {"input": "lista = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , \"<STR_LIT>\" , <NUM_LIT> , False , <NUM_LIT> ] \n print ( lista ) \n print ( \"<STR_LIT>\" )", "output": "lista . append ( <NUM_LIT> ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n lista2 = [ <NUM_LIT> , True , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n lista . extend ( lista2 ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n lista . insert ( <NUM_LIT> , <NUM_LIT> ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n lista . remove ( <NUM_LIT> ) \n lista . remove ( <NUM_LIT> ) \n print ( lista . remove ( '<STR_LIT>' ) ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n print ( lista . pop ( - <NUM_LIT> ) ) \n elemento_eliminado = lista . pop ( <NUM_LIT> ) \n print ( elemento_eliminado ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n print ( lista . index ( <NUM_LIT> ) ) \n print ( lista . index ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) \n print ( \"<STR_LIT>\" ) \n lista_copia = lista \n lista_medio_metro = lista . copy ( ) \n lista_medio_metro . pop ( ) \n lista . append ( <NUM_LIT> ) \n print ( lista_copia ) \n print ( lista ) \n print ( lista_medio_metro ) \n print ( \"<STR_LIT>\" ) \n lista . append ( <NUM_LIT> ) \n print ( lista ) \n print ( lista . count ( <NUM_LIT> ) ) \n print ( \"<STR_LIT>\" ) \n lista . remove ( \"<STR_LIT>\" ) \n print ( lista ) \n lista . sort ( ) \n print ( lista ) \n lista_palabras = [ \"<STR_LIT>\" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n print ( lista_palabras ) \n lista_palabras . sort ( ) \n print ( lista_palabras ) \n print ( \"<STR_LIT>\" ) \n print ( lista ) \n lista . reverse ( ) \n print ( lista ) \n print ( \"<STR_LIT>\" ) \n lista . clear ( ) \n print ( lista )"}, {"input": "import torch \n def feature_loss ( fmap_r , fmap_g ) : \n loss = <NUM_LIT> \n for dr , dg in zip ( fmap_r , fmap_g ) : \n for rl , gl in zip ( dr , dg ) : \n rl = rl . float ( ) . detach ( ) \n gl = gl . float ( ) \n loss += torch . mean ( torch . abs ( rl - gl ) ) \n return loss * <NUM_LIT> \n def discriminator_loss ( disc_real_outputs , disc_generated_outputs ) : \n loss = <NUM_LIT> \n r_losses = [ ] \n g_losses = [ ] \n for dr , dg in zip ( disc_real_outputs , disc_generated_outputs ) : \n dr = dr . float ( ) \n dg = dg . float ( ) \n r_loss = torch . mean ( ( <NUM_LIT> - dr ) ** <NUM_LIT> ) \n g_loss = torch . mean ( dg ** <NUM_LIT> ) \n loss += r_loss + g_loss \n r_losses . append ( r_loss . item ( ) ) \n g_losses . append ( g_loss . item ( ) ) \n return loss , r_losses , g_losses \n def generator_loss ( disc_outputs ) : \n loss = <NUM_LIT> \n gen_losses = [ ]", "output": "for dg in disc_outputs : \n dg = dg . float ( ) \n l = torch . mean ( ( <NUM_LIT> - dg ) ** <NUM_LIT> ) \n gen_losses . append ( l ) \n loss += l \n return loss , gen_losses \n def kl_loss ( z_p , logs_q , m_p , logs_p , z_mask ) : \n z_p = z_p . float ( ) \n logs_q = logs_q . float ( ) \n m_p = m_p . float ( ) \n logs_p = logs_p . float ( ) \n z_mask = z_mask . float ( ) \n kl = logs_p - logs_q - <NUM_LIT> \n kl += <NUM_LIT> * ( ( z_p - m_p ) ** <NUM_LIT> ) * torch . exp ( - <NUM_LIT> * logs_p ) \n kl = torch . sum ( kl * z_mask ) \n l = kl / torch . sum ( z_mask ) \n return l"}, {"input": "import math \n import numpy as np \n import torch \n from torch import nn \n from torch . nn import functional as F \n def init_weights ( m , mean = <NUM_LIT> , std = <NUM_LIT> ) : \n classname = m . __class__ . __name__ \n if classname . find ( \"<STR_LIT>\" ) != - <NUM_LIT> : \n m . weight . data . normal_ ( mean , std ) \n def get_padding ( kernel_size , dilation = <NUM_LIT> ) : \n return int ( ( kernel_size * dilation - dilation ) / <NUM_LIT> )", "output": "def convert_pad_shape ( pad_shape ) : \n l = pad_shape [ : : - <NUM_LIT> ] \n pad_shape = [ item for sublist in l for item in sublist ] \n return pad_shape \n def kl_divergence ( m_p , logs_p , m_q , logs_q ) : \n kl = ( logs_q - logs_p ) - <NUM_LIT> \n kl += ( \n <NUM_LIT> * ( torch . exp ( <NUM_LIT> * logs_p ) + ( ( m_p - m_q ) ** <NUM_LIT> ) ) * torch . exp ( - <NUM_LIT> * logs_q ) \n ) \n return kl \n def rand_gumbel ( shape ) : \n uniform_samples = torch . rand ( shape ) * <NUM_LIT> + <NUM_LIT> \n return - torch . log ( - torch . log ( uniform_samples ) ) \n def rand_gumbel_like ( x ) : \n g = rand_gumbel ( x . size ( ) ) . to ( dtype = x . dtype , device = x . device ) \n return g \n def slice_segments ( x , ids_str , segment_size = <NUM_LIT> ) : \n ret = torch . zeros_like ( x [ : , : , : segment_size ] ) \n for i in range ( x . size ( <NUM_LIT> ) ) : \n idx_str = ids_str [ i ] \n idx_end = idx_str + segment_size \n ret [ i ] = x [ i , : , idx_str : idx_end ] \n return ret \n def slice_segments2 ( x , ids_str , segment_size = <NUM_LIT> ) : \n ret = torch . zeros_like ( x [ : , : segment_size ] ) \n for i in range ( x . size ( <NUM_LIT> ) ) : \n idx_str = ids_str [ i ] \n idx_end = idx_str + segment_size \n ret [ i ] = x [ i , idx_str : idx_end ] \n return ret \n def rand_slice_segments ( x , x_lengths = None , segment_size = <NUM_LIT> ) : \n b , d , t = x . size ( ) \n if x_lengths is None : \n x_lengths = t \n ids_str_max = x_lengths - segment_size + <NUM_LIT> \n ids_str = ( torch . rand ( [ b ] ) . to ( device = x . device ) * ids_str_max ) . to ( dtype = torch . long ) \n ret = slice_segments ( x , ids_str , segment_size ) \n return ret , ids_str \n def get_timing_signal_1d ( length , channels , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> ) : \n position = torch . arange ( length , dtype = torch . float ) \n num_timescales = channels // <NUM_LIT> \n log_timescale_increment = math . log ( float ( max_timescale ) / float ( min_timescale ) ) / ( \n num_timescales - <NUM_LIT> \n ) \n inv_timescales = min_timescale * torch . exp ( \n torch . arange ( num_timescales , dtype = torch . float ) * - log_timescale_increment \n ) \n scaled_time = position . unsqueeze ( <NUM_LIT> ) * inv_timescales . unsqueeze ( <NUM_LIT> ) \n signal = torch . cat ( [ torch . sin ( scaled_time ) , torch . cos ( scaled_time ) ] , <NUM_LIT> ) \n signal = F . pad ( signal , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , channels % <NUM_LIT> ] ) \n signal = signal . view ( <NUM_LIT> , channels , length ) \n return signal \n def add_timing_signal_1d ( x , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> ) : \n b , channels , length = x . size ( ) \n signal = get_timing_signal_1d ( length , channels , min_timescale , max_timescale ) \n return x + signal . to ( dtype = x . dtype , device = x . device ) \n def cat_timing_signal_1d ( x , min_timescale = <NUM_LIT> , max_timescale = <NUM_LIT> , axis = <NUM_LIT> ) : \n b , channels , length = x . size ( ) \n signal = get_timing_signal_1d ( length , channels , min_timescale , max_timescale ) \n return torch . cat ( [ x , signal . to ( dtype = x . dtype , device = x . device ) ] , axis ) \n def subsequent_mask ( length ) : \n mask = torch . tril ( torch . ones ( length , length ) ) . unsqueeze ( <NUM_LIT> ) . unsqueeze ( <NUM_LIT> ) \n return mask \n @ torch . jit . script \n def fused_add_tanh_sigmoid_multiply ( input_a , input_b , n_channels ) : \n n_channels_int = n_channels [ <NUM_LIT> ] \n in_act = input_a + input_b \n t_act = torch . tanh ( in_act [ : , : n_channels_int , : ] ) \n s_act = torch . sigmoid ( in_act [ : , n_channels_int : , : ] ) \n acts = t_act * s_act \n return acts \n def convert_pad_shape ( pad_shape ) : \n l = pad_shape [ : : - <NUM_LIT> ] \n pad_shape = [ item for sublist in l for item in sublist ] \n return pad_shape \n def shift_1d ( x ) : \n x = F . pad ( x , convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) [ : , : , : - <NUM_LIT> ] \n return x \n def sequence_mask ( length , max_length = None ) : \n if max_length is None : \n max_length = length . max ( ) \n x = torch . arange ( max_length , dtype = length . dtype , device = length . device ) \n return x . unsqueeze ( <NUM_LIT> ) < length . unsqueeze ( <NUM_LIT> ) \n def generate_path ( duration , mask ) : \n device = duration . device \n b , _ , t_y , t_x = mask . shape \n cum_duration = torch . cumsum ( duration , - <NUM_LIT> ) \n cum_duration_flat = cum_duration . view ( b * t_x ) \n path = sequence_mask ( cum_duration_flat , t_y ) . to ( mask . dtype ) \n path = path . view ( b , t_x , t_y ) \n path = path - F . pad ( path , convert_pad_shape ( [ [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] , [ <NUM_LIT> , <NUM_LIT> ] ] ) ) [ : , : - <NUM_LIT> ] \n path = path . unsqueeze ( <NUM_LIT> ) . transpose ( <NUM_LIT> , <NUM_LIT> ) * mask \n return path \n def clip_grad_value_ ( parameters , clip_value , norm_type = <NUM_LIT> ) : \n if isinstance ( parameters , torch . Tensor ) : \n parameters = [ parameters ] \n parameters = list ( filter ( lambda p : p . grad is not None , parameters ) ) \n norm_type = float ( norm_type ) \n if clip_value is not None : \n clip_value = float ( clip_value ) \n total_norm = <NUM_LIT> \n for p in parameters : \n param_norm = p . grad . data . norm ( norm_type ) \n total_norm += param_norm . item ( ) ** norm_type \n if clip_value is not None : \n p . grad . data . clamp_ ( min = - clip_value , max = clip_value ) \n total_norm = total_norm ** ( <NUM_LIT> / norm_type ) \n return total_norm"}, {"input": "from densely_captioned_images . repro . eval . VLChecklist . vl_checklist . vlp_model import VLPModel \n import torch \n from PIL import Image \n class VLCtoHFCLIPWrap ( VLPModel ) : \n def __init__ ( self , model_id , model , processor , device = None ) : \n self . model = model \n self . processor = processor \n if device is None : \n device = model . device \n self . device = device \n self . model_id = model_id \n self . batch_size = <NUM_LIT> \n def model_name ( self ) : \n return self . model_id", "output": "def _load_data ( self , src_type , data ) : \n pass \n def predict ( self , \n images : list , \n texts : list , \n src_type : str = '<STR_LIT>' \n ) : \n loaded_images = [ Image . open ( p ) for p in images ] \n with torch . no_grad ( ) : \n inputs = self . processor ( text = texts , images = loaded_images , return_tensors = '<STR_LIT>' , padding = True ) \n res = self . model ( ** inputs . to ( self . device ) ) \n logits_per_image = res . logits_per_image \n probs = [ ] \n probs . extend ( logits_per_image . cpu ( ) . diag ( ) . numpy ( ) ) \n return { \"<STR_LIT>\" : [ ( None , p ) for p in probs ] }"}, {"input": "print ( <NUM_LIT> + <NUM_LIT> ) \n print ( ( <NUM_LIT> - <NUM_LIT> * <NUM_LIT> ) / <NUM_LIT> ) \n impuesto = <NUM_LIT> / <NUM_LIT> \n precio = <NUM_LIT> \n precio * impuesto \n esta_dormido = False \n status = \"<STR_LIT>\" if esta_dormido else \"<STR_LIT>\" \n print ( status ) \n print ( '<STR_LIT>' ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( '<STR_LIT>' ) \n print ( '<STR_LIT>' ) \n print ( r'<STR_LIT>' ) \n print ( ) \n print ( <NUM_LIT> * '<STR_LIT>' + '<STR_LIT>' ) \n print ( <NUM_LIT> * '<STR_LIT>' ) \n print ( '<STR_LIT>' '<STR_LIT>' ) \n print ( '<STR_LIT>' , '<STR_LIT>' ) \n print ( \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" ) \n palabra = \"<STR_LIT>\" \n print ( palabra [ <NUM_LIT> ] ) \n print ( palabra [ <NUM_LIT> ] ) \n print ( palabra [ - <NUM_LIT> ] ) \n print ( palabra [ - <NUM_LIT> ] ) \n print ( palabra [ - <NUM_LIT> ] )", "output": "print ( palabra [ <NUM_LIT> ] + palabra [ <NUM_LIT> ] ) \n print ( palabra [ <NUM_LIT> : <NUM_LIT> ] ) \n print ( palabra [ <NUM_LIT> : <NUM_LIT> ] ) \n print ( palabra [ <NUM_LIT> : ] ) \n print ( len ( palabra ) ) \n print ( palabra [ <NUM_LIT> : <NUM_LIT> ] + palabra [ <NUM_LIT> ] ) \n print ( palabra [ <NUM_LIT> : <NUM_LIT> + <NUM_LIT> ] )"}, {"input": "import sys \n import os \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai \n from main import reliableGPT \n import concurrent . futures \n openai . api_type = \"<STR_LIT>\" \n openai . api_base = os . getenv ( \"<STR_LIT>\" ) \n openai . api_version = \"<STR_LIT>\" \n openai . api_key = os . getenv ( \"<STR_LIT>\" ) \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n backup_openai_key = os . getenv ( \"<STR_LIT>\" ) , _test = True , verbose = True )", "output": "openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n backup_openai_key = os . getenv ( '<STR_LIT>' ) , \n verbose = True ) \n def simple_openai_call ( prompt ) : \n print ( f\"<STR_LIT>\" ) \n engine = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : prompt \n } , \n ] \n result = openai . ChatCompletion . create ( engine = engine , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n response = simple_openai_call ( \"<STR_LIT>\" ) \n print ( response ) \n text_string = \"<STR_LIT>\" \n embeddings = openai . Embedding . create ( deployment_id = \"<STR_LIT>\" , \n input = text_string )"}, {"input": "import json \n import pytest \n from flask import Flask \n from flask . testing import FlaskClient \n from app . controllers . horoscope_ctr_get import horoscope_blueprint_get \n from app . services . horoscope_service import HoroscopeService \n from app . utils . status_code import Status \n @ pytest . fixture \n def app ( ) : \n app = Flask ( __name__ ) \n app . register_blueprint ( horoscope_blueprint_get ) \n return app \n @ pytest . fixture \n def client ( app ) : \n return app . test_client ( ) \n def test_get_horoscope ( client ) : \n response = client . get ( '<STR_LIT>' ) \n assert response . status_code == Status . HTTP_OK \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_get_horoscope_when_date_is_future ( client ) :", "output": "data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . get ( f\"<STR_LIT>\" , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_get_horoscope_when_date_is_past ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . get ( f\"<STR_LIT>\" , json = data ) \n assert response . status_code == Status . HTTP_OK \n def test_get_horoscope_when_date_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . get ( f\"<STR_LIT>\" , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_get_horoscope_when_lang_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . get ( f\"<STR_LIT>\" , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_get_horoscope_when_sign_is_invalid ( client ) : \n data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n response = client . get ( f\"<STR_LIT>\" , json = data ) \n assert response . status_code == Status . HTTP_BAD_REQUEST \n assert '<STR_LIT>' in json . loads ( response . get_data ( as_text = True ) ) \n def test_get_horoscope_list ( client ) : \n response = client . get ( '<STR_LIT>' ) \n print ( response . text ) \n assert response . status_code == Status . HTTP_OK"}, {"input": "import logging \n class LogSystem ( object ) : \n handlerList = [ ] \n showOnCmd = True \n loggingLevel = logging . INFO \n loggingFile = None \n def __init__ ( self ) : \n self . logger = logging . getLogger ( '<STR_LIT>' ) \n self . logger . addHandler ( logging . NullHandler ( ) ) \n self . logger . setLevel ( self . loggingLevel ) \n self . cmdHandler = logging . StreamHandler ( ) \n self . fileHandler = None", "output": "self . logger . addHandler ( self . cmdHandler ) \n def set_logging ( self , showOnCmd = True , loggingFile = None , \n loggingLevel = logging . INFO ) : \n if showOnCmd != self . showOnCmd : \n if showOnCmd : \n self . logger . addHandler ( self . cmdHandler ) \n else : \n self . logger . removeHandler ( self . cmdHandler ) \n self . showOnCmd = showOnCmd \n if loggingFile != self . loggingFile : \n if self . loggingFile is not None : \n self . logger . removeHandler ( self . fileHandler ) \n self . fileHandler . close ( ) \n if loggingFile is not None : \n self . fileHandler = logging . FileHandler ( loggingFile ) \n self . logger . addHandler ( self . fileHandler ) \n self . loggingFile = loggingFile \n if loggingLevel != self . loggingLevel : \n self . logger . setLevel ( loggingLevel ) \n self . loggingLevel = loggingLevel \n ls = LogSystem ( ) \n set_logging = ls . set_logging"}, {"input": "import werobot \n import time \n from config import channel_conf \n from common import const \n from common . log import logger \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n import os \n robot = werobot . WeRoBot ( token = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n cache = { } \n @ robot . text \n def hello_world ( msg ) : \n with open ( '<STR_LIT>' , '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : \n sensitive_words = [ line . strip ( ) for line in f . readlines ( ) ] \n found = False \n for word in sensitive_words : \n if word != '<STR_LIT>' and word in msg . content : \n found = True \n break \n if found : \n return \"<STR_LIT>\" \n else : \n logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) \n key = msg . content + '<STR_LIT>' + msg . source \n if cache . get ( key ) : \n cache . get ( key ) [ '<STR_LIT>' ] += <NUM_LIT> \n return WechatSubsribeAccount ( ) . handle ( msg ) \n class WechatSubsribeAccount ( Channel ) : \n def startup ( self ) : \n logger . info ( '<STR_LIT>' ) \n robot . config [ '<STR_LIT>' ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) \n robot . config [ '<STR_LIT>' ] = '<STR_LIT>' \n robot . run ( ) \n def handle ( self , msg , count = <NUM_LIT> ) :", "output": "if msg . content == \"<STR_LIT>\" : \n return self . get_un_send_content ( msg . source ) \n context = dict ( ) \n context [ '<STR_LIT>' ] = msg . source \n key = msg . content + '<STR_LIT>' + msg . source \n res = cache . get ( key ) \n if not res : \n cache [ key ] = { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : <NUM_LIT> } \n thread_pool . submit ( self . _do_send , msg . content , context ) \n res = cache . get ( key ) \n logger . info ( \"<STR_LIT>\" . format ( count , res ) ) \n if res . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n res [ '<STR_LIT>' ] = \"<STR_LIT>\" \n cache . pop ( key ) \n return res . get ( \"<STR_LIT>\" ) \n if cache . get ( key ) [ '<STR_LIT>' ] == <NUM_LIT> and count >= <NUM_LIT> : \n logger . info ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n if count <= <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n if count == <NUM_LIT> : \n return None \n return self . handle ( msg , count + <NUM_LIT> ) \n def _do_send ( self , query , context ) : \n key = query + '<STR_LIT>' + context [ '<STR_LIT>' ] \n reply_text = super ( ) . build_reply_content ( query , context ) \n logger . info ( '<STR_LIT>' . format ( reply_text ) ) \n cache [ key ] [ '<STR_LIT>' ] = \"<STR_LIT>\" \n cache [ key ] [ '<STR_LIT>' ] = reply_text \n def get_un_send_content ( self , from_user_id ) : \n for key in cache : \n if from_user_id in key : \n value = cache [ key ] \n if value . get ( '<STR_LIT>' ) == \"<STR_LIT>\" : \n cache . pop ( key ) \n return value . get ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n return \"<STR_LIT>\""}, {"input": "import hashlib \n import web \n class Handle ( object ) : \n def GET ( self ) : \n try : \n data = web . input ( ) \n if len ( data ) == <NUM_LIT> : \n return \"<STR_LIT>\" \n signature = data . signature \n timestamp = data . timestamp \n nonce = data . nonce \n echostr = data . echostr \n token = \"<STR_LIT>\" \n list = [ token , timestamp , nonce ] \n list . sort ( ) \n sha1 = hashlib . sha1 ( ) \n sha1 . update ( list [ <NUM_LIT> ] . encode ( \"<STR_LIT>\" ) )", "output": "sha1 . update ( list [ <NUM_LIT> ] . encode ( \"<STR_LIT>\" ) ) \n sha1 . update ( list [ <NUM_LIT> ] . encode ( \"<STR_LIT>\" ) ) \n hashcode = sha1 . hexdigest ( ) \n print ( \"<STR_LIT>\" , hashcode , signature ) \n if hashcode == signature : \n return echostr \n else : \n return \"<STR_LIT>\" \n except Exception as Argument : \n return Argument"}, {"input": "import logging \n logger = logging . getLogger ( __name__ )", "output": "class DebugLogger : \n def __init__ ( self ) : \n self . debug = False \n def log ( self , level , message ) : \n if not self . debug : \n return \n message = f\"<STR_LIT>\" \n if level == \"<STR_LIT>\" : \n logger . info ( message ) \n elif level == \"<STR_LIT>\" : \n logger . warning ( message ) \n elif level == \"<STR_LIT>\" : \n logger . error ( message ) \n elif level == \"<STR_LIT>\" : \n if self . debug : \n logger . debug ( message ) \n else : \n logger . info ( message ) \n debugger = DebugLogger ( )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None", "output": "def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}, {"input": "from legged_gym . envs . base . legged_robot_config import LeggedRobotCfg , LeggedRobotCfgPPO \n class Go1RoughCfg ( LeggedRobotCfg ) : \n class init_state ( LeggedRobotCfg . init_state ) : \n pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class init_state_slope ( LeggedRobotCfg . init_state ) :", "output": "pos = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n default_joint_angles = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n '<STR_LIT>' : - <NUM_LIT> , \n } \n class control ( LeggedRobotCfg . control ) : \n control_type = '<STR_LIT>' \n stiffness = { '<STR_LIT>' : <NUM_LIT> } \n damping = { '<STR_LIT>' : <NUM_LIT> } \n action_scale = <NUM_LIT> \n decimation = <NUM_LIT> \n class asset ( LeggedRobotCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = \"<STR_LIT>\" \n penalize_contacts_on = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n terminate_after_contacts_on = [ \"<STR_LIT>\" ] \n self_collisions = <NUM_LIT> \n class rewards ( LeggedRobotCfg . rewards ) : \n soft_dof_pos_limit = <NUM_LIT> \n base_height_target = <NUM_LIT> \n class Go1RoughCfgPPO ( LeggedRobotCfgPPO ) : \n class algorithm ( LeggedRobotCfgPPO . algorithm ) : \n entropy_coef = <NUM_LIT> \n class runner ( LeggedRobotCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>'"}, {"input": "import torch \n def split_and_pad_trajectories ( tensor , dones ) : \n dones = dones . clone ( ) \n dones [ - <NUM_LIT> ] = <NUM_LIT> \n flat_dones = dones . transpose ( <NUM_LIT> , <NUM_LIT> ) . reshape ( - <NUM_LIT> , <NUM_LIT> ) \n done_indices = torch . cat ( ( flat_dones . new_tensor ( [ - <NUM_LIT> ] , dtype = torch . int64 ) , flat_dones . nonzero ( ) [ : , <NUM_LIT> ] ) ) \n trajectory_lengths = done_indices [ <NUM_LIT> : ] - done_indices [ : - <NUM_LIT> ] \n trajectory_lengths_list = trajectory_lengths . tolist ( ) \n trajectories = torch . split ( tensor . transpose ( <NUM_LIT> , <NUM_LIT> ) . flatten ( <NUM_LIT> , <NUM_LIT> ) , trajectory_lengths_list ) \n padded_trajectories = torch . nn . utils . rnn . pad_sequence ( trajectories )", "output": "trajectory_masks = trajectory_lengths > torch . arange ( <NUM_LIT> , tensor . shape [ <NUM_LIT> ] , device = tensor . device ) . unsqueeze ( <NUM_LIT> ) \n return padded_trajectories , trajectory_masks \n def unpad_trajectories ( trajectories , masks ) : \n return trajectories . transpose ( <NUM_LIT> , <NUM_LIT> ) [ masks . transpose ( <NUM_LIT> , <NUM_LIT> ) ] . view ( - <NUM_LIT> , trajectories . shape [ <NUM_LIT> ] , trajectories . shape [ - <NUM_LIT> ] ) . transpose ( <NUM_LIT> , <NUM_LIT> )"}, {"input": "import sys \n import random \n import logging \n import string \n from typing import Dict , List , Union , Iterable \n from urllib . parse import urlparse , urlunparse \n from bs4 import BeautifulSoup \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n from typing import Literal \n else : \n from typing_extensions import Literal \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Form = Dict [ \n Literal [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n Union [ str , set ] , \n ] \n def get_form ( action : str , inputs : Iterable , method : str = \"<STR_LIT>\" ) -> Form : \n method = method . upper ( ) \n if not action . startswith ( \"<STR_LIT>\" ) : \n action = \"<STR_LIT>\" + action \n assert method in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n return { \"<STR_LIT>\" : action , \"<STR_LIT>\" : method , \"<STR_LIT>\" : set ( inputs ) } \n def parse_forms ( url , html : Union [ str , BeautifulSoup ] ) -> List [ Form ] : \n parsed_url = urlparse ( url ) \n uri = parsed_url . path \n if isinstance ( html , str ) : \n bs_doc = BeautifulSoup ( html , \"<STR_LIT>\" ) \n elif isinstance ( html , BeautifulSoup ) : \n bs_doc = html \n details = [ ] \n for form_element in bs_doc . select ( \"<STR_LIT>\" ) : \n form = get_form ( \n action = form_element . attrs . get ( \"<STR_LIT>\" , uri ) , \n method = form_element . attrs . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) . upper ( ) , \n inputs = [ \n element . attrs [ \"<STR_LIT>\" ] \n for element in form_element . select ( \"<STR_LIT>\" ) \n if \"<STR_LIT>\" in element . attrs \n ] , \n ) \n details . append ( form ) \n return details \n def random_fill ( form : Form ) -> Dict [ str , str ] : \n return { \n k : \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase , k = <NUM_LIT> ) ) for k in form [ \"<STR_LIT>\" ] \n } \n def fill_form ( url , form , form_inputs = None , randomly_fill_other = True ) : \n if randomly_fill_other : \n fill = random_fill ( form ) \n if form_inputs is not None :", "output": "fill . update ( form_inputs ) \n else : \n fill = form_inputs \n return { \n \"<STR_LIT>\" : urlunparse ( urlparse ( url ) . _replace ( path = form [ \"<STR_LIT>\" ] ) ) , \n \"<STR_LIT>\" : form [ \"<STR_LIT>\" ] , \n ( \"<STR_LIT>\" if form [ \"<STR_LIT>\" ] == \"<STR_LIT>\" else \"<STR_LIT>\" ) : fill , \n }"}, {"input": "import os \n import re \n import uuid \n import feedgen . feed as feedgen \n import feedi . app as feedi_app \n import httpretty \n import pytest \n from feedi . models import db \n @ pytest . fixture ( scope = '<STR_LIT>' ) \n def app ( ) : \n assert os . getenv ( '<STR_LIT>' ) == '<STR_LIT>' , \"<STR_LIT>\" \n app = feedi_app . create_app ( ) \n httpretty . enable ( allow_net_connect = False , verbose = True ) \n yield app \n httpretty . disable ( ) \n with app . app_context ( ) : \n db . drop_all ( ) \n @ pytest . fixture \n def client ( app ) : \n \"<STR_LIT>\" \n email = f'<STR_LIT>' \n with app . app_context ( ) : \n from feedi import models \n user = models . User ( email = email ) \n user . set_password ( '<STR_LIT>' ) \n db . session . add ( user ) \n db . session . commit ( ) \n client = app . test_client ( ) \n response = client . post ( \n '<STR_LIT>' , data = { '<STR_LIT>' : email , '<STR_LIT>' : '<STR_LIT>' } , follow_redirects = True ) \n assert response . status_code == <NUM_LIT> \n httpretty . reset ( ) \n return client \n def create_feed ( client , domain , items , folder = None ) : \n feed_url = mock_feed ( domain , items ) \n return client . post ( '<STR_LIT>' , data = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : domain , \n '<STR_LIT>' : feed_url , \n '<STR_LIT>' : folder \n } , follow_redirects = True ) \n def mock_feed ( domain , items ) : \n base_url = f'<STR_LIT>' \n feed_url = f'<STR_LIT>' \n fg = feedgen . FeedGenerator ( ) \n fg . id ( base_url ) \n fg . link ( href = feed_url ) \n fg . title ( f'<STR_LIT>' ) \n fg . description ( f'<STR_LIT>' ) \n for item in items : \n entry_url = f'<STR_LIT>' \n entry = fg . add_entry ( ) \n entry . id ( ) \n entry . link ( href = entry_url ) \n entry . title ( item [ '<STR_LIT>' ] ) \n entry . author ( { \"<STR_LIT>\" : item . get ( '<STR_LIT>' , '<STR_LIT>' ) } ) \n entry . published ( item [ '<STR_LIT>' ] ) \n entry . updated ( item [ '<STR_LIT>' ] ) \n entry . description ( item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n mock_request ( entry_url , body = item . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n rssfeed = fg . rss_str ( ) \n mock_request ( base_url ) \n mock_request ( f'<STR_LIT>' , ctype = '<STR_LIT>' ) \n mock_request ( feed_url , body = rssfeed , ctype = '<STR_LIT>' ) \n return feed_url \n def mock_request ( url , body = '<STR_LIT>' , ctype = '<STR_LIT>' ) :", "output": "httpretty . register_uri ( httpretty . HEAD , url , adding_headers = { \n '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) \n httpretty . register_uri ( httpretty . GET , url , body = body , adding_headers = { \n '<STR_LIT>' : ctype } , priority = <NUM_LIT> ) \n def extract_entry_ids ( response ) : \n entry_ids_with_duplicates = re . findall ( r'<STR_LIT>' , response . text ) \n entry_ids = [ ] \n for e in entry_ids_with_duplicates : \n if e not in entry_ids : \n entry_ids . append ( e ) \n return entry_ids"}, {"input": "import os \n import wget \n url_base = \"<STR_LIT>\" \n pretraineds_v1_list = [ \n ( \n \"<STR_LIT>\" , \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n ) , \n ] \n pretraineds_v2_list = [ \n ( \n \"<STR_LIT>\" , \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n ) , \n ] \n models_list = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n executables_list = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n folder_mapping_list = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def prequisites_download_pipeline ( pretraineds_v1 , pretraineds_v2 , models , exe ) :", "output": "def download_files ( file_list ) : \n for file_name in file_list : \n destination_path = os . path . join ( file_name ) \n url = f\"<STR_LIT>\" \n if not os . path . exists ( destination_path ) : \n os . makedirs ( os . path . dirname ( destination_path ) or \"<STR_LIT>\" , exist_ok = True ) \n print ( f\"<STR_LIT>\" ) \n wget . download ( url , out = destination_path ) \n if models == \"<STR_LIT>\" : \n download_files ( models_list ) \n if exe == \"<STR_LIT>\" and os . name == \"<STR_LIT>\" : \n download_files ( executables_list ) \n if pretraineds_v1 == \"<STR_LIT>\" : \n for remote_folder , file_list in pretraineds_v1_list : \n local_folder = folder_mapping_list . get ( remote_folder , \"<STR_LIT>\" ) \n for file in file_list : \n destination_path = os . path . join ( local_folder , file ) \n url = f\"<STR_LIT>\" \n if not os . path . exists ( destination_path ) : \n os . makedirs ( os . path . dirname ( destination_path ) or \"<STR_LIT>\" , exist_ok = True ) \n print ( f\"<STR_LIT>\" ) \n wget . download ( url , out = destination_path ) \n if pretraineds_v2 == \"<STR_LIT>\" : \n for remote_folder , file_list in pretraineds_v2_list : \n local_folder = folder_mapping_list . get ( remote_folder , \"<STR_LIT>\" ) \n for file in file_list : \n destination_path = os . path . join ( local_folder , file ) \n url = f\"<STR_LIT>\" \n if not os . path . exists ( destination_path ) : \n os . makedirs ( os . path . dirname ( destination_path ) or \"<STR_LIT>\" , exist_ok = True ) \n print ( f\"<STR_LIT>\" ) \n wget . download ( url , out = destination_path )"}, {"input": "from __future__ import annotations \n from typing import Iterable \n import gradio as gr \n from gradio . themes . base import Base \n from gradio . themes . utils import colors , fonts , sizes \n import time \n class Applio ( Base ) : \n def __init__ ( \n self , \n * , \n primary_hue : colors . Color | str = colors . green , \n secondary_hue : colors . Color | str = colors . emerald , \n neutral_hue : colors . Color | str = colors . neutral , \n spacing_size : sizes . Size | str = sizes . spacing_md , \n radius_size : sizes . Size | str = sizes . radius_md , \n text_size : sizes . Size | str = sizes . text_lg , \n font : fonts . Font | str | Iterable [ fonts . Font | str ] = ( \n \"<STR_LIT>\" , \n fonts . GoogleFont ( \"<STR_LIT>\" ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n font_mono : fonts . Font | str | Iterable [ fonts . Font | str ] = ( \n \"<STR_LIT>\" , \n fonts . GoogleFont ( \"<STR_LIT>\" ) , \n ) , \n ) : \n super ( ) . __init__ ( \n primary_hue = primary_hue , \n secondary_hue = secondary_hue , \n neutral_hue = neutral_hue , \n spacing_size = spacing_size , \n radius_size = radius_size , \n text_size = text_size , \n font = font , \n font_mono = font_mono , \n ) \n self . name = ( \"<STR_LIT>\" , ) \n self . secondary_100 = ( \"<STR_LIT>\" , ) \n self . secondary_200 = ( \"<STR_LIT>\" , ) \n self . secondary_300 = ( \"<STR_LIT>\" , ) \n self . secondary_400 = ( \"<STR_LIT>\" , ) \n self . secondary_50 = ( \"<STR_LIT>\" , ) \n self . secondary_500 = ( \"<STR_LIT>\" , ) \n self . secondary_600 = ( \"<STR_LIT>\" , ) \n self . secondary_700 = ( \"<STR_LIT>\" , ) \n self . secondary_800 = ( \"<STR_LIT>\" , ) \n self . secondary_900 = ( \"<STR_LIT>\" , ) \n self . secondary_950 = ( \"<STR_LIT>\" , ) \n super ( ) . set ( \n background_fill_primary = \"<STR_LIT>\" , \n background_fill_primary_dark = \"<STR_LIT>\" , \n background_fill_secondary = \"<STR_LIT>\" , \n background_fill_secondary_dark = \"<STR_LIT>\" , \n block_background_fill = \"<STR_LIT>\" , \n block_background_fill_dark = \"<STR_LIT>\" , \n block_border_color = \"<STR_LIT>\" , \n block_border_color_dark = \"<STR_LIT>\" , \n block_border_width = \"<STR_LIT>\" , \n block_border_width_dark = \"<STR_LIT>\" , \n block_info_text_color = \"<STR_LIT>\" , \n block_info_text_color_dark = \"<STR_LIT>\" , \n block_info_text_size = \"<STR_LIT>\" , \n block_info_text_weight = \"<STR_LIT>\" , \n block_label_background_fill = \"<STR_LIT>\" , \n block_label_background_fill_dark = \"<STR_LIT>\" , \n block_label_border_color = \"<STR_LIT>\" , \n block_label_border_color_dark = \"<STR_LIT>\" , \n block_label_border_width = \"<STR_LIT>\" , \n block_label_border_width_dark = \"<STR_LIT>\" , \n block_label_margin = \"<STR_LIT>\" , \n block_label_padding = \"<STR_LIT>\" , \n block_label_radius = \"<STR_LIT>\" , \n block_label_right_radius = \"<STR_LIT>\" , \n block_label_shadow = \"<STR_LIT>\" , \n block_label_text_color = \"<STR_LIT>\" , \n block_label_text_color_dark = \"<STR_LIT>\" , \n block_label_text_weight = \"<STR_LIT>\" , \n block_padding = \"<STR_LIT>\" , \n block_radius = \"<STR_LIT>\" , \n block_shadow = \"<STR_LIT>\" , \n block_shadow_dark = \"<STR_LIT>\" , \n block_title_background_fill = \"<STR_LIT>\" ,", "output": "block_title_background_fill_dark = \"<STR_LIT>\" , \n block_title_border_color = \"<STR_LIT>\" , \n block_title_border_color_dark = \"<STR_LIT>\" , \n block_title_border_width = \"<STR_LIT>\" , \n block_title_padding = \"<STR_LIT>\" , \n block_title_radius = \"<STR_LIT>\" , \n block_title_text_color = \"<STR_LIT>\" , \n block_title_text_color_dark = \"<STR_LIT>\" , \n block_title_text_size = \"<STR_LIT>\" , \n block_title_text_weight = \"<STR_LIT>\" , \n body_background_fill = \"<STR_LIT>\" , \n body_background_fill_dark = \"<STR_LIT>\" , \n body_text_color = \"<STR_LIT>\" , \n body_text_color_dark = \"<STR_LIT>\" , \n body_text_color_subdued = \"<STR_LIT>\" , \n body_text_color_subdued_dark = \"<STR_LIT>\" , \n body_text_size = \"<STR_LIT>\" , \n body_text_weight = \"<STR_LIT>\" , \n border_color_accent = \"<STR_LIT>\" , \n border_color_accent_dark = \"<STR_LIT>\" , \n border_color_primary = \"<STR_LIT>\" , \n border_color_primary_dark = \"<STR_LIT>\" , \n button_border_width = \"<STR_LIT>\" , \n button_border_width_dark = \"<STR_LIT>\" , \n button_cancel_background_fill = \"<STR_LIT>\" , \n button_cancel_background_fill_dark = \"<STR_LIT>\" , \n button_cancel_background_fill_hover = \"<STR_LIT>\" , \n button_cancel_background_fill_hover_dark = \"<STR_LIT>\" , \n button_cancel_border_color = \"<STR_LIT>\" , \n button_cancel_border_color_dark = \"<STR_LIT>\" , \n button_cancel_border_color_hover = \"<STR_LIT>\" , \n button_cancel_border_color_hover_dark = \"<STR_LIT>\" , \n button_cancel_text_color = \"<STR_LIT>\" , \n button_cancel_text_color_dark = \"<STR_LIT>\" , \n button_cancel_text_color_hover = \"<STR_LIT>\" , \n button_cancel_text_color_hover_dark = \"<STR_LIT>\" , \n button_large_padding = \"<STR_LIT>\" , \n button_large_radius = \"<STR_LIT>\" , \n button_large_text_size = \"<STR_LIT>\" , \n button_large_text_weight = \"<STR_LIT>\" , \n button_primary_background_fill = \"<STR_LIT>\" , \n button_primary_background_fill_dark = \"<STR_LIT>\" , \n button_primary_background_fill_hover = \"<STR_LIT>\" , \n button_primary_background_fill_hover_dark = \"<STR_LIT>\" , \n button_primary_border_color = \"<STR_LIT>\" , \n button_primary_border_color_dark = \"<STR_LIT>\" , \n button_primary_border_color_hover = \"<STR_LIT>\" , \n button_primary_border_color_hover_dark = \"<STR_LIT>\" , \n button_primary_text_color = \"<STR_LIT>\" , \n button_primary_text_color_dark = \"<STR_LIT>\" , \n button_primary_text_color_hover = \"<STR_LIT>\" , \n button_primary_text_color_hover_dark = \"<STR_LIT>\" , \n button_secondary_background_fill = \"<STR_LIT>\" , \n button_secondary_background_fill_dark = \"<STR_LIT>\" , \n button_secondary_background_fill_hover = \"<STR_LIT>\" , \n button_secondary_background_fill_hover_dark = \"<STR_LIT>\" , \n button_secondary_border_color = \"<STR_LIT>\" , \n button_secondary_border_color_dark = \"<STR_LIT>\" , \n button_secondary_border_color_hover = \"<STR_LIT>\" , \n button_secondary_border_color_hover_dark = \"<STR_LIT>\" , \n button_secondary_text_color = \"<STR_LIT>\" , \n button_secondary_text_color_dark = \"<STR_LIT>\" , \n button_secondary_text_color_hover = \"<STR_LIT>\" , \n button_secondary_text_color_hover_dark = \"<STR_LIT>\" , \n button_shadow = \"<STR_LIT>\" , \n button_shadow_active = \"<STR_LIT>\" , \n button_shadow_hover = \"<STR_LIT>\" , \n button_small_padding = \"<STR_LIT>\" , \n button_small_radius = \"<STR_LIT>\" , \n button_small_text_size = \"<STR_LIT>\" , \n button_small_text_weight = \"<STR_LIT>\" , \n button_transition = \"<STR_LIT>\" , \n checkbox_background_color = \"<STR_LIT>\" , \n checkbox_background_color_dark = \"<STR_LIT>\" , \n checkbox_background_color_focus = \"<STR_LIT>\" , \n checkbox_background_color_focus_dark = \"<STR_LIT>\" , \n checkbox_background_color_hover = \"<STR_LIT>\" , \n checkbox_background_color_hover_dark = \"<STR_LIT>\" , \n checkbox_background_color_selected = \"<STR_LIT>\" , \n checkbox_background_color_selected_dark = \"<STR_LIT>\" , \n checkbox_border_color = \"<STR_LIT>\" , \n checkbox_border_color_dark = \"<STR_LIT>\" , \n checkbox_border_color_focus = \"<STR_LIT>\" , \n checkbox_border_color_focus_dark = \"<STR_LIT>\" , \n checkbox_border_color_hover = \"<STR_LIT>\" , \n checkbox_border_color_hover_dark = \"<STR_LIT>\" , \n checkbox_border_color_selected = \"<STR_LIT>\" , \n checkbox_border_color_selected_dark = \"<STR_LIT>\" , \n checkbox_border_radius = \"<STR_LIT>\" , \n checkbox_border_width = \"<STR_LIT>\" , \n checkbox_border_width_dark = \"<STR_LIT>\" , \n checkbox_check = \"<STR_LIT>\" , \n checkbox_label_background_fill = \"<STR_LIT>\" , \n checkbox_label_background_fill_dark = \"<STR_LIT>\" , \n checkbox_label_background_fill_hover = \"<STR_LIT>\" , \n checkbox_label_background_fill_hover_dark = \"<STR_LIT>\" , \n checkbox_label_background_fill_selected = \"<STR_LIT>\" , \n checkbox_label_background_fill_selected_dark = \"<STR_LIT>\" , \n checkbox_label_border_color = \"<STR_LIT>\" , \n checkbox_label_border_color_dark = \"<STR_LIT>\" , \n checkbox_label_border_color_hover = \"<STR_LIT>\" , \n checkbox_label_border_color_hover_dark = \"<STR_LIT>\" , \n checkbox_label_border_width = \"<STR_LIT>\" , \n checkbox_label_border_width_dark = \"<STR_LIT>\" , \n checkbox_label_gap = \"<STR_LIT>\" , \n checkbox_label_padding = \"<STR_LIT>\" , \n checkbox_label_shadow = \"<STR_LIT>\" , \n checkbox_label_text_color = \"<STR_LIT>\" , \n checkbox_label_text_color_dark = \"<STR_LIT>\" , \n checkbox_label_text_color_selected = \"<STR_LIT>\" , \n checkbox_label_text_color_selected_dark = \"<STR_LIT>\" , \n checkbox_label_text_size = \"<STR_LIT>\" , \n checkbox_label_text_weight = \"<STR_LIT>\" , \n checkbox_shadow = \"<STR_LIT>\" , \n color_accent = \"<STR_LIT>\" , \n color_accent_soft = \"<STR_LIT>\" , \n color_accent_soft_dark = \"<STR_LIT>\" , \n container_radius = \"<STR_LIT>\" , \n embed_radius = \"<STR_LIT>\" , \n error_background_fill = \"<STR_LIT>\" , \n error_background_fill_dark = \"<STR_LIT>\" , \n error_border_color = \"<STR_LIT>\" , \n error_border_color_dark = \"<STR_LIT>\" , \n error_border_width = \"<STR_LIT>\" , \n error_border_width_dark = \"<STR_LIT>\" , \n error_text_color = \"<STR_LIT>\" , \n error_text_color_dark = \"<STR_LIT>\" , \n form_gap_width = \"<STR_LIT>\" , \n input_background_fill = \"<STR_LIT>\" , \n input_background_fill_dark = \"<STR_LIT>\" , \n input_background_fill_focus = \"<STR_LIT>\" , \n input_background_fill_focus_dark = \"<STR_LIT>\" , \n input_background_fill_hover = \"<STR_LIT>\" , \n input_background_fill_hover_dark = \"<STR_LIT>\" , \n input_border_color = \"<STR_LIT>\" , \n input_border_color_dark = \"<STR_LIT>\" , \n input_border_color_focus = \"<STR_LIT>\" , \n input_border_color_focus_dark = \"<STR_LIT>\" , \n input_border_color_hover = \"<STR_LIT>\" , \n input_border_color_hover_dark = \"<STR_LIT>\" , \n input_border_width = \"<STR_LIT>\" , \n input_border_width_dark = \"<STR_LIT>\" , \n input_padding = \"<STR_LIT>\" , \n input_placeholder_color = \"<STR_LIT>\" , \n input_placeholder_color_dark = \"<STR_LIT>\" , \n input_radius = \"<STR_LIT>\" , \n input_shadow = \"<STR_LIT>\" , \n input_shadow_dark = \"<STR_LIT>\" , \n input_shadow_focus = \"<STR_LIT>\" , \n input_shadow_focus_dark = \"<STR_LIT>\" , \n input_text_size = \"<STR_LIT>\" , \n input_text_weight = \"<STR_LIT>\" , \n layout_gap = \"<STR_LIT>\" , \n link_text_color = \"<STR_LIT>\" , \n link_text_color_active = \"<STR_LIT>\" , \n link_text_color_active_dark = \"<STR_LIT>\" , \n link_text_color_dark = \"<STR_LIT>\" , \n link_text_color_hover = \"<STR_LIT>\" , \n link_text_color_hover_dark = \"<STR_LIT>\" , \n link_text_color_visited = \"<STR_LIT>\" , \n link_text_color_visited_dark = \"<STR_LIT>\" , \n loader_color = \"<STR_LIT>\" , \n loader_color_dark = \"<STR_LIT>\" , \n panel_background_fill = \"<STR_LIT>\" , \n panel_background_fill_dark = \"<STR_LIT>\" , \n panel_border_color = \"<STR_LIT>\" , \n panel_border_color_dark = \"<STR_LIT>\" , \n panel_border_width = \"<STR_LIT>\" , \n panel_border_width_dark = \"<STR_LIT>\" , \n prose_header_text_weight = \"<STR_LIT>\" , \n prose_text_size = \"<STR_LIT>\" , \n prose_text_weight = \"<STR_LIT>\" , \n radio_circle = \"<STR_LIT>\" , \n section_header_text_size = \"<STR_LIT>\" , \n section_header_text_weight = \"<STR_LIT>\" , \n shadow_drop = \"<STR_LIT>\" , \n shadow_drop_lg = \"<STR_LIT>\" , \n shadow_inset = \"<STR_LIT>\" , \n shadow_spread = \"<STR_LIT>\" , \n shadow_spread_dark = \"<STR_LIT>\" , \n slider_color = \"<STR_LIT>\" , \n slider_color_dark = \"<STR_LIT>\" , \n stat_background_fill = \"<STR_LIT>\" , \n stat_background_fill_dark = \"<STR_LIT>\" , \n table_border_color = \"<STR_LIT>\" , \n table_border_color_dark = \"<STR_LIT>\" , \n table_even_background_fill = \"<STR_LIT>\" , \n table_even_background_fill_dark = \"<STR_LIT>\" , \n table_odd_background_fill = \"<STR_LIT>\" , \n table_odd_background_fill_dark = \"<STR_LIT>\" , \n table_radius = \"<STR_LIT>\" , \n table_row_focus = \"<STR_LIT>\" , \n table_row_focus_dark = \"<STR_LIT>\" , \n )"}, {"input": "from typing import Dict , List , Tuple , Optional \n import torch \n import torch . nn as nn \n from transformers import CLIPVisionModel , CLIPImageProcessor \n from PIL import Image \n from multi_token . modalities . base_modality import Modality \n from multi_token . modalities . projectors import ( \n build_patch_mlp_projector , \n build_mlp_vector_projector , \n ) \n from multi_token . data_tools import load_image \n PATCH_LAYER = - <NUM_LIT> \n OUTPUT_LAYER = - <NUM_LIT> \n OUTPUT_EMB_SIZE = <NUM_LIT> \n class CLIPVisionModule ( nn . Module ) : \n def __init__ ( self , model_name_or_path : str , feature_layer : int = PATCH_LAYER ) : \n super ( ) . __init__ ( ) \n self . feature_layer = feature_layer \n self . model_name_or_path = model_name_or_path \n self . image_processor = None \n self . image_model = None \n self . load_model ( ) \n def load_model ( self ) : \n self . image_processor = CLIPImageProcessor . from_pretrained ( \n self . model_name_or_path \n ) \n self . image_model = CLIPVisionModel . from_pretrained ( self . model_name_or_path ) \n self . image_model . requires_grad_ ( False ) \n @ torch . no_grad ( ) \n def forward ( self , images ) -> torch . Tensor : \n if self . feature_layer == PATCH_LAYER : \n image_forward_outs = self . image_model ( \n images . to ( device = self . device , dtype = self . dtype ) , \n output_hidden_states = True , \n ) \n image_features = image_forward_outs . hidden_states [ self . feature_layer ] \n image_features = image_features [ : , <NUM_LIT> : ] . to ( images . dtype ) \n else : \n image_forward_outs = self . image_model ( \n images . to ( device = self . device , dtype = self . dtype ) , \n ) \n image_features = image_forward_outs . pooler_output . to ( images . dtype ) . view ( \n - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE \n ) \n return image_features \n @ property \n def dtype ( self ) : \n return self . image_model . dtype \n @ property \n def device ( self ) : \n return self . image_model . device \n @ property \n def config ( self ) : \n return self . image_model . config \n @ property \n def hidden_size ( self ) : \n return self . config . hidden_size \n @ property \n def num_patches ( self ) : \n return ( self . config . image_size // self . config . patch_size ) ** <NUM_LIT> \n def _expand2square ( pil_img : Image , background_color : Tuple ) -> Image : \n width , height = pil_img . size \n if width == height : \n return pil_img \n elif width > height : \n result = Image . new ( pil_img . mode , ( width , width ) , background_color ) \n result . paste ( pil_img , ( <NUM_LIT> , ( width - height ) // <NUM_LIT> ) ) \n return result \n else : \n result = Image . new ( pil_img . mode , ( height , height ) , background_color ) \n result . paste ( pil_img , ( ( height - width ) // <NUM_LIT> , <NUM_LIT> ) ) \n return result \n class CLIPVisionModality ( Modality ) : \n def __init__ ( \n self , \n model_name_or_path : str = \"<STR_LIT>\" , \n pad_non_square_images : bool = False , \n num_projector_layers : int = <NUM_LIT> , \n feature_layer : int = PATCH_LAYER , \n num_tokens_output : Optional [ int ] = None , \n ) : \n if feature_layer not in [ PATCH_LAYER , OUTPUT_LAYER ] : \n raise ValueError ( \n f\"<STR_LIT>\" \n ) \n if ( feature_layer == PATCH_LAYER ) != ( num_tokens_output is None ) : \n raise ValueError ( \n \"<STR_LIT>\" \n ) \n self . model_name_or_path = model_name_or_path \n self . module = CLIPVisionModule ( \n model_name_or_path = self . model_name_or_path , feature_layer = feature_layer \n ) \n self . pad_non_square_images = pad_non_square_images \n self . num_projector_layers = num_projector_layers \n self . num_tokens_output = num_tokens_output \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n if self . module . feature_layer == PATCH_LAYER : \n return build_patch_mlp_projector ( \n self . module . hidden_size , \n lm_hidden_size , \n num_layers = self . num_projector_layers , \n ) \n else : \n return build_mlp_vector_projector ( \n input_hidden_size = OUTPUT_EMB_SIZE , \n lm_hidden_size = lm_hidden_size , \n num_layers = self . num_projector_layers , \n num_tokens = self . num_tokens_output , \n ) \n @ property \n def name ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def data_key ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token_width ( self ) -> int : \n if self . module . feature_layer == PATCH_LAYER : \n return self . module . num_patches \n else : \n return self . num_tokens_output \n def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n self . module . to ( dtype = dtype , device = device ) \n return self", "output": "def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ torch . Tensor ] ] : \n row_values = [ ] \n for row in rows : \n images = [ ] \n for image_fn in row [ self . data_key ] : \n image_obj = load_image ( image_fn ) \n if self . pad_non_square_images : \n image_obj = _expand2square ( \n image_obj , \n tuple ( \n int ( x * <NUM_LIT> ) for x in self . module . image_processor . image_mean \n ) , \n ) \n image = self . module . image_processor . preprocess ( \n image_obj , return_tensors = \"<STR_LIT>\" \n ) [ \"<STR_LIT>\" ] [ <NUM_LIT> ] \n images . append ( image ) \n row_values . append ( torch . stack ( images ) if len ( images ) > <NUM_LIT> else None ) \n return row_values \n @ torch . no_grad ( ) \n def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : \n image_features = [ ] \n for image_batch in encoded_values : \n image_features . append ( self . module . forward ( image_batch ) ) \n return image_features"}, {"input": "import sys \n import os \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' )", "output": "import openai \n from main import reliableGPT \n import concurrent . futures \n openai . api_type = \"<STR_LIT>\" \n openai . api_base = os . getenv ( \"<STR_LIT>\" ) \n openai . api_version = \"<STR_LIT>\" \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n user_email = \"<STR_LIT>\" , \n azure_fallback_strategy = [ \"<STR_LIT>\" ] , _test = True , verbose = True ) \n def simple_openai_call ( prompt ) : \n print ( f\"<STR_LIT>\" ) \n engine = \"<STR_LIT>\" \n messages = [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : prompt \n } , \n ] \n result = openai . ChatCompletion . create ( engine = engine , messages = messages ) \n print ( f\"<STR_LIT>\" ) \n return result \n list_questions = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n openai . api_key = os . getenv ( \"<STR_LIT>\" ) \n for question in list_questions : \n response = simple_openai_call ( question ) \n print ( response )"}, {"input": "import functools \n import logging \n import random \n import re \n import sys \n import time \n from collections import namedtuple \n from string import ascii_lowercase \n from . payload_gen import TargetAndSubTargets , find_bad_exprs \n from . requester import HTTPRequester \n from . form import random_fill \n from . submitter import FormSubmitter , RequestSubmitter , Submitter \n from . colorize import colored \n from . const import ( \n PythonEnvironment , \n AutoFix500Code , \n ATTRIBUTE , \n CHAINED_ATTRIBUTE_ITEM , \n STRING , \n CONFIG , \n EVAL , \n OS_POPEN_READ , \n FLASK_CONTEXT_VAR , \n ) \n from . waf_func_gen import WafFuncGen \n from . full_payload_gen import FullPayloadGen \n from . context_vars import ContextVariableManager \n from . options import Options \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n from typing import Union , Callable , Dict , Tuple \n else : \n from typing_extensions import Union , Callable , Dict , Tuple , Literal \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n Result = namedtuple ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def guess_python_version ( url : str , requester : HTTPRequester ) -> PythonEnvironment : \n resp = requester . request ( method = \"<STR_LIT>\" , url = url ) \n if resp is None : \n return PythonEnvironment . UNKNOWN \n version_regexp = re . search ( r\"<STR_LIT>\" , resp . headers . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) \n if not version_regexp : \n return PythonEnvironment . UNKNOWN \n result = ( \n PythonEnvironment . PYTHON3 \n if version_regexp . group ( <NUM_LIT> ) == \"<STR_LIT>\" \n else PythonEnvironment . PYTHON2 \n ) \n logger . info ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , result . value , bold = True ) ) \n return result \n class EvalArgsModePayloadGen : \n def __init__ ( self , will_print ) : \n self . will_print = will_print \n def generate ( self , gen_type , * args ) : \n if gen_type == OS_POPEN_READ : \n return f\"<STR_LIT>\" , self . will_print \n elif gen_type == EVAL : \n req = args [ <NUM_LIT> ]", "output": "assert ( \n req [ <NUM_LIT> ] == STRING \n ) , \"<STR_LIT>\" + repr ( req ) \n return f\"<STR_LIT>\" , self . will_print \n elif gen_type == CONFIG : \n return ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" , \n self . will_print , \n ) \n return None , None \n class Cracker : \n test_cmd = \"<STR_LIT>\" \n test_eval = \"<STR_LIT>\" \n test_result = \"<STR_LIT>\" \n def __init__ ( \n self , \n submitter : Submitter , \n callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , \n options : Union [ Options , None ] = None , \n ) : \n self . options = options if options else Options ( ) \n self . subm = submitter \n self . _callback : Callable [ [ str , Dict ] , None ] = ( \n callback if callback else ( lambda x , y : None ) \n ) \n self . waf_func_gen = WafFuncGen ( submitter , callback = callback , options = options ) \n @ property \n def callback ( self ) : \n return self . _callback \n @ callback . setter \n def callback ( self , callback ) : \n self . _callback = callback \n self . waf_func_gen . callback = callback \n def test_payload ( self , payload : str , will_print : bool ) -> str : \n logger . info ( \n \"<STR_LIT>\" , \n ) \n result = self . subm . submit ( payload ) \n assert result is not None \n status_code , text = result \n if status_code == <NUM_LIT> : \n return \"<STR_LIT>\" \n return ( \n \"<STR_LIT>\" if self . test_result in text or not will_print else \"<STR_LIT>\" \n ) \n def test_payload_eval_args ( self , payload : str , subm : Submitter ) -> bool : \n logger . info ( \n \"<STR_LIT>\" , \n ) \n result = subm . submit ( payload ) \n assert result is not None \n _ , text = result \n return self . test_result in text \n def has_respond ( self ) -> bool : \n for _ in range ( <NUM_LIT> ) : \n content = random . choice ( ascii_lowercase ) * <NUM_LIT> \n resp = self . subm . submit ( content ) \n assert resp is not None , \"<STR_LIT>\" \n if content in resp . text : \n return True \n return False \n def crack_with_waf ( \n self , waf_func , waf_expr_func = None \n ) -> Union [ Tuple [ FullPayloadGen , bool , str , TargetAndSubTargets ] , None ] : \n full_payload_gen = FullPayloadGen ( \n waf_func , \n callback = None , \n options = self . options , \n waf_expr_func = waf_expr_func , \n ) \n result = full_payload_gen . generate_with_tree ( OS_POPEN_READ , self . test_cmd ) \n if result is None : \n return None \n payload , will_print , tree = result \n test_result = self . test_payload ( payload , will_print ) \n return full_payload_gen , will_print , test_result , tree \n def log_with_result ( self , will_print : bool , test_result : str ) : \n if will_print : \n if test_result == \"<STR_LIT>\" : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n elif test_result == \"<STR_LIT>\" : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n else : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n else : \n if test_result == \"<STR_LIT>\" : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n else : \n logger . info ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" , \n ) \n def expr_waf_not500 ( \n self , tree , outer_pattern , context_vars : ContextVariableManager \n ) : \n def is_expr_bad ( expr ) : \n payload = context_vars . get_payload ( \n context_vars . get_context ( ) \n ) + outer_pattern . replace ( \"<STR_LIT>\" , expr ) \n result = self . subm . submit ( payload ) \n assert result is not None \n status_code , _ = result \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , payload ) , \n colored ( \"<STR_LIT>\" , status_code ) , \n ) \n return status_code == <NUM_LIT> \n exprs = [ payload for payload , _ in find_bad_exprs ( tree , is_expr_bad ) ] \n @ functools . lru_cache ( <NUM_LIT> ) \n def new_waf ( s ) : \n return all ( expr not in s for expr in exprs ) and not is_expr_bad ( s ) \n return new_waf \n def crack ( self ) -> Union [ FullPayloadGen , None ] : \n logger . info ( \"<STR_LIT>\" ) \n waf_func = self . waf_func_gen . generate ( ) \n result = self . crack_with_waf ( waf_func ) \n if not result : \n return None \n full_payload_gen , will_print , test_result , tree = result \n assert ( \n full_payload_gen . context_vars is not None \n ) , \"<STR_LIT>\" \n self . log_with_result ( will_print , test_result ) \n if ( \n test_result == \"<STR_LIT>\" \n and self . options . autofix_500 == AutoFix500Code . ENABLED \n ) : \n logger . info ( colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) ) \n logger . info ( \n colored ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True \n ) \n ) \n logger . info ( \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) \n ) \n time . sleep ( <NUM_LIT> ) \n waf_expr_func = self . expr_waf_not500 ( \n tree , full_payload_gen . outer_pattern , full_payload_gen . context_vars \n ) \n result = self . crack_with_waf ( waf_func , waf_expr_func = waf_expr_func ) \n if result : \n full_payload_gen , will_print , test_result , tree = result \n if test_result == \"<STR_LIT>\" : \n logger . info ( \"<STR_LIT>\" ) \n self . log_with_result ( will_print , test_result ) \n return full_payload_gen \n def crack_eval_args ( self ) -> Union [ Tuple [ Submitter , EvalArgsModePayloadGen ] , None ] : \n args_target_field = \"<STR_LIT>\" \n logger . info ( \"<STR_LIT>\" ) \n assert isinstance ( \n self . subm , FormSubmitter \n ) , \"<STR_LIT>\" \n waf_func = self . waf_func_gen . generate ( ) \n full_payload_gen = FullPayloadGen ( waf_func , callback = None , options = self . options ) \n payload , will_print = full_payload_gen . generate ( \n EVAL , \n ( \n CHAINED_ATTRIBUTE_ITEM , \n ( FLASK_CONTEXT_VAR , \"<STR_LIT>\" ) , \n ( ATTRIBUTE , \"<STR_LIT>\" ) , \n ( ATTRIBUTE , args_target_field ) , \n ) , \n ) \n if payload is None : \n return None \n assert will_print is not None , \"<STR_LIT>\" \n payload_dict = { self . subm . target_field : payload } \n method = self . subm . form [ \"<STR_LIT>\" ] \n assert isinstance ( method , str ) \n payload_param = random_fill ( self . subm . form ) \n payload_param . update ( payload_dict ) \n new_subm = RequestSubmitter ( \n url = self . subm . url , \n method = method , \n target_field = args_target_field , \n params = payload_param if method == \"<STR_LIT>\" else { } , \n data = payload_param if method != \"<STR_LIT>\" else { } , \n requester = self . subm . req , \n ) \n if self . subm . tamperers : \n for tamperer in self . subm . tamperers : \n new_subm . add_tamperer ( tamperer ) \n if will_print : \n if self . test_payload_eval_args ( self . test_eval , new_subm ) : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n else : \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n else : \n logger . info ( \n \"<STR_LIT>\" \n + \"<STR_LIT>\" , \n ) \n return new_subm , EvalArgsModePayloadGen ( will_print )"}, {"input": "import os \n from pydantic_settings import BaseSettings \n import viztracer \n class Settings ( BaseSettings ) : \n app_name : str = \"<STR_LIT>\" \n project_dir : str = os . path . normpath ( \n os . path . join ( \n os . path . abspath ( __file__ ) , \n \"<STR_LIT>\" ,", "output": ") \n ) \n def get_path ( self , * args ) : \n return os . path . join ( \n self . project_dir , \n * args \n ) \n def get_viztracer_static_files ( self ) : \n return os . path . normpath ( os . path . join ( \n os . path . abspath ( viztracer . __file__ ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" \n ) ) \n settings = Settings ( )"}, {"input": "import argparse \n import json \n import logging \n import os \n logger = logging . getLogger ( __name__ ) \n parser = argparse . ArgumentParser ( description = \"<STR_LIT>\" ) \n parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' )", "output": "parser . add_argument ( '<STR_LIT>' , type = str , default = '<STR_LIT>' , help = '<STR_LIT>' ) \n parser . add_argument ( \"<STR_LIT>\" , action = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n kw_args , unknown_args = parser . parse_known_args ( ) \n arg_auths : dict = { kw_args . auth : \"<STR_LIT>\" } if kw_args . auth else None \n def first ( * args ) : \n result = next ( filter ( lambda x : x , args ) , None ) \n return result \n class DefaultConfig : \n def __init__ ( self ) : \n self . ip = '<STR_LIT>' \n self . port = <NUM_LIT> \n class ConfigFile : \n def __init__ ( self ) : \n json_config = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> \n } , \n \"<STR_LIT>\" : { } \n } \n file_path = os . path . join ( os . getcwd ( ) , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n try : \n with open ( file_path , \"<STR_LIT>\" ) as json_file : \n json_config = json . load ( json_file ) \n except FileNotFoundError : \n directory = os . path . dirname ( file_path ) \n if not os . path . exists ( directory ) : \n os . makedirs ( directory ) \n with open ( file_path , \"<STR_LIT>\" ) as json_file : \n json . dump ( json_config , json_file , indent = <NUM_LIT> ) \n self . auth : dict = json_config . get ( \"<STR_LIT>\" , { } ) \n self . server = json_config . get ( \"<STR_LIT>\" , { } ) \n self . port = self . server . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n self . ip = self . server . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n class EnvVar : \n def __init__ ( self ) : \n self . auth = os . environ . get ( '<STR_LIT>' , None ) \n self . port = os . environ . get ( '<STR_LIT>' , None ) \n self . auths = None \n if self . auth : \n self . auths : dict = { \n self . auth : \"<STR_LIT>\" \n } \n env_args = EnvVar ( ) \n config_args = ConfigFile ( ) \n default = DefaultConfig ( ) \n class GlobalArgs : \n def __init__ ( self ) : \n self . auth : dict = first ( env_args . auths , arg_auths , config_args . auth ) \n if type ( self . auth ) is not dict : \n self . auth : dict = { } \n self . port = first ( env_args . port , kw_args . port , config_args . port , default . port ) \n self . ip = first ( config_args . ip , default . ip ) \n self . debug = kw_args . debug \n self . version = \"<STR_LIT>\" \n def valid ( self , key ) : \n return key in self . auth . keys ( ) \n def permission ( self , key ) : \n return self . auth . get ( key , '<STR_LIT>' )"}, {"input": "import requests \n from . import storage \n class Core ( object ) : \n def __init__ ( self ) : \n self . alive , self . isLogging = False , False \n self . storageClass = storage . Storage ( self ) \n self . memberList = self . storageClass . memberList \n self . mpList = self . storageClass . mpList \n self . chatroomList = self . storageClass . chatroomList \n self . msgList = self . storageClass . msgList \n self . loginInfo = { } \n self . s = requests . Session ( ) \n self . uuid = None \n self . functionDict = { '<STR_LIT>' : { } , '<STR_LIT>' : { } , '<STR_LIT>' : { } } \n self . useHotReload , self . hotReloadDir = False , '<STR_LIT>' \n self . receivingRetryCount = <NUM_LIT> \n def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , \n loginCallback = None , exitCallback = None ) : \n raise NotImplementedError ( ) \n def get_QRuuid ( self ) : \n raise NotImplementedError ( ) \n def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : \n raise NotImplementedError ( ) \n def check_login ( self , uuid = None ) : \n raise NotImplementedError ( ) \n def web_init ( self ) : \n raise NotImplementedError ( ) \n def show_mobile_login ( self ) : \n raise NotImplementedError ( ) \n def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : \n raise NotImplementedError ( )", "output": "def get_msg ( self ) : \n raise NotImplementedError ( ) \n def logout ( self ) : \n raise NotImplementedError ( ) \n def update_chatroom ( self , userName , detailedMember = False ) : \n raise NotImplementedError ( ) \n def update_friend ( self , userName ) : \n raise NotImplementedError ( ) \n def get_contact ( self , update = False ) : \n raise NotImplementedError ( ) \n def get_friends ( self , update = False ) : \n raise NotImplementedError ( ) \n def get_chatrooms ( self , update = False , contactOnly = False ) : \n raise NotImplementedError ( ) \n def get_mps ( self , update = False ) : \n raise NotImplementedError ( ) \n def set_alias ( self , userName , alias ) : \n raise NotImplementedError ( ) \n def set_pinned ( self , userName , isPinned = True ) : \n raise NotImplementedError ( ) \n def accept_friend ( self , userName , v4 , autoUpdate = True ) : \n raise NotImplementedError ( ) \n def get_head_img ( self , userName = None , chatroomUserName = None , picDir = None ) : \n raise NotImplementedError ( ) \n def create_chatroom ( self , memberList , topic = '<STR_LIT>' ) : \n raise NotImplementedError ( ) \n def set_chatroom_name ( self , chatroomUserName , name ) : \n raise NotImplementedError ( ) \n def delete_member_from_chatroom ( self , chatroomUserName , memberList ) : \n raise NotImplementedError ( ) \n def add_member_into_chatroom ( self , chatroomUserName , memberList , \n useInvitation = False ) : \n raise NotImplementedError ( ) \n def send_raw_msg ( self , msgType , content , toUserName ) : \n raise NotImplementedError ( ) \n def send_msg ( self , msg = '<STR_LIT>' , toUserName = None ) : \n raise NotImplementedError ( ) \n def upload_file ( self , fileDir , isPicture = False , isVideo = False , \n toUserName = '<STR_LIT>' , file_ = None , preparedFile = None ) : \n raise NotImplementedError ( ) \n def send_file ( self , fileDir , toUserName = None , mediaId = None , file_ = None ) : \n raise NotImplementedError ( ) \n def send_image ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : \n raise NotImplementedError ( ) \n def send_video ( self , fileDir = None , toUserName = None , mediaId = None , file_ = None ) : \n raise NotImplementedError ( ) \n def send ( self , msg , toUserName = None , mediaId = None ) : \n raise NotImplementedError ( ) \n def revoke ( self , msgId , toUserName , localId = None ) : \n raise NotImplementedError ( ) \n def dump_login_status ( self , fileDir = None ) : \n raise NotImplementedError ( ) \n def load_login_status ( self , fileDir , \n loginCallback = None , exitCallback = None ) : \n raise NotImplementedError ( ) \n def auto_login ( self , hotReload = False , statusStorageDir = '<STR_LIT>' , \n enableCmdQR = False , picDir = None , qrCallback = None , \n loginCallback = None , exitCallback = None ) : \n raise NotImplementedError ( ) \n def configured_reply ( self ) : \n raise NotImplementedError ( ) \n def msg_register ( self , msgType , \n isFriendChat = False , isGroupChat = False , isMpChat = False ) : \n raise NotImplementedError ( ) \n def run ( self , debug = True , blockThread = True ) : \n raise NotImplementedError ( ) \n def search_friends ( self , name = None , userName = None , remarkName = None , nickName = None , \n wechatAccount = None ) : \n return self . storageClass . search_friends ( name , userName , remarkName , \n nickName , wechatAccount ) \n def search_chatrooms ( self , name = None , userName = None ) : \n return self . storageClass . search_chatrooms ( name , userName ) \n def search_mps ( self , name = None , userName = None ) : \n return self . storageClass . search_mps ( name , userName )"}, {"input": "from typing import List \n import random \n import argparse \n import json \n from huggingface_hub import hf_hub_download \n from datasets import Dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n PRETRAIN_PHRASES = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "] \n def _timestamp_to_seconds ( timestamp : str ) : \n parts = timestamp . split ( \"<STR_LIT>\" ) \n seconds = float ( parts [ - <NUM_LIT> ] ) \n seconds += float ( parts [ - <NUM_LIT> ] ) * <NUM_LIT> \n seconds += float ( parts [ - <NUM_LIT> ] ) * <NUM_LIT> * <NUM_LIT> \n return seconds \n def _write_convo ( row ) -> List : \n video = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + row [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : _timestamp_to_seconds ( row [ \"<STR_LIT>\" ] ) , \n \"<STR_LIT>\" : _timestamp_to_seconds ( row [ \"<STR_LIT>\" ] ) , \n } \n example = { \n \"<STR_LIT>\" : [ video ] , \n } \n phrase = random . choice ( PRETRAIN_PHRASES ) \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : phrase , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] , \n } , \n ] \n return example \n def main ( args ) : \n path = hf_hub_download ( \n repo_id = \"<STR_LIT>\" , filename = \"<STR_LIT>\" , repo_type = \"<STR_LIT>\" \n ) \n rows = [ ] \n with open ( path , \"<STR_LIT>\" ) as f : \n for line in f : \n rows . append ( json . loads ( line ) ) \n print ( \"<STR_LIT>\" , len ( rows ) ) \n if len ( rows ) > args . max_examples : \n rows = random . sample ( rows , k = args . max_examples ) \n def gen ( subset_rows ) : \n for row in subset_rows : \n try : \n yield _write_convo ( row ) \n except Exception as e : \n print ( e ) \n ds = Dataset . from_generator ( gen , gen_kwargs = { \"<STR_LIT>\" : rows } , num_proc = <NUM_LIT> ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , default = <NUM_LIT> ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import torch \n from torch import nn \n def get_grouped_diag ( logits : torch . Tensor ) -> torch . Tensor : \n dims = logits . shape \n bs = dims [ <NUM_LIT> ] \n N = dims [ <NUM_LIT> ] // bs \n def do_keep ( a ) : \n rowsize = N * bs \n curr_row = a // rowsize \n curr_col = ( a % rowsize ) // N \n return curr_col == curr_row \n index_ten = torch . arange ( N * bs * bs ) . reshape ( ( bs , N * bs ) ) \n return logits [ do_keep ( index_ten ) ] . reshape ( bs , N ) \n def get_pooled_groups ( logits : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : \n dims = logits . shape \n bs = dims [ <NUM_LIT> ] \n N = dims [ <NUM_LIT> ] // bs \n grouped = logits . reshape ( ( bs , bs , N ) ) \n if pool_type == '<STR_LIT>' : \n return torch . mean ( grouped , dim = <NUM_LIT> ) \n elif pool_type == '<STR_LIT>' : \n return torch . max ( grouped , dim = <NUM_LIT> ) . values \n elif pool_type == '<STR_LIT>' : \n return torch . min ( grouped , dim = <NUM_LIT> ) . values \n def get_pooled_diag ( logits : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : \n return get_pooled_groups ( logits , pool_type = pool_type ) . diag ( ) \n def contrastive_loss ( logits : torch . Tensor ) -> torch . Tensor : \n return nn . functional . cross_entropy ( logits , torch . arange ( len ( logits ) , device = logits . device ) ) \n def clip_loss ( similarity : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : \n if pool_type == '<STR_LIT>' : \n similarity = get_pooled_groups ( similarity , pool_type = '<STR_LIT>' ) \n min_sim_diag = get_pooled_diag ( similarity , pool_type = '<STR_LIT>' )", "output": "similarity [ range ( len ( min_sim_diag ) ) , range ( len ( min_sim_diag ) ) ] = min_sim_diag \n else : \n similarity = get_pooled_groups ( similarity ) \n caption_loss = contrastive_loss ( similarity ) \n image_loss = contrastive_loss ( similarity . t ( ) ) \n return ( caption_loss + image_loss ) / <NUM_LIT> \n def negatives_loss_pairwise ( scores : torch . Tensor ) -> torch . Tensor : \n label = torch . tensor ( [ [ <NUM_LIT> , <NUM_LIT> ] ] , device = scores . device ) . float ( ) \n labels = label . tile ( ( scores . size ( ) [ <NUM_LIT> ] , <NUM_LIT> ) ) \n neg_loss = nn . functional . binary_cross_entropy_with_logits ( scores , labels ) \n return neg_loss \n def negatives_loss ( pos_scores : torch . Tensor , neg_scores : torch . Tensor , pool_type = '<STR_LIT>' ) -> torch . Tensor : \n pos_pool_type = '<STR_LIT>' if pool_type == '<STR_LIT>' else pool_type \n neg_pool_type = '<STR_LIT>' if pool_type == '<STR_LIT>' else pool_type \n pos_diag = get_pooled_diag ( pos_scores , pool_type = pos_pool_type ) \n neg_diag = get_pooled_diag ( neg_scores , pool_type = neg_pool_type ) \n res = torch . stack ( ( pos_diag , neg_diag ) , axis = - <NUM_LIT> ) \n return negatives_loss_pairwise ( res )"}, {"input": "contactos = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] , \n '<STR_LIT>' : True , \n '<STR_LIT>' : '<STR_LIT>' \n } \n print ( contactos [ '<STR_LIT>' ] ) \n print ( contactos [ '<STR_LIT>' ] ) \n del contactos [ '<STR_LIT>' ] \n print ( contactos ) \n contactos [ '<STR_LIT>' ] = '<STR_LIT>' \n print ( contactos ) \n contactos [ '<STR_LIT>' ] = '<STR_LIT>' \n print ( contactos ) \n print ( list ( contactos ) ) \n print ( sorted ( contactos ) ) \n print ( contactos ) \n print ( '<STR_LIT>' in contactos ) \n print ( '<STR_LIT>' in contactos ) \n print ( \"<STR_LIT>\" ) \n telefonos = {", "output": "\"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n } \n celulares = dict ( [ \n ( '<STR_LIT>' , \"<STR_LIT>\" ) , \n ( '<STR_LIT>' , <NUM_LIT> ) , \n ( '<STR_LIT>' , \"<STR_LIT>\" ) , \n ( '<STR_LIT>' , \"<STR_LIT>\" ) , \n ( '<STR_LIT>' , \"<STR_LIT>\" ) \n ] ) \n print ( celulares ) \n celulares = dict ( \n camara = \"<STR_LIT>\" , \n bateria = <NUM_LIT> , \n pantalla = \"<STR_LIT>\" \n ) \n print ( celulares ) \n mi_diccionario = { numero : numero ** <NUM_LIT> for numero in ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) } \n print ( mi_diccionario ) \n pokemones = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( pokemones . items ( ) ) \n for nombre , tipo in pokemones . items ( ) : \n print ( nombre , tipo ) \n pokemones_datos = pokemones . items ( ) \n print ( list ( enumerate ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) ) ) \n for i , v in enumerate ( [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) : \n print ( i , v ) \n pregunta = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n repuestas = [ '<STR_LIT>' , '<STR_LIT>' ] \n asociacion = zip ( pregunta , repuestas ) \n print ( asociacion ) \n for p , r in asociacion : \n print ( p , r ) \n print ( pokemones . keys ( ) ) \n print ( pokemones . values ( ) ) \n print ( pokemones . items ( ) )"}, {"input": "import os , sys \n from statistics import mode \n sys . path . append ( \"<STR_LIT>\" ) \n import torch \n import torch . nn as nn \n from rsl_rl . modules . actor_critic import Actor , StateHistoryEncoder , get_activation , ActorCriticRMA \n from rsl_rl . modules . estimator import Estimator \n from rsl_rl . modules . depth_backbone import DepthOnlyFCBackbone58x87 , RecurrentDepthBackbone \n import argparse \n import code \n import shutil \n def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = \"<STR_LIT>\" ) : \n if not os . path . isdir ( root ) : \n model_name_cand = os . path . basename ( root ) \n model_parent = os . path . dirname ( root ) \n model_names = os . listdir ( model_parent ) \n model_names = [ name for name in model_names if os . path . isdir ( os . path . join ( model_parent , name ) ) ] \n for name in model_names : \n if len ( name ) >= <NUM_LIT> : \n if name [ : <NUM_LIT> ] == model_name_cand : \n root = os . path . join ( model_parent , name ) \n if checkpoint == - <NUM_LIT> : \n models = [ file for file in os . listdir ( root ) if model_name_include in file ] \n models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) \n model = models [ - <NUM_LIT> ] \n checkpoint = model . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n else : \n model = \"<STR_LIT>\" . format ( checkpoint ) \n load_path = os . path . join ( root , model ) \n return load_path , checkpoint \n class HardwareVisionNN ( nn . Module ) : \n def __init__ ( self , num_prop , \n num_scan , \n num_priv_latent , \n num_priv_explicit , \n num_hist , \n num_actions , \n tanh , \n actor_hidden_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n scan_encoder_dims = [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n depth_encoder_hidden_dim = <NUM_LIT> , \n activation = '<STR_LIT>' , \n priv_encoder_dims = [ <NUM_LIT> , <NUM_LIT> ] \n ) : \n super ( HardwareVisionNN , self ) . __init__ ( ) \n self . num_prop = num_prop \n self . num_scan = num_scan \n self . num_hist = num_hist \n self . num_actions = num_actions \n self . num_priv_latent = num_priv_latent \n self . num_priv_explicit = num_priv_explicit \n num_obs = num_prop + num_scan + num_hist * num_prop + num_priv_latent + num_priv_explicit \n self . num_obs = num_obs \n activation = get_activation ( activation ) \n self . actor = Actor ( num_prop , num_scan , num_actions , scan_encoder_dims , actor_hidden_dims , priv_encoder_dims , num_priv_latent , num_priv_explicit , num_hist , activation , tanh_encoder_output = tanh ) \n self . estimator = Estimator ( input_dim = num_prop , output_dim = num_priv_explicit , hidden_dims = [ <NUM_LIT> , <NUM_LIT> ] ) \n def forward ( self , obs , depth_latent ) : \n obs [ : , self . num_prop + self . num_scan : self . num_prop + self . num_scan + self . num_priv_explicit ] = self . estimator ( obs [ : , : self . num_prop ] ) \n return self . actor ( obs , hist_encoding = True , eval = False , scandots_latent = depth_latent ) \n def play ( args ) : \n load_run = \"<STR_LIT>\" + args . exptid \n checkpoint = args . checkpoint \n n_priv_explicit = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n n_priv_latent = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n num_scan = <NUM_LIT> \n num_actions = <NUM_LIT> \n depth_resized = ( <NUM_LIT> , <NUM_LIT> ) \n n_proprio = <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> + <NUM_LIT> \n history_len = <NUM_LIT> \n device = torch . device ( '<STR_LIT>' ) \n policy = HardwareVisionNN ( n_proprio , num_scan , n_priv_latent , n_priv_explicit , history_len , num_actions , args . tanh ) . to ( device ) \n load_path , checkpoint = get_load_path ( root = load_run , checkpoint = checkpoint ) \n load_run = os . path . dirname ( load_path ) \n print ( f\"<STR_LIT>\" ) \n ac_state_dict = torch . load ( load_path , map_location = device ) \n policy . actor . load_state_dict ( ac_state_dict [ '<STR_LIT>' ] , strict = True ) \n policy . estimator . load_state_dict ( ac_state_dict [ '<STR_LIT>' ] ) \n policy = policy . to ( device )", "output": "if not os . path . exists ( os . path . join ( load_run , \"<STR_LIT>\" ) ) : \n os . mkdir ( os . path . join ( load_run , \"<STR_LIT>\" ) ) \n state_dict = { '<STR_LIT>' : ac_state_dict [ '<STR_LIT>' ] } \n torch . save ( state_dict , os . path . join ( load_run , \"<STR_LIT>\" , args . exptid + \"<STR_LIT>\" + str ( checkpoint ) + \"<STR_LIT>\" ) ) \n policy . eval ( ) \n with torch . no_grad ( ) : \n num_envs = <NUM_LIT> \n obs_input = torch . ones ( num_envs , n_proprio + num_scan + n_priv_explicit + n_priv_latent + history_len * n_proprio , device = device ) \n depth_latent = torch . ones ( <NUM_LIT> , <NUM_LIT> , device = device ) \n test = policy ( obs_input , depth_latent ) \n traced_policy = torch . jit . trace ( policy , ( obs_input , depth_latent ) ) \n save_path = os . path . join ( load_run , \"<STR_LIT>\" , args . exptid + \"<STR_LIT>\" + str ( checkpoint ) + \"<STR_LIT>\" ) \n traced_policy . save ( save_path ) \n print ( \"<STR_LIT>\" , os . path . abspath ( save_path ) ) \n if __name__ == '<STR_LIT>' : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( '<STR_LIT>' , type = str ) \n parser . add_argument ( '<STR_LIT>' , type = int , default = - <NUM_LIT> ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' ) \n args = parser . parse_args ( ) \n play ( args )"}, {"input": "from curl_cffi import Curl , CurlOpt \n from io import BytesIO \n buffer = BytesIO ( ) \n c = Curl ( )", "output": "c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) \n c . setopt ( CurlOpt . WRITEDATA , buffer ) \n c . impersonate ( \"<STR_LIT>\" ) \n c . perform ( ) \n c . close ( ) \n body = buffer . getvalue ( ) \n print ( body . decode ( ) )"}, {"input": "import os \n import copy \n import torch \n import numpy as np \n import random \n from isaacgym import gymapi \n from isaacgym import gymutil \n import argparse \n from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR \n def class_to_dict ( obj ) -> dict : \n if not hasattr ( obj , \"<STR_LIT>\" ) : \n return obj \n result = { } \n for key in dir ( obj ) : \n if key . startswith ( \"<STR_LIT>\" ) : \n continue \n element = [ ] \n val = getattr ( obj , key ) \n if isinstance ( val , list ) : \n for item in val : \n element . append ( class_to_dict ( item ) ) \n else : \n element = class_to_dict ( val ) \n result [ key ] = element \n return result \n def update_class_from_dict ( obj , dict ) : \n for key , val in dict . items ( ) : \n attr = getattr ( obj , key , None ) \n if isinstance ( attr , type ) : \n update_class_from_dict ( attr , val ) \n else : \n setattr ( obj , key , val ) \n return \n def set_seed ( seed ) : \n if seed == - <NUM_LIT> : \n seed = np . random . randint ( <NUM_LIT> , <NUM_LIT> ) \n print ( \"<STR_LIT>\" . format ( seed ) ) \n random . seed ( seed ) \n np . random . seed ( seed ) \n torch . manual_seed ( seed ) \n os . environ [ '<STR_LIT>' ] = str ( seed ) \n torch . cuda . manual_seed ( seed ) \n torch . cuda . manual_seed_all ( seed ) \n def parse_sim_params ( args , cfg ) : \n sim_params = gymapi . SimParams ( ) \n if args . physics_engine == gymapi . SIM_FLEX : \n if args . device != \"<STR_LIT>\" : \n print ( \"<STR_LIT>\" ) \n elif args . physics_engine == gymapi . SIM_PHYSX : \n sim_params . physx . use_gpu = args . use_gpu \n sim_params . physx . num_subscenes = args . subscenes \n sim_params . use_gpu_pipeline = args . use_gpu_pipeline \n if \"<STR_LIT>\" in cfg : \n gymutil . parse_sim_config ( cfg [ \"<STR_LIT>\" ] , sim_params ) \n if args . physics_engine == gymapi . SIM_PHYSX and args . num_threads > <NUM_LIT> : \n sim_params . physx . num_threads = args . num_threads \n return sim_params \n def get_load_path ( root , load_run = - <NUM_LIT> , checkpoint = - <NUM_LIT> , model_name_include = \"<STR_LIT>\" ) : \n if not os . path . isdir ( root ) : \n model_name_cand = os . path . basename ( root ) \n model_parent = os . path . dirname ( root ) \n model_names = os . listdir ( model_parent ) \n model_names = [ name for name in model_names if os . path . isdir ( os . path . join ( model_parent , name ) ) ] \n for name in model_names : \n if len ( name ) >= <NUM_LIT> : \n if name [ : <NUM_LIT> ] == model_name_cand : \n root = os . path . join ( model_parent , name ) \n if checkpoint == - <NUM_LIT> : \n models = [ file for file in os . listdir ( root ) if model_name_include in file ] \n models . sort ( key = lambda m : '<STR_LIT>' . format ( m ) ) \n model = models [ - <NUM_LIT> ] \n else : \n model = \"<STR_LIT>\" . format ( checkpoint ) \n load_path = os . path . join ( root , model ) \n return load_path \n def update_cfg_from_args ( env_cfg , cfg_train , args ) : \n if env_cfg is not None : \n if args . use_camera : \n env_cfg . depth . use_camera = args . use_camera \n if env_cfg . depth . use_camera and args . headless : \n env_cfg . env . num_envs = env_cfg . depth . camera_num_envs \n env_cfg . terrain . num_rows = env_cfg . depth . camera_terrain_num_rows \n env_cfg . terrain . num_cols = env_cfg . depth . camera_terrain_num_cols \n env_cfg . terrain . max_error = env_cfg . terrain . max_error_camera \n env_cfg . terrain . horizontal_scale = env_cfg . terrain . horizontal_scale_camera \n env_cfg . terrain . simplify_grid = True \n env_cfg . terrain . terrain_dict [ \"<STR_LIT>\" ] = <NUM_LIT> \n env_cfg . terrain . terrain_dict [ \"<STR_LIT>\" ] = <NUM_LIT> \n env_cfg . terrain . terrain_dict [ \"<STR_LIT>\" ] = <NUM_LIT> \n env_cfg . terrain . terrain_dict [ \"<STR_LIT>\" ] = <NUM_LIT> \n env_cfg . terrain . terrain_dict [ \"<STR_LIT>\" ] = <NUM_LIT> \n env_cfg . terrain . terrain_proportions = list ( env_cfg . terrain . terrain_dict . values ( ) ) \n if env_cfg . depth . use_camera : \n env_cfg . terrain . y_range = [ - <NUM_LIT> , <NUM_LIT> ] \n if args . num_envs is not None : \n env_cfg . env . num_envs = args . num_envs \n if args . seed is not None : \n env_cfg . seed = args . seed \n if args . task_both : \n env_cfg . env . task_both = args . task_both \n if args . rows is not None : \n env_cfg . terrain . num_rows = args . rows \n if args . cols is not None : \n env_cfg . terrain . num_cols = args . cols \n if args . delay : \n env_cfg . domain_rand . action_delay = args . delay \n if not args . delay and not args . resume and not args . use_camera and args . headless : \n env_cfg . domain_rand . action_delay = True \n env_cfg . domain_rand . action_curr_step = env_cfg . domain_rand . action_curr_step_scratch \n if cfg_train is not None : \n if args . seed is not None : \n cfg_train . seed = args . seed \n if args . use_camera : \n cfg_train . depth_encoder . if_depth = args . use_camera \n if args . max_iterations is not None : \n cfg_train . runner . max_iterations = args . max_iterations \n if args . resume : \n cfg_train . runner . resume = args . resume \n cfg_train . algorithm . priv_reg_coef_schedual = cfg_train . algorithm . priv_reg_coef_schedual_resume \n if args . experiment_name is not None : \n cfg_train . runner . experiment_name = args . experiment_name \n if args . run_name is not None : \n cfg_train . runner . run_name = args . run_name \n if args . load_run is not None : \n cfg_train . runner . load_run = args . load_run \n if args . checkpoint is not None : \n cfg_train . runner . checkpoint = args . checkpoint \n return env_cfg , cfg_train \n def get_args ( ) : \n custom_parameters = [ \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : - <NUM_LIT> , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : '<STR_LIT>' } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : '<STR_LIT>' } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : int , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : str , \"<STR_LIT>\" : None , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } \n ] \n args = parse_arguments ( \n description = \"<STR_LIT>\" , \n custom_parameters = custom_parameters ) \n args . sim_device_id = args . compute_device_id \n args . sim_device = args . sim_device_type \n if args . sim_device == '<STR_LIT>' : \n args . sim_device += f\"<STR_LIT>\" \n return args \n def export_policy_as_jit ( actor_critic , path , name ) : \n if hasattr ( actor_critic , '<STR_LIT>' ) : \n exporter = PolicyExporterLSTM ( actor_critic ) \n exporter . export ( path ) \n else : \n os . makedirs ( path , exist_ok = True ) \n path = os . path . join ( path , name + \"<STR_LIT>\" ) \n model = copy . deepcopy ( actor_critic . actor ) . to ( '<STR_LIT>' ) \n traced_script_module = torch . jit . script ( model ) \n traced_script_module . save ( path ) \n class PolicyExporterLSTM ( torch . nn . Module ) : \n def __init__ ( self , actor_critic ) : \n super ( ) . __init__ ( ) \n self . actor = copy . deepcopy ( actor_critic . actor ) \n self . is_recurrent = actor_critic . is_recurrent \n self . memory = copy . deepcopy ( actor_critic . memory_a . rnn ) \n self . memory . cpu ( ) \n self . register_buffer ( f'<STR_LIT>' , torch . zeros ( self . memory . num_layers , <NUM_LIT> , self . memory . hidden_size ) ) \n self . register_buffer ( f'<STR_LIT>' , torch . zeros ( self . memory . num_layers , <NUM_LIT> , self . memory . hidden_size ) ) \n def forward ( self , x ) : \n out , ( h , c ) = self . memory ( x . unsqueeze ( <NUM_LIT> ) , ( self . hidden_state , self . cell_state ) ) \n self . hidden_state [ : ] = h \n self . cell_state [ : ] = c \n return self . actor ( out . squeeze ( <NUM_LIT> ) ) \n @ torch . jit . export \n def reset_memory ( self ) : \n self . hidden_state [ : ] = <NUM_LIT> \n self . cell_state [ : ] = <NUM_LIT> \n def export ( self , path ) : \n os . makedirs ( path , exist_ok = True ) \n path = os . path . join ( path , '<STR_LIT>' ) \n self . to ( '<STR_LIT>' ) \n traced_script_module = torch . jit . script ( self ) \n traced_script_module . save ( path ) \n def parse_device_str ( device_str ) : \n device = '<STR_LIT>' \n device_id = <NUM_LIT> \n if device_str == '<STR_LIT>' or device_str == '<STR_LIT>' : \n device = device_str \n device_id = <NUM_LIT> \n else : \n device_args = device_str . split ( '<STR_LIT>' ) \n assert len ( device_args ) == <NUM_LIT> and device_args [ <NUM_LIT> ] == '<STR_LIT>' , f'<STR_LIT>' \n device , device_id_s = device_args \n try : \n device_id = int ( device_id_s ) \n except ValueError : \n raise ValueError ( f'<STR_LIT>' ) \n return device , device_id \n def parse_arguments ( description = \"<STR_LIT>\" , headless = False , no_graphics = False , custom_parameters = [ ] ) : \n parser = argparse . ArgumentParser ( description = description ) \n if headless : \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) \n if no_graphics : \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , \n help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , default = \"<STR_LIT>\" , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = str , default = \"<STR_LIT>\" , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) \n physics_group = parser . add_mutually_exclusive_group ( ) \n physics_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) \n physics_group . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = int , default = <NUM_LIT> , help = '<STR_LIT>' ) \n parser . add_argument ( '<STR_LIT>' , type = int , help = '<STR_LIT>' ) \n for argument in custom_parameters : \n if ( \"<STR_LIT>\" in argument ) and ( \"<STR_LIT>\" in argument or \"<STR_LIT>\" in argument ) : \n help_str = \"<STR_LIT>\" \n if \"<STR_LIT>\" in argument : \n help_str = argument [ \"<STR_LIT>\" ] \n if \"<STR_LIT>\" in argument : \n if \"<STR_LIT>\" in argument : \n parser . add_argument ( argument [ \"<STR_LIT>\" ] , type = argument [ \"<STR_LIT>\" ] , default = argument [ \"<STR_LIT>\" ] , help = help_str ) \n else : \n parser . add_argument ( argument [ \"<STR_LIT>\" ] , type = argument [ \"<STR_LIT>\" ] , help = help_str ) \n elif \"<STR_LIT>\" in argument : \n parser . add_argument ( argument [ \"<STR_LIT>\" ] , action = argument [ \"<STR_LIT>\" ] , help = help_str ) \n else : \n print ( ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( ) \n args = parser . parse_args ( ) \n if args . device is not None : \n args . sim_device = args . device \n args . rl_device = args . device \n args . sim_device_type , args . compute_device_id = parse_device_str ( args . sim_device ) \n pipeline = args . pipeline . lower ( ) \n assert ( pipeline == '<STR_LIT>' or pipeline in ( '<STR_LIT>' , '<STR_LIT>' ) ) , f\"<STR_LIT>\" \n args . use_gpu_pipeline = ( pipeline in ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if args . sim_device_type != '<STR_LIT>' and args . flex : \n print ( \"<STR_LIT>\" ) \n args . sim_device = '<STR_LIT>' \n args . sim_device_type , args . compute_device_id = parse_device_str ( args . sim_device ) \n if ( args . sim_device_type != '<STR_LIT>' and pipeline == '<STR_LIT>' ) : \n print ( \"<STR_LIT>\" ) \n args . pipeline = '<STR_LIT>' \n args . use_gpu_pipeline = False \n args . physics_engine = gymapi . SIM_PHYSX", "output": "args . use_gpu = ( args . sim_device_type == '<STR_LIT>' ) \n if args . flex : \n args . physics_engine = gymapi . SIM_FLEX \n if no_graphics and args . nographics : \n args . headless = True \n if args . slices is None : \n args . slices = args . subscenes \n return args"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , \n server_default = None , \n nullable = True )", "output": "def downgrade ( ) -> None : \n pass"}, {"input": "import asyncio \n from contextlib import asynccontextmanager \n import pytest \n import svcs \n from tests . helpers import CloseMe \n try : \n from fastapi import FastAPI \n from fastapi . testclient import TestClient \n except ImportError : \n pytest . skip ( \"<STR_LIT>\" , allow_module_level = True ) \n @ pytest . mark . asyncio ( ) \n @ pytest . mark . parametrize ( \"<STR_LIT>\" , [ True , False ] ) \n @ pytest . mark . parametrize ( \"<STR_LIT>\" , [ True , False ] ) \n async def test_integration ( yield_something , cm ) : \n close_me_registry = CloseMe ( ) \n close_me_container = CloseMe ( ) \n async def factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n yield <NUM_LIT> \n await asyncio . sleep ( <NUM_LIT> ) \n await close_me_container . aclose ( ) \n async def close_registry ( ) : \n await close_me_registry . aclose ( ) \n if yield_something : \n async def lifespan ( app : FastAPI , registry : svcs . Registry ) : \n registry . register_factory ( \n int , factory , on_registry_close = close_registry \n ) \n yield { \"<STR_LIT>\" : \"<STR_LIT>\" } \n else : \n async def lifespan ( app : FastAPI , registry : svcs . Registry ) : \n registry . register_factory ( \n int , factory , on_registry_close = close_registry \n ) \n yield \n if cm : \n lifespan = asynccontextmanager ( lifespan ) \n app = FastAPI ( lifespan = svcs . fastapi . lifespan ( lifespan ) ) \n @ app . get ( \"<STR_LIT>\" ) \n async def view ( services : svcs . fastapi . DepContainer ) : \n return { \"<STR_LIT>\" : await services . aget ( int ) } \n with TestClient ( app ) as client : \n assert { \"<STR_LIT>\" : <NUM_LIT> } == client . get ( \"<STR_LIT>\" ) . json ( ) \n assert close_me_container . is_aclosed", "output": "assert close_me_registry . is_aclosed"}, {"input": "import pickle \n class idStore ( ) : \n def __init__ ( self ) :", "output": "self . id = None \n def get_id ( self ) : \n if self . id is None : \n try : \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : \n self . id = pickle . load ( f ) \n except FileNotFoundError : \n print ( \"<STR_LIT>\" ) \n return self . id \n def set_id ( self , id ) : \n self . id = id \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as f : \n pickle . dump ( self . id , f )"}, {"input": "import os \n import subprocess \n import sys \n import shutil \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n from core import ( \n run_preprocess_script , \n run_extract_script , \n run_train_script , \n run_index_script , \n run_prerequisites_script , \n ) \n from rvc . configs . config import max_vram_gpu , get_gpu_info \n from rvc . lib . utils import format_title \n from tabs . settings . restart import restart_applio \n i18n = I18nAuto ( ) \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n pretraineds_v1 = [ \n ( \n \"<STR_LIT>\" , \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n ) , \n ] \n folder_mapping = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n sup_audioext = { \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n } \n pretraineds_custom_path = os . path . join ( \n now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" \n ) \n pretraineds_custom_path_relative = os . path . relpath ( pretraineds_custom_path , now_dir ) \n if not os . path . exists ( pretraineds_custom_path_relative ) : \n os . makedirs ( pretraineds_custom_path_relative ) \n def get_pretrained_list ( suffix ) : \n return [ \n os . path . join ( dirpath , filename ) \n for dirpath , _ , filenames in os . walk ( pretraineds_custom_path_relative ) \n for filename in filenames \n if filename . endswith ( \"<STR_LIT>\" ) and suffix in filename \n ] \n pretraineds_list_d = get_pretrained_list ( \"<STR_LIT>\" ) \n pretraineds_list_g = get_pretrained_list ( \"<STR_LIT>\" ) \n def refresh_custom_pretraineds ( ) : \n return ( \n { \"<STR_LIT>\" : sorted ( get_pretrained_list ( \"<STR_LIT>\" ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( get_pretrained_list ( \"<STR_LIT>\" ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n datasets_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not os . path . exists ( datasets_path ) : \n os . makedirs ( datasets_path ) \n datasets_path_relative = os . path . relpath ( datasets_path , now_dir ) \n def get_datasets_list ( ) : \n return [ \n dirpath \n for dirpath , _ , filenames in os . walk ( datasets_path_relative ) \n if any ( filename . endswith ( tuple ( sup_audioext ) ) for filename in filenames ) \n ] \n def refresh_datasets ( ) : \n return { \"<STR_LIT>\" : sorted ( get_datasets_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } \n models_path = os . path . join ( now_dir , \"<STR_LIT>\" ) \n def get_models_list ( ) : \n return [ \n os . path . basename ( dirpath ) \n for dirpath in os . listdir ( models_path ) \n if os . path . isdir ( os . path . join ( models_path , dirpath ) ) \n and all ( excluded not in dirpath for excluded in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n ] \n def refresh_models ( ) : \n return { \"<STR_LIT>\" : sorted ( get_models_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } \n def refresh_models_and_datasets ( ) : \n return ( \n { \"<STR_LIT>\" : sorted ( get_models_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( get_datasets_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n def save_drop_model ( dropbox ) : \n if \"<STR_LIT>\" not in dropbox : \n gr . Info ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n else : \n file_name = os . path . basename ( dropbox ) \n pretrained_path = os . path . join ( pretraineds_custom_path_relative , file_name ) \n if os . path . exists ( pretrained_path ) : \n os . remove ( pretrained_path ) \n os . rename ( dropbox , pretrained_path ) \n gr . Info ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n return None \n def save_drop_dataset_audio ( dropbox , dataset_name ) : \n if not dataset_name : \n gr . Info ( \"<STR_LIT>\" ) \n return None , None \n else : \n file_extension = os . path . splitext ( dropbox ) [ <NUM_LIT> ] [ <NUM_LIT> : ] . lower ( ) \n if file_extension not in sup_audioext : \n gr . Info ( \"<STR_LIT>\" ) \n else : \n dataset_name = format_title ( dataset_name ) \n audio_file = format_title ( os . path . basename ( dropbox ) ) \n dataset_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , dataset_name ) \n if not os . path . exists ( dataset_path ) : \n os . makedirs ( dataset_path ) \n destination_path = os . path . join ( dataset_path , audio_file ) \n if os . path . exists ( destination_path ) : \n os . remove ( destination_path ) \n os . rename ( dropbox , destination_path ) \n gr . Info ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n dataset_path = os . path . dirname ( destination_path ) \n relative_dataset_path = os . path . relpath ( dataset_path , now_dir ) \n return None , relative_dataset_path \n def get_pth_list ( ) : \n return [ \n os . path . relpath ( os . path . join ( dirpath , filename ) , now_dir ) \n for dirpath , _ , filenames in os . walk ( models_path ) \n for filename in filenames \n if filename . endswith ( \"<STR_LIT>\" ) \n ] \n def get_index_list ( ) : \n return [ \n os . path . relpath ( os . path . join ( dirpath , filename ) , now_dir ) \n for dirpath , _ , filenames in os . walk ( models_path ) \n for filename in filenames \n if filename . endswith ( \"<STR_LIT>\" ) and \"<STR_LIT>\" not in filename \n ] \n def refresh_pth_and_index_list ( ) : \n return ( \n { \"<STR_LIT>\" : sorted ( get_pth_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : sorted ( get_index_list ( ) ) , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n def export_pth ( pth_path ) : \n if pth_path and os . path . exists ( pth_path ) : \n return pth_path \n return None \n def export_index ( index_path ) : \n if index_path and os . path . exists ( index_path ) : \n return index_path \n return None \n def upload_to_google_drive ( pth_path , index_path ) : \n def upload_file ( file_path ) : \n if file_path : \n try : \n gr . Info ( f\"<STR_LIT>\" ) \n google_drive_folder = \"<STR_LIT>\" \n if not os . path . exists ( google_drive_folder ) : \n os . makedirs ( google_drive_folder ) \n google_drive_file_path = os . path . join ( \n google_drive_folder , os . path . basename ( file_path ) \n ) \n if os . path . exists ( google_drive_file_path ) : \n os . remove ( google_drive_file_path ) \n shutil . copy2 ( file_path , google_drive_file_path ) \n gr . Info ( \"<STR_LIT>\" ) \n except Exception as error : \n print ( error ) \n gr . Info ( \"<STR_LIT>\" ) \n upload_file ( pth_path ) \n upload_file ( index_path ) \n def train_tab ( ) : \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n model_name = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_models_list ( ) , \n value = \"<STR_LIT>\" , \n interactive = True , \n allow_custom_value = True , \n ) \n dataset_path = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_datasets_list ( ) , \n allow_custom_value = True , \n interactive = True , \n ) \n refresh = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n dataset_creator = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n value = False , \n interactive = True , \n visible = True , \n ) \n with gr . Column ( visible = False ) as dataset_creator_settings : \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n dataset_name = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n interactive = True , \n ) \n upload_audio_dataset = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , \n type = \"<STR_LIT>\" , \n interactive = True , \n ) \n with gr . Column ( ) : \n sampling_rate = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n rvc_version = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n preprocess_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n interactive = False , \n ) \n with gr . Row ( ) : \n preprocess_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n preprocess_button . click ( \n run_preprocess_script , \n [ model_name , dataset_path , sampling_rate ] , \n preprocess_output_info , \n api_name = \"<STR_LIT>\" , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n with gr . Row ( ) : \n hop_length = gr . Slider ( \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n visible = False , \n ) \n with gr . Row ( ) : \n with gr . Column ( ) : \n f0method = gr . Radio ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n extract_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n interactive = False , \n ) \n extract_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n extract_button . click ( \n run_extract_script , \n [ model_name , rvc_version , f0method , hop_length , sampling_rate ] , \n extract_output_info , \n api_name = \"<STR_LIT>\" , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n with gr . Row ( ) : \n batch_size = gr . Slider ( \n <NUM_LIT> , \n <NUM_LIT> , \n max_vram_gpu ( <NUM_LIT> ) , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n ) \n save_every_epoch = gr . Slider ( \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n interactive = True , \n ) \n total_epoch = gr . Slider ( \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n ) \n with gr . Row ( ) : \n pitch_guidance = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = True , \n interactive = True , \n ) \n pretrained = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = True , \n interactive = True , \n ) \n save_only_latest = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = False , \n interactive = True , \n ) \n save_every_weights = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = True , \n interactive = True , \n ) \n custom_pretrained = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = False , \n interactive = True , \n ) \n multiple_gpu = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) , \n value = False , \n interactive = True , \n ) \n overtraining_detector = gr . Checkbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = False , \n interactive = True , \n ) \n with gr . Row ( ) : \n with gr . Column ( visible = False ) as pretrained_custom_settings : \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n upload_pretrained = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , \n type = \"<STR_LIT>\" , \n interactive = True , \n ) \n refresh_custom_pretaineds_button = gr . Button ( \n i18n ( \"<STR_LIT>\" ) \n ) \n g_pretrained_path = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = sorted ( pretraineds_list_g ) , \n interactive = True , \n allow_custom_value = True , \n ) \n d_pretrained_path = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n choices = sorted ( pretraineds_list_d ) , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Column ( visible = False ) as gpu_custom_settings : \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n gpu = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n interactive = True , \n ) \n gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = get_gpu_info ( ) , \n interactive = False , \n ) \n with gr . Column ( visible = False ) as overtraining_settings : \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) ) : \n overtraining_threshold = gr . Slider ( \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n step = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = True , \n ) \n with gr . Row ( ) : \n train_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n interactive = False , \n ) \n with gr . Row ( ) : \n train_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n train_button . click ( \n run_train_script , \n [ \n model_name , \n rvc_version , \n save_every_epoch , \n save_only_latest , \n save_every_weights , \n total_epoch , \n sampling_rate , \n batch_size , \n gpu , \n pitch_guidance , \n overtraining_detector , \n overtraining_threshold , \n pretrained , \n custom_pretrained , \n g_pretrained_path , \n d_pretrained_path , \n ] , \n train_output_info , \n api_name = \"<STR_LIT>\" , \n ) \n stop_train_button = gr . Button ( \n i18n ( \"<STR_LIT>\" ) , visible = False \n ) \n stop_train_button . click ( \n fn = restart_applio , \n inputs = [ ] , \n outputs = [ ] , \n ) \n index_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n index_button . click ( \n run_index_script , \n [ model_name , rvc_version ] , \n train_output_info , \n api_name = \"<STR_LIT>\" , \n ) \n with gr . Accordion ( i18n ( \"<STR_LIT>\" ) , open = False ) : \n if not os . name == \"<STR_LIT>\" : \n gr . Markdown ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n with gr . Row ( ) : \n with gr . Column ( ) : \n pth_file_export = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , \n type = \"<STR_LIT>\" , \n value = None , \n interactive = False , \n ) \n pth_dropdown_export = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_pth_list ( ) , \n value = None , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Column ( ) : \n index_file_export = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , \n type = \"<STR_LIT>\" , \n value = None , \n interactive = False , \n ) \n index_dropdown_export = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n choices = get_index_list ( ) , \n value = None , \n interactive = True , \n allow_custom_value = True , \n ) \n with gr . Row ( ) : \n with gr . Column ( ) :", "output": "refresh_export = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n if not os . name == \"<STR_LIT>\" : \n upload_exported = gr . Button ( i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\" ) \n upload_exported . click ( \n fn = upload_to_google_drive , \n inputs = [ pth_dropdown_export , index_dropdown_export ] , \n outputs = [ ] , \n ) \n def toggle_visible ( checkbox ) : \n return { \"<STR_LIT>\" : checkbox , \"<STR_LIT>\" : \"<STR_LIT>\" } \n def toggle_visible_hop_length ( f0method ) : \n if f0method == \"<STR_LIT>\" or f0method == \"<STR_LIT>\" : \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" } \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } \n def toggle_pretrained ( pretrained , custom_pretrained ) : \n if custom_pretrained == False : \n return { \"<STR_LIT>\" : pretrained , \"<STR_LIT>\" : \"<STR_LIT>\" } , { \n \"<STR_LIT>\" : False , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n else : \n return { \"<STR_LIT>\" : pretrained , \"<STR_LIT>\" : \"<STR_LIT>\" } , { \n \"<STR_LIT>\" : pretrained , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def enable_stop_train_button ( ) : \n return { \"<STR_LIT>\" : False , \"<STR_LIT>\" : \"<STR_LIT>\" } , { \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def disable_stop_train_button ( ) : \n return { \"<STR_LIT>\" : True , \"<STR_LIT>\" : \"<STR_LIT>\" } , { \n \"<STR_LIT>\" : False , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n def download_prerequisites ( version ) : \n for remote_folder , file_list in pretraineds_v1 : \n local_folder = folder_mapping . get ( remote_folder , \"<STR_LIT>\" ) \n missing = False \n for file in file_list : \n destination_path = os . path . join ( local_folder , file ) \n if not os . path . exists ( destination_path ) : \n missing = True \n if version == \"<STR_LIT>\" and missing == True : \n gr . Info ( \n \"<STR_LIT>\" \n ) \n run_prerequisites_script ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n gr . Info ( \n \"<STR_LIT>\" \n ) \n rvc_version . change ( \n fn = download_prerequisites , \n inputs = [ rvc_version ] , \n outputs = [ ] , \n ) \n refresh . click ( \n fn = refresh_models_and_datasets , \n inputs = [ ] , \n outputs = [ model_name , dataset_path ] , \n ) \n dataset_creator . change ( \n fn = toggle_visible , \n inputs = [ dataset_creator ] , \n outputs = [ dataset_creator_settings ] , \n ) \n upload_audio_dataset . upload ( \n fn = save_drop_dataset_audio , \n inputs = [ upload_audio_dataset , dataset_name ] , \n outputs = [ upload_audio_dataset , dataset_path ] , \n ) \n f0method . change ( \n fn = toggle_visible_hop_length , \n inputs = [ f0method ] , \n outputs = [ hop_length ] , \n ) \n pretrained . change ( \n fn = toggle_pretrained , \n inputs = [ pretrained , custom_pretrained ] , \n outputs = [ custom_pretrained , pretrained_custom_settings ] , \n ) \n custom_pretrained . change ( \n fn = toggle_visible , \n inputs = [ custom_pretrained ] , \n outputs = [ pretrained_custom_settings ] , \n ) \n refresh_custom_pretaineds_button . click ( \n fn = refresh_custom_pretraineds , \n inputs = [ ] , \n outputs = [ g_pretrained_path , d_pretrained_path ] , \n ) \n upload_pretrained . upload ( \n fn = save_drop_model , \n inputs = [ upload_pretrained ] , \n outputs = [ upload_pretrained ] , \n ) \n overtraining_detector . change ( \n fn = toggle_visible , \n inputs = [ overtraining_detector ] , \n outputs = [ overtraining_settings ] , \n ) \n multiple_gpu . change ( \n fn = toggle_visible , \n inputs = [ multiple_gpu ] , \n outputs = [ gpu_custom_settings ] , \n ) \n train_button . click ( \n fn = enable_stop_train_button , \n inputs = [ ] , \n outputs = [ train_button , stop_train_button ] , \n ) \n train_output_info . change ( \n fn = disable_stop_train_button , \n inputs = [ ] , \n outputs = [ train_button , stop_train_button ] , \n ) \n pth_dropdown_export . change ( \n fn = export_pth , \n inputs = [ pth_dropdown_export ] , \n outputs = [ pth_file_export ] , \n ) \n index_dropdown_export . change ( \n fn = export_index , \n inputs = [ index_dropdown_export ] , \n outputs = [ index_file_export ] , \n ) \n refresh_export . click ( \n fn = refresh_pth_and_index_list , \n inputs = [ ] , \n outputs = [ pth_dropdown_export , index_dropdown_export ] , \n )"}, {"input": "import json \n import os \n config = { } \n def load_config ( config_path = \"<STR_LIT>\" ) : \n global config \n if not os . path . exists ( config_path ) : \n raise Exception ( '<STR_LIT>' )", "output": "config_str = read_file ( config_path ) \n config = json . loads ( config_str ) \n print ( \"<STR_LIT>\" ) \n return config \n def get_root ( ) : \n return os . path . dirname ( os . path . abspath ( __file__ ) ) \n def read_file ( path ) : \n with open ( path , mode = '<STR_LIT>' , encoding = '<STR_LIT>' ) as f : \n return f . read ( ) \n def conf ( ) : \n return config \n def model_conf ( model_type ) : \n return config . get ( '<STR_LIT>' ) . get ( model_type ) \n def model_conf_val ( model_type , key ) : \n val = config . get ( '<STR_LIT>' ) . get ( model_type ) . get ( key ) \n if not val : \n return config . get ( '<STR_LIT>' ) . get ( key ) \n return val \n def channel_conf ( channel_type ) : \n return config . get ( '<STR_LIT>' ) . get ( channel_type ) \n def channel_conf_val ( channel_type , key , default = None ) : \n val = config . get ( '<STR_LIT>' ) . get ( channel_type ) . get ( key ) \n if not val : \n return config . get ( '<STR_LIT>' ) . get ( key , default ) \n return val \n def common_conf_val ( key , default = None ) : \n if not config . get ( '<STR_LIT>' ) : \n return default \n return config . get ( '<STR_LIT>' ) . get ( key , default )"}, {"input": "from multiprocessing import cpu_count \n import os \n import sys \n from scipy import signal \n from scipy . io import wavfile \n import librosa \n import numpy as np \n now_directory = os . getcwd ( ) \n sys . path . append ( now_directory ) \n from rvc . lib . utils import load_audio \n from rvc . train . slicer import Slicer \n experiment_directory = sys . argv [ <NUM_LIT> ] \n input_root = sys . argv [ <NUM_LIT> ] \n sampling_rate = int ( sys . argv [ <NUM_LIT> ] ) \n percentage = float ( sys . argv [ <NUM_LIT> ] ) \n num_processes = cpu_count ( ) \n import multiprocessing \n class PreProcess : \n def __init__ ( self , sr , exp_dir , per = <NUM_LIT> ) : \n self . slicer = Slicer ( \n sr = sr , \n threshold = - <NUM_LIT> , \n min_length = <NUM_LIT> , \n min_interval = <NUM_LIT> , \n hop_size = <NUM_LIT> , \n max_sil_kept = <NUM_LIT> , \n ) \n self . sr = sr \n self . b_high , self . a_high = signal . butter ( N = <NUM_LIT> , Wn = <NUM_LIT> , btype = \"<STR_LIT>\" , fs = self . sr ) \n self . per = per \n self . overlap = <NUM_LIT> \n self . tail = self . per + self . overlap \n self . max_amplitude = <NUM_LIT> \n self . alpha = <NUM_LIT> \n self . exp_dir = exp_dir \n self . gt_wavs_dir = f\"<STR_LIT>\" \n self . wavs16k_dir = f\"<STR_LIT>\" \n os . makedirs ( self . exp_dir , exist_ok = True ) \n os . makedirs ( self . gt_wavs_dir , exist_ok = True ) \n os . makedirs ( self . wavs16k_dir , exist_ok = True ) \n def normalize_and_write ( self , tmp_audio , idx0 , idx1 ) : \n tmp_max = np . abs ( tmp_audio ) . max ( ) \n if tmp_max > <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n return", "output": "tmp_audio = ( tmp_audio / tmp_max * ( self . max_amplitude * self . alpha ) ) + ( \n <NUM_LIT> - self . alpha \n ) * tmp_audio \n wavfile . write ( \n f\"<STR_LIT>\" , \n self . sr , \n tmp_audio . astype ( np . float32 ) , \n ) \n tmp_audio = librosa . resample ( \n tmp_audio , orig_sr = self . sr , target_sr = <NUM_LIT> \n ) \n wavfile . write ( \n f\"<STR_LIT>\" , \n <NUM_LIT> , \n tmp_audio . astype ( np . float32 ) , \n ) \n def process_audio ( self , path , idx0 ) : \n try : \n audio = load_audio ( path , self . sr ) \n audio = signal . lfilter ( self . b_high , self . a_high , audio ) \n idx1 = <NUM_LIT> \n for audio_segment in self . slicer . slice ( audio ) : \n i = <NUM_LIT> \n while <NUM_LIT> : \n start = int ( self . sr * ( self . per - self . overlap ) * i ) \n i += <NUM_LIT> \n if len ( audio_segment [ start : ] ) > self . tail * self . sr : \n tmp_audio = audio_segment [ \n start : start + int ( self . per * self . sr ) \n ] \n self . normalize_and_write ( tmp_audio , idx0 , idx1 ) \n idx1 += <NUM_LIT> \n else : \n tmp_audio = audio_segment [ start : ] \n idx1 += <NUM_LIT> \n break \n self . normalize_and_write ( tmp_audio , idx0 , idx1 ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n def process_audio_multiprocessing ( self , infos ) : \n for path , idx0 in infos : \n self . process_audio ( path , idx0 ) \n def process_audio_multiprocessing_input_directory ( self , input_root , num_processes ) : \n try : \n infos = [ \n ( f\"<STR_LIT>\" , idx ) \n for idx , name in enumerate ( sorted ( list ( os . listdir ( input_root ) ) ) ) \n ] \n processes = [ ] \n for i in range ( num_processes ) : \n p = multiprocessing . Process ( \n target = self . process_audio_multiprocessing , \n args = ( infos [ i : : num_processes ] , ) , \n ) \n processes . append ( p ) \n p . start ( ) \n for i in range ( num_processes ) : \n processes [ i ] . join ( ) \n except Exception as error : \n print ( error ) \n def preprocess_training_set ( input_root , sr , num_processes , exp_dir , per ) : \n pp = PreProcess ( sr , exp_dir , per ) \n print ( \"<STR_LIT>\" ) \n pp . process_audio_multiprocessing_input_directory ( input_root , num_processes ) \n print ( \"<STR_LIT>\" ) \n if __name__ == \"<STR_LIT>\" : \n preprocess_training_set ( \n input_root , sampling_rate , num_processes , experiment_directory , percentage \n )"}, {"input": "tu_edad = <NUM_LIT> \n while True : \n try : \n tu_edad = int ( input ( \"<STR_LIT>\" ) ) \n break \n except ValueError : \n pass \n print ( tu_edad ) \n class A ( Exception ) : \n pass \n class B ( A ) : \n pass \n class C ( B ) : \n pass \n for cls in [ A , B , C ] : \n try : \n raise cls ( ) \n except C : \n print ( \"<STR_LIT>\" ) \n except B : \n print ( \"<STR_LIT>\" ) \n except A : \n print ( \"<STR_LIT>\" ) \n try : \n raise Exception ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n except Exception as error : \n print ( type ( error ) ) \n print ( error . args ) \n print ( error ) \n x , y = error . args \n print ( \"<STR_LIT>\" , x )", "output": "print ( \"<STR_LIT>\" , y )"}, {"input": "from typing import List \n import random \n import argparse \n import json \n from datasets import Dataset , load_dataset \n from multi_token . constants import ROLE_ASSISTANT , ROLE_USER \n def _write_convo ( row ) -> List : \n video = \"<STR_LIT>\" + row [ \"<STR_LIT>\" ] [ <NUM_LIT> : ] \n example = { \n \"<STR_LIT>\" : [ video ] , \n } \n example [ \"<STR_LIT>\" ] = [ \n { \n \"<STR_LIT>\" : ROLE_USER , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] , \n } , \n { \n \"<STR_LIT>\" : ROLE_ASSISTANT , \n \"<STR_LIT>\" : row [ \"<STR_LIT>\" ] ,", "output": "} , \n ] \n return example \n def main ( args ) : \n data = load_dataset ( \"<STR_LIT>\" , split = \"<STR_LIT>\" ) \n def gen ( ) : \n for row in data : \n try : \n yield _write_convo ( row ) \n except Exception as e : \n print ( e ) \n ds = Dataset . from_generator ( gen ) \n ds . save_to_disk ( args . output_folder ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" \n ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import base64 \n import mutagen . flac \n from . import util \n from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY \n def get_pictures ( afile , norm_key ) : \n artworks = [ Artwork ( p . data , width = p . width , height = p . height , \n fmt = p . mime . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] , pic_type = p . type ) \n for p in afile . mfile . pictures ] \n return MetadataItem ( Artwork , None , artworks ) \n def set_pictures ( afile , norm_key , artworks ) : \n if not isinstance ( artworks , MetadataItem ) : \n raise TypeError ( ) \n afile . mfile . clear_pictures ( ) \n for i , art in enumerate ( artworks . values ) : \n if any ( v is None for v in ( art . mime , art . width , art . height , art . depth ) ) : \n raise ImportError ( \"<STR_LIT>\" ) \n pic = mutagen . flac . Picture ( ) \n pic . data = art . raw \n pic . type = art . pic_type \n pic . mime = art . mime \n pic . width = art . width \n pic . height = art . height \n pic . depth = art . depth \n afile . mfile . add_picture ( pic ) \n def rm_pictures ( afile , norm_key ) : \n afile . mfile . clear_pictures ( ) \n class FlacFile ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . flac . FLAC \n _TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int , sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures ,", "output": "remover = rm_pictures , \n type = Artwork ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n } \n def _ft_setter ( self , key , md_val , appendable = True ) : \n if self . appendable and appendable : \n self . mfile . tags [ key ] = [ str ( v ) for v in md_val . values ] \n else : \n self . mfile . tags [ key ] = str ( md_val . value )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = '<STR_LIT>' ) )", "output": "batch_op . create_index ( batch_op . f ( '<STR_LIT>' ) , [ '<STR_LIT>' ] , unique = False ) \n batch_op . create_foreign_key ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , [ '<STR_LIT>' ] ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_constraint ( '<STR_LIT>' , type_ = '<STR_LIT>' ) \n batch_op . drop_index ( batch_op . f ( '<STR_LIT>' ) ) \n batch_op . drop_column ( '<STR_LIT>' )"}, {"input": "from legged_gym import LEGGED_GYM_ROOT_DIR , LEGGED_GYM_ENVS_DIR \n from legged_gym . envs . a1 . a1_config import A1RoughCfg , A1RoughCfgPPO \n from . base . legged_robot import LeggedRobot \n from . anymal_c . anymal import Anymal \n from . anymal_c . mixed_terrains . anymal_c_rough_config import AnymalCRoughCfg , AnymalCRoughCfgPPO \n from . anymal_c . flat . anymal_c_flat_config import AnymalCFlatCfg , AnymalCFlatCfgPPO \n from . anymal_b . anymal_b_config import AnymalBRoughCfg , AnymalBRoughCfgPPO \n from . cassie . cassie import Cassie \n from . cassie . cassie_config import CassieRoughCfg , CassieRoughCfgPPO \n from . a1 . a1_config import A1RoughCfg , A1RoughCfgPPO \n from . a1 . a1_parkour_config import A1ParkourCfg , A1ParkourCfgPPO \n from . go1 . go1_config import Go1RoughCfg , Go1RoughCfgPPO \n import os \n from legged_gym . utils . task_registry import task_registry \n task_registry . register ( \"<STR_LIT>\" , LeggedRobot , A1ParkourCfg ( ) , A1ParkourCfgPPO ( ) )", "output": "task_registry . register ( \"<STR_LIT>\" , LeggedRobot , Go1RoughCfg ( ) , Go1RoughCfgPPO ( ) )"}, {"input": "import mutagen . mp4 \n import mutagen . easymp4 \n from mutagen . mp4 import MP4FreeForm \n from . import util \n from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY \n mutagen . easymp4 . EasyMP4Tags . RegisterTextKey ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n _MP4_ISRC_KEY = '<STR_LIT>' \n def get_tracknum ( afile , norm_key ) : \n trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] \n try : \n return trkn [ <NUM_LIT> ] \n except IndexError : \n return None \n def set_tracknum ( afile , norm_key , val ) : \n trkn = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) \n trkn += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( trkn ) ) \n trkn [ <NUM_LIT> ] = int ( val ) \n trkn = tuple ( [ <NUM_LIT> if i is None else int ( i ) for i in trkn ] ) \n afile . mfile . tags [ '<STR_LIT>' ] = [ trkn ] \n def get_totaltracks ( afile , norm_key ) : \n trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] \n try : \n return trkn [ <NUM_LIT> ] \n except IndexError : \n return None \n def set_totaltracks ( afile , norm_key , val ) : \n trkn = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) \n trkn += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( trkn ) ) \n trkn [ <NUM_LIT> ] = int ( val ) \n trkn = tuple ( [ <NUM_LIT> if i is None else int ( i ) for i in trkn ] ) \n afile . mfile . tags [ '<STR_LIT>' ] = [ trkn ] \n def get_discnum ( afile , norm_key ) : \n trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] \n try : \n return trkn [ <NUM_LIT> ] \n except IndexError : \n return None \n def set_discnum ( afile , norm_key , val ) : \n disc = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) \n disc += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( disc ) ) \n disc [ <NUM_LIT> ] = int ( val ) \n disc = [ <NUM_LIT> if i is None else i for i in disc ] \n afile . mfile . tags [ '<STR_LIT>' ] = [ tuple ( disc ) ] \n def get_totaldiscs ( afile , norm_key ) : \n trkn = afile . mfile . get ( '<STR_LIT>' , [ ( None , None ) ] ) [ <NUM_LIT> ] \n try : \n return trkn [ <NUM_LIT> ] \n except IndexError : \n return None \n def set_totaldiscs ( afile , norm_key , val ) : \n disc = list ( afile . mfile . tags . get ( '<STR_LIT>' , [ ( <NUM_LIT> , <NUM_LIT> ) ] ) [ <NUM_LIT> ] ) \n disc += [ <NUM_LIT> ] * ( <NUM_LIT> - len ( disc ) ) \n disc [ <NUM_LIT> ] = int ( val ) \n disc = [ <NUM_LIT> if i is None else i for i in disc ] \n afile . mfile . tags [ '<STR_LIT>' ] = [ tuple ( disc ) ] \n def get_artwork ( afile , norm_key ) : \n fmt_lut = { mutagen . mp4 . MP4Cover . FORMAT_JPEG : '<STR_LIT>' , \n mutagen . mp4 . MP4Cover . FORMAT_PNG : '<STR_LIT>' , \n } \n artworks = [ Artwork ( bytes ( p ) ) for p in afile . mfile . tags [ '<STR_LIT>' ] ] \n return MetadataItem ( Artwork , None , artworks ) \n def set_artwork ( afile , norm_key , artworks ) : \n if not isinstance ( artworks , MetadataItem ) : \n raise TypeError ( ) \n pics = [ ] \n for art in artworks . values : \n if any ( v is None for v in ( art . mime , ) ) : \n raise ImportError ( \"<STR_LIT>\" ) \n mime_fmt = art . mime . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . upper ( ) \n if mime_fmt == '<STR_LIT>' : \n img_fmt = mutagen . mp4 . MP4Cover . FORMAT_JPEG", "output": "elif mime_fmt == '<STR_LIT>' : \n img_fmt = mutagen . mp4 . MP4Cover . FORMAT_PNG \n else : \n raise TypeError ( '<STR_LIT>' ) \n pics . append ( mutagen . mp4 . MP4Cover ( art . raw , imageformat = img_fmt ) ) \n afile . mfile . tags [ '<STR_LIT>' ] = pics \n def freeform_get ( afile , norm_key ) : \n return [ val . decode ( ) for val in afile . mfile . get ( norm_key , [ ] ) ] \n def freeform_set ( afile , norm_key , val ) : \n ff_vals = [ MP4FreeForm ( v . encode ( '<STR_LIT>' ) ) for v in val . values ] \n afile . mfile . tags [ norm_key ] = ff_vals \n class Mp4File ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . mp4 . MP4 \n _TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_tracknum , \n setter = set_tracknum , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaltracks , \n setter = set_totaltracks , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_discnum , \n setter = set_discnum , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_totaldiscs , \n setter = set_totaldiscs , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda f , k : freeform_get ( f , _MP4_ISRC_KEY ) , \n setter = lambda f , k , v : freeform_set ( f , _MP4_ISRC_KEY , v ) , \n remover = _MP4_ISRC_KEY , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = bool , \n sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_artwork , setter = set_artwork , \n type = Artwork ) , \n } \n class EasyMp4File ( Mp4File ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . easymp4 . EasyMP4 \n _TAG_MAP = Mp4File . _TAG_MAP . copy ( ) \n _TAG_MAP . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_tracknum , \n setter = util . set_easy_tracknum , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_totaltracks , \n setter = util . set_easy_totaltracks , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_discnum , \n setter = util . set_easy_discnum , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = util . get_easy_totaldiscs , \n setter = util . set_easy_totaldiscs , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = int , \n sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , type = Artwork ) , \n } )"}, {"input": "import datetime \n import json \n import dateparser \n from bs4 import BeautifulSoup \n from feedi import scraping \n from feedi . requests import requests \n def fetch ( url ) : \n response = requests . get ( url ) \n response . raise_for_status ( ) \n if not response . ok : \n raise Exception ( ) \n soup = BeautifulSoup ( response . content , '<STR_LIT>' ) \n metadata = scraping . all_meta ( soup ) \n title = metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' , getattr ( soup . title , '<STR_LIT>' ) ) ) \n if not title : \n raise ValueError ( f\"<STR_LIT>\" ) \n if '<STR_LIT>' in metadata : \n display_date = dateparser . parse ( metadata [ '<STR_LIT>' ] ) \n else :", "output": "display_date = datetime . datetime . utcnow ( ) \n username = metadata . get ( '<STR_LIT>' , '<STR_LIT>' ) . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n icon_url = scraping . get_favicon ( url , html = response . content ) \n entry = { \n '<STR_LIT>' : url , \n '<STR_LIT>' : title , \n '<STR_LIT>' : username , \n '<STR_LIT>' : display_date , \n '<STR_LIT>' : datetime . datetime . utcnow ( ) , \n '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , \n '<STR_LIT>' : metadata . get ( '<STR_LIT>' , metadata . get ( '<STR_LIT>' ) ) , \n '<STR_LIT>' : url , \n '<STR_LIT>' : url , \n '<STR_LIT>' : json . dumps ( metadata ) , \n '<STR_LIT>' : icon_url } \n return entry"}, {"input": "from densely_captioned_images . dataset . utils import get_clip_token_length \n from densely_captioned_images . dataset . config import DATASET_PHOTO_PATH , DATASET_COMPLETE_PATH \n from PIL import Image \n from io import BytesIO \n import numpy as np \n import os \n import json \n import base64 \n from typing import Optional , List , Dict , TypedDict , Union", "output": "class DCIEntry ( TypedDict ) : \n image : np . ndarray \n caption : str \n key : str \n class PointDict ( TypedDict ) : \n x : int \n y : int \n class BoundDict ( TypedDict ) : \n topLeft : PointDict \n bottomRight : PointDict \n class NegativeEntry ( TypedDict ) : \n swaps : List [ str ] \n layout : List [ str ] \n basic : List [ str ] \n DCISummaries = Dict [ str , str ] \n DCINegatives = Dict [ str , NegativeEntry ] \n class MaskDataEntry ( TypedDict ) : \n outer_mask : str \n area : int \n bounds : BoundDict \n idx : int \n requirements : List [ int ] \n parent : int \n label : str \n caption : str \n mask_quality : int \n class DCIBaseData ( TypedDict ) : \n short_caption : str \n extra_caption : str \n image : str \n mask_data : Dict [ str , MaskDataEntry ] \n mask_keys : List [ str ] \n class DCIExtendedData ( DCIBaseData ) : \n img : np . ndarray \n height : int \n _id : int \n _entry_key : str \n _source : str \n ENTRIES = None \n ENTRIES_MAP = None \n ENTRIES_REVERSE_MAP = None \n def init_entries ( source : str = DATASET_COMPLETE_PATH ) -> None : \n global ENTRIES , ENTRIES_MAP , ENTRIES_REVERSE_MAP \n ENTRIES = os . listdir ( source ) \n ENTRIES_MAP = { str ( i ) : e for i , e in enumerate ( ENTRIES ) } \n ENTRIES_REVERSE_MAP = { str ( e ) : i for i , e in enumerate ( ENTRIES ) } \n def get_dci_count ( ) -> int : \n if ENTRIES_MAP is None : \n init_entries ( ) \n return len ( ENTRIES_MAP ) \n def get_key_for ( idx : Union [ int , str ] ) -> str : \n if ENTRIES_MAP is None : \n init_entries ( ) \n return ENTRIES_MAP . get ( str ( idx ) ) \n def load_image ( entry_key : str ) -> DCIExtendedData : \n if ENTRIES is None : \n init_entries ( ) \n if entry_key in ENTRIES_MAP : \n entry_key = ENTRIES_MAP [ entry_key ] \n complete_path = os . path . join ( DATASET_COMPLETE_PATH , entry_key ) \n with open ( complete_path ) as entry_file : \n base_data : DCIBaseData = json . load ( entry_file ) \n img = Image . open ( os . path . join ( DATASET_PHOTO_PATH , base_data [ '<STR_LIT>' ] ) ) \n width , height = img . size \n base_data [ '<STR_LIT>' ] = width \n base_data [ '<STR_LIT>' ] = height \n base_data [ '<STR_LIT>' ] = np . array ( img ) \n base_data [ '<STR_LIT>' ] = ENTRIES_REVERSE_MAP [ entry_key ] \n base_data [ '<STR_LIT>' ] = entry_key \n base_data [ '<STR_LIT>' ] = os . path . join ( DATASET_COMPLETE_PATH , entry_key ) \n return base_data \n class DenseCaptionedImage ( ) : \n def __init__ ( self , img_id : int ) : \n self . _data : DCIExtendedData = load_image ( str ( img_id ) ) \n self . _id : int = img_id \n def get_summaries ( self ) -> Optional [ DCISummaries ] : \n return self . _data . get ( '<STR_LIT>' ) \n def get_negatives ( self ) -> Optional [ DCINegatives ] : \n return self . _data . get ( '<STR_LIT>' ) \n def get_mask ( self , idx : int ) -> MaskDataEntry : \n return self . _data [ '<STR_LIT>' ] . get ( str ( idx ) ) \n def get_all_masks ( self ) -> List [ MaskDataEntry ] : \n return list ( self . _data [ '<STR_LIT>' ] . values ( ) ) \n def filter_masks_by_size ( \n self , \n min_height : int = <NUM_LIT> , \n min_width : int = <NUM_LIT> , \n min_length : int = <NUM_LIT> , \n max_length : Optional [ int ] = None \n ) -> List [ MaskDataEntry ] : \n all_masks = self . get_all_masks ( ) \n if min_height == <NUM_LIT> and min_width == <NUM_LIT> and min_length == <NUM_LIT> and max_length is None : \n return all_masks \n def mask_is_bigger ( mask : MaskDataEntry ) -> bool : \n if min_length > <NUM_LIT> or max_length is not None : \n caption_len = get_clip_token_length ( self . _extract_caption ( mask ) ) \n if caption_len < min_length or ( max_length is not None and caption_len > max_length ) : \n return False \n crop_dims = mask [ '<STR_LIT>' ] \n width = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n height = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n return width >= min_width and height >= min_height \n return [ m for m in all_masks if mask_is_bigger ( m ) ] \n def _extract_caption ( self , mask ) -> Optional [ str ] : \n if mask [ '<STR_LIT>' ] == <NUM_LIT> : \n return None \n elif mask [ '<STR_LIT>' ] == <NUM_LIT> : \n return mask [ '<STR_LIT>' ] \n return f\"<STR_LIT>\" \n def _get_max_depth ( self , mask ) -> int : \n submasks = [ self . get_mask ( m ) for m in mask [ '<STR_LIT>' ] ] \n submasks = [ m for m in submasks if m is not None ] \n if len ( submasks ) == <NUM_LIT> : \n return <NUM_LIT> \n return <NUM_LIT> + max ( [ self . _get_max_depth ( m ) for m in submasks ] ) \n def get_all_submasks_dfs ( \n self , \n mask : MaskDataEntry , \n max_depth : int , \n include_self : bool = True \n ) -> List [ MaskDataEntry ] : \n if include_self : \n res = [ mask ] \n else : \n res = [ ] \n if max_depth == <NUM_LIT> : \n return res \n submasks = [ self . get_mask ( m ) for m in mask [ '<STR_LIT>' ] ] \n submasks = [ m for m in submasks if m is not None and m [ '<STR_LIT>' ] != <NUM_LIT> ] \n for submask in submasks : \n res += self . get_all_submasks_dfs ( submask , max_depth - <NUM_LIT> ) \n return res \n def get_caption_with_subcaptions ( self , mask , max_depth = <NUM_LIT> ) -> List [ DCIEntry ] : \n submasks = self . get_all_submasks_dfs ( mask , max_depth = max_depth , include_self = False ) \n base_caption = f\"<STR_LIT>\" \n captions = [ self . _extract_caption ( m ) for m in submasks ] \n captions = [ c for c in captions if c is not None ] \n all_captions = \"<STR_LIT>\" . join ( captions ) \n if len ( captions ) > <NUM_LIT> : \n caption = f\"<STR_LIT>\" \n else : \n caption = base_caption \n return [ \n { '<STR_LIT>' : self . get_subimage ( mask ) , '<STR_LIT>' : caption , '<STR_LIT>' : f'<STR_LIT>' } \n ] \n def get_image ( self ) -> np . ndarray : \n return self . _data [ '<STR_LIT>' ] \n def get_subimage ( self , mask , pad_amount = <NUM_LIT> , apply_mask = False ) -> np . ndarray : \n base_image = self . get_image ( ) \n if apply_mask : \n mask_array = np . array ( Image . open ( BytesIO ( base64 . b64decode ( mask [ '<STR_LIT>' ] ) ) ) ) \n mask_tiled = np . tile ( mask_array , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) . transpose ( ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) \n base_image = base_image * mask_tiled + ( base_image // <NUM_LIT> + <NUM_LIT> ) * ( <NUM_LIT> - mask_tiled ) \n base_image = base_image . astype ( np . uint8 ) \n x1 , y1 , x2 , y2 = <NUM_LIT> , <NUM_LIT> , self . _data [ '<STR_LIT>' ] , self . _data [ '<STR_LIT>' ] \n crop_dims = mask [ '<STR_LIT>' ] \n width = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n height = crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n w_pad_int = int ( width * pad_amount ) \n h_pad_int = int ( height * pad_amount ) \n cx1 = max ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - w_pad_int , x1 ) \n cy1 = max ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] - h_pad_int , y1 ) \n cx2 = min ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] + w_pad_int , x2 ) \n cy2 = min ( crop_dims [ '<STR_LIT>' ] [ '<STR_LIT>' ] + h_pad_int , y2 ) \n return base_image [ cy1 : cy2 , cx1 : cx2 ] \n def get_base_caption ( self ) -> List [ DCIEntry ] : \n return [ \n { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : self . _data [ '<STR_LIT>' ] , '<STR_LIT>' : '<STR_LIT>' } \n ] \n def get_extended_caption ( self ) -> List [ DCIEntry ] : \n caption = f\"<STR_LIT>\" \n return [ \n { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : caption , '<STR_LIT>' : '<STR_LIT>' } \n ] \n def get_formatted_description_min_size ( self , min_height = <NUM_LIT> , min_width = <NUM_LIT> ) -> List [ DCIEntry ] : \n base_caption = f\"<STR_LIT>\" \n masks = self . filter_masks_by_size ( min_height = min_height , min_width = min_width ) \n captions = [ self . _extract_caption ( m ) for m in masks ] \n captions = [ c for c in captions if c is not None ] \n all_captions = \"<STR_LIT>\" . join ( captions ) \n if len ( captions ) > <NUM_LIT> : \n caption = f\"<STR_LIT>\" \n else : \n caption = base_caption \n return [ \n { '<STR_LIT>' : self . get_image ( ) , '<STR_LIT>' : caption , '<STR_LIT>' : '<STR_LIT>' } \n ] \n def get_formatted_complete_description ( self ) -> List [ DCIEntry ] : \n return self . get_formatted_description_min_size ( min_height = <NUM_LIT> , min_width = <NUM_LIT> ) \n def get_positive_mask_samples ( \n self , \n min_height : int = <NUM_LIT> , \n min_width : int = <NUM_LIT> , \n min_length : int = <NUM_LIT> , \n max_length : Optional [ int ] = None \n ) -> List [ DCIEntry ] : \n masks = self . filter_masks_by_size ( \n min_height = min_height , min_width = min_width , min_length = min_length , max_length = max_length \n ) \n return [ \n { \n '<STR_LIT>' : self . get_subimage ( mask ) , \n '<STR_LIT>' : self . _extract_caption ( mask ) , \n '<STR_LIT>' : f'<STR_LIT>' \n } for mask in masks \n ]"}, {"input": "def puedes_conducir ( edad = <NUM_LIT> ) : \n if edad < <NUM_LIT> : \n return False \n elif edad >= <NUM_LIT> and edad < <NUM_LIT> : \n return True , \"<STR_LIT>\" \n elif edad >= <NUM_LIT> and edad < <NUM_LIT> : \n return True , \"<STR_LIT>\" \n else : \n return False \n def tipo_licencia ( tipo_vehiculo = \"<STR_LIT>\" ) : \n match tipo_vehiculo : \n case \"<STR_LIT>\" : \n return \"<STR_LIT>\" \n case \"<STR_LIT>\" : \n return \"<STR_LIT>\" \n case \"<STR_LIT>\" : \n return \"<STR_LIT>\" \n case _ :", "output": "return False"}, {"input": "from curl_cffi import requests , Curl , CurlOpt \n from io import BytesIO \n import json \n import re \n def send_message ( ) : \n url = \"<STR_LIT>\" \n attachments = [ ] \n prompt = \"<STR_LIT>\" \n organization_id = \"<STR_LIT>\" \n conversation_id = \"<STR_LIT>\" \n cookie = \"<STR_LIT>\" \n proxies = \"<STR_LIT>\" \n payload = json . dumps ( { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : f\"<STR_LIT>\" , \n \"<STR_LIT>\" : attachments \n } ) \n headers = [ b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' % cookie . encode ( '<STR_LIT>' ) , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' , \n b'<STR_LIT>' ] \n buffer = BytesIO ( ) \n c = Curl ( ) \n def stream_callback ( data ) : \n json_str = data . decode ( '<STR_LIT>' ) \n decoded_data = re . sub ( '<STR_LIT>' , '<STR_LIT>' , json_str ) . strip ( ) \n data_strings = decoded_data . split ( '<STR_LIT>' ) \n for data_string in data_strings : \n json_str = data_string [ <NUM_LIT> : ] . strip ( ) \n _data = json . loads ( json_str ) \n if '<STR_LIT>' in _data : \n buffer . write ( str ( _data [ '<STR_LIT>' ] ) . encode ( '<STR_LIT>' ) ) \n print ( _data [ '<STR_LIT>' ] , end = \"<STR_LIT>\" ) \n c . setopt ( CurlOpt . URL , b'<STR_LIT>' ) \n c . setopt ( CurlOpt . WRITEFUNCTION , stream_callback ) \n c . setopt ( CurlOpt . HTTPHEADER , headers ) \n c . setopt ( CurlOpt . POSTFIELDS , payload ) \n c . setopt ( CurlOpt . PROXY , proxies . encode ( ) ) \n c . impersonate ( \"<STR_LIT>\" ) \n c . perform ( ) \n c . close ( )", "output": "body = buffer . getvalue ( ) \n print ( body . decode ( ) ) \n send_message ( )"}, {"input": "import time \n import itchat \n import json \n from itchat . content import * \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n from common . log import logger \n from common import const \n from config import channel_conf_val \n import requests \n from plugins . plugin_manager import * \n from common . sensitive_word import SensitiveWord \n import io", "output": "thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n sw = SensitiveWord ( ) \n @ itchat . msg_register ( TEXT ) \n def handler_single_msg ( msg ) : \n WechatChannel ( ) . handle ( msg ) \n return None \n @ itchat . msg_register ( TEXT , isGroupChat = True ) \n def handler_group_msg ( msg ) : \n WechatChannel ( ) . handle_group ( msg ) \n return None \n class WechatChannel ( Channel ) : \n def __init__ ( self ) : \n pass \n def startup ( self ) : \n hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) \n if channel_conf_val ( const . WECHAT , '<STR_LIT>' ) : \n itchat . auto_login ( enableCmdQR = <NUM_LIT> , hot_reload = hot_reload , qrCallback = self . login ) \n else : \n itchat . auto_login ( enableCmdQR = <NUM_LIT> , hotReload = hot_reload ) \n itchat . run ( ) \n def login ( self , uuid = None , status = '<STR_LIT>' , qrcode = None ) : \n print ( '<STR_LIT>' , uuid ) \n print ( '<STR_LIT>' , status ) \n print ( '<STR_LIT>' , '<STR_LIT>' + uuid ) \n def handle ( self , msg ) : \n logger . debug ( \"<STR_LIT>\" + json . dumps ( msg , ensure_ascii = False ) ) \n from_user_id = msg [ '<STR_LIT>' ] \n to_user_id = msg [ '<STR_LIT>' ] \n other_user_id = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n create_time = msg [ '<STR_LIT>' ] \n content = msg [ '<STR_LIT>' ] \n hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) \n if hot_reload == True and int ( create_time ) < int ( time . time ( ) ) - <NUM_LIT> : \n logger . debug ( \"<STR_LIT>\" ) \n return \n if sw . process_text ( content ) : \n self . send ( '<STR_LIT>' , from_user_id ) \n return \n match_prefix = self . check_prefix ( content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) \n if from_user_id == other_user_id and match_prefix is not None : \n if match_prefix != '<STR_LIT>' : \n str_list = content . split ( match_prefix , <NUM_LIT> ) \n if len ( str_list ) == <NUM_LIT> : \n content = str_list [ <NUM_LIT> ] . strip ( ) \n thread_pool . submit ( self . _do_send , content , from_user_id ) \n elif to_user_id == other_user_id and match_prefix : \n str_list = content . split ( match_prefix , <NUM_LIT> ) \n if len ( str_list ) == <NUM_LIT> : \n content = str_list [ <NUM_LIT> ] . strip ( ) \n thread_pool . submit ( self . _do_send , content , to_user_id ) \n def handle_group ( self , msg ) : \n logger . debug ( \"<STR_LIT>\" + json . dumps ( msg , ensure_ascii = False ) ) \n group_name = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) \n group_id = msg [ '<STR_LIT>' ] . get ( '<STR_LIT>' , None ) \n create_time = msg [ '<STR_LIT>' ] \n hot_reload = channel_conf_val ( const . WECHAT , '<STR_LIT>' , True ) \n if hot_reload == True and int ( create_time ) < int ( time . time ( ) ) - <NUM_LIT> : \n logger . debug ( \"<STR_LIT>\" ) \n return \n if not group_name : \n return None \n origin_content = msg [ '<STR_LIT>' ] \n content = msg [ '<STR_LIT>' ] \n content_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) \n context_special_list = content . split ( '<STR_LIT>' , <NUM_LIT> ) \n if len ( context_special_list ) == <NUM_LIT> : \n content = context_special_list [ <NUM_LIT> ] \n elif len ( content_list ) == <NUM_LIT> : \n content = content_list [ <NUM_LIT> ] \n match_prefix = ( msg [ '<STR_LIT>' ] and not channel_conf_val ( const . WECHAT , \"<STR_LIT>\" , False ) ) or self . check_prefix ( origin_content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) or self . check_contain ( origin_content , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) \n if match_prefix is True : \n if sw . process_text ( content ) : \n self . send ( '<STR_LIT>' , group_id ) \n return \n group_white_list = channel_conf_val ( const . WECHAT , '<STR_LIT>' ) \n if ( '<STR_LIT>' in group_white_list or group_name in group_white_list or self . check_contain ( group_name , channel_conf_val ( const . WECHAT , '<STR_LIT>' ) ) ) and match_prefix : \n thread_pool . submit ( self . _do_send_group , content , msg ) \n return None \n def send ( self , msg , receiver ) : \n logger . info ( '<STR_LIT>' . format ( msg , receiver ) ) \n itchat . send ( msg , toUserName = receiver ) \n def _do_send ( self , query , reply_user_id ) : \n try : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = reply_user_id \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { \n '<STR_LIT>' : self , '<STR_LIT>' : query , \"<STR_LIT>\" : context } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if not e_context . is_pass ( ) : \n reply = super ( ) . build_reply_content ( e_context [ \"<STR_LIT>\" ] , e_context [ \"<STR_LIT>\" ] ) \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { \n '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , \"<STR_LIT>\" : e_context [ \"<STR_LIT>\" ] } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if reply : \n self . send ( channel_conf_val ( const . WECHAT , \"<STR_LIT>\" ) + reply , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def _do_send_img ( self , query , context ) : \n try : \n if not query : \n return \n reply_user_id = context [ '<STR_LIT>' ] \n img_urls = super ( ) . build_reply_content ( query , context ) \n if not img_urls : \n return \n if not isinstance ( img_urls , list ) : \n self . send ( channel_conf_val ( const . WECHAT , \"<STR_LIT>\" ) + img_urls , reply_user_id ) \n return \n for url in img_urls : \n pic_res = requests . get ( url , stream = True ) \n image_storage = io . BytesIO ( ) \n for block in pic_res . iter_content ( <NUM_LIT> ) : \n image_storage . write ( block ) \n image_storage . seek ( <NUM_LIT> ) \n logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) \n itchat . send_image ( image_storage , reply_user_id ) \n except Exception as e : \n logger . exception ( e ) \n def _do_send_group ( self , query , msg ) : \n if not query : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_HANDLE_CONTEXT , { \n '<STR_LIT>' : self , '<STR_LIT>' : query , \"<STR_LIT>\" : context } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if not e_context . is_pass ( ) : \n context [ '<STR_LIT>' ] = msg [ '<STR_LIT>' ] \n reply = super ( ) . build_reply_content ( e_context [ \"<STR_LIT>\" ] , e_context [ \"<STR_LIT>\" ] ) \n e_context = PluginManager ( ) . emit_event ( EventContext ( Event . ON_DECORATE_REPLY , { \n '<STR_LIT>' : self , '<STR_LIT>' : context , '<STR_LIT>' : reply , \"<STR_LIT>\" : e_context [ \"<STR_LIT>\" ] } ) ) \n reply = e_context [ '<STR_LIT>' ] \n if reply : \n reply = '<STR_LIT>' + msg [ '<STR_LIT>' ] + '<STR_LIT>' + reply . strip ( ) \n self . send ( channel_conf_val ( const . WECHAT , \"<STR_LIT>\" , \"<STR_LIT>\" ) + reply , msg [ '<STR_LIT>' ] [ '<STR_LIT>' ] ) \n def check_prefix ( self , content , prefix_list ) : \n for prefix in prefix_list : \n if content . startswith ( prefix ) : \n return prefix \n return None \n def check_contain ( self , content , keyword_list ) : \n if not keyword_list : \n return None \n for ky in keyword_list : \n if content . find ( ky ) != - <NUM_LIT> : \n return True \n return None"}, {"input": "import logging \n import sys \n from . import payload_gen \n from . colorize import colored \n from . context_vars import ( \n get_context_vars_manager , \n ContextVariableManager , \n ) \n from . const import ( \n DetectMode , \n SET_STMT_PATTERNS , \n CALLBACK_PREPARE_FULLPAYLOADGEN , \n CALLBACK_GENERATE_FULLPAYLOAD , \n STRING , \n INTEGER , \n OS_POPEN_READ , \n EVAL , \n WafFunc , \n ) \n from . options import Options \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n from typing import Callable , Tuple , Union , Dict , Any , List , Literal \n else : \n from typing_extensions import Callable , Tuple , Union , Dict , Any , List , Literal \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n def get_outer_pattern ( \n waf_func : Callable , \n ) -> Union [ Tuple [ str , bool ] , Tuple [ None , None ] ] : \n outer_payloads = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" , True ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" , True ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" , True ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" , False ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" , False ) , \n ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n False , \n ) , \n ] \n for test_payload , outer_pattern , will_print in outer_payloads : \n if waf_func ( test_payload ) : \n return outer_pattern , will_print \n else : \n logger . warning ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , outer_pattern ) ) \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" , bold = True ) , \n ) \n return None , None \n def context_payloads_to_context ( \n context_payload : Dict [ str , Dict [ str , Any ] ] \n ) -> Dict [ str , Any ] : \n return { \n var_name : var_value \n for _ , d in context_payload . items ( ) \n for var_name , var_value in d . items ( ) \n } \n class FullPayloadGen : \n def __init__ ( \n self , \n waf_func : WafFunc , \n callback : Union [ Callable [ [ str , Dict ] , None ] , None ] = None , \n options : Union [ Options , None ] = None , \n waf_expr_func : Union [ WafFunc , None ] = None , \n ) : \n self . waf_func = waf_func \n self . prepared = False \n self . extra_context_vars_prepared = False \n self . added_extra_context_vars = set ( ) \n self . _callback : Callable [ [ str , Dict ] , None ] = ( \n callback if callback else ( lambda x , y : None ) \n ) \n self . context_vars : Union [ ContextVariableManager , None ] = None \n self . outer_pattern , self . will_print = None , None \n self . payload_gen = None \n self . options = options if options else Options ( ) \n self . waf_expr_func = waf_expr_func \n @ property \n def callback ( self ) : \n return self . _callback \n @ callback . setter \n def callback ( self , callback ) : \n self . _callback = callback \n if self . payload_gen : \n self . payload_gen . callback = callback \n def do_prepare ( self ) -> bool : \n if self . prepared : \n return True \n self . context_vars = get_context_vars_manager ( self . waf_func , self . options ) \n self . outer_pattern , self . will_print = get_outer_pattern ( self . waf_func ) \n if not self . outer_pattern : \n return False \n if self . will_print : \n logger . info ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , self . outer_pattern ) ) \n else : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , self . outer_pattern ) , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ) \n self . payload_gen = payload_gen . PayloadGenerator ( \n self . waf_func ,", "output": "self . context_vars . get_context ( ) , \n self . callback , \n options = self . options , \n waf_expr_func = self . waf_expr_func , \n ) \n self . prepared = True \n self . callback ( \n CALLBACK_PREPARE_FULLPAYLOADGEN , \n { \n \"<STR_LIT>\" : self . context_vars . get_context ( ) , \n \"<STR_LIT>\" : self . outer_pattern , \n \"<STR_LIT>\" : self . will_print , \n } , \n ) \n return True \n def try_add_context_var ( self , value : str , clean_cache = True ) -> Literal [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] : \n if not self . prepared and not self . do_prepare ( ) : \n return \"<STR_LIT>\" \n assert self . payload_gen and self . context_vars , \"<STR_LIT>\" \n pattern = None \n for fill_pattern , test_pattern in SET_STMT_PATTERNS : \n if self . waf_func ( test_pattern ) : \n pattern = fill_pattern \n break \n if pattern is None : \n return \"<STR_LIT>\" \n value_type = { str : STRING , int : INTEGER } [ type ( value ) ] \n ret = self . payload_gen . generate_detailed ( value_type , value ) \n if ret is None : \n return \"<STR_LIT>\" \n expression , used_context , _ = ret \n if len ( expression ) - len ( repr ( value ) ) < <NUM_LIT> : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , expression ) , \n ) \n return \"<STR_LIT>\" \n var_name = self . context_vars . generate_related_variable_name ( value ) \n if not var_name : \n var_name = self . context_vars . generate_random_variable_name ( ) \n if not var_name : \n return \"<STR_LIT>\" \n payload = pattern . replace ( \"<STR_LIT>\" , var_name ) . replace ( \"<STR_LIT>\" , expression ) \n success = self . add_context_variable ( \n payload , { var_name : value } , check_waf = True , depends_on = used_context \n ) \n if not success : \n return \"<STR_LIT>\" \n if clean_cache : \n self . payload_gen . cache_by_repr . clear ( ) \n else : \n self . payload_gen . delete_from_cache ( STRING , value ) \n logger . info ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( value ) ) , \n colored ( \"<STR_LIT>\" , payload ) , \n ) \n return \"<STR_LIT>\" \n def prepare_extra_context_vars ( self , append_targets : List [ str ] ) : \n targets = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] + append_targets \n if not self . prepared and not self . do_prepare ( ) : \n return \n if not any ( \n self . waf_func ( test_pattern ) for _ , test_pattern in SET_STMT_PATTERNS \n ) : \n logger . warning ( \"<STR_LIT>\" ) \n return \n assert self . payload_gen is not None , \"<STR_LIT>\" \n logger . info ( \n \"<STR_LIT>\" , \n ) \n for target in targets : \n if target in self . added_extra_context_vars : \n continue \n result = self . try_add_context_var ( target , clean_cache = False ) \n if result == \"<STR_LIT>\" : \n logger . warning ( \"<STR_LIT>\" , colored ( \"<STR_LIT>\" , repr ( target ) ) ) \n continue \n if result == \"<STR_LIT>\" : \n self . added_extra_context_vars . add ( target ) \n def add_context_variable ( \n self , \n payload : str , \n context_vars : Dict [ str , Any ] , \n check_waf : bool = True , \n depends_on : Union [ Dict [ str , Any ] , None ] = None , \n ) -> bool : \n if not self . prepared : \n raise RuntimeError ( \"<STR_LIT>\" ) \n assert self . payload_gen is not None and self . context_vars is not None \n success = self . context_vars . add_payload ( \n payload = payload , \n variables = context_vars , \n depends_on = depends_on , \n check_waf = check_waf , \n ) \n if not success : \n return False \n self . payload_gen . context = self . context_vars . get_context ( ) \n return True \n def generate_with_tree ( \n self , gen_type , * args \n ) -> Union [ Tuple [ str , bool , payload_gen . TargetAndSubTargets ] , None ] : \n if not self . prepared and not self . do_prepare ( ) : \n return None \n assert self . payload_gen is not None and self . context_vars is not None \n assert isinstance ( self . outer_pattern , str ) and self . will_print is not None \n if self . options . detect_mode != DetectMode . FAST : \n extra_strings = [ ] \n if gen_type == OS_POPEN_READ : \n extra_strings = [ args [ <NUM_LIT> ] ] \n elif gen_type == EVAL and args [ <NUM_LIT> ] [ <NUM_LIT> ] == STRING : \n extra_strings = [ args [ <NUM_LIT> ] [ <NUM_LIT> ] ] \n self . prepare_extra_context_vars ( extra_strings ) \n logger . info ( \"<STR_LIT>\" ) \n ret = self . payload_gen . generate_detailed ( gen_type , * args ) \n if ret is None : \n logger . warning ( \"<STR_LIT>\" ) \n return None \n inner_payload , used_context , tree = ret \n context_payload = self . context_vars . get_payload ( used_context ) \n payload = context_payload + self . outer_pattern . replace ( \"<STR_LIT>\" , inner_payload ) \n self . callback ( \n CALLBACK_GENERATE_FULLPAYLOAD , \n { \n \"<STR_LIT>\" : gen_type , \n \"<STR_LIT>\" : args , \n \"<STR_LIT>\" : payload , \n \"<STR_LIT>\" : self . will_print , \n } , \n ) \n if not self . will_print : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , self . outer_pattern ) , \n colored ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ) \n return ( payload , self . will_print , tree ) \n def generate ( self , gen_type , * args ) -> Tuple [ Union [ str , None ] , Union [ bool , None ] ] : \n result = self . generate_with_tree ( gen_type , * args ) \n if result : \n return result [ : <NUM_LIT> ] \n return None , None"}, {"input": "from unittest . mock import Mock \n import pytest \n from starlette . testclient import TestClient \n from simple_starlette_app import Database , app , lifespan \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _client ( ) : \n with TestClient ( app ) as client : \n yield client", "output": "def test_db_goes_boom ( client ) : \n db = Mock ( spec_set = Database ) \n db . get_user . side_effect = Exception ( \"<STR_LIT>\" ) \n lifespan . registry . register_value ( Database , db ) \n resp = client . get ( \"<STR_LIT>\" ) \n assert { \"<STR_LIT>\" : \"<STR_LIT>\" } == resp . json ( )"}, {"input": "from time import time \n import numpy as np \n import os \n from isaacgym . torch_utils import * \n from isaacgym import gymtorch , gymapi , gymutil \n import torch \n from typing import Tuple , Dict \n from legged_gym . envs import LeggedRobot \n from legged_gym import LEGGED_GYM_ROOT_DIR \n from . mixed_terrains . anymal_c_rough_config import AnymalCRoughCfg \n class Anymal ( LeggedRobot ) : \n cfg : AnymalCRoughCfg \n def __init__ ( self , cfg , sim_params , physics_engine , sim_device , headless ) : \n super ( ) . __init__ ( cfg , sim_params , physics_engine , sim_device , headless ) \n if self . cfg . control . use_actuator_network : \n actuator_network_path = self . cfg . control . actuator_net_file . format ( LEGGED_GYM_ROOT_DIR = LEGGED_GYM_ROOT_DIR ) \n self . actuator_network = torch . jit . load ( actuator_network_path ) . to ( self . device ) \n def reset_idx ( self , env_ids ) : \n super ( ) . reset_idx ( env_ids ) \n self . sea_hidden_state_per_env [ : , env_ids ] = <NUM_LIT> \n self . sea_cell_state_per_env [ : , env_ids ] = <NUM_LIT> \n def _init_buffers ( self ) : \n super ( ) . _init_buffers ( ) \n self . sea_input = torch . zeros ( self . num_envs * self . num_actions , <NUM_LIT> , <NUM_LIT> , device = self . device , requires_grad = False ) \n self . sea_hidden_state = torch . zeros ( <NUM_LIT> , self . num_envs * self . num_actions , <NUM_LIT> , device = self . device , requires_grad = False ) \n self . sea_cell_state = torch . zeros ( <NUM_LIT> , self . num_envs * self . num_actions , <NUM_LIT> , device = self . device , requires_grad = False ) \n self . sea_hidden_state_per_env = self . sea_hidden_state . view ( <NUM_LIT> , self . num_envs , self . num_actions , <NUM_LIT> )", "output": "self . sea_cell_state_per_env = self . sea_cell_state . view ( <NUM_LIT> , self . num_envs , self . num_actions , <NUM_LIT> ) \n def _compute_torques ( self , actions ) : \n if self . cfg . control . use_actuator_network : \n with torch . inference_mode ( ) : \n self . sea_input [ : , <NUM_LIT> , <NUM_LIT> ] = ( actions * self . cfg . control . action_scale + self . default_dof_pos - self . dof_pos ) . flatten ( ) \n self . sea_input [ : , <NUM_LIT> , <NUM_LIT> ] = self . dof_vel . flatten ( ) \n torques , ( self . sea_hidden_state [ : ] , self . sea_cell_state [ : ] ) = self . actuator_network ( self . sea_input , ( self . sea_hidden_state , self . sea_cell_state ) ) \n return torques \n else : \n return super ( ) . _compute_torques ( actions )"}, {"input": "import sys \n import logging \n from instld . module . context_manager import pip_context \n from instld . module . runner import run_python \n from instld . module . empty_logger import EmptyLogger \n from instld . common_utils . convert_options import convert_options \n class ProxyModule ( sys . modules [ __name__ ] . __class__ ) : \n def __call__ ( self , * packages_names , logger = logging , runner = run_python , catch_output = False , where = None , ** options ) : \n if logger is None :", "output": "logger = EmptyLogger ( ) \n options = convert_options ( options ) \n return pip_context ( packages_names , options , logger , runner , catch_output , where )"}, {"input": "import os \n import sys \n import time \n import torch \n import logging \n import numpy as np \n import soundfile as sf \n import librosa \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from rvc . infer . pipeline import VC \n from scipy . io import wavfile \n import noisereduce as nr \n from rvc . lib . utils import load_audio \n from rvc . lib . tools . split_audio import process_audio , merge_audio \n from fairseq import checkpoint_utils \n from rvc . lib . infer_pack . models import ( \n SynthesizerTrnMs256NSFsid , \n SynthesizerTrnMs256NSFsid_nono , \n SynthesizerTrnMs768NSFsid , \n SynthesizerTrnMs768NSFsid_nono , \n ) \n from rvc . configs . config import Config \n logging . getLogger ( \"<STR_LIT>\" ) . setLevel ( logging . WARNING ) \n logging . getLogger ( \"<STR_LIT>\" ) . setLevel ( logging . WARNING ) \n logging . getLogger ( \"<STR_LIT>\" ) . setLevel ( logging . WARNING ) \n config = Config ( ) \n hubert_model = None \n tgt_sr = None \n net_g = None \n vc = None \n cpt = None \n version = None \n n_spk = None \n def load_hubert ( ) : \n global hubert_model \n models , _ , _ = checkpoint_utils . load_model_ensemble_and_task ( \n [ \"<STR_LIT>\" ] , \n suffix = \"<STR_LIT>\" , \n ) \n hubert_model = models [ <NUM_LIT> ] \n hubert_model = hubert_model . to ( config . device ) \n if config . is_half : \n hubert_model = hubert_model . half ( ) \n else : \n hubert_model = hubert_model . float ( ) \n hubert_model . eval ( ) \n def remove_audio_noise ( input_audio_path , reduction_strength = <NUM_LIT> ) : \n try : \n rate , data = wavfile . read ( input_audio_path ) \n reduced_noise = nr . reduce_noise ( \n y = data , \n sr = rate , \n prop_decrease = reduction_strength , \n ) \n return reduced_noise \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n return None \n def convert_audio_format ( input_path , output_path , output_format ) : \n try : \n if output_format != \"<STR_LIT>\" : \n print ( f\"<STR_LIT>\" ) \n audio , sample_rate = librosa . load ( input_path , sr = None ) \n common_sample_rates = [ \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n <NUM_LIT> , \n ] \n target_sr = min ( common_sample_rates , key = lambda x : abs ( x - sample_rate ) ) \n audio = librosa . resample ( audio , orig_sr = sample_rate , target_sr = target_sr ) \n sf . write ( output_path , audio , target_sr , format = output_format . lower ( ) ) \n return output_path \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n def vc_single ( \n sid = <NUM_LIT> , \n input_audio_path = None , \n f0_up_key = None , \n f0_file = None , \n f0_method = None , \n file_index = None , \n index_rate = None , \n resample_sr = <NUM_LIT> , \n rms_mix_rate = None , \n protect = None , \n hop_length = None , \n output_path = None , \n split_audio = False , \n f0autotune = False , \n filter_radius = None , \n ) : \n global tgt_sr , net_g , vc , hubert_model , version \n f0_up_key = int ( f0_up_key ) \n try : \n audio = load_audio ( input_audio_path , <NUM_LIT> ) \n audio_max = np . abs ( audio ) . max ( ) / <NUM_LIT> \n if audio_max > <NUM_LIT> : \n audio /= audio_max \n if not hubert_model : \n load_hubert ( ) \n if_f0 = cpt . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n file_index = ( \n file_index . strip ( \"<STR_LIT>\" ) \n . strip ( '<STR_LIT>' ) \n . strip ( \"<STR_LIT>\" ) \n . strip ( '<STR_LIT>' ) \n . strip ( \"<STR_LIT>\" ) \n . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n ) \n if tgt_sr != resample_sr >= <NUM_LIT> : \n tgt_sr = resample_sr \n if split_audio == \"<STR_LIT>\" : \n result , new_dir_path = process_audio ( input_audio_path ) \n if result == \"<STR_LIT>\" : \n return \"<STR_LIT>\" , None \n dir_path = ( \n new_dir_path . strip ( \"<STR_LIT>\" ) . strip ( '<STR_LIT>' ) . strip ( \"<STR_LIT>\" ) . strip ( '<STR_LIT>' ) . strip ( \"<STR_LIT>\" ) \n ) \n if dir_path != \"<STR_LIT>\" : \n paths = [ \n os . path . join ( root , name ) \n for root , _ , files in os . walk ( dir_path , topdown = False ) \n for name in files \n if name . endswith ( \"<STR_LIT>\" ) and root == dir_path \n ] \n try : \n for path in paths : \n vc_single ( \n sid , \n path , \n f0_up_key , \n None , \n f0_method , \n file_index , \n index_rate , \n resample_sr , \n rms_mix_rate , \n protect , \n hop_length , \n path , \n False , \n f0autotune , \n ) \n except Exception as error : \n print ( error ) \n return f\"<STR_LIT>\" \n print ( \"<STR_LIT>\" ) \n merge_timestamps_file = os . path . join ( \n os . path . dirname ( new_dir_path ) , \n f\"<STR_LIT>\" , \n ) \n tgt_sr , audio_opt = merge_audio ( merge_timestamps_file ) \n os . remove ( merge_timestamps_file ) \n else : \n audio_opt = vc . pipeline ( \n hubert_model , \n net_g , \n sid , \n audio , \n input_audio_path , \n f0_up_key , \n f0_method , \n file_index , \n index_rate , \n if_f0 , \n filter_radius , \n tgt_sr , \n resample_sr , \n rms_mix_rate , \n version , \n protect , \n hop_length , \n f0autotune , \n f0_file = f0_file , \n ) \n if output_path is not None : \n sf . write ( output_path , audio_opt , tgt_sr , format = \"<STR_LIT>\" ) \n return ( tgt_sr , audio_opt ) \n except Exception as error : \n print ( error ) \n def get_vc ( weight_root , sid ) : \n global n_spk , tgt_sr , net_g , vc , cpt , version \n if sid == \"<STR_LIT>\" or sid == [ ] : \n global hubert_model \n if hubert_model is not None : \n print ( \"<STR_LIT>\" ) \n del net_g , n_spk , vc , hubert_model , tgt_sr \n hubert_model = net_g = n_spk = vc = hubert_model = tgt_sr = None \n if torch . cuda . is_available ( ) : \n torch . cuda . empty_cache ( ) \n if_f0 = cpt . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n version = cpt . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if version == \"<STR_LIT>\" : \n if if_f0 == <NUM_LIT> : \n net_g = SynthesizerTrnMs256NSFsid ( \n * cpt [ \"<STR_LIT>\" ] , is_half = config . is_half \n ) \n else : \n net_g = SynthesizerTrnMs256NSFsid_nono ( * cpt [ \"<STR_LIT>\" ] ) \n elif version == \"<STR_LIT>\" : \n if if_f0 == <NUM_LIT> :", "output": "net_g = SynthesizerTrnMs768NSFsid ( \n * cpt [ \"<STR_LIT>\" ] , is_half = config . is_half \n ) \n else : \n net_g = SynthesizerTrnMs768NSFsid_nono ( * cpt [ \"<STR_LIT>\" ] ) \n del net_g , cpt \n if torch . cuda . is_available ( ) : \n torch . cuda . empty_cache ( ) \n cpt = None \n person = weight_root \n cpt = torch . load ( person , map_location = \"<STR_LIT>\" ) \n tgt_sr = cpt [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] \n cpt [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] = cpt [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] . shape [ <NUM_LIT> ] \n if_f0 = cpt . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n version = cpt . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if version == \"<STR_LIT>\" : \n if if_f0 == <NUM_LIT> : \n net_g = SynthesizerTrnMs256NSFsid ( * cpt [ \"<STR_LIT>\" ] , is_half = config . is_half ) \n else : \n net_g = SynthesizerTrnMs256NSFsid_nono ( * cpt [ \"<STR_LIT>\" ] ) \n elif version == \"<STR_LIT>\" : \n if if_f0 == <NUM_LIT> : \n net_g = SynthesizerTrnMs768NSFsid ( * cpt [ \"<STR_LIT>\" ] , is_half = config . is_half ) \n else : \n net_g = SynthesizerTrnMs768NSFsid_nono ( * cpt [ \"<STR_LIT>\" ] ) \n del net_g . enc_q \n print ( net_g . load_state_dict ( cpt [ \"<STR_LIT>\" ] , strict = False ) ) \n net_g . eval ( ) . to ( config . device ) \n if config . is_half : \n net_g = net_g . half ( ) \n else : \n net_g = net_g . float ( ) \n vc = VC ( tgt_sr , config ) \n n_spk = cpt [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] \n def infer_pipeline ( \n f0up_key , \n filter_radius , \n index_rate , \n rms_mix_rate , \n protect , \n hop_length , \n f0method , \n audio_input_path , \n audio_output_path , \n model_path , \n index_path , \n split_audio , \n f0autotune , \n clean_audio , \n clean_strength , \n export_format , \n ) : \n global tgt_sr , net_g , vc , cpt \n get_vc ( model_path , <NUM_LIT> ) \n try : \n start_time = time . time ( ) \n vc_single ( \n sid = <NUM_LIT> , \n input_audio_path = audio_input_path , \n f0_up_key = f0up_key , \n f0_file = None , \n f0_method = f0method , \n file_index = index_path , \n index_rate = index_rate , \n rms_mix_rate = rms_mix_rate , \n protect = protect , \n hop_length = hop_length , \n output_path = audio_output_path , \n split_audio = split_audio , \n f0autotune = f0autotune , \n filter_radius = filter_radius , \n ) \n if clean_audio == \"<STR_LIT>\" : \n cleaned_audio = remove_audio_noise ( audio_output_path , clean_strength ) \n if cleaned_audio is not None : \n sf . write ( audio_output_path , cleaned_audio , tgt_sr , format = \"<STR_LIT>\" ) \n output_path_format = audio_output_path . replace ( \n \"<STR_LIT>\" , f\"<STR_LIT>\" \n ) \n audio_output_path = convert_audio_format ( \n audio_output_path , output_path_format , export_format \n ) \n end_time = time . time ( ) \n elapsed_time = end_time - start_time \n print ( \n f\"<STR_LIT>\" \n ) \n except Exception as error : \n print ( f\"<STR_LIT>\" )"}, {"input": "from . bard_bot import BardBot \n from config import model_conf_val \n from model . model import Model \n from common import log \n user_session = dict ( ) \n class BardModel ( Model ) : \n bot : BardBot = None \n def __init__ ( self ) : \n try : \n self . cookies = model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n self . bot = BardBot ( self . cookies ) \n except Exception as e : \n log . warn ( e ) \n def reply ( self , query : str , context = None ) -> dict [ str , str ] : \n if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n bot = user_session . get ( context [ '<STR_LIT>' ] , None ) \n if bot is None : \n bot = self . bot \n user_session [ context [ '<STR_LIT>' ] ] = bot \n log . info ( f\"<STR_LIT>\" ) \n answer = bot . ask ( query ) \n reply = answer [ '<STR_LIT>' ] \n if answer [ '<STR_LIT>' ] : \n reference = [ ( { '<STR_LIT>' : item [ <NUM_LIT> ] , '<STR_LIT>' : item [ <NUM_LIT> ] [ <NUM_LIT> ] if item [ <NUM_LIT> ] [ <NUM_LIT> ] else item [ <NUM_LIT> ] [ <NUM_LIT> ] } ) for item in answer [ '<STR_LIT>' ] [ <NUM_LIT> ] ] \n reference . sort ( key = lambda x : x [ '<STR_LIT>' ] , reverse = True ) \n reply = self . insert_reference ( reply , reference ) \n log . warn ( f\"<STR_LIT>\" ) \n return reply \n async def reply_text_stream ( self , query : str , context = None ) -> dict : \n reply = self . reply ( query , context ) \n yield True , reply", "output": "def insert_reference ( self , reply : str , reference : list ) -> str : \n refer = '<STR_LIT>' \n length = len ( reference ) \n for i , item in enumerate ( reference ) : \n index = item [ \"<STR_LIT>\" ] - <NUM_LIT> \n reply = reply [ : index ] + f'<STR_LIT>' + reply [ index : ] \n refer += f'<STR_LIT>' \n refer += '<STR_LIT>' \n return reply + refer"}, {"input": "HTTP_PARAMS_LIST = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ]"}, {"input": "import io \n import os \n import sys \n import traceback \n from typing import List , Dict , Any , Optional \n from dataclasses import dataclass \n from subprocess import CalledProcessError \n from contextlib import redirect_stdout , redirect_stderr \n import pytest \n from instld . cli . main import main \n @ dataclass \n class MainRunResult : \n stdout : bytes \n stderr : bytes \n before_stderr : bytes \n returncode : int \n command : List [ str ] \n def check_returncode ( self ) : \n if self . returncode != <NUM_LIT> : \n raise CalledProcessError ( self . returncode , self . command ) \n @ pytest . fixture \n def main_runner ( ) : \n def runner_function ( arguments : List [ str ] , env : Optional [ Dict [ str , str ] ] = None , universal_newlines : Optional [ bool ] = None , ** kwargs : Any ) : \n old_excepthook = sys . excepthook \n old_argv = sys . argv \n old_environ = os . environ \n old_exit = sys . exit \n old_env = os . environ \n if env is not None : \n os . environ = env \n sys . argv = arguments \n class LocalError ( Exception ) : pass \n def exit_handler ( number ) : raise LocalError \n sys . exit = exit_handler \n stdout_buffer = io . StringIO ( ) \n stderr_buffer = io . StringIO ( ) \n returncode = <NUM_LIT> \n with redirect_stdout ( stdout_buffer ) as stdout , redirect_stderr ( stderr_buffer ) as stderr : \n try : \n main ( ) \n stdout = stdout_buffer . getvalue ( ) \n stderr = stderr_buffer . getvalue ( ) \n before_stderr = str . encode ( stderr ) \n except LocalError : \n returncode = <NUM_LIT> \n stdout = stdout_buffer . getvalue ( ) \n stderr = stderr_buffer . getvalue ( ) \n before_stderr = str . encode ( stderr ) \n except Exception as e : \n returncode = <NUM_LIT> \n sys . excepthook ( type ( e ) , e , e . __traceback__ ) \n stdout = stdout_buffer . getvalue ( ) \n stderr = stderr_buffer . getvalue ( )", "output": "before_stderr = str . encode ( stderr ) \n if sys . platform . lower ( ) in ( '<STR_LIT>' , ) : \n stdout = stdout . replace ( '<STR_LIT>' , os . linesep ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n stderr = stderr . replace ( '<STR_LIT>' , os . linesep ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n finally : \n if not ( universal_newlines is not None and universal_newlines ) : \n stdout = str . encode ( stdout ) \n stderr = str . encode ( stderr ) \n result = MainRunResult ( command = arguments , stdout = stdout , stderr = stderr , before_stderr = before_stderr , returncode = returncode ) \n sys . excepthook = old_excepthook \n sys . argv = old_argv \n os . environ = old_environ \n sys . exit = old_exit \n os . environ = old_env \n return result \n return runner_function"}, {"input": "import os \n import yaml \n DENSE_CAPS_DATASET_PACKAGE_DIR = os . path . dirname ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) \n DENSE_CAPS_DIR = os . path . dirname ( DENSE_CAPS_DATASET_PACKAGE_DIR ) \n CONFIG_FILE = os . path . join ( DENSE_CAPS_DATASET_PACKAGE_DIR , '<STR_LIT>' ) \n def init_config ( ) : \n config = { '<STR_LIT>' : True } \n print ( \"<STR_LIT>\" ) \n default_data_dir = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) \n data_dir = input ( f\"<STR_LIT>\" ) \n if data_dir . strip ( ) == \"<STR_LIT>\" : \n config [ '<STR_LIT>' ] = default_data_dir \n else : \n config [ '<STR_LIT>' ] = data_dir . strip ( ) \n with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : \n yaml . dump ( config , config_file ) \n return config \n def get_config ( ) : \n if not os . path . exists ( CONFIG_FILE ) : \n return init_config ( ) \n with open ( CONFIG_FILE , '<STR_LIT>' ) as config_file : \n config = yaml . safe_load ( config_file ) \n return config \n DCI_CONFIG = get_config ( )", "output": "DATASET_BASE = os . path . join ( DCI_CONFIG [ '<STR_LIT>' ] , '<STR_LIT>' ) \n MODEL_BASE = os . path . join ( DENSE_CAPS_DIR , '<STR_LIT>' ) \n DATASET_PHOTO_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' ) \n DATASET_ANNOTATIONS_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' ) \n DATASET_COMPLETE_PATH = os . path . join ( DATASET_BASE , '<STR_LIT>' )"}, {"input": "import gradio as gr \n from assets . version_checker import compare_version \n from assets . i18n . i18n import I18nAuto \n i18n = I18nAuto ( ) \n def version_tab ( ) : \n with gr . Row ( ) : \n with gr . Column ( ) : \n version_check = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n interactive = False , \n ) \n version_button = gr . Button ( i18n ( \"<STR_LIT>\" ) ) \n version_button . click ( \n fn = compare_version , \n inputs = [ ] , \n outputs = [ version_check ] ,", "output": ")"}, {"input": "import asyncio \n import os , time , re , io \n import threading \n import json \n import random \n import traceback \n import logging \n try : \n from httplib import BadStatusLine \n except ImportError : \n from http . client import BadStatusLine \n import requests \n from pyqrcode import QRCode \n from . . import config , utils \n from . . returnvalues import ReturnValue \n from . . storage . templates import wrap_user_dict \n from . contact import update_local_chatrooms , update_local_friends \n from . messages import produce_msg \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_login ( core ) : \n core . login = login \n core . get_QRuuid = get_QRuuid \n core . get_QR = get_QR \n core . check_login = check_login \n core . web_init = web_init \n core . show_mobile_login = show_mobile_login \n core . start_receiving = start_receiving \n core . get_msg = get_msg \n core . logout = logout \n async def login ( self , enableCmdQR = False , picDir = None , qrCallback = None , EventScanPayload = None , ScanStatus = None , event_stream = None , \n loginCallback = None , exitCallback = None ) : \n if self . alive or self . isLogging : \n logger . warning ( '<STR_LIT>' ) \n return \n self . isLogging = True \n while self . isLogging : \n uuid = await push_login ( self ) \n if uuid : \n payload = EventScanPayload ( \n status = ScanStatus . Waiting , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n else : \n logger . info ( '<STR_LIT>' ) \n self . get_QRuuid ( ) \n payload = EventScanPayload ( \n status = ScanStatus . Waiting , \n qrcode = f\"<STR_LIT>\" \n ) \n print ( f\"<STR_LIT>\" ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n isLoggedIn = False \n while not isLoggedIn : \n status = await self . check_login ( ) \n if status == '<STR_LIT>' : \n isLoggedIn = True \n payload = EventScanPayload ( \n status = ScanStatus . Scanned , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n elif status == '<STR_LIT>' : \n if isLoggedIn is not None : \n logger . info ( '<STR_LIT>' ) \n isLoggedIn = None \n payload = EventScanPayload ( \n status = ScanStatus . Waiting , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n elif status != '<STR_LIT>' : \n payload = EventScanPayload ( \n status = ScanStatus . Cancel , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n break \n if isLoggedIn : \n payload = EventScanPayload ( \n status = ScanStatus . Confirmed , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n break \n elif self . isLogging : \n logger . info ( '<STR_LIT>' ) \n payload = EventScanPayload ( \n status = ScanStatus . Timeout , \n qrcode = f\"<STR_LIT>\" \n ) \n event_stream . emit ( '<STR_LIT>' , payload ) \n await asyncio . sleep ( <NUM_LIT> ) \n else : \n return \n logger . info ( '<STR_LIT>' ) \n await self . web_init ( ) \n await self . show_mobile_login ( ) \n self . get_contact ( True ) \n if hasattr ( loginCallback , '<STR_LIT>' ) : \n r = await loginCallback ( self . storageClass . userName ) \n else : \n utils . clear_screen ( ) \n if os . path . exists ( picDir or config . DEFAULT_QR ) : \n os . remove ( picDir or config . DEFAULT_QR ) \n logger . info ( '<STR_LIT>' % self . storageClass . nickName ) \n await self . start_receiving ( exitCallback ) \n self . isLogging = False \n async def push_login ( core ) : \n cookiesDict = core . s . cookies . get_dict ( ) \n if '<STR_LIT>' in cookiesDict : \n url = '<STR_LIT>' % ( \n config . BASE_URL , cookiesDict [ '<STR_LIT>' ] ) \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = core . s . get ( url , headers = headers ) . json ( ) \n if '<STR_LIT>' in r and r . get ( '<STR_LIT>' ) in ( <NUM_LIT> , '<STR_LIT>' ) : \n core . uuid = r [ '<STR_LIT>' ] \n return r [ '<STR_LIT>' ] \n return False \n def get_QRuuid ( self ) : \n url = '<STR_LIT>' % config . BASE_URL \n params = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' ,", "output": "'<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . get ( url , params = params , headers = headers ) \n regx = r'<STR_LIT>' \n data = re . search ( regx , r . text ) \n if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : \n self . uuid = data . group ( <NUM_LIT> ) \n return self . uuid \n async def get_QR ( self , uuid = None , enableCmdQR = False , picDir = None , qrCallback = None ) : \n uuid = uuid or self . uuid \n picDir = picDir or config . DEFAULT_QR \n qrStorage = io . BytesIO ( ) \n qrCode = QRCode ( '<STR_LIT>' + uuid ) \n qrCode . png ( qrStorage , scale = <NUM_LIT> ) \n if hasattr ( qrCallback , '<STR_LIT>' ) : \n await qrCallback ( uuid = uuid , status = '<STR_LIT>' , qrcode = qrStorage . getvalue ( ) ) \n else : \n with open ( picDir , '<STR_LIT>' ) as f : \n f . write ( qrStorage . getvalue ( ) ) \n if enableCmdQR : \n utils . print_cmd_qr ( qrCode . text ( <NUM_LIT> ) , enableCmdQR = enableCmdQR ) \n else : \n utils . print_qr ( picDir ) \n return qrStorage \n async def check_login ( self , uuid = None ) : \n uuid = uuid or self . uuid \n url = '<STR_LIT>' % config . BASE_URL \n localTime = int ( time . time ( ) ) \n params = '<STR_LIT>' % ( \n uuid , int ( - localTime / <NUM_LIT> ) , localTime ) \n headers = { '<STR_LIT>' : config . USER_AGENT } \n r = self . s . get ( url , params = params , headers = headers ) \n regx = r'<STR_LIT>' \n data = re . search ( regx , r . text ) \n if data and data . group ( <NUM_LIT> ) == '<STR_LIT>' : \n if await process_login_info ( self , r . text ) : \n return '<STR_LIT>' \n else : \n return '<STR_LIT>' \n elif data : \n return data . group ( <NUM_LIT> ) \n else : \n return '<STR_LIT>' \n async def process_login_info ( core , loginContent ) : \n regx = r'<STR_LIT>' \n core . loginInfo [ '<STR_LIT>' ] = re . search ( regx , loginContent ) . group ( <NUM_LIT> ) \n headers = { '<STR_LIT>' : config . USER_AGENT , \n '<STR_LIT>' : config . UOS_PATCH_CLIENT_VERSION , \n '<STR_LIT>' : config . UOS_PATCH_EXTSPAM , \n '<STR_LIT>' : '<STR_LIT>' \n } \n r = core . s . get ( core . loginInfo [ '<STR_LIT>' ] , headers = headers , allow_redirects = False ) \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ : core . loginInfo [ '<STR_LIT>' ] . rfind ( '<STR_LIT>' ) ] \n for indexUrl , detailedUrl in ( \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) , \n ( \"<STR_LIT>\" , ( \"<STR_LIT>\" , \"<STR_LIT>\" ) ) ) : \n fileUrl , syncUrl = [ '<STR_LIT>' % url for url in detailedUrl ] \n if indexUrl in core . loginInfo [ '<STR_LIT>' ] : \n core . loginInfo [ '<STR_LIT>' ] , core . loginInfo [ '<STR_LIT>' ] = fileUrl , syncUrl \n break \n else : \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] \n core . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] \n core . loginInfo [ '<STR_LIT>' ] = int ( time . time ( ) * <NUM_LIT> ) \n core . loginInfo [ '<STR_LIT>' ] = { } \n cookies = core . s . cookies . get_dict ( ) \n skey = re . findall ( '<STR_LIT>' , r . text , re . S ) [ <NUM_LIT> ] \n pass_ticket = re . findall ( '<STR_LIT>' , r . text , re . S ) [ <NUM_LIT> ] \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = skey \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ \"<STR_LIT>\" ] \n core . loginInfo [ '<STR_LIT>' ] = core . loginInfo [ '<STR_LIT>' ] [ '<STR_LIT>' ] = cookies [ \"<STR_LIT>\" ] \n core . loginInfo [ '<STR_LIT>' ] = pass_ticket \n if not all ( [ key in core . loginInfo for key in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) ] ) : \n logger . error ( '<STR_LIT>' % r . text ) \n core . isLogging = False \n return False \n return True \n async def web_init ( self ) : \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n params = { \n '<STR_LIT>' : int ( - time . time ( ) / <NUM_LIT> ) , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n data = { '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , } \n r = self . s . post ( url , params = params , data = json . dumps ( data ) , headers = headers ) \n dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n utils . emoji_formatter ( dic [ '<STR_LIT>' ] , '<STR_LIT>' ) \n self . loginInfo [ '<STR_LIT>' ] = int ( dic [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] = wrap_user_dict ( utils . struct_friend_info ( dic [ '<STR_LIT>' ] ) ) \n self . memberList . append ( self . loginInfo [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) \n for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) \n self . storageClass . userName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n self . storageClass . nickName = dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n contactList = dic . get ( '<STR_LIT>' , [ ] ) \n chatroomList , otherList = [ ] , [ ] \n for m in contactList : \n if m [ '<STR_LIT>' ] != <NUM_LIT> : \n otherList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n m [ '<STR_LIT>' ] = [ ] \n chatroomList . append ( m ) \n elif '<STR_LIT>' in m [ '<STR_LIT>' ] : \n otherList . append ( m ) \n if chatroomList : \n update_local_chatrooms ( self , chatroomList ) \n if otherList : \n update_local_friends ( self , otherList ) \n return dic \n async def show_mobile_login ( self ) : \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : self . storageClass . userName , \n '<STR_LIT>' : self . storageClass . userName , \n '<STR_LIT>' : int ( time . time ( ) ) , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT , } \n r = self . s . post ( url , data = json . dumps ( data ) , headers = headers ) \n return ReturnValue ( rawResponse = r ) \n async def start_receiving ( self , exitCallback = None , getReceivingFnOnly = False ) : \n self . alive = True \n def maintain_loop ( ) : \n retryCount = <NUM_LIT> \n while self . alive : \n try : \n i = sync_check ( self ) \n if i is None : \n self . alive = False \n elif i == '<STR_LIT>' : \n pass \n else : \n msgList , contactList = self . get_msg ( ) \n if msgList : \n msgList = produce_msg ( self , msgList ) \n for msg in msgList : \n self . msgList . put ( msg ) \n if contactList : \n chatroomList , otherList = [ ] , [ ] \n for contact in contactList : \n if '<STR_LIT>' in contact [ '<STR_LIT>' ] : \n chatroomList . append ( contact ) \n else : \n otherList . append ( contact ) \n chatroomMsg = update_local_chatrooms ( self , chatroomList ) \n chatroomMsg [ '<STR_LIT>' ] = self . loginInfo [ '<STR_LIT>' ] \n self . msgList . put ( chatroomMsg ) \n update_local_friends ( self , otherList ) \n retryCount = <NUM_LIT> \n except requests . exceptions . ReadTimeout : \n pass \n except : \n retryCount += <NUM_LIT> \n logger . error ( traceback . format_exc ( ) ) \n if self . receivingRetryCount < retryCount : \n self . alive = False \n else : \n time . sleep ( <NUM_LIT> ) \n self . logout ( ) \n if hasattr ( exitCallback , '<STR_LIT>' ) : \n exitCallback ( self . storageClass . userName ) \n else : \n logger . info ( '<STR_LIT>' ) \n if getReceivingFnOnly : \n return maintain_loop \n else : \n maintainThread = threading . Thread ( target = maintain_loop ) \n maintainThread . setDaemon ( True ) \n maintainThread . start ( ) \n def sync_check ( self ) : \n url = '<STR_LIT>' % self . loginInfo . get ( '<STR_LIT>' , self . loginInfo [ '<STR_LIT>' ] ) \n params = { \n '<STR_LIT>' : int ( time . time ( ) * <NUM_LIT> ) , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n self . loginInfo [ '<STR_LIT>' ] += <NUM_LIT> \n try : \n r = self . s . get ( url , params = params , headers = headers , timeout = config . TIMEOUT ) \n except requests . exceptions . ConnectionError as e : \n try : \n if not isinstance ( e . args [ <NUM_LIT> ] . args [ <NUM_LIT> ] , BadStatusLine ) : \n raise \n return '<STR_LIT>' \n except : \n raise \n r . raise_for_status ( ) \n regx = r'<STR_LIT>' \n pm = re . search ( regx , r . text ) \n if pm is None or pm . group ( <NUM_LIT> ) != '<STR_LIT>' : \n logger . debug ( '<STR_LIT>' % r . text ) \n return None \n return pm . group ( <NUM_LIT> ) \n def get_msg ( self ) : \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' + repr ( random . random ( ) ) [ <NUM_LIT> : <NUM_LIT> ] \n url = '<STR_LIT>' % ( \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] , \n self . loginInfo [ '<STR_LIT>' ] , self . loginInfo [ '<STR_LIT>' ] ) \n data = { \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , \n '<STR_LIT>' : ~ int ( time . time ( ) ) , } \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : config . USER_AGENT } \n r = self . s . post ( url , data = json . dumps ( data ) , headers = headers , timeout = config . TIMEOUT ) \n dic = json . loads ( r . content . decode ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] != <NUM_LIT> : return None , None \n self . loginInfo [ '<STR_LIT>' ] = dic [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = '<STR_LIT>' . join ( [ '<STR_LIT>' % ( item [ '<STR_LIT>' ] , item [ '<STR_LIT>' ] ) \n for item in dic [ '<STR_LIT>' ] [ '<STR_LIT>' ] ] ) \n return dic [ '<STR_LIT>' ] , dic [ '<STR_LIT>' ] \n def logout ( self ) : \n if self . alive : \n url = '<STR_LIT>' % self . loginInfo [ '<STR_LIT>' ] \n params = { \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : <NUM_LIT> , \n '<STR_LIT>' : self . loginInfo [ '<STR_LIT>' ] , } \n headers = { '<STR_LIT>' : config . USER_AGENT } \n self . s . get ( url , params = params , headers = headers ) \n self . alive = False \n self . isLogging = False \n self . s . cookies . clear ( ) \n del self . chatroomList [ : ] \n del self . memberList [ : ] \n del self . mpList [ : ] \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , } } )"}, {"input": "import torch \n import os \n import sys \n from densely_captioned_images . repro . config import MODEL_PATH , ELEVATER_MODEL_CONFIG , ELEVATER_DATASET_CONFIG_ROOT , ELEVATER_DATASET_ROOT , EVAL_LOG_PATH \n from densely_captioned_images . repro . eval . ElevaterIC . vision_benchmark . commands . linear_probe import main as linear_probe \n from densely_captioned_images . repro . eval . ElevaterIC . vision_benchmark . commands . zeroshot import main as zeroshot \n DATASETS_LIST = [ \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' \n ] \n DATASETS_NAME_MAP = { \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : '<STR_LIT>' \n } \n SHOT_OPTIONS = [ \n <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> \n ] \n def run_elevater_on ( \n model , \n path_key , \n run_full = False , \n do_finetune = True , \n shot_options = None , \n dataset_option = None , \n ) : \n submodel_path = os . path . join ( MODEL_PATH , path_key , '<STR_LIT>' ) \n if not os . path . exists ( submodel_path ) : \n torch . save ( model . state_dict ( ) , submodel_path ) \n run_elevater ( \n submodel_path , \n path_key , \n run_full = run_full , \n do_finetune = do_finetune , \n shot_options = shot_options , \n dataset_option = dataset_option , \n ) \n def run_elevater ( \n model_path , \n path_key , \n run_full = False , \n do_finetune = True , \n shot_options = None , \n rerun_existing = False , \n dataset_option = None \n ) : \n if shot_options is None : \n shot_options = SHOT_OPTIONS \n use_datasets = DATASETS_LIST \n if dataset_option is not None : \n use_datasets = [ use_datasets [ dataset_option ] ] \n if run_full is False : \n shot_options = shot_options [ : <NUM_LIT> ] \n use_datasets = use_datasets [ : <NUM_LIT> ] \n def dataset_already_run_for_all_shots ( d ) : \n for shot in shot_options : \n if shot == <NUM_LIT> : \n subpath = \"<STR_LIT>\" \n elif shot == - <NUM_LIT> : \n subpath = '<STR_LIT>' \n else : \n subpath = f'<STR_LIT>' \n full_path = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' , path_key , '<STR_LIT>' , subpath ) \n if not os . path . exists ( full_path ) : \n return False \n all_tasks = '<STR_LIT>' . join ( os . listdir ( full_path ) ) \n if DATASETS_NAME_MAP [ d ] not in all_tasks : \n return False \n return True \n if not rerun_existing : \n use_datasets = [ d for d in use_datasets if not dataset_already_run_for_all_shots ( d ) ] \n print ( f\"<STR_LIT>\" ) \n os . makedirs ( f'<STR_LIT>' , exist_ok = True ) \n for dataset in use_datasets : \n for num_shots in shot_options : \n if num_shots != <NUM_LIT> : \n args_list = [ \n '<STR_LIT>' , ELEVATER_MODEL_CONFIG , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n ] \n saved_argv = sys . argv \n try : \n sys . argv = [ '<STR_LIT>' ] + args_list \n linear_probe ( ) \n finally : \n sys . argv = saved_argv \n else : \n args_list = [ \n '<STR_LIT>' , ELEVATER_MODEL_CONFIG , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , '<STR_LIT>' , \n '<STR_LIT>' , f'<STR_LIT>' , \n ] \n saved_argv = sys . argv \n try : \n sys . argv = [ '<STR_LIT>' ] + args_list \n zeroshot ( ) \n finally : \n sys . argv = saved_argv \n def run_elevator_from_lora_checkpoint ( lora_path_key , run_full = False , shot_options = None ) : \n from transformers import CLIPModel \n from peft import PeftModel \n if shot_option is not None and isinstance ( shot_option , int ) : \n shot_option = [ shot_option ] \n submodel_path = os . path . join ( MODEL_PATH , lora_path_key ) \n base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n loaded = PeftModel . from_pretrained ( base_clip_model , submodel_path ) \n loaded = loaded . merge_and_unload ( ) \n run_elevater_on ( loaded , lora_path_key , run_full = run_full , shot_options = shot_options )", "output": "if __name__ == '<STR_LIT>' : \n clip_path = os . path . join ( MODEL_PATH , '<STR_LIT>' ) \n if not os . path . exists ( clip_path ) : \n from transformers import CLIPModel \n base_clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n torch . save ( base_clip_model . state_dict ( ) , clip_path ) \n run_elevater ( clip_path , '<STR_LIT>' , run_full = True , shot_options = [ <NUM_LIT> , <NUM_LIT> ] )"}, {"input": "from infer_pack . modules . F0Predictor . F0Predictor import F0Predictor \n import pyworld \n import numpy as np \n class HarvestF0Predictor ( F0Predictor ) : \n def __init__ ( self , hop_length = <NUM_LIT> , f0_min = <NUM_LIT> , f0_max = <NUM_LIT> , sampling_rate = <NUM_LIT> ) : \n self . hop_length = hop_length \n self . f0_min = f0_min \n self . f0_max = f0_max \n self . sampling_rate = sampling_rate \n def interpolate_f0 ( self , f0 ) : \n data = np . reshape ( f0 , ( f0 . size , <NUM_LIT> ) ) \n vuv_vector = np . zeros ( ( data . size , <NUM_LIT> ) , dtype = np . float32 ) \n vuv_vector [ data > <NUM_LIT> ] = <NUM_LIT> \n vuv_vector [ data <= <NUM_LIT> ] = <NUM_LIT> \n ip_data = data \n frame_number = data . size \n last_value = <NUM_LIT> \n for i in range ( frame_number ) : \n if data [ i ] <= <NUM_LIT> :", "output": "j = i + <NUM_LIT> \n for j in range ( i + <NUM_LIT> , frame_number ) : \n if data [ j ] > <NUM_LIT> : \n break \n if j < frame_number - <NUM_LIT> : \n if last_value > <NUM_LIT> : \n step = ( data [ j ] - data [ i - <NUM_LIT> ] ) / float ( j - i ) \n for k in range ( i , j ) : \n ip_data [ k ] = data [ i - <NUM_LIT> ] + step * ( k - i + <NUM_LIT> ) \n else : \n for k in range ( i , j ) : \n ip_data [ k ] = data [ j ] \n else : \n for k in range ( i , frame_number ) : \n ip_data [ k ] = last_value \n else : \n ip_data [ i ] = data [ i ] \n last_value = data [ i ] \n return ip_data [ : , <NUM_LIT> ] , vuv_vector [ : , <NUM_LIT> ] \n def resize_f0 ( self , x , target_len ) : \n source = np . array ( x ) \n source [ source < <NUM_LIT> ] = np . nan \n target = np . interp ( \n np . arange ( <NUM_LIT> , len ( source ) * target_len , len ( source ) ) / target_len , \n np . arange ( <NUM_LIT> , len ( source ) ) , \n source , \n ) \n res = np . nan_to_num ( target ) \n return res \n def compute_f0 ( self , wav , p_len = None ) : \n if p_len is None : \n p_len = wav . shape [ <NUM_LIT> ] // self . hop_length \n f0 , t = pyworld . harvest ( \n wav . astype ( np . double ) , \n fs = self . sampling_rate , \n f0_ceil = self . f0_max , \n f0_floor = self . f0_min , \n frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , \n ) \n f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . fs ) \n return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) ) [ <NUM_LIT> ] \n def compute_f0_uv ( self , wav , p_len = None ) : \n if p_len is None : \n p_len = wav . shape [ <NUM_LIT> ] // self . hop_length \n f0 , t = pyworld . harvest ( \n wav . astype ( np . double ) , \n fs = self . sampling_rate , \n f0_floor = self . f0_min , \n f0_ceil = self . f0_max , \n frame_period = <NUM_LIT> * self . hop_length / self . sampling_rate , \n ) \n f0 = pyworld . stonemask ( wav . astype ( np . double ) , f0 , t , self . sampling_rate ) \n return self . interpolate_f0 ( self . resize_f0 ( f0 , p_len ) )"}, {"input": "from __future__ import annotations \n import sys \n from contextlib import asynccontextmanager \n from typing import AsyncGenerator \n from fastapi import Depends , FastAPI \n from fastapi . responses import JSONResponse \n import svcs \n if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : \n from typing_extensions import Annotated \n else : \n from typing import Annotated \n @ svcs . fastapi . lifespan \n async def lifespan ( \n app : FastAPI , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n yield { } \n reg : svcs . Registry = lifespan . registry \n @ svcs . fastapi . lifespan \n async def lifespan2 ( \n app : FastAPI , registry : svcs . Registry \n ) -> AsyncGenerator [ None , None ] : \n yield", "output": "@ svcs . fastapi . lifespan \n @ asynccontextmanager \n async def lifespan3 ( \n app : FastAPI , registry : svcs . Registry \n ) -> AsyncGenerator [ dict [ str , object ] , None ] : \n yield { } \n @ svcs . fastapi . lifespan \n @ asynccontextmanager \n async def lifespan4 ( \n app : FastAPI , registry : svcs . Registry \n ) -> AsyncGenerator [ None , None ] : \n yield \n app = FastAPI ( lifespan = lifespan ) \n @ app . get ( \"<STR_LIT>\" ) \n async def view ( \n services : Annotated [ svcs . Container , Depends ( svcs . fastapi . container ) ] \n ) -> JSONResponse : \n x : int = services . get ( int ) \n return JSONResponse ( { } , <NUM_LIT> ) \n @ app . get ( \"<STR_LIT>\" ) \n async def view2 ( services : svcs . fastapi . DepContainer ) -> JSONResponse : \n x : int = services . get ( int ) \n return JSONResponse ( { } , <NUM_LIT> )"}, {"input": "from collections import deque", "output": "alumnos = deque ( [ \"<STR_LIT>\" , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] ) \n print ( alumnos ) \n alumnos . append ( \"<STR_LIT>\" ) \n alumnos . append ( \"<STR_LIT>\" ) \n print ( alumnos ) \n alumnos . popleft ( ) \n print ( alumnos ) \n alumnos . popleft ( ) \n print ( alumnos ) \n print ( \"<STR_LIT>\" ) \n cola1 = [ ] \n cola1 . append ( \"<STR_LIT>\" ) \n cola1 . append ( \"<STR_LIT>\" ) \n cola1 . append ( \"<STR_LIT>\" ) \n print ( cola1 ) \n cola1 . pop ( <NUM_LIT> ) \n print ( cola1 ) \n cola1 . pop ( <NUM_LIT> ) \n print ( cola1 )"}, {"input": "import logging \n import sys \n from waitress import serve \n from mod import check_update \n from mod . args import GlobalArgs \n from mod . dev . debugger import debugger \n from api import * \n from api import __import__ \n args = GlobalArgs ( ) \n def run_server ( debug = False ) : \n if not debug : \n serve ( app , host = args . ip , port = args . port , threads = <NUM_LIT> , channel_timeout = <NUM_LIT> ) \n else : \n debugger . debug = True \n debugger . log ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n debugger . log ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) \n if args . auth : \n debugger . log ( \"<STR_LIT>\" , f\"<STR_LIT>\" ) \n app . run ( host = '<STR_LIT>' , port = args . port , debug = True ) \n if __name__ == '<STR_LIT>' : \n if sys . version_info < ( <NUM_LIT> , <NUM_LIT> ) : \n raise RuntimeError ( \n \"<STR_LIT>\" . format ( * sys . version_info [ : <NUM_LIT> ] ) \n ) \n logging . basicConfig ( level = logging . INFO , format = '<STR_LIT>' )", "output": "logger = logging . getLogger ( '<STR_LIT>' ) \n logger . info ( \"<STR_LIT>\" ) \n check_update . run ( version = args . version ) \n app . register_blueprint ( v1_bp ) \n run_server ( args . debug )"}, {"input": "tupla1 = ( <NUM_LIT> , <NUM_LIT> , \"<STR_LIT>\" , True ) \n lista1 = [ <NUM_LIT> , <NUM_LIT> , \"<STR_LIT>\" , True ] \n super_tupla = tupla1 , ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) , [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] \n print ( super_tupla ) \n lista1 [ <NUM_LIT> ] = '<STR_LIT>'", "output": "print ( lista1 ) \n super_tupla [ <NUM_LIT> ] [ <NUM_LIT> ] = '<STR_LIT>' \n print ( super_tupla ) \n super_lista = list ( super_tupla ) \n print ( super_lista ) \n vacia = ( ) \n singleton_uwu = \"<STR_LIT>\" \n print ( vacia ) \n print ( singleton_uwu )"}, {"input": "import os \n import glob \n import json \n import torch \n import argparse \n import numpy as np \n from scipy . io . wavfile import read \n def load_checkpoint ( checkpoint_path , model , optimizer = None , load_opt = <NUM_LIT> ) : \n assert os . path . isfile ( checkpoint_path ) \n checkpoint_dict = torch . load ( checkpoint_path , map_location = \"<STR_LIT>\" ) \n saved_state_dict = checkpoint_dict [ \"<STR_LIT>\" ] \n if hasattr ( model , \"<STR_LIT>\" ) : \n state_dict = model . module . state_dict ( ) \n else : \n state_dict = model . state_dict ( ) \n new_state_dict = { } \n for k , v in state_dict . items ( ) : \n try : \n new_state_dict [ k ] = saved_state_dict [ k ] \n if saved_state_dict [ k ] . shape != state_dict [ k ] . shape : \n print ( \n \"<STR_LIT>\" , \n k , \n state_dict [ k ] . shape , \n saved_state_dict [ k ] . shape , \n ) \n raise KeyError \n except : \n print ( \"<STR_LIT>\" , k ) \n new_state_dict [ k ] = v \n if hasattr ( model , \"<STR_LIT>\" ) : \n model . module . load_state_dict ( new_state_dict , strict = False ) \n else : \n model . load_state_dict ( new_state_dict , strict = False ) \n iteration = checkpoint_dict [ \"<STR_LIT>\" ] \n learning_rate = checkpoint_dict [ \"<STR_LIT>\" ] \n if optimizer is not None and load_opt == <NUM_LIT> : \n optimizer . load_state_dict ( checkpoint_dict [ \"<STR_LIT>\" ] ) \n print ( f\"<STR_LIT>\" ) \n return model , optimizer , learning_rate , iteration \n def save_checkpoint ( model , optimizer , learning_rate , iteration , checkpoint_path ) : \n print ( f\"<STR_LIT>\" ) \n if hasattr ( model , \"<STR_LIT>\" ) : \n state_dict = model . module . state_dict ( ) \n else : \n state_dict = model . state_dict ( ) \n torch . save ( \n { \n \"<STR_LIT>\" : state_dict , \n \"<STR_LIT>\" : iteration , \n \"<STR_LIT>\" : optimizer . state_dict ( ) , \n \"<STR_LIT>\" : learning_rate , \n } , \n checkpoint_path , \n ) \n def summarize ( \n writer , \n global_step , \n scalars = { } , \n histograms = { } , \n images = { } , \n audios = { } , \n audio_sampling_rate = <NUM_LIT> , \n ) : \n for k , v in scalars . items ( ) : \n writer . add_scalar ( k , v , global_step ) \n for k , v in histograms . items ( ) : \n writer . add_histogram ( k , v , global_step ) \n for k , v in images . items ( ) : \n writer . add_image ( k , v , global_step , dataformats = \"<STR_LIT>\" ) \n for k , v in audios . items ( ) : \n writer . add_audio ( k , v , global_step , audio_sampling_rate ) \n def latest_checkpoint_path ( dir_path , regex = \"<STR_LIT>\" ) : \n f_list = glob . glob ( os . path . join ( dir_path , regex ) ) \n f_list . sort ( key = lambda f : int ( \"<STR_LIT>\" . join ( filter ( str . isdigit , f ) ) ) ) \n x = f_list [ - <NUM_LIT> ] \n return x \n def plot_spectrogram_to_numpy ( spectrogram ) : \n import matplotlib . pylab as plt \n import numpy as np \n fig , ax = plt . subplots ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) \n im = ax . imshow ( spectrogram , aspect = \"<STR_LIT>\" , origin = \"<STR_LIT>\" , interpolation = \"<STR_LIT>\" ) \n plt . colorbar ( im , ax = ax ) \n plt . xlabel ( \"<STR_LIT>\" ) \n plt . ylabel ( \"<STR_LIT>\" ) \n plt . tight_layout ( ) \n fig . canvas . draw ( ) \n data = np . fromstring ( fig . canvas . tostring_rgb ( ) , dtype = np . uint8 , sep = \"<STR_LIT>\" ) \n data = data . reshape ( fig . canvas . get_width_height ( ) [ : : - <NUM_LIT> ] + ( <NUM_LIT> , ) ) \n plt . close ( ) \n return data \n def load_wav_to_torch ( full_path ) : \n sampling_rate , data = read ( full_path ) \n return torch . FloatTensor ( data . astype ( np . float32 ) ) , sampling_rate \n def load_filepaths_and_text ( filename , split = \"<STR_LIT>\" ) : \n with open ( filename , encoding = \"<STR_LIT>\" ) as f : \n filepaths_and_text = [ line . strip ( ) . split ( split ) for line in f ] \n return filepaths_and_text \n def get_hparams ( ) : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , required = True , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" , help = \"<STR_LIT>\" ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = int , required = True , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , required = True , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , required = True , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = str , \n default = \"<STR_LIT>\" , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \"<STR_LIT>\" , type = str , required = True , help = \"<STR_LIT>\" \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n required = True , \n help = \"<STR_LIT>\" , \n ) \n parser . add_argument ( \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n type = int , \n default = <NUM_LIT> , \n help = \"<STR_LIT>\" , \n ) \n args = parser . parse_args ( ) \n name = args . experiment_dir \n experiment_dir = os . path . join ( \"<STR_LIT>\" , args . experiment_dir ) \n config_save_path = os . path . join ( experiment_dir , \"<STR_LIT>\" ) \n with open ( config_save_path , \"<STR_LIT>\" ) as f : \n config = json . load ( f ) \n hparams = HParams ( ** config ) \n hparams . model_dir = hparams . experiment_dir = experiment_dir \n hparams . save_every_epoch = args . save_every_epoch \n hparams . name = name \n hparams . total_epoch = args . total_epoch \n hparams . pretrainG = args . pretrainG \n hparams . pretrainD = args . pretrainD \n hparams . version = args . version \n hparams . gpus = args . gpus \n hparams . train . batch_size = args . batch_size \n hparams . sample_rate = args . sample_rate \n hparams . if_f0 = args . if_f0 \n hparams . if_latest = args . if_latest \n hparams . save_every_weights = args . save_every_weights \n hparams . if_cache_data_in_gpu = args . if_cache_data_in_gpu \n hparams . data . training_files = f\"<STR_LIT>\" \n hparams . overtraining_detector = args . overtraining_detector \n hparams . overtraining_threshold = args . overtraining_threshold \n return hparams \n class HParams : \n def __init__ ( self , ** kwargs ) : \n for k , v in kwargs . items ( ) : \n if type ( v ) == dict :", "output": "v = HParams ( ** v ) \n self [ k ] = v \n def keys ( self ) : \n return self . __dict__ . keys ( ) \n def items ( self ) : \n return self . __dict__ . items ( ) \n def values ( self ) : \n return self . __dict__ . values ( ) \n def __len__ ( self ) : \n return len ( self . __dict__ ) \n def __getitem__ ( self , key ) : \n return getattr ( self , key ) \n def __setitem__ ( self , key , value ) : \n return setattr ( self , key , value ) \n def __contains__ ( self , key ) : \n return key in self . __dict__ \n def __repr__ ( self ) : \n return self . __dict__ . __repr__ ( )"}, {"input": "import config \n from common . log import logger \n from channel import channel_factory \n def run ( ) : \n try : \n config . load_config ( ) \n channel = channel_factory . create_channel ( \"<STR_LIT>\" ) \n channel . startup ( ) \n except Exception as e :", "output": "logger . error ( \"<STR_LIT>\" ) \n logger . exception ( e ) \n if __name__ == \"<STR_LIT>\" : \n run ( )"}, {"input": "from typing import List , Dict , Sequence \n from dataclasses import dataclass , field \n import logging \n import os \n from torch . utils . data import Dataset \n from datasets import load_from_disk , load_dataset , Dataset as HFDataset \n import transformers \n import torch \n from multi_token . modalities . base_modality import Modality \n from multi_token . constants import IGNORE_INDEX \n from multi_token . data_tools import encode_chat \n @ dataclass \n class DataArguments : \n dataset_path : str = field ( \n default = None , metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n ) \n def _resolve_dataset ( path : str ) -> HFDataset : \n if os . path . exists ( path ) : \n return load_from_disk ( path ) \n else : \n return load_dataset ( path , split = \"<STR_LIT>\" , data_files = \"<STR_LIT>\" ) \n class LMMDataset ( Dataset ) : \n def __init__ ( \n self , \n data_args : DataArguments , \n tokenizer : transformers . PreTrainedTokenizer , \n modalities : List [ Modality ] , \n ) : \n super ( LMMDataset , self ) . __init__ ( ) \n self . dataset = _resolve_dataset ( data_args . dataset_path ) \n self . tokenizer = tokenizer \n self . modalities = modalities \n def __len__ ( self ) :", "output": "return len ( self . dataset ) \n def get_example ( self ) -> Dict : \n return self . dataset [ <NUM_LIT> ] \n def __getitem__ ( self , i ) -> Dict : \n try : \n item = self . dataset [ i ] \n return encode_chat ( item , self . tokenizer , self . modalities ) \n except Exception as e : \n new_i = i + <NUM_LIT> \n if new_i >= len ( self ) : \n new_i = <NUM_LIT> \n logging . error ( f\"<STR_LIT>\" ) \n return self . __getitem__ ( new_i ) \n @ dataclass \n class DataCollatorForSupervisedLMMDataset : \n tokenizer : transformers . PreTrainedTokenizer \n modalities : List [ Modality ] \n def __call__ ( self , instances : Sequence [ Dict ] ) -> Dict [ str , torch . Tensor ] : \n input_ids , labels = tuple ( \n [ instance [ key ] for instance in instances ] for key in [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n ) \n input_ids = torch . nn . utils . rnn . pad_sequence ( \n input_ids , batch_first = True , padding_value = self . tokenizer . pad_token_id \n ) \n labels = torch . nn . utils . rnn . pad_sequence ( \n labels , batch_first = True , padding_value = IGNORE_INDEX \n ) \n input_ids = input_ids [ : , : self . tokenizer . model_max_length ] \n labels = labels [ : , : self . tokenizer . model_max_length ] \n batch = dict ( \n input_ids = input_ids , \n labels = labels , \n attention_mask = input_ids . ne ( self . tokenizer . pad_token_id ) , \n ) \n for m in self . modalities : \n batch [ m . name ] = [ instance [ m . name ] for instance in instances ] \n return batch"}, {"input": "class Animal : \n def __init__ ( self , nombre ) : \n self . nombre = nombre \n class Perro ( Animal ) : \n def __init__ ( self , nombre , raza ) : \n super ( ) . __init__ ( nombre ) \n self . raza = raza \n def hacer_sonido ( self ) : \n print ( f\"<STR_LIT>\" ) \n def correr ( self ) : \n print ( \"<STR_LIT>\" ) \n class Gato ( Animal ) : \n def __init__ ( self , nombre , raza ) : \n super ( ) . __init__ ( nombre ) \n self . raza = raza \n def hacer_sonido ( self ) : \n print ( f\"<STR_LIT>\" ) \n def correr ( self ) : \n print ( \"<STR_LIT>\" ) \n class Persona ( Animal ) : \n def __init__ ( self , nombre ) :", "output": "super ( ) . __init__ ( nombre ) \n def hacer_sonido ( self ) : \n print ( f\"<STR_LIT>\" ) \n def correr ( self ) : \n print ( \"<STR_LIT>\" ) \n def sonido_animal ( animal ) : \n animal . hacer_sonido ( ) \n def correr ( animal ) : \n animal . correr ( ) \n brujita = Gato ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n jesus = Persona ( \"<STR_LIT>\" ) \n sonido_animal ( brujita ) \n correr ( brujita ) \n sonido_animal ( jesus ) \n correr ( jesus )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_index ( '<STR_LIT>' )", "output": "batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) \n batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' ) \n batch_op . alter_column ( '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}, {"input": "from typing import Any \n from pydantic import BaseModel , Json , computed_field \n class Trace ( BaseModel ) : \n data : Json [ Any ] = { } \n name : str \n duration : float = <NUM_LIT> \n timestamp : str = \"<STR_LIT>\" \n id : int \n class TraceCreate ( BaseModel ) : \n data : Json [ Any ] = { } \n name : str \n @ computed_field \n @ property \n def duration ( self ) -> float : \n any_trace_to_analize = any ( \n True \n for trace in self . data . get ( \"<STR_LIT>\" , [ ] ) \n if trace . get ( \"<STR_LIT>\" ) \n ) \n if not any_trace_to_analize : \n return <NUM_LIT> \n start = min ( \n trace . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n for trace in self . data . get ( \"<STR_LIT>\" , [ ] ) \n if trace . get ( \"<STR_LIT>\" ) \n ) \n end = max (", "output": "trace . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n for trace in self . data . get ( \"<STR_LIT>\" , [ ] ) \n ) \n return end - start"}, {"input": "import csv \n import datetime \n from functools import wraps \n import click \n import flask \n import opml \n import sqlalchemy as sa \n from huey import crontab \n from huey . contrib . mini import MiniHuey \n import feedi . models as models \n import feedi . parsers as parsers \n from feedi . app import create_huey_app \n from feedi . models import db \n app = create_huey_app ( ) \n huey = MiniHuey ( pool_size = app . config [ '<STR_LIT>' ] ) \n feed_cli = flask . cli . AppGroup ( '<STR_LIT>' ) \n user_cli = flask . cli . AppGroup ( '<STR_LIT>' ) \n flask . current_app . cli . add_command ( feed_cli ) \n flask . current_app . cli . add_command ( user_cli ) \n def huey_task ( * huey_args ) : \n \"<STR_LIT>\" \n huey_decorator = huey . task ( * huey_args ) \n def with_context ( f ) : \n @ wraps ( f ) \n def decorator ( * args , ** kwargs ) : \n with app . app_context ( ) : \n fargs = '<STR_LIT>' . join ( [ str ( arg ) for arg in args ] ) \n fkwargs = '<STR_LIT>' . join ( [ f'<STR_LIT>' for ( k , v ) in kwargs . items ( ) ] ) \n app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) \n try : \n f ( * args , ** kwargs ) \n app . logger . info ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) \n except Exception : \n app . logger . exception ( \"<STR_LIT>\" , f . __name__ , fargs , fkwargs ) \n return decorator \n def composed_decorator ( f ) : \n return huey_decorator ( with_context ( f ) ) \n return composed_decorator \n @ feed_cli . command ( '<STR_LIT>' ) \n @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) \n def sync_all_feeds ( ) : \n feeds = db . session . execute ( db . select ( models . Feed . id , models . Feed . name ) ) . all ( ) \n tasks = [ ]", "output": "for feed in feeds : \n tasks . append ( ( feed . name , sync_feed ( feed . id , feed . name ) ) ) \n for name , task in tasks : \n try : \n task . get ( ) \n except Exception : \n app . logger . exception ( \"<STR_LIT>\" , name ) \n continue \n @ huey_task ( ) \n def sync_feed ( feed_id , _feed_name , force = False ) : \n db_feed = db . session . get ( models . Feed , feed_id ) \n db_feed . sync_with_remote ( force = force ) \n db . session . commit ( ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ huey_task ( crontab ( minute = app . config [ '<STR_LIT>' ] ) ) \n def content_prefetch ( ) : \n for user_id in db . session . scalars ( db . select ( models . User . id ) ) : \n start_at = datetime . datetime . utcnow ( ) \n query = models . Entry . sorted_by ( \n user_id , models . Entry . ORDER_FREQUENCY , start_at , hide_seen = True ) . filter ( models . Entry . content_full . is_ ( None ) , models . Entry . content_url . isnot ( None ) ) . limit ( <NUM_LIT> ) \n for entry in db . session . scalars ( query ) : \n app . logger . debug ( '<STR_LIT>' , entry . content_url ) \n entry . fetch_content ( ) \n db . session . commit ( ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = app . config [ '<STR_LIT>' ] ) ) \n def delete_old_entries ( ) : \n older_than_date = ( datetime . datetime . utcnow ( ) - \n datetime . timedelta ( days = app . config [ '<STR_LIT>' ] ) ) \n minimum = app . config [ '<STR_LIT>' ] \n feeds_q = db . select ( models . Feed . id , models . Feed . name ) . join ( models . Feed . entries ) . filter ( models . Entry . sort_date < older_than_date , \n models . Entry . favorited . is_ ( None ) , \n models . Entry . backlogged . is_ ( None ) , \n models . Entry . pinned . is_ ( None ) \n ) . group_by ( models . Feed . id ) . having ( sa . func . count ( models . Feed . entries ) > <NUM_LIT> ) \n for ( feed_id , feed_name ) in db . session . execute ( feeds_q ) . all ( ) : \n min_sort_date = db . session . scalar ( \n db . select ( models . Entry . sort_date ) \n . filter_by ( feed_id = feed_id ) \n . order_by ( models . Entry . sort_date . desc ( ) ) \n . limit ( <NUM_LIT> ) \n . offset ( minimum - <NUM_LIT> ) ) \n if not min_sort_date : \n continue \n q = db . delete ( models . Entry ) . where ( \n models . Entry . favorited . is_ ( None ) , \n models . Entry . backlogged . is_ ( None ) , \n models . Entry . pinned . is_ ( None ) , \n models . Entry . feed_id == feed_id , \n models . Entry . sort_date < min_sort_date , \n models . Entry . sort_date < older_than_date ) \n res = db . session . execute ( q ) \n db . session . commit ( ) \n if res . rowcount : \n app . logger . info ( \"<STR_LIT>\" , res . rowcount , feed_id , feed_name ) \n q = db . delete ( models . Entry ) . where ( \n models . Entry . feed_id . is_ ( None ) , \n models . Entry . favorited . is_ ( None ) , \n models . Entry . pinned . is_ ( None ) , \n models . Entry . sort_date < older_than_date ) \n res = db . session . execute ( q ) \n db . session . commit ( ) \n if res . rowcount : \n app . logger . info ( \"<STR_LIT>\" , res . rowcount ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ huey_task ( crontab ( minute = '<STR_LIT>' , hour = '<STR_LIT>' ) ) \n def pop_backlog ( ) : \n \"<STR_LIT>\" \n week_ago = datetime . datetime . utcnow ( ) - datetime . timedelta ( days = <NUM_LIT> ) \n backlogged_date = sa . func . min ( models . Entry . backlogged ) . label ( '<STR_LIT>' ) \n query = db . select ( models . Entry ) . group_by ( models . Entry . user_id ) . having ( backlogged_date < week_ago ) \n for entry in db . session . scalars ( query ) : \n entry . unbacklog ( ) \n app . logger . info ( \"<STR_LIT>\" , entry . user_id , entry . target_url ) \n db . session . commit ( ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ click . argument ( '<STR_LIT>' ) \n def debug_feed ( url ) : \n parsers . rss . pretty_print ( url ) \n def load_user_arg ( _ctx , _param , email ) : \n if not email : \n email = app . config . get ( '<STR_LIT>' ) \n if not email : \n raise click . UsageError ( '<STR_LIT>' ) \n user = db . session . scalar ( db . select ( models . User ) . filter_by ( email = email ) ) \n if not user : \n raise click . UsageError ( f'<STR_LIT>' ) \n return user \n @ feed_cli . command ( '<STR_LIT>' ) \n @ click . argument ( \"<STR_LIT>\" ) \n @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) \n def csv_load ( file , user ) : \n \"<STR_LIT>\" \n with open ( file ) as csv_file : \n for values in csv . reader ( csv_file ) : \n cls = models . Feed . resolve ( values [ <NUM_LIT> ] ) \n feed = cls . from_valuelist ( * values ) \n feed . user_id = user . id \n add_if_not_exists ( feed ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ click . argument ( \"<STR_LIT>\" ) \n @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) \n def csv_dump ( file , user ) : \n \"<STR_LIT>\" \n with open ( file , '<STR_LIT>' ) as csv_file : \n feed_writer = csv . writer ( csv_file ) \n for feed in db . session . execute ( db . select ( models . Feed ) \n . filter_by ( user_id = user . id , is_mastodon = False ) ) . scalars ( ) : \n feed_writer . writerow ( feed . to_valuelist ( ) ) \n app . logger . info ( '<STR_LIT>' , feed ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ click . argument ( \"<STR_LIT>\" ) \n @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) \n def opml_load ( file , user ) : \n document = opml . OpmlDocument . load ( file ) \n for outline in document . outlines : \n if outline . outlines : \n folder = outline . text \n for feed in outline . outlines : \n add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , \n user_id = user . id , \n url = feed . xml_url , \n folder = folder ) ) \n else : \n add_if_not_exists ( models . RssFeed ( name = feed . title or feed . text , \n user_id = user . id , \n url = feed . xml_url ) ) \n @ feed_cli . command ( '<STR_LIT>' ) \n @ click . argument ( \"<STR_LIT>\" ) \n @ click . argument ( '<STR_LIT>' , required = False , callback = load_user_arg ) \n def opml_dump ( file , user ) : \n document = opml . OpmlDocument ( ) \n folder_outlines = { } \n for feed in db . session . execute ( db . select ( models . RssFeed ) \n . filter_by ( user_id = user . id ) \n ) . scalars ( ) : \n if feed . folder : \n if feed . folder in folder_outlines : \n folder_outlines [ feed . folder ] = document . add_outline ( feed . folder ) \n target = folder_outlines [ feed . folder ] \n else : \n target = document \n target . add_rss ( feed . name , \n feed . url , \n title = feed . name , \n categories = [ feed . folder ] if feed . folder else [ ] , \n created = datetime . datetime . now ( ) ) \n document . dump ( file ) \n def add_if_not_exists ( feed ) : \n query = db . select ( db . exists ( models . Feed ) \n . where ( models . Feed . name == feed . name , models . Feed . user_id == feed . user_id ) ) \n if db . session . execute ( query ) . scalar ( ) : \n app . logger . info ( '<STR_LIT>' , feed . name ) \n return \n db . session . add ( feed ) \n db . session . commit ( ) \n feed . load_icon ( ) \n db . session . commit ( ) \n app . logger . info ( '<STR_LIT>' , feed ) \n @ user_cli . command ( '<STR_LIT>' ) \n @ click . argument ( '<STR_LIT>' ) \n @ click . password_option ( ) \n def user_add ( email , password ) : \n user = models . User ( email = email ) \n user . set_password ( password ) \n db . session . add ( user ) \n db . session . commit ( ) \n @ user_cli . command ( '<STR_LIT>' ) \n @ click . argument ( '<STR_LIT>' ) \n def user_delete ( email ) : \n stmt = db . delete ( models . User ) . where ( models . User . email == email ) \n db . session . execute ( stmt ) \n db . session . commit ( )"}, {"input": "import json \n import math \n from typing import List , Dict , Any \n import requests \n from fastchat . model . model_adapter import get_conversation_template \n from long_captions . utils import get_clip_token_length \n from long_captions . dense_image import DenseCaptionedImage , MaskDataEntry \n ALTER_PROMPTS = { \n '<STR_LIT>' : ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) , \n '<STR_LIT>' : ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) , \n '<STR_LIT>' : ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n } \n MULTI_CAPTION_PROMPT = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n class RemoteLanguageModel : \n def __init__ ( \n self , \n model_path : str , \n worker_addr : str = \"<STR_LIT>\" , \n debug : bool = False \n ) -> None : \n self . model_path = model_path \n self . worker_addr = worker_addr \n self . debug = debug \n def generate ( \n self , \n message : str , \n system : str = None , \n temperature : float = <NUM_LIT> , \n max_new_tokens : int = <NUM_LIT> \n ) -> Dict [ str , Any ] : \n conv = get_conversation_template ( self . model_path ) \n if system is not None : \n conv . system_message = f\"<STR_LIT>\" \n conv . append_message ( conv . roles [ <NUM_LIT> ] , message ) \n conv . append_message ( conv . roles [ <NUM_LIT> ] , None ) \n prompt = conv . get_prompt ( ) \n if self . debug : \n print ( prompt ) \n headers = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n gen_params = { \n \"<STR_LIT>\" : self . model_path , \n \"<STR_LIT>\" : prompt , \n \"<STR_LIT>\" : temperature , \n \"<STR_LIT>\" : max_new_tokens , \n \"<STR_LIT>\" : conv . stop_str , \n \"<STR_LIT>\" : conv . stop_token_ids , \n \"<STR_LIT>\" : False , \n } \n response = requests . post ( \n self . worker_addr + \"<STR_LIT>\" , \n headers = headers , \n json = gen_params \n ) \n return json . loads ( response . text ) \n def get_short_enough_full_caption ( self , dci : DenseCaptionedImage , tok_bound = <NUM_LIT> ) -> str : \n full_caption = dci . get_formatted_complete_description ( ) [ <NUM_LIT> ] [ '<STR_LIT>' ] \n while get_clip_token_length ( full_caption ) > tok_bound :", "output": "full_caption = \"<STR_LIT>\" . join ( full_caption . split ( \"<STR_LIT>\" ) [ : - <NUM_LIT> ] ) \n return full_caption \n def get_extracted_captions ( self , resp ) -> List [ str ] : \n _ , content = resp . split ( \"<STR_LIT>\" ) \n c1 , content = content . split ( \"<STR_LIT>\" ) \n c2 , content = content . split ( \"<STR_LIT>\" ) \n c3 , content = content . split ( \"<STR_LIT>\" ) \n c4 , content = content . split ( \"<STR_LIT>\" ) \n c5 = content . split ( \"<STR_LIT>\" ) [ <NUM_LIT> ] \n return [ c1 . strip ( ) , c2 . strip ( ) , c3 . strip ( ) , c4 . strip ( ) , c5 . strip ( ) ] \n def get_new_captions_for_dci ( \n self , dci : DenseCaptionedImage , \n max_len = <NUM_LIT> , target_count = <NUM_LIT> , max_retry = <NUM_LIT> ) -> List [ str ] : \n new_captions = [ ] \n full_caption = self . get_short_enough_full_caption ( dci ) \n while len ( new_captions ) < target_count and max_retry > <NUM_LIT> : \n max_retry -= <NUM_LIT> \n try : \n message = f\"<STR_LIT>\" \n res = self . generate ( \n message = message , \n system = MULTI_CAPTION_PROMPT , \n max_new_tokens = <NUM_LIT> , \n ) \n for c in self . get_extracted_captions ( res [ '<STR_LIT>' ] ) : \n if get_clip_token_length ( c ) < max_len : \n new_captions . append ( c ) \n except : \n pass \n assert len ( new_captions ) > target_count \n return new_captions \n def get_short_enough_mask_caption ( \n self , dci : DenseCaptionedImage , \n mask : MaskDataEntry , tok_bound = <NUM_LIT> ) -> str : \n depth = dci . _get_max_depth ( mask ) \n caption_length = <NUM_LIT> \n while caption_length > tok_bound : \n full_caption = dci . get_caption_with_subcaptions ( mask , max_depth = depth ) [ <NUM_LIT> ] [ '<STR_LIT>' ] \n depth -= <NUM_LIT> \n caption_length = get_clip_token_length ( full_caption ) \n return full_caption \n def get_new_captions_for_mask ( \n self , dci : DenseCaptionedImage , \n mask : MaskDataEntry , max_len = <NUM_LIT> , \n target_count = None , max_retry = <NUM_LIT> ) -> list [ str ] : \n mask_subcaption = self . get_short_enough_mask_caption ( dci , mask ) \n caption_length = get_clip_token_length ( mask_subcaption ) \n if target_count is None : \n target_count = max ( <NUM_LIT> , math . log ( caption_length ) ) \n new_captions = [ ] \n while len ( new_captions ) < target_count and max_retry > <NUM_LIT> : \n max_retry -= <NUM_LIT> \n try : \n message = f\"<STR_LIT>\" \n res = self . generate ( \n message = message , \n system = MULTI_CAPTION_PROMPT , \n max_new_tokens = <NUM_LIT> , \n ) \n for c in self . get_extracted_captions ( res [ '<STR_LIT>' ] ) : \n if get_clip_token_length ( c ) < max_len : \n new_captions . append ( c ) \n except : \n pass \n assert len ( new_captions ) > target_count \n return new_captions \n def get_summary ( self , message : str , short = False ) : \n if not short : \n SYSTEM_PROMPT = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n SYSTEM_PROMPT = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n res = self . generate ( message , system = SYSTEM_PROMPT , max_new_tokens = <NUM_LIT> ) \n if \"<STR_LIT>\" in res [ '<STR_LIT>' ] : \n assert \"<STR_LIT>\" not in res [ '<STR_LIT>' ] [ <NUM_LIT> : ] , f\"<STR_LIT>\" \n res [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] \n return res [ '<STR_LIT>' ] . strip ( ) \n def get_negative ( self , message : str , negative_type = '<STR_LIT>' ) : \n alter_prompt = ALTER_PROMPTS [ negative_type ] \n res = self . generate ( message , system = alter_prompt , max_new_tokens = <NUM_LIT> ) \n if \"<STR_LIT>\" in res [ '<STR_LIT>' ] : \n assert \"<STR_LIT>\" not in res [ '<STR_LIT>' ] [ <NUM_LIT> : ] , f\"<STR_LIT>\" \n res [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] \n return res [ '<STR_LIT>' ] . strip ( ) \n def reduce_length ( self , message : str , target_tokens = <NUM_LIT> , failures = <NUM_LIT> ) : \n last_len = get_clip_token_length ( message ) \n while ( last_len ) > target_tokens : \n print ( f\"<STR_LIT>\" , end = '<STR_LIT>' , flush = True ) \n if last_len - target_tokens < <NUM_LIT> : \n ALTER_PROMPT = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n else : \n ALTER_PROMPT = ( \n \"<STR_LIT>\" \n \"<STR_LIT>\" \n ) \n res = self . generate ( message , system = ALTER_PROMPT , max_new_tokens = <NUM_LIT> ) \n new_len = get_clip_token_length ( res [ '<STR_LIT>' ] . strip ( ) ) \n if new_len >= last_len or \"<STR_LIT>\" in res [ '<STR_LIT>' ] : \n assert failures > <NUM_LIT> , f\"<STR_LIT>\" \n failures -= <NUM_LIT> \n else : \n last_len = new_len \n message = res [ '<STR_LIT>' ] . strip ( ) \n return message"}, {"input": "import torch \n import sys \n import os \n import datetime \n from utils import ( \n get_hparams , \n plot_spectrogram_to_numpy , \n summarize , \n load_checkpoint , \n save_checkpoint , \n latest_checkpoint_path , \n ) \n from random import randint , shuffle \n from time import sleep \n from time import time as ttime \n from torch . cuda . amp import GradScaler , autocast \n from torch . nn import functional as F \n from torch . nn . parallel import DistributedDataParallel as DDP \n from torch . utils . data import DataLoader \n from torch . utils . tensorboard import SummaryWriter \n import torch . distributed as dist \n import torch . multiprocessing as mp \n now_dir = os . getcwd ( ) \n sys . path . append ( os . path . join ( now_dir ) ) \n from data_utils import ( \n DistributedBucketSampler , \n TextAudioCollate , \n TextAudioCollateMultiNSFsid , \n TextAudioLoader , \n TextAudioLoaderMultiNSFsid , \n ) \n from losses import ( \n discriminator_loss , \n feature_loss , \n generator_loss , \n kl_loss , \n ) \n from mel_processing import mel_spectrogram_torch , spec_to_mel_torch \n from rvc . train . process . extract_model import extract_model \n from rvc . lib . infer_pack import commons \n hps = get_hparams ( ) \n if hps . version == \"<STR_LIT>\" : \n from rvc . lib . infer_pack . models import MultiPeriodDiscriminator \n from rvc . lib . infer_pack . models import SynthesizerTrnMs256NSFsid as RVC_Model_f0 \n from rvc . lib . infer_pack . models import ( \n SynthesizerTrnMs256NSFsid_nono as RVC_Model_nof0 , \n ) \n elif hps . version == \"<STR_LIT>\" : \n from rvc . lib . infer_pack . models import ( \n SynthesizerTrnMs768NSFsid as RVC_Model_f0 , \n SynthesizerTrnMs768NSFsid_nono as RVC_Model_nof0 , \n MultiPeriodDiscriminatorV2 as MultiPeriodDiscriminator , \n ) \n os . environ [ \"<STR_LIT>\" ] = hps . gpus . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n n_gpus = len ( hps . gpus . split ( \"<STR_LIT>\" ) ) \n torch . backends . cudnn . deterministic = False \n torch . backends . cudnn . benchmark = False \n global_step = <NUM_LIT> \n lowest_value = { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : float ( \"<STR_LIT>\" ) , \"<STR_LIT>\" : <NUM_LIT> } \n last_loss_gen_all = <NUM_LIT> \n epochs_since_last_lowest = <NUM_LIT> \n class EpochRecorder : \n def __init__ ( self ) : \n self . last_time = ttime ( ) \n def record ( self ) : \n now_time = ttime ( ) \n elapsed_time = now_time - self . last_time \n self . last_time = now_time \n elapsed_time = round ( elapsed_time , <NUM_LIT> ) \n elapsed_time_str = str ( datetime . timedelta ( seconds = int ( elapsed_time ) ) ) \n current_time = datetime . datetime . now ( ) . strftime ( \"<STR_LIT>\" ) \n return f\"<STR_LIT>\" \n def main ( ) : \n n_gpus = torch . cuda . device_count ( ) \n if torch . cuda . is_available ( ) == False and torch . backends . mps . is_available ( ) == True : \n n_gpus = <NUM_LIT> \n if n_gpus < <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n n_gpus = <NUM_LIT> \n children = [ ] \n pid_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n with open ( pid_file_path , \"<STR_LIT>\" ) as pid_file : \n for i in range ( n_gpus ) : \n subproc = mp . Process ( \n target = run , \n args = ( i , n_gpus , hps ) , \n ) \n children . append ( subproc ) \n subproc . start ( ) \n pid_file . write ( str ( subproc . pid ) + \"<STR_LIT>\" ) \n for i in range ( n_gpus ) : \n children [ i ] . join ( ) \n def run ( \n rank , \n n_gpus , \n hps , \n ) : \n global global_step \n if rank == <NUM_LIT> : \n writer = SummaryWriter ( log_dir = hps . model_dir ) \n writer_eval = SummaryWriter ( log_dir = os . path . join ( hps . model_dir , \"<STR_LIT>\" ) ) \n os . environ [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n os . environ [ \"<STR_LIT>\" ] = str ( randint ( <NUM_LIT> , <NUM_LIT> ) ) \n dist . init_process_group ( \n backend = \"<STR_LIT>\" , init_method = \"<STR_LIT>\" , world_size = n_gpus , rank = rank \n ) \n torch . manual_seed ( hps . train . seed ) \n if torch . cuda . is_available ( ) : \n torch . cuda . set_device ( rank ) \n if hps . if_f0 == <NUM_LIT> : \n train_dataset = TextAudioLoaderMultiNSFsid ( hps . data ) \n else : \n train_dataset = TextAudioLoader ( hps . data ) \n train_sampler = DistributedBucketSampler ( \n train_dataset , \n hps . train . batch_size * n_gpus , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n num_replicas = n_gpus , \n rank = rank , \n shuffle = True , \n ) \n if hps . if_f0 == <NUM_LIT> : \n collate_fn = TextAudioCollateMultiNSFsid ( ) \n else : \n collate_fn = TextAudioCollate ( ) \n train_loader = DataLoader ( \n train_dataset , \n num_workers = <NUM_LIT> , \n shuffle = False , \n pin_memory = True , \n collate_fn = collate_fn , \n batch_sampler = train_sampler , \n persistent_workers = True , \n prefetch_factor = <NUM_LIT> , \n ) \n if hps . if_f0 == <NUM_LIT> : \n net_g = RVC_Model_f0 ( \n hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , \n hps . train . segment_size // hps . data . hop_length , \n ** hps . model , \n is_half = hps . train . fp16_run , \n sr = hps . sample_rate , \n ) \n else : \n net_g = RVC_Model_nof0 ( \n hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , \n hps . train . segment_size // hps . data . hop_length , \n ** hps . model , \n is_half = hps . train . fp16_run , \n ) \n if torch . cuda . is_available ( ) : \n net_g = net_g . cuda ( rank ) \n net_d = MultiPeriodDiscriminator ( hps . model . use_spectral_norm ) \n if torch . cuda . is_available ( ) : \n net_d = net_d . cuda ( rank ) \n optim_g = torch . optim . AdamW ( \n net_g . parameters ( ) , \n hps . train . learning_rate , \n betas = hps . train . betas , \n eps = hps . train . eps , \n ) \n optim_d = torch . optim . AdamW ( \n net_d . parameters ( ) , \n hps . train . learning_rate , \n betas = hps . train . betas , \n eps = hps . train . eps , \n ) \n if torch . cuda . is_available ( ) : \n net_g = DDP ( net_g , device_ids = [ rank ] ) \n net_d = DDP ( net_d , device_ids = [ rank ] ) \n else : \n net_g = DDP ( net_g ) \n net_d = DDP ( net_d ) \n try : \n print ( \"<STR_LIT>\" ) \n _ , _ , _ , epoch_str = load_checkpoint ( \n latest_checkpoint_path ( hps . model_dir , \"<STR_LIT>\" ) , net_d , optim_d \n ) \n _ , _ , _ , epoch_str = load_checkpoint ( \n latest_checkpoint_path ( hps . model_dir , \"<STR_LIT>\" ) , net_g , optim_g \n ) \n global_step = ( epoch_str - <NUM_LIT> ) * len ( train_loader ) \n except : \n epoch_str = <NUM_LIT> \n global_step = <NUM_LIT> \n if hps . pretrainG != \"<STR_LIT>\" : \n if rank == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n if hasattr ( net_g , \"<STR_LIT>\" ) : \n print ( \n net_g . module . load_state_dict ( \n torch . load ( hps . pretrainG , map_location = \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n ) \n ) \n else : \n print ( \n net_g . load_state_dict ( \n torch . load ( hps . pretrainG , map_location = \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n ) \n ) \n if hps . pretrainD != \"<STR_LIT>\" : \n if rank == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n if hasattr ( net_d , \"<STR_LIT>\" ) : \n print ( \n net_d . module . load_state_dict ( \n torch . load ( hps . pretrainD , map_location = \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n ) \n ) \n else : \n print ( \n net_d . load_state_dict ( \n torch . load ( hps . pretrainD , map_location = \"<STR_LIT>\" ) [ \"<STR_LIT>\" ] \n ) \n ) \n scheduler_g = torch . optim . lr_scheduler . ExponentialLR ( \n optim_g , gamma = hps . train . lr_decay , last_epoch = epoch_str - <NUM_LIT> \n ) \n scheduler_d = torch . optim . lr_scheduler . ExponentialLR ( \n optim_d , gamma = hps . train . lr_decay , last_epoch = epoch_str - <NUM_LIT> \n ) \n scaler = GradScaler ( enabled = hps . train . fp16_run ) \n cache = [ ] \n for epoch in range ( epoch_str , hps . train . epochs + <NUM_LIT> ) : \n if rank == <NUM_LIT> : \n train_and_evaluate ( \n rank , \n epoch , \n hps , \n [ net_g , net_d ] , \n [ optim_g , optim_d ] , \n scaler , \n [ train_loader , None ] , \n [ writer , writer_eval ] , \n cache , \n ) \n else : \n train_and_evaluate ( \n rank , \n epoch , \n hps , \n [ net_g , net_d ] , \n [ optim_g , optim_d ] , \n scaler , \n [ train_loader , None ] , \n None , \n cache , \n ) \n scheduler_g . step ( ) \n scheduler_d . step ( ) \n def train_and_evaluate ( rank , epoch , hps , nets , optims , scaler , loaders , writers , cache ) : \n global global_step , last_loss_gen_all , lowest_value , epochs_since_last_lowest \n if epoch == <NUM_LIT> : \n lowest_value = { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : float ( \"<STR_LIT>\" ) , \"<STR_LIT>\" : <NUM_LIT> } \n last_loss_gen_all = <NUM_LIT> \n epochs_since_last_lowest = <NUM_LIT> \n net_g , net_d = nets \n optim_g , optim_d = optims \n train_loader = loaders [ <NUM_LIT> ] if loaders is not None else None \n if writers is not None : \n writer = writers [ <NUM_LIT> ] \n train_loader . batch_sampler . set_epoch ( epoch ) \n net_g . train ( ) \n net_d . train ( ) \n if hps . if_cache_data_in_gpu == True : \n data_iterator = cache \n if cache == [ ] : \n for batch_idx , info in enumerate ( train_loader ) : \n if hps . if_f0 == <NUM_LIT> : \n ( \n phone , \n phone_lengths , \n pitch , \n pitchf , \n spec , \n spec_lengths , \n wave , \n wave_lengths , \n sid , \n ) = info \n else : \n ( \n phone , \n phone_lengths , \n spec , \n spec_lengths , \n wave , \n wave_lengths , \n sid , \n ) = info \n if torch . cuda . is_available ( ) : \n phone = phone . cuda ( rank , non_blocking = True ) \n phone_lengths = phone_lengths . cuda ( rank , non_blocking = True ) \n if hps . if_f0 == <NUM_LIT> : \n pitch = pitch . cuda ( rank , non_blocking = True ) \n pitchf = pitchf . cuda ( rank , non_blocking = True ) \n sid = sid . cuda ( rank , non_blocking = True ) \n spec = spec . cuda ( rank , non_blocking = True ) \n spec_lengths = spec_lengths . cuda ( rank , non_blocking = True ) \n wave = wave . cuda ( rank , non_blocking = True ) \n wave_lengths = wave_lengths . cuda ( rank , non_blocking = True ) \n if hps . if_f0 == <NUM_LIT> : \n cache . append ( \n ( \n batch_idx , \n ( \n phone , \n phone_lengths , \n pitch , \n pitchf , \n spec , \n spec_lengths , \n wave , \n wave_lengths , \n sid , \n ) , \n ) \n ) \n else : \n cache . append ( \n ( \n batch_idx , \n ( \n phone , \n phone_lengths , \n spec , \n spec_lengths , \n wave , \n wave_lengths , \n sid , \n ) , \n ) \n ) \n else : \n shuffle ( cache ) \n else : \n data_iterator = enumerate ( train_loader ) \n epoch_recorder = EpochRecorder ( ) \n for batch_idx , info in data_iterator : \n if hps . if_f0 == <NUM_LIT> : \n ( \n phone , \n phone_lengths , \n pitch , \n pitchf , \n spec , \n spec_lengths , \n wave , \n wave_lengths , \n sid , \n ) = info \n else : \n phone , phone_lengths , spec , spec_lengths , wave , wave_lengths , sid = info \n if ( hps . if_cache_data_in_gpu == False ) and torch . cuda . is_available ( ) : \n phone = phone . cuda ( rank , non_blocking = True ) \n phone_lengths = phone_lengths . cuda ( rank , non_blocking = True ) \n if hps . if_f0 == <NUM_LIT> : \n pitch = pitch . cuda ( rank , non_blocking = True ) \n pitchf = pitchf . cuda ( rank , non_blocking = True ) \n sid = sid . cuda ( rank , non_blocking = True ) \n spec = spec . cuda ( rank , non_blocking = True ) \n spec_lengths = spec_lengths . cuda ( rank , non_blocking = True ) \n wave = wave . cuda ( rank , non_blocking = True ) \n with autocast ( enabled = hps . train . fp16_run ) : \n if hps . if_f0 == <NUM_LIT> : \n ( \n y_hat , \n ids_slice , \n x_mask , \n z_mask , \n ( z , z_p , m_p , logs_p , m_q , logs_q ) , \n ) = net_g ( phone , phone_lengths , pitch , pitchf , spec , spec_lengths , sid ) \n else : \n ( \n y_hat , \n ids_slice , \n x_mask , \n z_mask , \n ( z , z_p , m_p , logs_p , m_q , logs_q ) , \n ) = net_g ( phone , phone_lengths , spec , spec_lengths , sid ) \n mel = spec_to_mel_torch ( \n spec , \n hps . data . filter_length , \n hps . data . n_mel_channels , \n hps . data . sampling_rate , \n hps . data . mel_fmin , \n hps . data . mel_fmax , \n ) \n y_mel = commons . slice_segments ( \n mel , ids_slice , hps . train . segment_size // hps . data . hop_length \n ) \n with autocast ( enabled = False ) : \n y_hat_mel = mel_spectrogram_torch ( \n y_hat . float ( ) . squeeze ( <NUM_LIT> ) , \n hps . data . filter_length , \n hps . data . n_mel_channels , \n hps . data . sampling_rate , \n hps . data . hop_length , \n hps . data . win_length , \n hps . data . mel_fmin , \n hps . data . mel_fmax , \n ) \n if hps . train . fp16_run == True : \n y_hat_mel = y_hat_mel . half ( ) \n wave = commons . slice_segments ( \n wave , ids_slice * hps . data . hop_length , hps . train . segment_size \n ) \n y_d_hat_r , y_d_hat_g , _ , _ = net_d ( wave , y_hat . detach ( ) ) \n with autocast ( enabled = False ) : \n loss_disc , losses_disc_r , losses_disc_g = discriminator_loss ( \n y_d_hat_r , y_d_hat_g \n ) \n optim_d . zero_grad ( ) \n scaler . scale ( loss_disc ) . backward ( ) \n scaler . unscale_ ( optim_d ) \n grad_norm_d = commons . clip_grad_value_ ( net_d . parameters ( ) , None ) \n scaler . step ( optim_d ) \n with autocast ( enabled = hps . train . fp16_run ) : \n y_d_hat_r , y_d_hat_g , fmap_r , fmap_g = net_d ( wave , y_hat ) \n with autocast ( enabled = False ) : \n loss_mel = F . l1_loss ( y_mel , y_hat_mel ) * hps . train . c_mel \n loss_kl = kl_loss ( z_p , logs_q , m_p , logs_p , z_mask ) * hps . train . c_kl \n loss_fm = feature_loss ( fmap_r , fmap_g ) \n loss_gen , losses_gen = generator_loss ( y_d_hat_g ) \n loss_gen_all = loss_gen + loss_fm + loss_mel + loss_kl \n if loss_gen_all < lowest_value [ \"<STR_LIT>\" ] : \n lowest_value [ \"<STR_LIT>\" ] = loss_gen_all \n lowest_value [ \"<STR_LIT>\" ] = global_step \n lowest_value [ \"<STR_LIT>\" ] = epoch \n if epoch > lowest_value [ \"<STR_LIT>\" ] : \n print ( \n \"<STR_LIT>\" \n ) \n optim_g . zero_grad ( ) \n scaler . scale ( loss_gen_all ) . backward ( ) \n scaler . unscale_ ( optim_g ) \n grad_norm_g = commons . clip_grad_value_ ( net_g . parameters ( ) , None ) \n scaler . step ( optim_g ) \n scaler . update ( ) \n if rank == <NUM_LIT> : \n if global_step % hps . train . log_interval == <NUM_LIT> : \n lr = optim_g . param_groups [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n if loss_mel > <NUM_LIT> : \n loss_mel = <NUM_LIT> \n if loss_kl > <NUM_LIT> : \n loss_kl = <NUM_LIT> \n scalar_dict = { \n \"<STR_LIT>\" : loss_gen_all , \n \"<STR_LIT>\" : loss_disc , \n \"<STR_LIT>\" : lr , \n \"<STR_LIT>\" : grad_norm_d , \n \"<STR_LIT>\" : grad_norm_g , \n } \n scalar_dict . update ( \n { \n \"<STR_LIT>\" : loss_fm , \n \"<STR_LIT>\" : loss_mel , \n \"<STR_LIT>\" : loss_kl , \n } \n ) \n scalar_dict . update ( \n { \"<STR_LIT>\" . format ( i ) : v for i , v in enumerate ( losses_gen ) } \n ) \n scalar_dict . update ( \n { \"<STR_LIT>\" . format ( i ) : v for i , v in enumerate ( losses_disc_r ) } \n ) \n scalar_dict . update ( \n { \"<STR_LIT>\" . format ( i ) : v for i , v in enumerate ( losses_disc_g ) } \n ) \n image_dict = { \n \"<STR_LIT>\" : plot_spectrogram_to_numpy ( \n y_mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) \n ) , \n \"<STR_LIT>\" : plot_spectrogram_to_numpy ( \n y_hat_mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) \n ) , \n \"<STR_LIT>\" : plot_spectrogram_to_numpy ( mel [ <NUM_LIT> ] . data . cpu ( ) . numpy ( ) ) , \n } \n summarize ( \n writer = writer , \n global_step = global_step , \n images = image_dict , \n scalars = scalar_dict , \n ) \n global_step += <NUM_LIT> \n if epoch % hps . save_every_epoch == <NUM_LIT> and rank == <NUM_LIT> : \n checkpoint_suffix = \"<STR_LIT>\" . format ( \n global_step if hps . if_latest == <NUM_LIT> else <NUM_LIT> \n ) \n save_checkpoint ( \n net_g , \n optim_g , \n hps . train . learning_rate , \n epoch , \n os . path . join ( hps . model_dir , \"<STR_LIT>\" + checkpoint_suffix ) , \n ) \n save_checkpoint ( \n net_d , \n optim_d , \n hps . train . learning_rate , \n epoch , \n os . path . join ( hps . model_dir , \"<STR_LIT>\" + checkpoint_suffix ) , \n ) \n if rank == <NUM_LIT> and hps . save_every_weights == \"<STR_LIT>\" : \n if hasattr ( net_g , \"<STR_LIT>\" ) : \n ckpt = net_g . module . state_dict ( ) \n else : \n ckpt = net_g . state_dict ( ) \n extract_model ( \n ckpt , \n hps . sample_rate , \n hps . if_f0 , \n hps . name , \n os . path . join ( \n hps . model_dir , \"<STR_LIT>\" . format ( hps . name , epoch , global_step ) \n ) , \n epoch , \n global_step , \n hps . version , \n hps , \n ) \n if hps . overtraining_detector == <NUM_LIT> : \n if lowest_value [ \"<STR_LIT>\" ] < last_loss_gen_all : \n epochs_since_last_lowest += <NUM_LIT> \n else : \n epochs_since_last_lowest = <NUM_LIT> \n if epochs_since_last_lowest >= hps . overtraining_threshold : \n print ( \n \"<STR_LIT>\" . format ( \n lowest_value [ \"<STR_LIT>\" ] , lowest_value [ \"<STR_LIT>\" ] , lowest_value [ \"<STR_LIT>\" ] \n ) \n ) \n os . _exit ( <NUM_LIT> ) \n if rank == <NUM_LIT> : \n if epoch > <NUM_LIT> : \n print ( hps . overtraining_threshold ) \n print ( \n f\"<STR_LIT>\" \n ) \n else : \n print ( \n f\"<STR_LIT>\" \n ) \n last_loss_gen_all = loss_gen_all \n if epoch >= hps . total_epoch and rank == <NUM_LIT> : \n print ( \n f\"<STR_LIT>\" \n ) \n print ( \n f\"<STR_LIT>\" \n ) \n pid_file_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n os . remove ( pid_file_path ) \n if hasattr ( net_g , \"<STR_LIT>\" ) : \n ckpt = net_g . module . state_dict ( ) \n else : \n ckpt = net_g . state_dict ( ) \n extract_model ( \n ckpt , \n hps . sample_rate , \n hps . if_f0 , \n hps . name , \n os . path . join (", "output": "hps . model_dir , \"<STR_LIT>\" . format ( hps . name , epoch , global_step ) \n ) , \n epoch , \n global_step , \n hps . version , \n hps , \n ) \n sleep ( <NUM_LIT> ) \n os . _exit ( <NUM_LIT> ) \n if __name__ == \"<STR_LIT>\" : \n torch . multiprocessing . set_start_method ( \"<STR_LIT>\" ) \n main ( )"}, {"input": "from __future__ import annotations \n import flask \n import svcs \n bp = flask . Blueprint ( \"<STR_LIT>\" , __name__ ) \n @ bp . get ( \"<STR_LIT>\" ) \n def healthy ( ) -> flask . ResponseValue : \n ok : list [ str ] = [ ] \n failing : list [ dict [ str , str ] ] = [ ] \n code = <NUM_LIT> \n for svc in svcs . flask . get_pings ( ) : \n try : \n svc . ping ( ) \n ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } )", "output": "code = <NUM_LIT> \n return { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } , code"}, {"input": "import random \n import spacy \n from densely_captioned_images . dataset . utils import get_clip_token_length \n _loaded_en_nlp = None \n ADJ_ANTONYM_RATE = <NUM_LIT> \n def get_english ( ) : \n global _loaded_en_nlp \n if _loaded_en_nlp is None : \n _loaded_en_nlp = spacy . load ( \"<STR_LIT>\" ) \n return _loaded_en_nlp \n def get_wordnet ( ) : \n from nltk . corpus import wordnet \n return wordnet \n def get_antonyms ( phrase ) : \n wordnet = get_wordnet ( ) \n antonyms = [ ] \n for syn in wordnet . synsets ( phrase ) : \n for l in syn . lemmas ( ) :", "output": "if l . antonyms ( ) : \n antonyms . append ( l . antonyms ( ) [ <NUM_LIT> ] . name ( ) ) \n return list ( set ( antonyms ) ) \n def levenshtein_distance ( s1 , s2 ) : \n if len ( s1 ) > len ( s2 ) : \n s1 , s2 = s2 , s1 \n distances = range ( len ( s1 ) + <NUM_LIT> ) \n for i2 , c2 in enumerate ( s2 ) : \n distances_ = [ i2 + <NUM_LIT> ] \n for i1 , c1 in enumerate ( s1 ) : \n if c1 == c2 : \n distances_ . append ( distances [ i1 ] ) \n else : \n distances_ . append ( <NUM_LIT> + min ( ( distances [ i1 ] , distances [ i1 + <NUM_LIT> ] , distances_ [ - <NUM_LIT> ] ) ) ) \n distances = distances_ \n return distances [ - <NUM_LIT> ] \n def different_enough ( s1 , s2 ) : \n if s1 . lower ( ) in s2 . lower ( ) or s2 . lower ( ) in s1 . lower ( ) : \n return False \n return levenshtein_distance ( s1 , s2 ) / min ( len ( s1 ) , len ( s2 ) ) > <NUM_LIT> \n def process_swaps_single_pass ( in_text , swaps ) : \n chunks = { } \n starts = [ ] \n ends = [ ] \n for ( ( s1 , e1 ) , tuple_or_word ) in swaps : \n if isinstance ( tuple_or_word , str ) : \n chunks [ ( s1 , e1 ) ] = tuple_or_word \n starts . append ( s1 ) \n ends . append ( e1 ) \n else : \n s2 , e2 = tuple_or_word \n chunks [ ( s1 , e1 ) ] = in_text [ s2 : e2 ] \n chunks [ ( s2 , e2 ) ] = in_text [ s1 : e1 ] \n starts += [ s1 , s2 ] \n ends += [ e1 , e2 ] \n starts . sort ( ) \n ends . sort ( ) \n if starts [ <NUM_LIT> ] != <NUM_LIT> : \n ends = [ starts [ <NUM_LIT> ] ] + ends \n starts = [ <NUM_LIT> ] + starts \n if ends [ - <NUM_LIT> ] != len ( in_text ) : \n starts = starts + [ ends [ - <NUM_LIT> ] ] \n ends = ends + [ len ( in_text ) ] \n for s in starts : \n if s != <NUM_LIT> and s not in ends : \n ends . append ( s ) \n for e in ends : \n if e != len ( in_text ) and e not in starts : \n starts . append ( e ) \n starts . sort ( ) \n ends . sort ( ) \n res_text = \"<STR_LIT>\" \n for ( s , e ) in zip ( starts , ends ) : \n if ( s , e ) in chunks : \n to_add = chunks [ ( s , e ) ] \n else : \n to_add = in_text [ s : e ] \n res_text += to_add \n return res_text \n def get_spacy_negative ( in_text , swap_count = <NUM_LIT> , use_antonyms = False ) : \n nlp = get_english ( ) \n out_text = in_text \n doc = nlp ( out_text ) \n def get_possible_swaps ( swap_source , max_swaps = <NUM_LIT> ) : \n used_elems = set ( ) \n swaps = [ ] \n while len ( swaps ) < max_swaps and len ( swap_source ) > <NUM_LIT> : \n elem = swap_source . pop ( <NUM_LIT> ) \n if elem . text in used_elems : \n continue \n used_elems . add ( elem . text ) \n if use_antonyms and hasattr ( elem , '<STR_LIT>' ) and elem . pos_ == \"<STR_LIT>\" and random . random ( ) < ADJ_ANTONYM_RATE : \n antonyms = get_antonyms ( elem . text ) \n if len ( antonyms ) > <NUM_LIT> : \n swaps . append ( ( ( elem . idx , elem . idx + len ( elem ) ) , random . choice ( antonyms ) ) ) \n continue \n possible_swaps = [ n for n in swap_source if different_enough ( n . text , elem . text ) and n . text not in used_elems ] \n if len ( possible_swaps ) == <NUM_LIT> : \n continue \n swap_elem = random . choice ( possible_swaps ) \n used_elems . add ( swap_elem . text ) \n try : \n swaps . append ( ( ( elem . start_char , elem . end_char ) , ( swap_elem . start_char , swap_elem . end_char ) ) ) \n except : \n swaps . append ( ( \n ( elem . idx , elem . idx + len ( elem ) ) , \n ( swap_elem . idx , swap_elem . idx + len ( swap_elem ) ) \n ) ) \n return swaps \n noun_phrases = [ noun_phrase for noun_phrase in doc . noun_chunks ] \n random . shuffle ( noun_phrases ) \n swaps = get_possible_swaps ( noun_phrases , swap_count ) \n if len ( swaps ) > <NUM_LIT> : \n out_text = process_swaps_single_pass ( out_text , swaps ) \n doc = nlp ( out_text ) \n adjectives = [ tok for tok in doc if tok . pos_ == \"<STR_LIT>\" ] \n random . shuffle ( adjectives ) \n swaps = get_possible_swaps ( adjectives , swap_count ) \n verbs = [ tok for tok in doc if tok . pos_ == \"<STR_LIT>\" ] \n random . shuffle ( verbs ) \n swaps += get_possible_swaps ( verbs , swap_count ) \n if len ( swaps ) == <NUM_LIT> : \n return out_text \n out_text = process_swaps_single_pass ( out_text , swaps ) \n if get_clip_token_length ( out_text ) >= <NUM_LIT> : \n return get_spacy_negative ( in_text , swap_count = swap_count , use_antonyms = False ) \n return out_text"}, {"input": "from concurrent . futures import ThreadPoolExecutor \n import io \n import requests \n import telebot \n from common import const \n from common . log import logger \n from channel . channel import Channel \n from config import channel_conf_val , channel_conf \n bot = telebot . TeleBot ( token = channel_conf ( const . TELEGRAM ) . get ( '<STR_LIT>' ) ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ bot . message_handler ( commands = [ '<STR_LIT>' ] ) \n def send_welcome ( message ) : \n bot . send_message ( message . chat . id , \"<STR_LIT>\" , parse_mode = \"<STR_LIT>\" ) \n @ bot . message_handler ( content_types = [ '<STR_LIT>' ] ) \n def send_welcome ( msg ) : \n TelegramChannel ( ) . handle ( msg ) \n class TelegramChannel ( Channel ) : \n def __init__ ( self ) : \n pass \n def startup ( self ) : \n logger . info ( \"<STR_LIT>\" ) \n bot . infinity_polling ( ) \n def handle ( self , msg ) : \n logger . debug ( \"<STR_LIT>\" + msg . text ) \n img_match_prefix = self . check_prefix ( msg , channel_conf_val ( const . TELEGRAM , '<STR_LIT>' ) ) \n if img_match_prefix : \n thread_pool . submit ( self . _do_send_img , msg , str ( msg . chat . id ) ) \n else : \n thread_pool . submit ( self . _dosend , msg . text , msg ) \n def _dosend ( self , query , msg ) : \n context = dict ( ) \n context [ '<STR_LIT>' ] = str ( msg . chat . id ) \n reply_text = super ( ) . build_reply_content ( query , context ) \n logger . info ( '<STR_LIT>' . format ( reply_text ) ) \n bot . reply_to ( msg , reply_text ) \n def _do_send_img ( self , msg , reply_user_id ) : \n try : \n if not msg : \n return \n context = dict ( ) \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n img_urls = super ( ) . build_reply_content ( msg . text , context ) \n if not img_urls : \n return \n if not isinstance ( img_urls , list ) : \n bot . reply_to ( msg , img_urls ) \n return \n for url in img_urls : \n pic_res = requests . get ( url , stream = True ) \n image_storage = io . BytesIO ( )", "output": "for block in pic_res . iter_content ( <NUM_LIT> ) : \n image_storage . write ( block ) \n image_storage . seek ( <NUM_LIT> ) \n logger . info ( '<STR_LIT>' . format ( reply_user_id ) ) \n bot . send_photo ( msg . chat . id , image_storage ) \n except Exception as e : \n logger . exception ( e ) \n def check_prefix ( self , msg , prefix_list ) : \n if not prefix_list : \n return None \n for prefix in prefix_list : \n if msg . text . startswith ( prefix ) : \n return prefix \n return None"}, {"input": "from typing import Optional \n from flask_login import UserMixin \n from itsdangerous import URLSafeTimedSerializer as Serializer , SignatureExpired , BadSignature \n from os import environ \n from . base import db \n DEFAULT_PROFILE_PICTURE = '<STR_LIT>' \n class User ( db . Model , UserMixin ) :", "output": "__tablename__ = '<STR_LIT>' \n id = db . Column ( db . Integer , primary_key = True ) \n username = db . Column ( db . String ( <NUM_LIT> ) , unique = True , nullable = False ) \n email = db . Column ( db . String , unique = True , nullable = False ) \n password = db . Column ( db . String ( <NUM_LIT> ) , nullable = False ) \n profile_picture = db . Column ( db . String ( <NUM_LIT> ) , nullable = False , default = DEFAULT_PROFILE_PICTURE ) \n messages = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True ) \n allowed_channels = db . relationship ( '<STR_LIT>' , backref = '<STR_LIT>' , lazy = True ) \n def __repr__ ( self ) -> str : \n return f\"<STR_LIT>\" \n def generate_api_token ( self , expiration : int = <NUM_LIT> ) -> str : \n s = Serializer ( environ [ '<STR_LIT>' ] , expires_in = expiration , salt = '<STR_LIT>' ) \n return s . dumps ( { '<STR_LIT>' : self . id } ) . decode ( '<STR_LIT>' ) \n @ staticmethod \n def verify_api_token ( token : str ) -> Optional [ '<STR_LIT>' ] : \n s = Serializer ( environ [ '<STR_LIT>' ] , salt = '<STR_LIT>' ) \n try : \n data = s . loads ( token ) \n except SignatureExpired : \n return None \n except BadSignature : \n return None \n user = User . query . get ( data [ '<STR_LIT>' ] ) \n return user"}, {"input": "from datetime import datetime , timezone \n from typing import Any , Optional , Tuple , List \n from werkzeug . wrappers import Response \n from flask import jsonify , url_for , redirect , flash \n from flask_login import current_user \n from app . models import db , ChannelAllowList , User , Channel \n from app . models . channel_allowlist import UserRole \n from app . bcrypt . utils import hash_password , check_hashed_password \n from app . forms . channel import UpdateChannelForm , AddChannelForm , JoinChannelForm \n def convert_time_to_string ( time : datetime ) -> str : \n return time . replace ( tzinfo = timezone . utc ) . astimezone ( ) . strftime ( '<STR_LIT>' ) \n def get_messages ( channel_name : str , counter : int ) -> Any : \n num_messages_loaded_at_once = <NUM_LIT> \n channel = Channel . query . filter_by ( name = channel_name ) . first ( ) \n messages = channel . messages \n messages_response = [ \n { \n '<STR_LIT>' : User . query . filter_by ( id = message . author_id ) . first ( ) . username , \n '<STR_LIT>' : f\"<STR_LIT>\" \n f\"<STR_LIT>\" , \n '<STR_LIT>' : message . content , \n '<STR_LIT>' : convert_time_to_string ( message . time ) \n } \n for message in messages [ max ( counter - num_messages_loaded_at_once , <NUM_LIT> ) : counter ] \n ] \n return jsonify ( { '<STR_LIT>' : messages_response } ) \n def is_valid_channel ( channel : Optional [ Channel ] , form : AddChannelForm ) -> bool : \n if isinstance ( channel , Channel ) : \n return check_hashed_password ( channel . password , form . password . data ) \n else : \n return False \n def process_add_channel_form ( form : AddChannelForm ) -> Response : \n hashed_password = hash_password ( form . password . data ) \n db . session . add ( Channel ( \n name = form . name . data , password = hashed_password \n ) ) \n channel_id = Channel . query . filter_by ( password = hashed_password ) . first ( ) . id \n db . session . add ( ChannelAllowList ( \n channel_id = channel_id , user_id = current_user . id , user_role = UserRole . ADMIN . value \n ) ) \n db . session . commit ( ) \n flash ( f'<STR_LIT>' , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n def process_join_channel_form ( form : JoinChannelForm ) -> str : \n channel = Channel . query . filter_by ( name = form . name . data ) . first ( ) \n if is_valid_channel ( channel , form ) : \n db . session . add ( ChannelAllowList ( \n channel_id = channel . id , user_id = current_user . id \n ) ) \n db . session . commit ( ) \n flash ( f'<STR_LIT>' , '<STR_LIT>' ) \n else : \n flash ( '<STR_LIT>' , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n def get_number_of_channels_users ( channel : Channel ) -> int : \n return ChannelAllowList . query . filter_by ( channel_id = channel . id ) . count ( ) \n def get_number_of_channels_messages ( channel : Channel ) -> int : \n return len ( channel . messages ) \n def get_channels_users ( channel : Channel ) -> List [ User ] : \n channel_allowed_records = ChannelAllowList . query . filter_by ( channel_id = channel . id ) . all ( ) \n return [ User . query . get ( record . user_id ) for record in channel_allowed_records ] \n def is_admin ( channel : Channel , user : User ) -> bool : \n allowed_record = ( ChannelAllowList . query . filter_by ( channel_id = channel . id ) \n . filter_by ( user_id = user . id ) . first ( ) ) \n return allowed_record and allowed_record . user_role == UserRole . ADMIN . value \n def check_channel_settings_form ( channel_id : str , user_id : str ) -> Optional [ Tuple [ Channel , User ] ] : \n if not channel_id or not user_id : \n return None \n try : \n channel_id = int ( channel_id ) \n user_id = int ( user_id ) \n except ValueError :", "output": "return None \n channel = Channel . query . get ( channel_id ) \n if not channel : \n return None \n user = User . query . get ( user_id ) \n if not ( user and is_admin ( channel , current_user ) ) : \n return None \n else : \n return channel , user \n def admin_manager ( command : str , channel_id : str , user_id : str ) : \n checked_value = check_channel_settings_form ( channel_id , user_id ) \n if not checked_value : \n return admin_invalid ( ) \n else : \n channel , user = checked_value \n allow_record = ( ChannelAllowList . query . filter_by ( channel_id = channel_id ) \n . filter_by ( user_id = user_id ) . first ( ) ) \n if not allow_record : \n return admin_invalid ( ) \n if command == '<STR_LIT>' : \n allow_record . user_role = UserRole . ADMIN . value \n db . session . commit ( ) \n message = '<STR_LIT>' \n elif command == '<STR_LIT>' : \n allow_record . user_role = UserRole . NORMAL_USER . value \n db . session . commit ( ) \n message = '<STR_LIT>' \n else : \n return admin_invalid ( ) \n flash ( f'<STR_LIT>' , '<STR_LIT>' ) \n return redirect ( f'<STR_LIT>' ) \n def admin_invalid ( ) -> str : \n flash ( \"<STR_LIT>\" , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) ) \n def no_channel ( ) -> str : \n flash ( \"<STR_LIT>\" , '<STR_LIT>' ) \n return redirect ( url_for ( '<STR_LIT>' ) )"}, {"input": "from typing import Dict , List , Optional \n import torch \n import torch . nn as nn \n from transformers import ClapModel , ClapProcessor \n from multi_token . data_tools import load_audio \n from multi_token . modalities . base_modality import Modality \n from multi_token . modalities . projectors import ( \n build_mlp_vector_projector , \n ) \n OUTPUT_EMB_SIZE = <NUM_LIT> \n class CLAPAudioModule ( nn . Module ) : \n def __init__ ( self , model_name_or_path : str ) : \n super ( ) . __init__ ( ) \n self . model_name_or_path = model_name_or_path \n self . model = None \n self . processor = None \n self . load_model ( ) \n def load_model ( self ) : \n self . model = ClapModel . from_pretrained ( self . model_name_or_path ) \n self . processor = ClapProcessor . from_pretrained ( self . model_name_or_path ) \n self . model . requires_grad_ ( False ) \n @ torch . no_grad ( ) \n def forward ( self , audios ) -> torch . Tensor : \n embs = [ ] \n for audio_features in audios : \n features = self . model . get_audio_features ( \n input_features = audio_features [ \"<STR_LIT>\" ] . to ( torch . float32 ) , \n is_longer = audio_features [ \"<STR_LIT>\" ] , \n ) \n embs . append ( features ) \n embs = torch . stack ( embs ) \n return embs . view ( - <NUM_LIT> , <NUM_LIT> , OUTPUT_EMB_SIZE ) \n @ property \n def dtype ( self ) : \n return self . model . dtype \n @ property \n def device ( self ) : \n return self . model . device \n class CLAPAudioModality ( Modality ) : \n def __init__ ( \n self , \n model_name_or_path : str = \"<STR_LIT>\" , \n num_projector_layers : int = <NUM_LIT> , \n num_tokens_output : int = <NUM_LIT> , \n ) : \n self . model_name_or_path = model_name_or_path \n self . module = CLAPAudioModule ( model_name_or_path = self . model_name_or_path ) \n self . num_projector_layers = num_projector_layers \n self . num_tokens_output = num_tokens_output \n self . dtype = torch . float32 \n def build_projector ( self , lm_hidden_size : int ) -> nn . Module : \n return build_mlp_vector_projector ( \n input_hidden_size = OUTPUT_EMB_SIZE , \n lm_hidden_size = lm_hidden_size , \n num_layers = self . num_projector_layers , \n num_tokens = self . num_tokens_output , \n ) \n @ property \n def name ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def data_key ( self ) -> str : \n return \"<STR_LIT>\" \n @ property \n def token_width ( self ) -> int : \n return self . num_tokens_output", "output": "def to ( self , dtype : torch . dtype , device : torch . device ) -> \"<STR_LIT>\" : \n self . dtype = dtype \n self . module . to ( device = device ) \n return self \n def preprocess_rows ( self , rows : List [ Dict ] ) -> List [ Optional [ Dict ] ] : \n row_values = [ ] \n for row in rows : \n audios = [ ] \n for audio_dict in row [ self . data_key ] : \n audio_dict = load_audio ( \n audio_dict , \n target_sampling_rate = self . module . processor . feature_extractor . sampling_rate , \n ) \n audio_processed = self . module . processor ( \n audios = audio_dict [ \"<STR_LIT>\" ] , \n return_tensors = \"<STR_LIT>\" , \n sampling_rate = audio_dict [ \"<STR_LIT>\" ] , \n ) \n audios . append ( audio_processed ) \n row_values . append ( audios ) \n return row_values \n @ torch . no_grad ( ) \n def forward ( self , encoded_values : List [ torch . Tensor ] ) -> List [ torch . Tensor ] : \n audio_features = [ ] \n for audio_batch in encoded_values : \n audio_features . append ( self . module . forward ( audio_batch ) . to ( dtype = self . dtype ) ) \n return audio_features"}, {"input": "from secrets import token_hex \n from os import path , remove \n from PIL import Image \n from flask_login import current_user \n from typing import Optional , Final \n from werkzeug . datastructures import FileStorage \n from app . path import APP_PATH \n from app . bcrypt . utils import hash_password , check_hashed_password \n from app . forms . registration import RegistrationForm \n from app . forms . login import LoginForm \n from app . models import db , ChannelAllowList , Message \n from app . models . user import User , DEFAULT_PROFILE_PICTURE \n IMAGE_SIDE_SIZE : Final = <NUM_LIT> \n def add_user ( form : RegistrationForm ) -> None : \n hashed_password = hash_password ( form . password . data )", "output": "db . session . add ( User ( \n username = form . username . data , email = form . email . data , password = hashed_password \n ) ) \n db . session . commit ( ) \n def is_valid_user ( user : Optional [ User ] , form : LoginForm ) -> bool : \n if isinstance ( user , User ) : \n return check_hashed_password ( user . password , form . password . data ) \n else : \n return False \n def get_number_of_all_messages ( ) -> int : \n number_of_all_messages : int = Message . query . filter_by ( author_id = current_user . id ) . count ( ) \n return number_of_all_messages \n def get_number_of_all_channels ( ) -> int : \n number_of_all_channels : int = ChannelAllowList . query . filter_by ( user_id = current_user . id ) . count ( ) \n return number_of_all_channels \n def update_user ( username : str , email : str ) -> None : \n current_user . username = username \n current_user . email = email \n db . session . commit ( ) \n def get_profile_picture_full_path ( profile_picture_filename : str ) -> str : \n return path . join ( APP_PATH , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , profile_picture_filename ) \n def remove_old_profile_picture ( ) -> None : \n if current_user . profile_picture != DEFAULT_PROFILE_PICTURE : \n old_profile_picture_path = get_profile_picture_full_path ( current_user . profile_picture ) \n remove ( old_profile_picture_path ) \n def make_square ( image : Image ) -> Image : \n x , y = image . size \n fill_color = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n new_image = Image . new ( '<STR_LIT>' , ( IMAGE_SIDE_SIZE , IMAGE_SIDE_SIZE ) , fill_color ) \n new_image . paste ( image , ( int ( ( IMAGE_SIDE_SIZE - x ) / <NUM_LIT> ) , int ( ( IMAGE_SIDE_SIZE - y ) / <NUM_LIT> ) ) ) \n return new_image \n def save_profile_picture ( picture : FileStorage ) -> str : \n random_hex = token_hex ( <NUM_LIT> ) \n _ , file_extension = path . splitext ( picture . filename ) \n assert isinstance ( file_extension , str ) \n picture_filename = random_hex + file_extension \n picture_path = get_profile_picture_full_path ( picture_filename ) \n output_size = ( IMAGE_SIDE_SIZE , IMAGE_SIDE_SIZE ) \n image = Image . open ( picture ) \n image . thumbnail ( output_size ) \n square_image = make_square ( image ) \n square_image . save ( picture_path ) \n return picture_filename"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' ) \n def downgrade ( ) -> None :", "output": "op . alter_column ( '<STR_LIT>' , '<STR_LIT>' , new_column_name = '<STR_LIT>' )"}, {"input": "from . import * \n import os \n import concurrent . futures \n from flask import request , abort , jsonify , render_template_string \n from urllib . parse import unquote_plus \n from mod import search , lrc \n from mod import searchx \n from mod import tools \n from mod import tag \n from mod . auth import webui \n from mod . auth . authentication import require_auth \n def read_file_with_encoding ( file_path , encodings ) : \n for encoding in encodings : \n try : \n with open ( file_path , '<STR_LIT>' , encoding = encoding ) as f : \n return f . read ( ) \n except UnicodeDecodeError : \n continue \n return None \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) \n def lyrics ( ) : \n match require_auth ( request = request ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n if not bool ( request . args ) : \n abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n path = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n if path : \n lrc_path = os . path . splitext ( path ) [ <NUM_LIT> ] + '<STR_LIT>' \n if os . path . isfile ( lrc_path ) : \n file_content = read_file_with_encoding ( lrc_path , [ '<STR_LIT>' , '<STR_LIT>' ] ) \n if file_content is not None :", "output": "return lrc . standard ( file_content ) \n try : \n lrc_in = tag . read ( path ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if type ( lrc_in ) is str and len ( lrc_in ) > <NUM_LIT> : \n return lrc_in \n except : \n pass \n try : \n title = unquote_plus ( request . args . get ( '<STR_LIT>' ) ) \n artist = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n album = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n result : list = searchx . search_all ( title = title , artist = artist , album = album , timeout = <NUM_LIT> ) \n if not result [ <NUM_LIT> ] . get ( '<STR_LIT>' ) : \n return \"<STR_LIT>\" , <NUM_LIT> \n return result [ <NUM_LIT> ] . get ( '<STR_LIT>' ) \n except : \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) \n def lrc_json ( ) : \n match require_auth ( request = request ) : \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n case - <NUM_LIT> : \n return render_template_string ( webui . error ( ) ) , <NUM_LIT> \n if not bool ( request . args ) : \n abort ( <NUM_LIT> , \"<STR_LIT>\" ) \n path = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n title = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n artist = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n album = unquote_plus ( request . args . get ( '<STR_LIT>' , '<STR_LIT>' ) ) \n response = [ ] \n if path : \n lrc_path = os . path . splitext ( path ) [ <NUM_LIT> ] + '<STR_LIT>' \n if os . path . isfile ( lrc_path ) : \n file_content = read_file_with_encoding ( lrc_path , [ '<STR_LIT>' , '<STR_LIT>' ] ) \n if file_content is not None : \n file_content = lrc . standard ( file_content ) \n response . append ( { \n \"<STR_LIT>\" : tools . calculate_md5 ( file_content ) , \n \"<STR_LIT>\" : title , \n \"<STR_LIT>\" : artist , \n \"<STR_LIT>\" : file_content \n } ) \n lyrics_list = searchx . search_all ( title , artist , album ) \n if lyrics_list : \n for i in lyrics_list : \n if not i : \n continue \n i [ '<STR_LIT>' ] = lrc . standard ( i [ '<STR_LIT>' ] ) \n response . append ( i ) \n _response = jsonify ( response ) \n _response . headers [ '<STR_LIT>' ] = '<STR_LIT>' \n return jsonify ( response )"}, {"input": "import os \n import sys \n import numpy as np \n import pyworld \n import torchcrepe \n import torch \n import parselmouth \n import tqdm \n from multiprocessing import Process , cpu_count \n current_directory = os . getcwd ( ) \n sys . path . append ( current_directory ) \n from rvc . lib . utils import load_audio \n exp_dir = sys . argv [ <NUM_LIT> ] \n f0_method = sys . argv [ <NUM_LIT> ] \n num_processes = cpu_count ( ) \n try : \n hop_length = int ( sys . argv [ <NUM_LIT> ] ) \n except ValueError : \n hop_length = <NUM_LIT> \n DoFormant = False \n Quefrency = <NUM_LIT> \n Timbre = <NUM_LIT> \n class FeatureInput : \n def __init__ ( self , sample_rate = <NUM_LIT> , hop_size = <NUM_LIT> ) : \n self . fs = sample_rate \n self . hop = hop_size \n self . f0_method_dict = self . get_f0_method_dict ( ) \n self . f0_bin = <NUM_LIT> \n self . f0_max = <NUM_LIT> \n self . f0_min = <NUM_LIT> \n self . f0_mel_min = <NUM_LIT> * np . log ( <NUM_LIT> + self . f0_min / <NUM_LIT> ) \n self . f0_mel_max = <NUM_LIT> * np . log ( <NUM_LIT> + self . f0_max / <NUM_LIT> ) \n def mncrepe ( self , method , x , p_len , hop_length ) : \n f0 = None \n torch_device_index = <NUM_LIT> \n torch_device = ( \n torch . device ( f\"<STR_LIT>\" ) \n if torch . cuda . is_available ( ) \n else ( \n torch . device ( \"<STR_LIT>\" ) \n if torch . backends . mps . is_available ( ) \n else torch . device ( \"<STR_LIT>\" ) \n ) \n ) \n audio = torch . from_numpy ( x . astype ( np . float32 ) ) . to ( torch_device , copy = True ) \n audio /= torch . quantile ( torch . abs ( audio ) , <NUM_LIT> ) \n audio = torch . unsqueeze ( audio , dim = <NUM_LIT> ) \n if audio . ndim == <NUM_LIT> and audio . shape [ <NUM_LIT> ] > <NUM_LIT> : \n audio = torch . mean ( audio , dim = <NUM_LIT> , keepdim = True ) . detach ( ) \n audio = audio . detach ( ) \n if method == \"<STR_LIT>\" : \n pitch = torchcrepe . predict ( \n audio , \n self . fs , \n hop_length , \n self . f0_min , \n self . f0_max , \n \"<STR_LIT>\" , \n batch_size = hop_length * <NUM_LIT> , \n device = torch_device , \n pad = True , \n ) \n p_len = p_len or x . shape [ <NUM_LIT> ] // hop_length \n source = np . array ( pitch . squeeze ( <NUM_LIT> ) . cpu ( ) . float ( ) . numpy ( ) ) \n source [ source < <NUM_LIT> ] = np . nan \n target = np . interp ( \n np . arange ( <NUM_LIT> , len ( source ) * p_len , len ( source ) ) / p_len , \n np . arange ( <NUM_LIT> , len ( source ) ) , \n source , \n ) \n f0 = np . nan_to_num ( target ) \n return f0 \n def get_pm ( self , x , p_len ) : \n f0 = ( \n parselmouth . Sound ( x , self . fs ) \n . to_pitch_ac ( \n time_step = <NUM_LIT> / <NUM_LIT> , \n voicing_threshold = <NUM_LIT> , \n pitch_floor = self . f0_min , \n pitch_ceiling = self . f0_max , \n ) \n . selected_array [ \"<STR_LIT>\" ] \n ) \n return np . pad ( \n f0 , \n [ \n [ \n max ( <NUM_LIT> , ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> ) , \n max ( <NUM_LIT> , p_len - len ( f0 ) - ( p_len - len ( f0 ) + <NUM_LIT> ) // <NUM_LIT> ) , \n ] \n ] , \n mode = \"<STR_LIT>\" , \n ) \n def get_harvest ( self , x ) : \n f0_spectral = pyworld . harvest ( \n x . astype ( np . double ) , \n fs = self . fs , \n f0_ceil = self . f0_max , \n f0_floor = self . f0_min , \n frame_period = <NUM_LIT> * self . hop / self . fs , \n ) \n return pyworld . stonemask ( x . astype ( np . double ) , * f0_spectral , self . fs ) \n def get_dio ( self , x ) : \n f0_spectral = pyworld . dio ( \n x . astype ( np . double ) , \n fs = self . fs , \n f0_ceil = self . f0_max , \n f0_floor = self . f0_min , \n frame_period = <NUM_LIT> * self . hop / self . fs , \n ) \n return pyworld . stonemask ( x . astype ( np . double ) , * f0_spectral , self . fs ) \n def get_rmvpe ( self , x ) : \n if not hasattr ( self , \"<STR_LIT>\" ) : \n from rvc . lib . rmvpe import RMVPE \n self . model_rmvpe = RMVPE ( \"<STR_LIT>\" , is_half = False , device = \"<STR_LIT>\" ) \n return self . model_rmvpe . infer_from_audio ( x , thred = <NUM_LIT> ) \n def get_f0_method_dict ( self ) : \n return { \n \"<STR_LIT>\" : self . get_pm , \n \"<STR_LIT>\" : self . get_harvest , \n \"<STR_LIT>\" : self . get_dio , \n \"<STR_LIT>\" : self . get_rmvpe , \n } \n def compute_f0 ( self , path , f0_method , hop_length ) : \n x = load_audio ( path , self . fs ) \n p_len = x . shape [ <NUM_LIT> ] // self . hop \n if f0_method in self . f0_method_dict : \n f0 = ( \n self . f0_method_dict [ f0_method ] ( x , p_len ) \n if f0_method == \"<STR_LIT>\" \n else self . f0_method_dict [ f0_method ] ( x ) \n ) \n elif f0_method == \"<STR_LIT>\" : \n f0 = self . mncrepe ( f0_method , x , p_len , hop_length ) \n return f0 \n def coarse_f0 ( self , f0 ) : \n f0_mel = <NUM_LIT> * np . log ( <NUM_LIT> + f0 / <NUM_LIT> ) \n f0_mel [ f0_mel > <NUM_LIT> ] = ( f0_mel [ f0_mel > <NUM_LIT> ] - self . f0_mel_min ) * ( \n self . f0_bin - <NUM_LIT> \n ) / ( self . f0_mel_max - self . f0_mel_min ) + <NUM_LIT> \n f0_mel [ f0_mel <= <NUM_LIT> ] = <NUM_LIT> \n f0_mel [ f0_mel > self . f0_bin - <NUM_LIT> ] = self . f0_bin - <NUM_LIT> \n f0_coarse = np . rint ( f0_mel ) . astype ( int ) \n assert f0_coarse . max ( ) <= <NUM_LIT> and f0_coarse . min ( ) >= <NUM_LIT> , ( \n f0_coarse . max ( ) , \n f0_coarse . min ( ) , \n ) \n return f0_coarse \n def process_paths ( self , paths , f0_method , hop_length , thread_n ) : \n if len ( paths ) == <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n return \n with tqdm . tqdm ( total = len ( paths ) , leave = True , position = thread_n ) as pbar : \n description = f\"<STR_LIT>\" \n pbar . set_description ( description ) \n for idx , ( inp_path , opt_path1 , opt_path2 ) in enumerate ( paths ) : \n try : \n if os . path . exists ( opt_path1 + \"<STR_LIT>\" ) and os . path . exists ( \n opt_path2 + \"<STR_LIT>\" \n ) : \n pbar . update ( <NUM_LIT> ) \n continue \n feature_pit = self . compute_f0 ( inp_path , f0_method , hop_length ) \n np . save ( \n opt_path2 , \n feature_pit , \n allow_pickle = False , \n ) \n coarse_pit = self . coarse_f0 ( feature_pit ) \n np . save ( \n opt_path1 , \n coarse_pit , \n allow_pickle = False , \n ) \n pbar . update ( <NUM_LIT> ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n if __name__ == \"<STR_LIT>\" : \n feature_input = FeatureInput ( ) \n paths = [ ] \n input_root = f\"<STR_LIT>\" \n output_root1 = f\"<STR_LIT>\" \n output_root2 = f\"<STR_LIT>\" \n os . makedirs ( output_root1 , exist_ok = True ) \n os . makedirs ( output_root2 , exist_ok = True ) \n for name in sorted ( list ( os . listdir ( input_root ) ) ) :", "output": "input_path = f\"<STR_LIT>\" \n if \"<STR_LIT>\" in input_path : \n continue \n output_path1 = f\"<STR_LIT>\" \n output_path2 = f\"<STR_LIT>\" \n paths . append ( [ input_path , output_path1 , output_path2 ] ) \n processes = [ ] \n print ( \"<STR_LIT>\" + f0_method ) \n for i in range ( num_processes ) : \n p = Process ( \n target = feature_input . process_paths , \n args = ( paths [ i : : num_processes ] , f0_method , hop_length , i ) , \n ) \n processes . append ( p ) \n p . start ( ) \n for i in range ( num_processes ) : \n processes [ i ] . join ( )"}, {"input": "import gradio as gr \n import sys \n import os \n import logging \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from tabs . inference . inference import inference_tab \n from tabs . train . train import train_tab \n from tabs . extra . extra import extra_tab \n from tabs . report . report import report_tab \n from tabs . download . download import download_tab \n from tabs . tts . tts import tts_tab \n from tabs . voice_blender . voice_blender import voice_blender_tab \n from tabs . settings . presence import presence_tab , load_config_presence \n from tabs . settings . flask_server import flask_server_tab \n from tabs . settings . fake_gpu import fake_gpu_tab , gpu_available , load_fake_gpu \n from tabs . settings . themes import theme_tab \n from tabs . plugins . plugins import plugins_tab \n from tabs . settings . version import version_tab \n from tabs . settings . lang import lang_tab \n from tabs . settings . restart import restart_tab \n import assets . themes . loadThemes as loadThemes \n from assets . i18n . i18n import I18nAuto \n import assets . installation_checker as installation_checker \n from assets . discord_presence import RPCManager \n from assets . flask . server import start_flask , load_config_flask \n from core import run_prerequisites_script \n run_prerequisites_script ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n i18n = I18nAuto ( ) \n if load_config_presence ( ) == True : \n RPCManager . start_presence ( ) \n installation_checker . check_installation ( ) \n logging . getLogger ( \"<STR_LIT>\" ) . disabled = True \n logging . getLogger ( \"<STR_LIT>\" ) . disabled = True \n if load_config_flask ( ) == True : \n print ( \"<STR_LIT>\" ) \n start_flask ( ) \n my_applio = loadThemes . load_json ( ) \n if my_applio : \n pass \n else : \n my_applio = \"<STR_LIT>\" \n with gr . Blocks ( theme = my_applio , title = \"<STR_LIT>\" ) as Applio : \n gr . Markdown ( \"<STR_LIT>\" ) \n gr . Markdown ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n gr . Markdown ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n inference_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n if gpu_available ( ) or load_fake_gpu ( ) : \n train_tab ( ) \n else : \n gr . Markdown ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n tts_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n voice_blender_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n plugins_tab ( )", "output": "with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n download_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n report_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n extra_tab ( ) \n with gr . Tab ( i18n ( \"<STR_LIT>\" ) ) : \n presence_tab ( ) \n flask_server_tab ( ) \n if not gpu_available ( ) : \n fake_gpu_tab ( ) \n theme_tab ( ) \n version_tab ( ) \n lang_tab ( ) \n restart_tab ( ) \n if __name__ == \"<STR_LIT>\" : \n port = <NUM_LIT> \n if \"<STR_LIT>\" in sys . argv : \n port_index = sys . argv . index ( \"<STR_LIT>\" ) + <NUM_LIT> \n if port_index < len ( sys . argv ) : \n port = int ( sys . argv [ port_index ] ) \n Applio . launch ( \n favicon_path = \"<STR_LIT>\" , \n share = \"<STR_LIT>\" in sys . argv , \n inbrowser = \"<STR_LIT>\" in sys . argv , \n server_port = port , \n )"}, {"input": "import requests \n import concurrent . futures \n import time \n url = \"<STR_LIT>\" \n params = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n queries = [ ] \n for i in range ( <NUM_LIT> ) : \n query = f\"<STR_LIT>\" \n queries . append ( query ) \n def make_request ( query ) : \n params [ \"<STR_LIT>\" ] = query \n print ( f\"<STR_LIT>\" ) \n response = requests . get ( url , params = params ) \n print ( response ) \n return response . text", "output": "start_time = time . time ( ) \n with concurrent . futures . ThreadPoolExecutor ( max_workers = <NUM_LIT> ) as executor : \n futures = [ executor . submit ( make_request , query ) for query in queries ] \n concurrent . futures . wait ( futures ) \n results = [ future . result ( ) for future in futures ] \n end_time = time . time ( ) \n print ( f\"<STR_LIT>\" )"}, {"input": "import torch \n from datetime import datetime \n def prettify_date ( date_str ) : \n date_time_obj = datetime . strptime ( date_str , \"<STR_LIT>\" ) \n return date_time_obj . strftime ( \"<STR_LIT>\" ) \n def model_information ( path ) : \n model_data = torch . load ( path , map_location = \"<STR_LIT>\" ) \n print ( f\"<STR_LIT>\" ) \n epochs = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n steps = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n sr = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n f0 = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" )", "output": "version = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n creation_date = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n model_hash = model_data . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n pitch_guidance = \"<STR_LIT>\" if f0 == <NUM_LIT> else \"<STR_LIT>\" \n return ( \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n f\"<STR_LIT>\" \n )"}, {"input": "edad1 = <NUM_LIT> \n class Persona :", "output": "def __init__ ( self , nombre , edad , altura ) : \n self . nombre = nombre \n self . edad = edad \n self . altura = altura \n def leer_edad ( self ) : \n return self . nombre + \"<STR_LIT>\" + str ( self . edad ) + \"<STR_LIT>\" \n jesus = Persona ( \"<STR_LIT>\" , <NUM_LIT> , <NUM_LIT> ) \n print ( jesus . leer_edad ( ) ) \n print ( jesus . altura ) \n mari = Persona ( \"<STR_LIT>\" , <NUM_LIT> , <NUM_LIT> ) \n print ( mari . leer_edad ( ) ) \n print ( mari . altura ) \n print ( \"<STR_LIT>\" ) \n class Perro : \n tipo = \"<STR_LIT>\" \n def __init__ ( self , nombre ) : \n self . nombre = nombre \n self . trucos = [ ] \n def aprender_truco ( self , truco ) : \n self . trucos . append ( truco ) \n puppy = Perro ( \"<STR_LIT>\" ) \n puppy . aprender_truco ( \"<STR_LIT>\" ) \n manchas = Perro ( \"<STR_LIT>\" ) \n manchas . aprender_truco ( \"<STR_LIT>\" ) \n print ( puppy . tipo ) \n print ( manchas . tipo ) \n print ( puppy . nombre ) \n print ( manchas . nombre ) \n print ( puppy . trucos ) \n print ( manchas . trucos )"}, {"input": "import sys \n if sys . version_info >= ( <NUM_LIT> , <NUM_LIT> ) : \n from typing import Annotated \n else : \n from typing_extensions import Annotated \n def nop ( * _ , ** __ ) : \n pass \n class CloseMe : \n is_aclosed = is_closed = False \n def close ( self ) : \n self . is_closed = True \n async def aclose ( self ) :", "output": "self . is_aclosed = True"}, {"input": "import asyncio \n from model . model import Model \n from config import model_conf_val , common_conf_val \n from common import log \n from EdgeGPT import Chatbot , ConversationStyle \n from ImageGen import ImageGen \n from common import functions \n from model . bing . jailbroken_sydney import SydneyBot \n user_session = dict ( ) \n suggestion_session = dict ( ) \n class BingModel ( Model ) : \n style = ConversationStyle . creative \n bot : Chatbot = None \n cookies : list = None \n def __init__ ( self ) : \n try : \n self . cookies = model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n self . jailbreak = model_conf_val ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n self . bot = SydneyBot ( cookies = self . cookies , options = { } ) if ( \n self . jailbreak ) else Chatbot ( cookies = self . cookies ) \n except Exception as e : \n log . warn ( e ) \n async def reply_text_stream ( self , query : str , context = None ) -> dict : \n async def handle_answer ( final , answer ) : \n if final : \n try : \n reply = self . build_source_attributions ( answer , context ) \n log . info ( \"<STR_LIT>\" , reply ) \n yield True , reply \n except Exception as e : \n log . warn ( answer ) \n log . warn ( e ) \n await user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) \n yield True , answer \n else : \n try : \n yield False , answer \n except Exception as e : \n log . warn ( answer ) \n log . warn ( e ) \n await user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) \n yield True , answer \n if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n clear_memory_commands = common_conf_val ( \n '<STR_LIT>' , [ '<STR_LIT>' ] ) \n if query in clear_memory_commands : \n user_session [ context [ '<STR_LIT>' ] ] = None \n yield True , '<STR_LIT>' \n bot = user_session . get ( context [ '<STR_LIT>' ] , None ) \n if not bot : \n bot = self . bot \n else : \n query = self . get_quick_ask_query ( query , context ) \n user_session [ context [ '<STR_LIT>' ] ] = bot \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n if self . jailbreak : \n async for final , answer in bot . ask_stream ( query , conversation_style = self . style , message_id = bot . user_message_id ) : \n async for result in handle_answer ( final , answer ) : \n yield result \n else : \n async for final , answer in bot . ask_stream ( query , conversation_style = self . style ) : \n async for result in handle_answer ( final , answer ) : \n yield result \n def reply ( self , query : str , context = None ) -> tuple [ str , dict ] : \n if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n clear_memory_commands = common_conf_val ( \n '<STR_LIT>' , [ '<STR_LIT>' ] ) \n if query in clear_memory_commands : \n user_session [ context [ '<STR_LIT>' ] ] = None \n return '<STR_LIT>' \n bot = user_session . get ( context [ '<STR_LIT>' ] , None ) \n if ( bot == None ) : \n bot = self . bot \n else : \n query = self . get_quick_ask_query ( query , context )", "output": "user_session [ context [ '<STR_LIT>' ] ] = bot \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n if ( self . jailbreak ) : \n task = bot . ask ( query , conversation_style = self . style , \n message_id = bot . user_message_id ) \n else : \n task = bot . ask ( query , conversation_style = self . style ) \n answer = asyncio . run ( task ) \n if isinstance ( answer , str ) : \n return answer \n try : \n reply = answer [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] \n except Exception as e : \n user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) \n log . warn ( answer ) \n return \"<STR_LIT>\" \n return self . build_source_attributions ( answer , context ) \n elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : \n if functions . contain_chinese ( query ) : \n return \"<STR_LIT>\" \n return self . create_img ( query ) \n def create_img ( self , query ) : \n try : \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n cookie_value = self . cookies [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n image_generator = ImageGen ( cookie_value ) \n img_list = image_generator . get_images ( query ) \n log . info ( \"<STR_LIT>\" . format ( img_list ) ) \n return img_list \n except Exception as e : \n log . warn ( e ) \n return \"<STR_LIT>\" \n def get_quick_ask_query ( self , query , context ) : \n if ( len ( query ) == <NUM_LIT> and query . isdigit ( ) and query != \"<STR_LIT>\" ) : \n suggestion_dict = suggestion_session [ context [ '<STR_LIT>' ] ] \n if ( suggestion_dict != None ) : \n query = suggestion_dict [ int ( query ) - <NUM_LIT> ] \n if ( query == None ) : \n return \"<STR_LIT>\" \n else : \n query = \"<STR_LIT>\" + query \n return query \n def build_source_attributions ( self , answer , context ) : \n reference = \"<STR_LIT>\" \n reply = answer [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ - <NUM_LIT> ] \n reply_text = reply [ \"<STR_LIT>\" ] \n if \"<STR_LIT>\" in reply : \n for i , attribution in enumerate ( reply [ \"<STR_LIT>\" ] ) : \n display_name = attribution [ \"<STR_LIT>\" ] \n url = attribution [ \"<STR_LIT>\" ] \n reference += f\"<STR_LIT>\" \n if len ( reference ) > <NUM_LIT> : \n reference = \"<STR_LIT>\" + reference \n suggestion = \"<STR_LIT>\" \n if \"<STR_LIT>\" in reply : \n suggestion_dict = dict ( ) \n for i , attribution in enumerate ( reply [ \"<STR_LIT>\" ] ) : \n suggestion_dict [ i ] = attribution [ \"<STR_LIT>\" ] \n suggestion += f\"<STR_LIT>\" \n suggestion_session [ context [ '<STR_LIT>' ] \n ] = suggestion_dict \n if len ( suggestion ) > <NUM_LIT> : \n suggestion = \"<STR_LIT>\" + suggestion \n throttling = answer [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n throttling_str = \"<STR_LIT>\" \n if throttling [ \"<STR_LIT>\" ] == throttling [ \"<STR_LIT>\" ] : \n user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) \n throttling_str = \"<STR_LIT>\" \n else : \n throttling_str = f\"<STR_LIT>\" \n response = f\"<STR_LIT>\" \n log . info ( \"<STR_LIT>\" , response ) \n return response \n else : \n user_session . get ( context [ '<STR_LIT>' ] , None ) . reset ( ) \n log . warn ( \"<STR_LIT>\" , answer ) \n return \"<STR_LIT>\""}, {"input": "from densely_captioned_images . repro . eval . ARO . dataset_zoo import VG_Relation , VG_Attribution , COCO_Order \n from densely_captioned_images . repro . config import COCO_DIR , ARO_DIR \n import os \n def run_downloads ( ) : \n if not os . path . exists ( ARO_DIR ) : \n os . makedirs ( ARO_DIR , exist_ok = True ) \n _ = VG_Relation ( image_preprocess = lambda x : x , download = True , root_dir = ARO_DIR ) \n _ = VG_Attribution ( image_preprocess = lambda x : x , download = True , root_dir = ARO_DIR ) \n if not os . path . exists ( COCO_DIR ) : \n raise Exception ( \"<STR_LIT>\" ) \n _ = COCO_Order ( image_preprocess = lambda x : x , download = True , root_dir = COCO_DIR )", "output": "if __name__ == '<STR_LIT>' : \n run_downloads ( )"}, {"input": "import logging \n import sys \n sys . path . append ( \"<STR_LIT>\" ) \n import unittest \n import os \n import click \n from fenjing import cli , waf_func_gen , options \n import tempfile \n SLEEP_INTERVAL = float ( os . environ . get ( \"<STR_LIT>\" , <NUM_LIT> ) ) \n VULUNSERVER_ADDR = os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n waf_func_gen . logger . setLevel ( logging . ERROR ) \n TEST_REQUEST = \n class TestCLI ( unittest . TestCase ) : \n def crack_test ( self , params ) : \n ctx = click . Context ( cli . crack ) \n ctx . params = { \n param . name : param . default \n for param in cli . crack . get_params ( ctx ) \n if param . name != \"<STR_LIT>\" \n } \n ctx . params . update ( params ) \n cli . crack . invoke ( ctx ) \n def crack_path_test ( self , params ) : \n ctx = click . Context ( cli . crack_path ) \n ctx . params = { \n param . name : param . default \n for param in cli . crack_path . get_params ( ctx ) \n if param . name != \"<STR_LIT>\" \n } \n ctx . params . update ( params ) \n cli . crack_path . invoke ( ctx ) \n def scan_test ( self , params ) : \n ctx = click . Context ( cli . scan ) \n ctx . params = { \n param . name : param . default \n for param in cli . scan . get_params ( ctx ) \n if param . name != \"<STR_LIT>\" \n } \n ctx . params . update ( params ) \n cli . scan . invoke ( ctx ) \n def crack_request_test ( self , params ) : \n ctx = click . Context ( cli . crack_request ) \n ctx . params = { \n param . name : param . default \n for param in cli . crack_request . get_params ( ctx ) \n if param . name != \"<STR_LIT>\" \n } \n ctx . params . update ( params ) \n cli . crack_request . invoke ( ctx ) \n def test_crack_basic ( self ) : \n for uri in [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + uri , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n } \n ) \n def test_crack_notexist ( self ) : \n try : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n except cli . RunFailed : \n return \n else : \n assert False \n def test_crack_nonrespond ( self ) : \n try : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n except cli . RunFailed : \n return \n else : \n assert False \n def test_crack_ua ( self ) : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ \"<STR_LIT>\" ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_crack_fast ( self ) : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_crack_eval_args ( self ) : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : True , \n \"<STR_LIT>\" : options . TemplateEnvironment . FLASK \n } \n ) \n def test_crack_tamperer ( self ) : \n self . crack_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_crack_path_basic ( self ) : \n self . crack_path_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_crack_path_tamperer ( self ) : \n self . crack_path_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n ) \n def test_crack_path_extra ( self ) : \n self . crack_path_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_scan_basic ( self ) : \n self . scan_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_scan_burstparam ( self ) : \n self . scan_test ( \n {", "output": "\"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n def test_scan_nonrespond ( self ) : \n try : \n self . scan_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n ) \n except cli . RunFailed : \n pass \n else : \n assert False \n def test_scan_tamperer ( self ) : \n self . scan_test ( \n { \n \"<STR_LIT>\" : VULUNSERVER_ADDR + \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n ) \n def test_crack_request_basic ( self ) : \n protocol , sep , addr = VULUNSERVER_ADDR . partition ( \"<STR_LIT>\" ) \n host , sep , port = addr . partition ( \"<STR_LIT>\" ) \n temp_file = tempfile . NamedTemporaryFile ( ) \n temp_file_path = temp_file . name \n with open ( temp_file_path , '<STR_LIT>' ) as file : \n file . write ( TEST_REQUEST ) \n self . crack_request_test ( \n { \n \"<STR_LIT>\" : host , \n \"<STR_LIT>\" : int ( port ) , \n \"<STR_LIT>\" : temp_file_path , \n \"<STR_LIT>\" : protocol == \"<STR_LIT>\" , \n \"<STR_LIT>\" : SLEEP_INTERVAL , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n )"}, {"input": "from dataclasses import dataclass \n from . const import ( \n DetectMode , \n TemplateEnvironment , \n PythonEnvironment , \n ReplacedKeywordStrategy , \n AutoFix500Code , \n )", "output": "@ dataclass \n class Options : \n detect_mode : DetectMode = DetectMode . ACCURATE \n environment : TemplateEnvironment = TemplateEnvironment . FLASK \n replaced_keyword_strategy : ReplacedKeywordStrategy = ReplacedKeywordStrategy . AVOID \n python_version : PythonEnvironment = PythonEnvironment . UNKNOWN \n autofix_500 : AutoFix500Code = AutoFix500Code . ENABLED"}, {"input": "import numpy as np \n import matplotlib . pyplot as plt \n from . mask_creation_utils import FinalGrouping \n from typing import Optional \n def show_mask ( mask , ax , random_color = False ) : \n if random_color : \n color = np . concatenate ( [ np . random . random ( <NUM_LIT> ) , np . array ( [ <NUM_LIT> ] ) ] , axis = <NUM_LIT> ) \n else : \n color = np . array ( [ <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT> / <NUM_LIT> , <NUM_LIT> ] ) \n h , w = mask . shape [ - <NUM_LIT> : ] \n mask_image = mask . reshape ( h , w , <NUM_LIT> ) * color . reshape ( <NUM_LIT> , <NUM_LIT> , - <NUM_LIT> ) \n ax . imshow ( mask_image ) \n def show_masks ( masks , ax ) : \n sorted_masks = sorted ( masks , key = lambda m : np . sum ( m * <NUM_LIT> ) , reverse = True ) \n for mask in sorted_masks : \n show_mask ( mask , ax , random_color = True ) \n def show_points ( coords , labels , ax , marker_size = <NUM_LIT> ) :", "output": "pos_points = coords [ labels == <NUM_LIT> ] \n neg_points = coords [ labels == <NUM_LIT> ] \n ax . scatter ( pos_points [ : , <NUM_LIT> ] , pos_points [ : , <NUM_LIT> ] , color = '<STR_LIT>' , marker = '<STR_LIT>' , s = marker_size , edgecolor = '<STR_LIT>' , linewidth = <NUM_LIT> ) \n ax . scatter ( neg_points [ : , <NUM_LIT> ] , neg_points [ : , <NUM_LIT> ] , color = '<STR_LIT>' , marker = '<STR_LIT>' , s = marker_size , edgecolor = '<STR_LIT>' , linewidth = <NUM_LIT> ) \n def display_all_levels ( image : np . ndarray , full_group_tree : FinalGrouping , label : Optional [ str ] = None ) : \n unexplored_depth = list ( full_group_tree . values ( ) ) \n depth_levels = [ ] \n while len ( unexplored_depth ) > <NUM_LIT> : \n depth_levels . append ( [ r [ '<STR_LIT>' ] . mask for r in unexplored_depth ] ) \n next_depth = [ ] \n for elem in unexplored_depth : \n next_depth += list ( elem [ '<STR_LIT>' ] . values ( ) ) \n unexplored_depth = next_depth \n for idx , level in enumerate ( depth_levels ) : \n plt . figure ( figsize = ( <NUM_LIT> , <NUM_LIT> ) ) \n plt . imshow ( image ) \n plt . title ( f\"<STR_LIT>\" , fontsize = <NUM_LIT> ) \n show_masks ( level , plt . gca ( ) ) \n plt . axis ( '<STR_LIT>' ) \n plt . show ( )"}, {"input": "import argparse \n import shutil \n import os \n from huggingface_hub import HfApi \n USEFUL_FILES = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n def main ( args ) : \n api = HfApi ( ) \n api . create_repo ( args . repo , exist_ok = True , repo_type = \"<STR_LIT>\" ) \n checkpoints = [ fn for fn in os . listdir ( args . model_folder ) if fn . startswith ( \"<STR_LIT>\" ) ] \n checkpoints . sort ( key = lambda x : int ( x . split ( \"<STR_LIT>\" ) [ - <NUM_LIT> ] ) ) \n if ( \n not os . path . exists ( os . path . join ( args . model_folder , \"<STR_LIT>\" ) ) \n and len ( checkpoints ) > <NUM_LIT> \n ) : \n last_checkpoint = os . path . join ( args . model_folder , checkpoints [ - <NUM_LIT> ] ) \n for fn in USEFUL_FILES : \n checkpoint_fn = os . path . join ( last_checkpoint , fn ) \n new_fn = os . path . join ( args . model_folder , fn )", "output": "if os . path . exists ( checkpoint_fn ) and not os . path . exists ( new_fn ) : \n shutil . copy ( checkpoint_fn , args . model_folder ) \n api . upload_folder ( \n repo_id = args . repo , allow_patterns = USEFUL_FILES , folder_path = args . model_folder \n ) \n if __name__ == \"<STR_LIT>\" : \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n parser . add_argument ( \"<STR_LIT>\" , \"<STR_LIT>\" , type = str ) \n args = parser . parse_args ( ) \n main ( args )"}, {"input": "import requests \n from bot . bot import Bot \n class BaiduUnitBot ( Bot ) : \n def reply ( self , query , context = None ) : \n token = self . get_token ( ) \n url = '<STR_LIT>' + token \n post_data = \"<STR_LIT>\" + query + \"<STR_LIT>\" \n print ( post_data ) \n headers = { '<STR_LIT>' : '<STR_LIT>' } \n response = requests . post ( url , data = post_data . encode ( ) , headers = headers ) \n if response : \n return response . json ( ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ <NUM_LIT> ] \n def get_token ( self ) : \n access_key = '<STR_LIT>' \n secret_key = '<STR_LIT>' \n host = '<STR_LIT>' + access_key + '<STR_LIT>' + secret_key \n response = requests . get ( host ) \n if response :", "output": "print ( response . json ( ) ) \n return response . json ( ) [ '<STR_LIT>' ]"}, {"input": "import re \n from flask_login import current_user \n from flask_wtf import FlaskForm \n from wtforms import StringField , SubmitField , PasswordField \n from wtforms . validators import DataRequired , ValidationError , Length , EqualTo \n from app . models import Channel , ChannelAllowList \n class ChannelForm ( FlaskForm ) : \n @ staticmethod \n def _channel_has_invalid_name ( name : str ) -> bool : \n valid_pattern = re . compile ( r'<STR_LIT>' ) \n if not name : \n return True \n else : \n return not ( bool ( re . fullmatch ( valid_pattern , name ) ) ) or name . startswith ( '<STR_LIT>' ) or name . endswith ( '<STR_LIT>' ) \n @ staticmethod \n def _channel_already_exists ( name : str ) -> bool : \n channel = Channel . query . filter_by ( name = name ) . first ( ) \n return bool ( channel ) \n class AddChannelForm ( ChannelForm ) : \n name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) \n password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) \n confirm_password = PasswordField ( '<STR_LIT>' , validators = [ \n DataRequired ( ) , EqualTo ( '<STR_LIT>' , message = '<STR_LIT>' ) \n ] ) \n submit_add = SubmitField ( '<STR_LIT>' ) \n def validate_name ( self , name : StringField ) -> None : \n if self . _channel_already_exists ( name . data ) : \n raise ValidationError ( '<STR_LIT>' ) \n elif self . _channel_has_invalid_name ( name . data ) : \n raise ValidationError ( '<STR_LIT>' )", "output": "class JoinChannelForm ( ChannelForm ) : \n name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) \n password = PasswordField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) \n submit_join = SubmitField ( '<STR_LIT>' ) \n def validate_name ( self , name : StringField ) -> None : \n if channel := Channel . query . filter_by ( name = name . data ) . first ( ) : \n if ChannelAllowList . query . filter_by ( user_id = current_user . id ) . filter_by ( channel_id = channel . id ) . first ( ) : \n raise ValueError ( \"<STR_LIT>\" ) \n class UpdateChannelForm ( ChannelForm ) : \n name = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) ] ) \n submit_update = SubmitField ( '<STR_LIT>' ) \n def validate_name ( self , name : StringField ) -> None : \n if self . _channel_already_exists ( name . data ) : \n raise ValidationError ( '<STR_LIT>' ) \n elif self . _channel_has_invalid_name ( name . data ) : \n raise ValidationError ( '<STR_LIT>' )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False , server_default = \"<STR_LIT>\" ) ) \n def downgrade ( ) -> None :", "output": "op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "from typing import Sequence , Union \n from alembic import op \n import sqlalchemy as sa \n revision : str = '<STR_LIT>'", "output": "down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) \n op . create_index ( op . f ( '<STR_LIT>' ) , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False ) \n def downgrade ( ) -> None : \n op . drop_index ( op . f ( '<STR_LIT>' ) , table_name = '<STR_LIT>' ) \n op . drop_column ( '<STR_LIT>' , '<STR_LIT>' )"}, {"input": "from __future__ import annotations \n from fastapi import FastAPI \n from fastapi . responses import JSONResponse \n import svcs \n app = FastAPI ( ... ) \n @ app . get ( \"<STR_LIT>\" ) \n async def healthy ( services : svcs . fastapi . DepContainer ) -> JSONResponse : \n ok : list [ str ] = [ ] \n failing : list [ dict [ str , str ] ] = [ ] \n code = <NUM_LIT> \n for svc in services . get_pings ( ) : \n try : \n await svc . aping ( )", "output": "ok . append ( svc . name ) \n except Exception as e : \n failing . append ( { svc . name : repr ( e ) } ) \n code = <NUM_LIT> \n return JSONResponse ( \n content = { \"<STR_LIT>\" : ok , \"<STR_LIT>\" : failing } , status_code = code \n )"}, {"input": "import uuid \n import json \n from curl_cffi import requests \n url = requests . get ( \"<STR_LIT>\" , impersonate = \"<STR_LIT>\" ) \n headers = { \n '<STR_LIT>' : \n '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' ,", "output": "'<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : f'<STR_LIT>' \n } \n proxies = { \"<STR_LIT>\" : \"<STR_LIT>\" } \n response = requests . get ( \"<STR_LIT>\" , impersonate = \"<STR_LIT>\" , headers = headers , proxies = proxies , timeout = <NUM_LIT> ) \n if response . status_code == <NUM_LIT> : \n res = json . loads ( response . text ) \n uuid = res [ <NUM_LIT> ] [ '<STR_LIT>' ] \n print ( f\"<STR_LIT>\" ) \n else : \n print ( f\"<STR_LIT>\" )"}, {"input": "import fnmatch \n import json \n import re \n from typing import Optional \n from dataclasses import dataclass \n from tempfile import NamedTemporaryFile \n from viztracer import VizTracer \n from profyle . application . trace . store import store_trace \n from profyle . domain . trace_repository import TraceRepository \n from profyle . domain . trace import TraceCreate \n @ dataclass \n class profyle : \n name : str \n repo : TraceRepository \n max_stack_depth : int = - <NUM_LIT> \n min_duration : float = <NUM_LIT> \n pattern : Optional [ str ] = None \n tracer : Optional [ VizTracer ] = None \n def __enter__ ( self ) -> None : \n if self . should_trace ( ) : \n self . tracer = VizTracer ( \n log_func_args = True , \n log_print = True , \n log_func_retval = True , \n log_async = True , \n file_info = True , \n min_duration = self . min_duration , \n max_stack_depth = self . max_stack_depth \n ) \n self . tracer . start ( )", "output": "def __exit__ ( \n self , \n * args , \n ) -> None : \n if self . tracer and self . tracer . enable : \n self . tracer . stop ( ) \n temp_file = NamedTemporaryFile ( suffix = \"<STR_LIT>\" ) \n self . tracer . save ( temp_file . name ) \n temp_file . close ( ) \n new_trace = TraceCreate ( \n data = json . dumps ( self . tracer . data ) , \n name = self . name \n ) \n store_trace ( \n new_trace = new_trace , \n repo = self . repo \n ) \n def should_trace ( self ) -> bool : \n if not self . pattern : \n return True \n regex = fnmatch . translate ( self . pattern ) \n reobj = re . compile ( regex ) \n method_and_name = self . name . split ( '<STR_LIT>' ) \n if len ( method_and_name ) > <NUM_LIT> : \n return bool ( reobj . match ( method_and_name [ <NUM_LIT> ] ) ) \n return bool ( reobj . match ( self . name ) )"}, {"input": "from flask_wtf import FlaskForm \n from sqlalchemy import func \n from wtforms import StringField , SubmitField , ValidationError \n from flask_wtf . file import FileField , FileAllowed \n from wtforms . validators import DataRequired , Length , Email \n from flask_login import current_user \n from app . models . user import User \n class UpdateProfileForm ( FlaskForm ) : \n username = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Length ( min = <NUM_LIT> , max = <NUM_LIT> ) ] ) \n email = StringField ( '<STR_LIT>' , validators = [ DataRequired ( ) , Email ( ) ] ) \n profile_picture = FileField ( '<STR_LIT>' , validators = [ FileAllowed ( [ '<STR_LIT>' , '<STR_LIT>' ] , '<STR_LIT>' ) ] ) \n submit = SubmitField ( '<STR_LIT>' ) \n def validate_username ( self , username : StringField ) -> None : \n user = User . query . filter ( func . lower ( User . username ) == func . lower ( username . data ) ) . first ( ) \n if user != current_user : \n if user : \n raise ValidationError ( '<STR_LIT>' ) \n def validate_email ( self , email : StringField ) -> None : \n user = User . query . filter ( func . lower ( User . email ) == func . lower ( email . data ) ) . first ( )", "output": "if user != current_user : \n if user : \n raise ValidationError ( '<STR_LIT>' )"}, {"input": "from flask import Flask , request \n import time \n import openai \n import sys \n import traceback \n import dotenv \n import vecs \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n from CacheDecorator import reliableCache \n import openai \n from main import reliableGPT \n import os \n openai . api_key = os . getenv ( '<STR_LIT>' ) \n DB_CONNECTION = os . getenv ( \"<STR_LIT>\" ) \n vx = vecs . create_client ( DB_CONNECTION ) \n vectorstore = vx . get_collection ( name = \"<STR_LIT>\" ) \n cache = reliableCache ( max_threads = <NUM_LIT> , query_arg = \"<STR_LIT>\" , customer_instance_arg = \"<STR_LIT>\" , user_email = \"<STR_LIT>\" ) \n def logging_fn ( * args , ** kwargs ) : \n pass \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" ) \n @ cache . cache_wrapper \n def test_fn ( ) : \n print ( \"<STR_LIT>\" )", "output": "try : \n question = request . args . get ( \"<STR_LIT>\" ) \n query_embedding = openai . Embedding . create ( model = \"<STR_LIT>\" , \n input = question ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n response = vectorstore . query ( \n query_vector = query_embedding , \n limit = <NUM_LIT> , \n include_value = True , \n include_metadata = True , \n ) \n input_prompt = \n result = openai . ChatCompletion . create ( model = \"<STR_LIT>\" , messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : input_prompt } ] ) \n print ( f\"<STR_LIT>\" ) \n return result \n except : \n traceback . print_exc ( ) \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . route ( '<STR_LIT>' ) \n def index ( ) : \n return '<STR_LIT>' \n if __name__ == \"<STR_LIT>\" : \n from waitress import serve \n serve ( app , host = \"<STR_LIT>\" , port = <NUM_LIT> , threads = <NUM_LIT> )"}, {"input": "import pickle , os \n import logging \n import requests \n from . . config import VERSION \n from . . returnvalues import ReturnValue \n from . . storage import templates \n from . contact import update_local_chatrooms , update_local_friends \n from . messages import produce_msg \n logger = logging . getLogger ( '<STR_LIT>' ) \n def load_hotreload ( core ) : \n core . dump_login_status = dump_login_status \n core . load_login_status = load_login_status \n async def dump_login_status ( self , fileDir = None ) : \n fileDir = fileDir or self . hotReloadDir \n try : \n with open ( fileDir , '<STR_LIT>' ) as f : \n f . write ( '<STR_LIT>' ) \n os . remove ( fileDir ) \n except :", "output": "raise Exception ( '<STR_LIT>' ) \n status = { \n '<STR_LIT>' : VERSION , \n '<STR_LIT>' : self . loginInfo , \n '<STR_LIT>' : self . s . cookies . get_dict ( ) , \n '<STR_LIT>' : self . storageClass . dumps ( ) } \n with open ( fileDir , '<STR_LIT>' ) as f : \n pickle . dump ( status , f ) \n logger . debug ( '<STR_LIT>' ) \n async def load_login_status ( self , fileDir , \n loginCallback = None , exitCallback = None ) : \n try : \n with open ( fileDir , '<STR_LIT>' ) as f : \n j = pickle . load ( f ) \n except Exception as e : \n logger . debug ( '<STR_LIT>' ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n if j . get ( '<STR_LIT>' , '<STR_LIT>' ) != VERSION : \n logger . debug ( ( '<STR_LIT>' + \n '<STR_LIT>' ) % ( \n j . get ( '<STR_LIT>' , '<STR_LIT>' ) , VERSION ) ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n self . loginInfo = j [ '<STR_LIT>' ] \n self . loginInfo [ '<STR_LIT>' ] = templates . User ( self . loginInfo [ '<STR_LIT>' ] ) \n self . loginInfo [ '<STR_LIT>' ] . core = self \n self . s . cookies = requests . utils . cookiejar_from_dict ( j [ '<STR_LIT>' ] ) \n self . storageClass . loads ( j [ '<STR_LIT>' ] ) \n try : \n msgList , contactList = self . get_msg ( ) \n except : \n msgList = contactList = None \n if ( msgList or contactList ) is None : \n self . logout ( ) \n await load_last_login_status ( self . s , j [ '<STR_LIT>' ] ) \n logger . debug ( '<STR_LIT>' ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : - <NUM_LIT> , } } ) \n else : \n if contactList : \n for contact in contactList : \n if '<STR_LIT>' in contact [ '<STR_LIT>' ] : \n update_local_chatrooms ( self , [ contact ] ) \n else : \n update_local_friends ( self , [ contact ] ) \n if msgList : \n msgList = produce_msg ( self , msgList ) \n for msg in msgList : self . msgList . put ( msg ) \n await self . start_receiving ( exitCallback ) \n logger . debug ( '<STR_LIT>' ) \n if hasattr ( loginCallback , '<STR_LIT>' ) : \n await loginCallback ( self . storageClass . userName ) \n return ReturnValue ( { '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : <NUM_LIT> , } } ) \n async def load_last_login_status ( session , cookiesDict ) : \n try : \n session . cookies = requests . utils . cookiejar_from_dict ( { \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] + '<STR_LIT>' , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : cookiesDict [ '<STR_LIT>' ] , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , } ) \n except : \n logger . info ( '<STR_LIT>' ) \n logger . info ( '<STR_LIT>' )"}, {"input": "import pyaes \n class Crypto : \n def __init__ ( self ) : \n self . key = self . gen_key ( ) \n @ staticmethod \n def gen_key ( ) -> str : \n import random \n import string \n aes_key : str = '<STR_LIT>' . join ( random . choices ( string . ascii_letters + string . digits , k = <NUM_LIT> ) ) \n return aes_key \n def encrypt ( self , data : str ) -> str : \n aes = pyaes . AESModeOfOperationCTR ( self . key . encode ( ) ) \n return aes . encrypt ( data ) . hex ( ) \n def decrypt ( self , data : str ) -> str :", "output": "aes = pyaes . AESModeOfOperationCTR ( self . key . encode ( ) ) \n return aes . decrypt ( bytes . fromhex ( data ) ) . decode ( ) \n def change_key ( self ) : \n self . key = self . gen_key ( ) \n crypto = Crypto ( )"}, {"input": "from uuid import uuid4 \n import os \n import json \n from transformers import CLIPProcessor , CLIPModel \n from densely_captioned_images . repro . eval . VLChecklist . vl_checklist . evaluate import Evaluate \n from densely_captioned_images . repro . config import VLC_ROOT_PATH , EVAL_LOG_PATH \n from densely_captioned_images . repro . eval . clip_vlc_wrap import VLCtoHFCLIPWrap \n ATTRIBUTE_YAML = \n OBJECT_YAML = \n RELATION_SPATIAL_YAML = \n RELATION_ACTION_YAML = \n CORPUS_PATH = os . path . join ( VLC_ROOT_PATH , '<STR_LIT>' ) \n LOG_PATH = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' ) \n def score_vlc ( model_name , swig_only = False ) : \n m = json . load ( open ( CORPUS_PATH ) ) \n if swig_only : \n m = { \n task_name : { \n ds_name : ds_split \n for ds_name , ds_split in task_split . items ( ) \n if '<STR_LIT>' in ds_name \n } for task_name , task_split in m . items ( ) \n } \n score_list = [ ] \n filepath = os . path . join ( LOG_PATH , model_name , '<STR_LIT>' ) \n for item in m . keys ( ) : \n data_num = len ( m [ item ] . keys ( ) ) \n data_score = [ ] \n if data_num == <NUM_LIT> : \n score_list . append ( <NUM_LIT> ) \n continue \n for data in m [ item ] . keys ( ) : \n score = <NUM_LIT> \n file_num = len ( m [ item ] [ data ] ) \n if file_num == <NUM_LIT> : \n data_score . append ( <NUM_LIT> ) \n continue \n for file in m [ item ] [ data ] : \n json_name = os . path . join ( filepath , f\"<STR_LIT>\" ) \n if not os . path . exists ( json_name ) : \n print ( f\"<STR_LIT>\" ) \n return \n else : \n m1 = json . load ( open ( json_name ) ) \n score += m1 [ \"<STR_LIT>\" ] \n data_score . append ( score / file_num ) \n score_list . append ( sum ( data_score ) / data_num ) \n print ( \"<STR_LIT>\" ) \n print ( list ( zip ( score_list , [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) ) ) \n overall_scores = [ sum ( score_list [ <NUM_LIT> : <NUM_LIT> ] ) / <NUM_LIT> , sum ( score_list [ <NUM_LIT> : <NUM_LIT> ] ) / <NUM_LIT> , sum ( score_list [ <NUM_LIT> : ] ) / <NUM_LIT> ] \n print ( \"<STR_LIT>\" ) \n print ( list ( zip ( overall_scores , [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] ) ) ) \n def run_vlc_on_model ( model : CLIPModel , processor : CLIPProcessor , model_name = None ) : \n if model_name is None : \n model_name = str ( uuid4 ( ) ) \n wrap_model = VLCtoHFCLIPWrap ( model_name , model , processor )", "output": "tasks = [ '<STR_LIT>' ] \n output_dirname = os . path . join ( EVAL_LOG_PATH , '<STR_LIT>' , model_name ) \n os . makedirs ( output_dirname , exist_ok = True ) \n for task in tasks : \n for BASE_YAML in [ ATTRIBUTE_YAML , OBJECT_YAML , RELATION_SPATIAL_YAML , RELATION_ACTION_YAML ] : \n yaml = BASE_YAML . format ( model_name = model_name , task = task , output_dirname = output_dirname ) \n yaml_path = os . path . join ( EVAL_LOG_PATH , f\"<STR_LIT>\" ) \n with open ( yaml_path , '<STR_LIT>' ) as yaml_file : \n yaml_file . write ( yaml ) \n evaluator = Evaluate ( config_file = yaml_path , model = wrap_model ) \n evaluator . start ( ) \n os . unlink ( yaml_path ) \n score_vlc ( model_name ) \n if __name__ == '<STR_LIT>' : \n from transformers import CLIPProcessor , CLIPModel \n clip_model = CLIPModel . from_pretrained ( \"<STR_LIT>\" ) \n clip_processor = CLIPProcessor . from_pretrained ( \"<STR_LIT>\" ) \n run_vlc_on_model ( clip_model , clip_processor , model_name = '<STR_LIT>' )"}, {"input": "import random \n import gc \n from flask import Flask , request , render_template_string \n from jinja2 import Template \n app = Flask ( __name__ ) \n blacklist = [ \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n def waf_words ( s ) : \n return [ word for word in blacklist if word in s ] \n def waf_pass ( s ) : \n return waf_words ( s ) == [ ] \n def lengthlimit1_waf_pass ( s ) : \n if len ( s ) > <NUM_LIT> : \n return False \n blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for ban in blacklist : \n if ban in s : \n return False \n return True \n def lengthlimit2_waf_pass ( inp ) : \n blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for b in blacklist : \n if b in inp : \n return False \n if len ( inp ) <= <NUM_LIT> : \n return True \n if len ( inp ) > <NUM_LIT> : \n return False \n @ app . after_request \n def garbasecollect ( resp ) : \n if random . randint ( <NUM_LIT> , <NUM_LIT> ) == <NUM_LIT> : \n gc . collect ( <NUM_LIT> ) \n return resp \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def index ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n template = . format ( \n name \n ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def nonrespond ( ) : \n template = \"<STR_LIT>\" \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def verifyheader ( ) : \n user_agent = request . headers . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n custom_key = request . headers . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n cookie_data = request . cookies . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if \"<STR_LIT>\" not in user_agent : \n return \"<STR_LIT>\" \n if \"<STR_LIT>\" not in custom_key : \n return \"<STR_LIT>\" \n if \"<STR_LIT>\" not in cookie_data : \n return \"<STR_LIT>\" \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" ) \n def crackpath ( name ) : \n return render_template_string ( \"<STR_LIT>\" . format ( name ) ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def scan_burstkeywords ( ) : \n name = request . args . get ( \"<STR_LIT>\" ) \n if not name : \n return \"<STR_LIT>\" \n if not waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def static_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" ] ) \n def static_waf2 ( ) : \n url_black_list = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n black_list = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n url = request . url \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if any ( w in url for w in url_black_list ) or any ( w in name for w in black_list ) : \n return \"<STR_LIT>\" \n return render_template_string ( \"<STR_LIT>\" . format ( name ) ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def dynamic_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not waf_pass ( name ) : \n return waf_words ( name ) [ <NUM_LIT> ] \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def weird_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not waf_pass ( name ) and len ( name ) < <NUM_LIT> and random . random ( ) < <NUM_LIT> : \n return \"<STR_LIT>\" \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def reversed_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) [ : : - <NUM_LIT> ] \n if not waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = ( \n \"<STR_LIT>\" . format ( name ) \n + \n ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def lengthlimit1_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not lengthlimit1_waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def lengthlimit2_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not lengthlimit2_waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def replace_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n words = waf_words ( name ) \n for word in words : \n if len ( word ) >= <NUM_LIT> : \n name = name . replace ( word , \"<STR_LIT>\" ) \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def replace_waf2 ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n words = waf_words ( name ) \n for word in words : \n if len ( word ) >= <NUM_LIT> : \n name = name . replace ( word , \"<STR_LIT>\" ) \n template = \"<STR_LIT>\" . format ( name ) \n return render_template_string ( template ) \n @ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def jinja_env_waf ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if not waf_pass ( name ) : \n return \"<STR_LIT>\" \n template = Template ( \"<STR_LIT>\" . format ( name ) ) \n return template . render ( ) \n @ app . route ( \"<STR_LIT>\" ) \n def crackpath_extra ( name ) : \n isdebug = request . args . get ( \"<STR_LIT>\" ) is not None \n if isdebug : \n return render_template_string ( \"<STR_LIT>\" . format ( name ) ) \n return \"<STR_LIT>\" \n if __name__ == \"<STR_LIT>\" : \n app . run ( host = \"<STR_LIT>\" , port = <NUM_LIT> )"}, {"input": "from doctest import ELLIPSIS \n import pytest \n from sybil import Sybil \n from sybil . parsers import myst , rest \n import svcs \n from tests . helpers import CloseMe \n from tests . ifaces import Service \n markdown_examples = Sybil ( \n parsers = [ \n myst . DocTestDirectiveParser ( optionflags = ELLIPSIS ) , \n myst . PythonCodeBlockParser ( doctest_optionflags = ELLIPSIS ) , \n myst . SkipParser ( ) , \n ] , \n patterns = [ \"<STR_LIT>\" ] , \n ) \n rest_examples = Sybil ( \n parsers = [ \n rest . DocTestParser ( optionflags = ELLIPSIS ) , \n rest . PythonCodeBlockParser ( ) , \n ] , \n patterns = [ \"<STR_LIT>\" ] , \n ) \n pytest_collect_file = ( markdown_examples + rest_examples ) . pytest ( ) \n collect_ignore = [ ] \n try : \n import sphinx \n except ImportError : \n collect_ignore . extend ( [ \"<STR_LIT>\" ] ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _svc ( ) : \n return Service ( ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _rs ( svc ) : \n return svcs . RegisteredService ( Service , Service , False , True , None ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _registry ( ) : \n return svcs . Registry ( ) \n @ pytest . fixture ( name = \"<STR_LIT>\" )", "output": "def _container ( registry ) : \n return svcs . Container ( registry ) \n @ pytest . fixture ( name = \"<STR_LIT>\" ) \n def _close_me ( ) : \n return CloseMe ( )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None", "output": "depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' ) \n def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n batch_op . add_column ( sa . Column ( '<STR_LIT>' , sa . BOOLEAN ( ) , nullable = True ) )"}, {"input": "import sys \n from threading import Thread , Lock \n from subprocess import Popen , PIPE \n from instld . module . empty_logger import EmptyLogger \n from instld . errors import RestartingCommandError , RunningCommandError \n class CommandExecuter : \n def __init__ ( self , arguments , catch_output = True , logger = EmptyLogger ( ) ) : \n self . arguments = arguments \n self . arguments_string_representation = '<STR_LIT>' . join ( self . arguments ) \n self . catch_output = catch_output \n self . logger = logger \n self . stdout = [ ]", "output": "self . stderr = [ ] \n self . running = False \n self . done = False \n self . lock = Lock ( ) \n def run ( self ) : \n with self . lock : \n if self . done or self . running : \n raise RestartingCommandError ( '<STR_LIT>' ) \n self . running = True \n self . logger . info ( f'<STR_LIT>' ) \n with Popen ( self . arguments , stdout = PIPE , stderr = PIPE , bufsize = <NUM_LIT> , universal_newlines = True ) as process : \n stderr_reading_thread = Thread ( target = self . read_stderr , args = ( process , ) ) \n stderr_reading_thread . start ( ) \n for line in process . stdout : \n self . stdout . append ( line ) \n if not self . catch_output : \n print ( line , end = '<STR_LIT>' ) \n stderr_reading_thread . join ( ) \n self . done = True \n self . running = False \n if process . returncode != <NUM_LIT> : \n message = f'<STR_LIT>' \n self . logger . error ( message ) \n exception = RunningCommandError ( message ) \n exception . stdout = '<STR_LIT>' . join ( self . stdout ) \n exception . stderr = '<STR_LIT>' . join ( self . stderr ) \n raise exception \n self . logger . info ( f'<STR_LIT>' ) \n def read_stderr ( self , process ) : \n for line in process . stderr : \n self . stderr . append ( line ) \n if not self . catch_output : \n sys . stderr . write ( line )"}, {"input": "from datetime import datetime \n from flask import url_for \n from flask_socketio import emit \n from flask_login import current_user \n from app . main . utils import convert_time_to_string \n from app . models import db , Channel , Message \n def add_message ( data : dict ) -> None : \n channel = data [ '<STR_LIT>' ] \n message_content = data [ '<STR_LIT>' ] \n username = current_user . username \n user_id = current_user . id \n user_picture = f\"<STR_LIT>\" \n full_time = datetime . utcnow ( ) \n channel_id = Channel . query . filter_by ( name = channel ) . first ( ) . id \n db . session . add ( Message ( \n content = message_content , author_id = user_id , time = full_time , target_channel = channel_id \n ) ) \n db . session . commit ( ) \n return announce_message ( username , user_picture , convert_time_to_string ( full_time ) , channel , message_content ) \n def announce_message ( user_name : str , user_picture : str , time : str , channel : str , message_content : str ) -> None : \n response = { \n '<STR_LIT>' : user_name ,", "output": "'<STR_LIT>' : user_picture , \n '<STR_LIT>' : time , \n '<STR_LIT>' : channel , \n '<STR_LIT>' : message_content \n } \n emit ( '<STR_LIT>' , response , room = channel )"}, {"input": "from flask import Flask \n import time \n import openai \n import sys \n import traceback \n import dotenv \n from dotenv import load_dotenv", "output": "load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai \n from main import reliableGPT \n import os \n openai . api_key = os . getenv ( '<STR_LIT>' ) \n def logging_fn ( * args , ** kwargs ) : \n pass \n openai . ChatCompletion . create = reliableGPT ( \n openai . ChatCompletion . create , \n caching = True , \n user_email = [ \"<STR_LIT>\" ] , \n max_threads = <NUM_LIT> , verbose = True ) \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" ) \n def test_fn ( ) : \n print ( \"<STR_LIT>\" ) \n try : \n result = openai . ChatCompletion . create ( model = \"<STR_LIT>\" , messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : \"<STR_LIT>\" } ] ) \n print ( f\"<STR_LIT>\" ) \n return result \n except : \n traceback . print_exc ( ) \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . route ( '<STR_LIT>' ) \n def index ( ) : \n return '<STR_LIT>' \n if __name__ == \"<STR_LIT>\" : \n from waitress import serve \n serve ( app , host = \"<STR_LIT>\" , port = <NUM_LIT> , threads = <NUM_LIT> )"}, {"input": "from typing import List , Optional \n import logging \n import math \n import threading \n import numpy as np \n import torch \n from legged_gym import LEGGED_GYM_ROOT_DIR \n import os \n try : \n import flask \n except ImportError : \n flask = None \n try : \n import imageio \n import isaacgym \n import isaacgym . torch_utils as torch_utils \n from isaacgym import gymapi \n except ImportError : \n imageio = None \n isaacgym = None \n torch_utils = None \n gymapi = None \n def cartesian_to_spherical ( x , y , z ) : \n r = np . sqrt ( x ** <NUM_LIT> + y ** <NUM_LIT> + z ** <NUM_LIT> ) \n theta = np . arccos ( z / r ) if r != <NUM_LIT> else <NUM_LIT> \n phi = np . arctan2 ( y , x ) \n return r , theta , phi \n def spherical_to_cartesian ( r , theta , phi ) : \n x = r * np . sin ( theta ) * np . cos ( phi ) \n y = r * np . sin ( theta ) * np . sin ( phi ) \n z = r * np . cos ( theta ) \n return x , y , z \n class WebViewer : \n def __init__ ( self , host : str = \"<STR_LIT>\" , port : int = <NUM_LIT> ) -> None : \n self . _app = flask . Flask ( __name__ ) \n self . _app . add_url_rule ( \"<STR_LIT>\" , view_func = self . _route_index ) \n self . _app . add_url_rule ( \"<STR_LIT>\" , view_func = self . _route_stream ) \n self . _app . add_url_rule ( \"<STR_LIT>\" , view_func = self . _route_stream_depth ) \n self . _app . add_url_rule ( \"<STR_LIT>\" , view_func = self . _route_input_event , methods = [ \"<STR_LIT>\" ] ) \n self . _log = logging . getLogger ( '<STR_LIT>' ) \n self . _log . disabled = True \n self . _app . logger . disabled = True \n self . _image = None \n self . _image_depth = None \n self . _camera_id = <NUM_LIT> \n self . _camera_type = gymapi . IMAGE_COLOR \n self . _notified = False \n self . _wait_for_page = True \n self . _pause_stream = False \n self . _event_load = threading . Event ( ) \n self . _event_stream = threading . Event ( ) \n self . _event_stream_depth = threading . Event ( ) \n self . _thread = threading . Thread ( target = lambda : self . _app . run ( host = host , port = port , debug = False , use_reloader = False ) , daemon = True ) \n self . _thread . start ( ) \n print ( f\"<STR_LIT>\" ) \n def _route_index ( self ) -> '<STR_LIT>' : \n with open ( os . path . join ( LEGGED_GYM_ROOT_DIR , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) , '<STR_LIT>' , encoding = '<STR_LIT>' ) as file : \n template = file . read ( ) \n self . _event_load . set ( ) \n return flask . render_template_string ( template ) \n def _route_stream ( self ) -> '<STR_LIT>' : \n return flask . Response ( self . _stream ( ) , mimetype = '<STR_LIT>' ) \n def _route_stream_depth ( self ) -> '<STR_LIT>' : \n return flask . Response ( self . _stream_depth ( ) , mimetype = '<STR_LIT>' ) \n def _route_input_event ( self ) -> '<STR_LIT>' : \n data = flask . request . get_json ( ) \n key , mouse = data . get ( \"<STR_LIT>\" , None ) , data . get ( \"<STR_LIT>\" , None ) \n dx , dy , dz = data . get ( \"<STR_LIT>\" , None ) , data . get ( \"<STR_LIT>\" , None ) , data . get ( \"<STR_LIT>\" , None ) \n transform = self . _gym . get_camera_transform ( self . _sim , \n self . _envs [ self . _camera_id ] , \n self . _cameras [ self . _camera_id ] ) \n if mouse == \"<STR_LIT>\" : \n r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) \n r += <NUM_LIT> * dz \n self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) \n elif mouse == \"<STR_LIT>\" : \n dx *= <NUM_LIT> * math . pi / <NUM_LIT> \n dy *= <NUM_LIT> * math . pi / <NUM_LIT> \n r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) \n theta -= dy \n phi -= dx \n self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) \n elif mouse == \"<STR_LIT>\" : \n dx *= - <NUM_LIT> * math . pi / <NUM_LIT> \n dy *= - <NUM_LIT> * math . pi / <NUM_LIT> \n r , theta , phi = cartesian_to_spherical ( * self . cam_pos_rel ) \n theta += dy \n phi += dx \n self . cam_pos_rel = spherical_to_cartesian ( r , theta , phi ) \n elif key == <NUM_LIT> : \n self . _camera_id = ( self . _camera_id - <NUM_LIT> ) % self . _env . num_envs \n return flask . Response ( status = <NUM_LIT> ) \n elif key == <NUM_LIT> : \n self . _camera_id = ( self . _camera_id + <NUM_LIT> ) % self . _env . num_envs \n return flask . Response ( status = <NUM_LIT> ) \n elif key == <NUM_LIT> : \n self . _pause_stream = not self . _pause_stream \n return flask . Response ( status = <NUM_LIT> ) \n elif key == <NUM_LIT> : \n if self . _camera_type == gymapi . IMAGE_COLOR : \n self . _camera_type = gymapi . IMAGE_DEPTH \n elif self . _camera_type == gymapi . IMAGE_DEPTH : \n self . _camera_type = gymapi . IMAGE_COLOR \n return flask . Response ( status = <NUM_LIT> ) \n else : \n return flask . Response ( status = <NUM_LIT> ) \n return flask . Response ( status = <NUM_LIT> ) \n def _stream ( self ) -> bytes : \n while True : \n self . _event_stream . wait ( ) \n image = imageio . imwrite ( \"<STR_LIT>\" , self . _image , format = \"<STR_LIT>\" ) \n yield ( b'<STR_LIT>' \n b'<STR_LIT>' + image + b'<STR_LIT>' ) \n self . _event_stream . clear ( ) \n self . _notified = False \n def _stream_depth ( self ) -> bytes : \n while self . _env . cfg . depth . use_camera : \n self . _event_stream_depth . wait ( ) \n image = imageio . imwrite ( \"<STR_LIT>\" , self . _image_depth , format = \"<STR_LIT>\" ) \n yield ( b'<STR_LIT>' \n b'<STR_LIT>' + image + b'<STR_LIT>' ) \n self . _event_stream_depth . clear ( ) \n def attach_view_camera ( self , i , env_handle , actor_handle , root_pos ) : \n if True : \n camera_props = gymapi . CameraProperties ( ) \n camera_props . width = <NUM_LIT> \n camera_props . height = <NUM_LIT> \n camera_handle = self . _gym . create_camera_sensor ( env_handle , camera_props ) \n self . _cameras . append ( camera_handle ) \n cam_pos = root_pos + np . array ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) \n self . _gym . set_camera_location ( camera_handle , env_handle , gymapi . Vec3 ( * cam_pos ) , gymapi . Vec3 ( * root_pos ) ) \n def setup ( self , env ) -> None : \n self . _gym = env . gym \n self . _sim = env . sim \n self . _envs = env . envs \n self . _cameras = [ ] \n self . _env = env \n self . cam_pos_rel = np . array ( [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] ) \n for i in range ( self . _env . num_envs ) : \n root_pos = self . _env . root_states [ i , : <NUM_LIT> ] . cpu ( ) . numpy ( ) \n self . attach_view_camera ( i , self . _envs [ i ] , self . _env . actor_handles [ i ] , root_pos ) \n def render ( self , \n fetch_results : bool = True , \n step_graphics : bool = True , \n render_all_camera_sensors : bool = True , \n wait_for_page_load : bool = True ) -> None : \n if self . _wait_for_page : \n if wait_for_page_load : \n if not self . _event_load . is_set ( ) : \n print ( \"<STR_LIT>\" ) \n self . _event_load . wait ( ) \n self . _event_load . clear ( ) \n self . _wait_for_page = False \n if self . _pause_stream : \n return \n if self . _notified : \n return \n if fetch_results : \n self . _gym . fetch_results ( self . _sim , True ) \n if step_graphics : \n self . _gym . step_graphics ( self . _sim ) \n if render_all_camera_sensors : \n self . _gym . render_all_camera_sensors ( self . _sim ) \n image = self . _gym . get_camera_image ( self . _sim , \n self . _envs [ self . _camera_id ] , \n self . _cameras [ self . _camera_id ] , \n self . _camera_type ) \n if self . _camera_type == gymapi . IMAGE_COLOR : \n self . _image = image . reshape ( image . shape [ <NUM_LIT> ] , - <NUM_LIT> , <NUM_LIT> ) [ ... , : <NUM_LIT> ] \n elif self . _camera_type == gymapi . IMAGE_DEPTH :", "output": "self . _image = - image . reshape ( image . shape [ <NUM_LIT> ] , - <NUM_LIT> ) \n minimum = <NUM_LIT> if np . isinf ( np . min ( self . _image ) ) else np . min ( self . _image ) \n maximum = <NUM_LIT> if np . isinf ( np . max ( self . _image ) ) else np . max ( self . _image ) \n self . _image = np . clip ( <NUM_LIT> - ( self . _image - minimum ) / ( maximum - minimum ) , <NUM_LIT> , <NUM_LIT> ) \n self . _image = np . uint8 ( <NUM_LIT> * self . _image ) \n else : \n raise ValueError ( \"<STR_LIT>\" ) \n if self . _env . cfg . depth . use_camera : \n self . _image_depth = self . _env . depth_buffer [ self . _camera_id , - <NUM_LIT> ] . cpu ( ) . numpy ( ) + <NUM_LIT> \n self . _image_depth = np . uint8 ( <NUM_LIT> * self . _image_depth ) \n root_pos = self . _env . root_states [ self . _camera_id , : <NUM_LIT> ] . cpu ( ) . numpy ( ) \n cam_pos = root_pos + self . cam_pos_rel \n self . _gym . set_camera_location ( self . _cameras [ self . _camera_id ] , self . _envs [ self . _camera_id ] , gymapi . Vec3 ( * cam_pos ) , gymapi . Vec3 ( * root_pos ) ) \n self . _event_stream . set ( ) \n if self . _env . cfg . depth . use_camera : \n self . _event_stream_depth . set ( ) \n self . _notified = True \n def ik ( jacobian_end_effector : torch . Tensor , \n current_position : torch . Tensor , \n current_orientation : torch . Tensor , \n goal_position : torch . Tensor , \n goal_orientation : Optional [ torch . Tensor ] = None , \n damping_factor : float = <NUM_LIT> , \n squeeze_output : bool = True ) -> torch . Tensor : \n if goal_orientation is None : \n goal_orientation = current_orientation \n q = torch_utils . quat_mul ( goal_orientation , torch_utils . quat_conjugate ( current_orientation ) ) \n error = torch . cat ( [ goal_position - current_position , \n q [ : , <NUM_LIT> : <NUM_LIT> ] * torch . sign ( q [ : , <NUM_LIT> ] ) . unsqueeze ( - <NUM_LIT> ) ] , \n dim = - <NUM_LIT> ) . unsqueeze ( - <NUM_LIT> ) \n transpose = torch . transpose ( jacobian_end_effector , <NUM_LIT> , <NUM_LIT> ) \n lmbda = torch . eye ( <NUM_LIT> , device = jacobian_end_effector . device ) * ( damping_factor ** <NUM_LIT> ) \n if squeeze_output : \n return ( transpose @ torch . inverse ( jacobian_end_effector @ transpose + lmbda ) @ error ) . squeeze ( dim = <NUM_LIT> ) \n else : \n return transpose @ torch . inverse ( jacobian_end_effector @ transpose + lmbda ) @ error \n def print_arguments ( args ) : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n for a in args . __dict__ : \n print ( f\"<STR_LIT>\" ) \n def print_asset_options ( asset_options : '<STR_LIT>' , asset_name : str = \"<STR_LIT>\" ) : \n attrs = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n print ( \"<STR_LIT>\" . format ( f\"<STR_LIT>\" if asset_name else \"<STR_LIT>\" ) ) \n for attr in attrs : \n print ( \"<STR_LIT>\" . format ( attr , getattr ( asset_options , attr ) if hasattr ( asset_options , attr ) else \"<STR_LIT>\" ) ) \n if attr == \"<STR_LIT>\" and hasattr ( asset_options , attr ) and getattr ( asset_options , attr ) : \n vhacd_attrs = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \n \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n print ( \"<STR_LIT>\" ) \n for vhacd_attr in vhacd_attrs : \n print ( \"<STR_LIT>\" . format ( vhacd_attr , getattr ( asset_options . vhacd_params , vhacd_attr ) if hasattr ( asset_options . vhacd_params , vhacd_attr ) else \"<STR_LIT>\" ) ) \n def print_sim_components ( gym , sim ) : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , gym . get_env_count ( sim ) ) \n print ( \"<STR_LIT>\" , gym . get_sim_actor_count ( sim ) ) \n print ( \"<STR_LIT>\" , gym . get_sim_rigid_body_count ( sim ) ) \n print ( \"<STR_LIT>\" , gym . get_sim_joint_count ( sim ) ) \n print ( \"<STR_LIT>\" , gym . get_sim_dof_count ( sim ) ) \n print ( \"<STR_LIT>\" , gym . get_sim_force_sensor_count ( sim ) ) \n def print_env_components ( gym , env ) : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , gym . get_actor_count ( env ) ) \n print ( \"<STR_LIT>\" , gym . get_env_rigid_body_count ( env ) ) \n print ( \"<STR_LIT>\" , gym . get_env_joint_count ( env ) ) \n print ( \"<STR_LIT>\" , gym . get_env_dof_count ( env ) ) \n def print_actor_components ( gym , env , actor ) : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , gym . get_actor_rigid_body_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_joint_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_dof_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_actuator_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_rigid_shape_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_soft_body_count ( env , actor ) ) \n print ( \"<STR_LIT>\" , gym . get_actor_tendon_count ( env , actor ) ) \n def print_dof_properties ( gymapi , props ) : \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" . format ( int ( gymapi . DOF_MODE_NONE ) ) ) \n print ( \"<STR_LIT>\" . format ( int ( gymapi . DOF_MODE_POS ) ) ) \n print ( \"<STR_LIT>\" . format ( int ( gymapi . DOF_MODE_VEL ) ) ) \n print ( \"<STR_LIT>\" . format ( int ( gymapi . DOF_MODE_EFFORT ) ) ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n print ( \"<STR_LIT>\" , props [ \"<STR_LIT>\" ] ) \n def print_links_and_dofs ( gym , asset ) : \n link_dict = gym . get_asset_rigid_body_dict ( asset ) \n dof_dict = gym . get_asset_dof_dict ( asset ) \n print ( \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n for k in link_dict : \n print ( f\"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" ) \n for k in dof_dict : \n print ( f\"<STR_LIT>\" )"}, {"input": "import os \n import torch \n import hashlib \n import datetime \n from collections import OrderedDict \n def replace_keys_in_dict ( d , old_key_part , new_key_part ) : \n if isinstance ( d , OrderedDict ) : \n updated_dict = OrderedDict ( ) \n else : \n updated_dict = { } \n for key , value in d . items ( ) : \n new_key = key . replace ( old_key_part , new_key_part ) \n if isinstance ( value , dict ) :", "output": "value = replace_keys_in_dict ( value , old_key_part , new_key_part ) \n updated_dict [ new_key ] = value \n return updated_dict \n def extract_model ( ckpt , sr , if_f0 , name , model_dir , epoch , step , version , hps ) : \n try : \n print ( f\"<STR_LIT>\" ) \n pth_file = f\"<STR_LIT>\" \n pth_file_old_version_path = os . path . join ( \n model_dir , f\"<STR_LIT>\" \n ) \n opt = OrderedDict ( \n weight = { \n key : value . half ( ) for key , value in ckpt . items ( ) if \"<STR_LIT>\" not in key \n } \n ) \n opt [ \"<STR_LIT>\" ] = [ \n hps . data . filter_length // <NUM_LIT> + <NUM_LIT> , \n <NUM_LIT> , \n hps . model . inter_channels , \n hps . model . hidden_channels , \n hps . model . filter_channels , \n hps . model . n_heads , \n hps . model . n_layers , \n hps . model . kernel_size , \n hps . model . p_dropout , \n hps . model . resblock , \n hps . model . resblock_kernel_sizes , \n hps . model . resblock_dilation_sizes , \n hps . model . upsample_rates , \n hps . model . upsample_initial_channel , \n hps . model . upsample_kernel_sizes , \n hps . model . spk_embed_dim , \n hps . model . gin_channels , \n hps . data . sampling_rate , \n ] \n opt [ \"<STR_LIT>\" ] = epoch \n opt [ \"<STR_LIT>\" ] = step \n opt [ \"<STR_LIT>\" ] = sr \n opt [ \"<STR_LIT>\" ] = if_f0 \n opt [ \"<STR_LIT>\" ] = version \n opt [ \"<STR_LIT>\" ] = datetime . datetime . now ( ) . isoformat ( ) \n hash_input = f\"<STR_LIT>\" \n model_hash = hashlib . sha256 ( hash_input . encode ( ) ) . hexdigest ( ) \n opt [ \"<STR_LIT>\" ] = model_hash \n torch . save ( opt , model_dir ) \n model = torch . load ( model_dir , map_location = torch . device ( \"<STR_LIT>\" ) ) \n torch . save ( \n replace_keys_in_dict ( \n replace_keys_in_dict ( \n model , \"<STR_LIT>\" , \"<STR_LIT>\" \n ) , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ) , \n pth_file_old_version_path , \n ) \n os . remove ( model_dir ) \n os . rename ( pth_file_old_version_path , model_dir ) \n except Exception as error : \n print ( error )"}, {"input": "import os , sys \n import gradio as gr \n import shutil \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n from assets . i18n . i18n import I18nAuto \n from core import run_model_blender_script \n i18n = I18nAuto ( ) \n def update_model_fusion ( dropbox ) : \n return dropbox , None \n def voice_blender_tab ( ) : \n gr . Markdown ( i18n ( \"<STR_LIT>\" ) ) \n gr . Markdown ( \n i18n ( \n \"<STR_LIT>\" \n ) \n ) \n with gr . Column ( ) : \n model_fusion_name = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n max_lines = <NUM_LIT> , \n interactive = True , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n ) \n with gr . Row ( ) : \n with gr . Column ( ) : \n model_fusion_a_dropbox = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , type = \"<STR_LIT>\" \n ) \n model_fusion_a = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n interactive = True , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n ) \n with gr . Column ( ) : \n model_fusion_b_dropbox = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , type = \"<STR_LIT>\" \n ) \n model_fusion_b = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n interactive = True , \n placeholder = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n ) \n alpha_a = gr . Slider ( \n minimum = <NUM_LIT> , \n maximum = <NUM_LIT> , \n label = i18n ( \"<STR_LIT>\" ) , \n value = <NUM_LIT> , \n interactive = True ,", "output": "info = i18n ( \n \"<STR_LIT>\" \n ) , \n ) \n model_fusion_button = gr . Button ( i18n ( \"<STR_LIT>\" ) , variant = \"<STR_LIT>\" ) \n with gr . Row ( ) : \n model_fusion_output_info = gr . Textbox ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \"<STR_LIT>\" ) , \n value = \"<STR_LIT>\" , \n ) \n model_fusion_pth_output = gr . File ( \n label = i18n ( \"<STR_LIT>\" ) , type = \"<STR_LIT>\" , interactive = False \n ) \n model_fusion_button . click ( \n fn = run_model_blender_script , \n inputs = [ \n model_fusion_name , \n model_fusion_a , \n model_fusion_b , \n alpha_a , \n ] , \n outputs = [ model_fusion_output_info , model_fusion_pth_output ] , \n ) \n model_fusion_a_dropbox . upload ( \n fn = update_model_fusion , \n inputs = model_fusion_a_dropbox , \n outputs = [ model_fusion_a , model_fusion_a_dropbox ] , \n ) \n model_fusion_b_dropbox . upload ( \n fn = update_model_fusion , \n inputs = model_fusion_b_dropbox , \n outputs = [ model_fusion_b , model_fusion_b_dropbox ] , \n )"}, {"input": "from dataclasses import dataclass \n from enum import IntEnum , auto \n from typing import Optional \n class RunType ( IntEnum ) : \n script = auto ( ) \n REPL = auto ( )", "output": "module = auto ( ) \n @ dataclass \n class StateStorage : \n run_type : RunType = RunType . module \n last_string : Optional [ str ] = None \n state_storage = StateStorage ( )"}, {"input": "from flask import Flask \n from flask_pymongo import PyMongo \n from flask_uploads import UploadSet , configure_uploads , IMAGES \n from werkzeug . utils import secure_filename \n from werkzeug . datastructures import FileStorage \n app = Flask ( __name__ ) \n app . config [ \"<STR_LIT>\" ] = \"<STR_LIT>\" \n mongo = PyMongo ( app ) \n photos = UploadSet ( '<STR_LIT>' , IMAGES ) \n app . config [ '<STR_LIT>' ] = '<STR_LIT>'", "output": "configure_uploads ( app , photos ) \n from app import routes"}, {"input": "import smtplib \n import imaplib \n import email \n import re \n import base64 \n import time \n from random import randrange \n from email . mime . text import MIMEText \n from email . header import decode_header \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n from common import const \n from config import channel_conf_val , channel_conf \n smtp_ssl_host = '<STR_LIT>' \n imap_ssl_host = '<STR_LIT>' \n MAX_DELAY = <NUM_LIT> \n MIN_DELAY = <NUM_LIT> \n STEP_TIME = <NUM_LIT> \n LATESTN = <NUM_LIT> \n wait_time = <NUM_LIT> \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n def checkEmail ( email ) : \n regex = r'<STR_LIT>' \n if re . search ( regex , email ) : \n return True \n else : \n return False \n def process ( max , speed ) : \n global wait_time \n i = <NUM_LIT> \n while i <= max : \n i = i + <NUM_LIT> \n time . sleep ( speed ) \n print ( \"<STR_LIT>\" + \"<STR_LIT>\" + str ( i + wait_time ) + \"<STR_LIT>\" , end = '<STR_LIT>' ) \n wait_time += max * speed \n class GmailChannel ( Channel ) : \n def __init__ ( self ) : \n self . host_email = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) \n self . host_password = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) \n self . subject_keyword = channel_conf_val ( const . GMAIL , '<STR_LIT>' ) \n def startup ( self ) : \n global wait_time \n ques_list = list ( ) \n lastques = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } \n print ( \"<STR_LIT>\" ) \n while ( True ) : \n ques_list = self . receiveEmail ( ) \n if ques_list : \n for ques in ques_list : \n if ques [ '<STR_LIT>' ] is None : \n print ( \"<STR_LIT>\" % ques [ '<STR_LIT>' ] ) \n elif ( lastques [ '<STR_LIT>' ] == ques [ '<STR_LIT>' ] and lastques [ '<STR_LIT>' ] == ques [ '<STR_LIT>' ] ) : \n print ( \"<STR_LIT>\" % ( ques [ '<STR_LIT>' ] ) ) \n else : \n if ques [ '<STR_LIT>' ] : \n print ( \"<STR_LIT>\" , end = '<STR_LIT>' ) \n self . handle ( ques ) \n lastques = ques \n wait_time = <NUM_LIT> \n else : \n print ( \"<STR_LIT>\" ) \n else : \n process ( randrange ( MIN_DELAY , MAX_DELAY ) , STEP_TIME ) \n def handle ( self , question ) : \n message = dict ( ) \n context = dict ( ) \n print ( \"<STR_LIT>\" % ( question [ '<STR_LIT>' ] , question [ '<STR_LIT>' ] ) ) \n context [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] \n answer = super ( ) . build_reply_content ( question [ '<STR_LIT>' ] , context ) \n message = MIMEText ( answer ) \n message [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] \n message [ '<STR_LIT>' ] = self . host_email", "output": "message [ '<STR_LIT>' ] = question [ '<STR_LIT>' ] \n thread_pool . submit ( self . sendEmail , message ) \n def sendEmail ( self , message : list ) -> dict : \n smtp_server = smtplib . SMTP ( smtp_ssl_host ) \n smtp_server . starttls ( ) \n smtp_server . login ( self . host_email , self . host_password ) \n output = { '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> , '<STR_LIT>' : <NUM_LIT> } \n try : \n smtp_server . sendmail ( message [ '<STR_LIT>' ] , message [ '<STR_LIT>' ] , message . as_string ( ) ) \n print ( \"<STR_LIT>\" . format ( message [ '<STR_LIT>' ] ) ) \n output [ '<STR_LIT>' ] += <NUM_LIT> \n except Exception as e : \n print ( \"<STR_LIT>\" . format ( e ) ) \n output [ '<STR_LIT>' ] += <NUM_LIT> \n print ( \"<STR_LIT>\" . format ( output [ '<STR_LIT>' ] , output [ '<STR_LIT>' ] ) ) \n smtp_server . quit ( ) \n return output \n def receiveEmail ( self ) : \n question_list = list ( ) \n question = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } \n imap_server = imaplib . IMAP4_SSL ( imap_ssl_host ) \n imap_server . login ( self . host_email , self . host_password ) \n imap_server . select ( '<STR_LIT>' ) \n status , data = imap_server . search ( None , '<STR_LIT>' ) \n mail_ids = [ ] \n for block in data : \n mail_ids += block . split ( ) \n mail_ids = mail_ids [ - LATESTN : ] \n for i in mail_ids : \n status , data = imap_server . fetch ( i , '<STR_LIT>' ) \n for response in data : \n if isinstance ( response , tuple ) : \n message = email . message_from_bytes ( response [ <NUM_LIT> ] ) \n mail_from = message [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n mail_subject = decode_header ( message [ '<STR_LIT>' ] ) [ <NUM_LIT> ] [ <NUM_LIT> ] \n if isinstance ( mail_subject , bytes ) : \n try : \n mail_subject = mail_subject . decode ( ) \n except UnicodeDecodeError : \n mail_subject = mail_subject . decode ( '<STR_LIT>' ) \n if not self . check_contain ( mail_subject , self . subject_keyword ) : \n continue \n if message . is_multipart ( ) : \n mail_content = '<STR_LIT>' \n for part in message . get_payload ( ) : \n flag = False \n if isinstance ( part . get_payload ( ) , list ) : \n part = part . get_payload ( ) [ <NUM_LIT> ] \n flag = True \n if part . get_content_type ( ) in [ '<STR_LIT>' , '<STR_LIT>' ] : \n if flag : \n mail_content += str ( part . get_payload ( ) ) \n else : \n try : \n mail_content += base64 . b64decode ( str ( part . get_payload ( ) ) ) . decode ( \"<STR_LIT>\" ) \n except UnicodeDecodeError : \n mail_content += base64 . b64decode ( str ( part . get_payload ( ) ) ) . decode ( '<STR_LIT>' ) \n else : \n mail_content = message . get_payload ( ) \n question [ '<STR_LIT>' ] = mail_from \n question [ '<STR_LIT>' ] = '<STR_LIT>' . join ( mail_subject . split ( '<STR_LIT>' ) [ <NUM_LIT> : ] ) \n question [ '<STR_LIT>' ] = mail_content \n print ( f'<STR_LIT>' ) \n question_list . append ( question ) \n question = { '<STR_LIT>' : None , '<STR_LIT>' : None , '<STR_LIT>' : None } \n imap_server . store ( i , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n print ( \"<STR_LIT>\" % mail_subject ) \n imap_server . expunge ( ) \n imap_server . close ( ) \n imap_server . logout ( ) \n return question_list \n def check_contain ( self , content , keyword_list ) : \n if not keyword_list : \n return None \n for ky in keyword_list : \n if content . find ( ky ) != - <NUM_LIT> : \n return True \n return None"}, {"input": "import transformers \n import logging \n from multi_token . training import ( \n TrainingArguments , \n ModelArguments , \n train_for_modalities , \n ) \n from multi_token . training_data import ( \n DataArguments , \n ) \n from multi_token . language_models import LANGUAGE_MODEL_NAME_TO_CLASS \n from multi_token . modalities import MODALITY_BUILDERS \n if __name__ == \"<STR_LIT>\" : \n logging . getLogger ( ) . setLevel ( logging . INFO ) \n parser = transformers . HfArgumentParser ( \n ( TrainingArguments , ModelArguments , DataArguments ) \n )", "output": "training_args , model_args , data_args , _ = parser . parse_args_into_dataclasses ( \n return_remaining_strings = True \n ) \n modalities = MODALITY_BUILDERS [ model_args . modality_builder ] ( ) \n model_cls = LANGUAGE_MODEL_NAME_TO_CLASS [ model_args . model_cls ] \n train_for_modalities ( model_cls , training_args , model_args , data_args , modalities )"}, {"input": "import os \n import sys \n import base64 \n import pathlib \n import tempfile \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n i18n = I18nAuto ( ) \n recorder_js_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n main_js_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n record_button_js_path = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n recorder_js = pathlib . Path ( recorder_js_path ) . read_text ( ) \n main_js = pathlib . Path ( main_js_path ) . read_text ( ) \n record_button_js = ( \n pathlib . Path ( record_button_js_path ) \n . read_text ( ) \n . replace ( \"<STR_LIT>\" , recorder_js ) \n . replace ( \"<STR_LIT>\" , main_js ) \n ) \n def save_base64_video ( base64_string ) : \n base64_video = base64_string \n video_data = base64 . b64decode ( base64_video ) \n with tempfile . NamedTemporaryFile ( suffix = \"<STR_LIT>\" , delete = False ) as temp_file : \n temp_filename = temp_file . name \n temp_file . write ( video_data ) \n print ( f\"<STR_LIT>\" ) \n return temp_filename \n def report_tab ( ) : \n instructions = [ \n i18n ( \"<STR_LIT>\" ) , \n i18n (", "output": "\"<STR_LIT>\" \n ) , \n i18n ( \n \"<STR_LIT>\" \n ) , \n i18n ( \n \"<STR_LIT>\" \n ) , \n i18n ( \n \"<STR_LIT>\" \n ) , \n ] \n components = [ gr . Markdown ( value = instruction ) for instruction in instructions ] \n start_button = gr . Button ( \"<STR_LIT>\" ) \n video_component = gr . Video ( interactive = False ) \n def toggle_button_label ( returned_string ) : \n if returned_string . startswith ( \"<STR_LIT>\" ) : \n return gr . Button ( value = \"<STR_LIT>\" ) , None \n else : \n try : \n temp_filename = save_base64_video ( returned_string ) \n except Exception as error : \n return gr . Button ( value = \"<STR_LIT>\" ) , gr . Warning ( \n f\"<STR_LIT>\" \n ) \n return gr . Button ( value = \"<STR_LIT>\" ) , gr . Video ( \n value = temp_filename , interactive = False \n ) \n start_button . click ( \n toggle_button_label , \n start_button , \n [ start_button , video_component ] , \n js = record_button_js , \n )"}, {"input": "from functools import lru_cache \n from instld . errors import InstallingPackageError \n from instld . state_management . storage import state_storage , RunType \n def get_comment_substring_from_string ( string ) : \n splitted_line = string . split ( '<STR_LIT>' ) \n right_part = splitted_line [ <NUM_LIT> : ] \n right_part = '<STR_LIT>' . join ( right_part ) \n right_part = right_part . strip ( ) \n if right_part . startswith ( '<STR_LIT>' ) : \n right_part = right_part [ <NUM_LIT> : ] . strip ( ) \n if right_part : \n return right_part \n else : \n raise InstallingPackageError ( '<STR_LIT>' ) \n @ lru_cache ( ) \n def get_comment_string_from_file ( line_number , file_name ) : \n try :", "output": "with open ( file_name , '<STR_LIT>' ) as file : \n for index , line in enumerate ( file ) : \n if index + <NUM_LIT> == line_number : \n return get_comment_substring_from_string ( line ) \n except ( FileNotFoundError , OSError ) : \n return None \n def get_comment_string_by_frame ( frame ) : \n if state_storage . run_type == RunType . script : \n line_number = frame . f_lineno \n code = frame . f_code \n file_name = code . co_filename \n return get_comment_string_from_file ( line_number , file_name ) \n elif state_storage . run_type == RunType . REPL : \n return get_comment_substring_from_string ( state_storage . last_string )"}, {"input": "import os \n import numpy as np \n import torch \n import torch . utils . data \n from mel_processing import spectrogram_torch \n from utils import load_filepaths_and_text , load_wav_to_torch \n class TextAudioLoaderMultiNSFsid ( torch . utils . data . Dataset ) : \n def __init__ ( self , hparams ) : \n self . audiopaths_and_text = load_filepaths_and_text ( hparams . training_files ) \n self . max_wav_value = hparams . max_wav_value \n self . sampling_rate = hparams . sampling_rate \n self . filter_length = hparams . filter_length \n self . hop_length = hparams . hop_length \n self . win_length = hparams . win_length \n self . sampling_rate = hparams . sampling_rate \n self . min_text_len = getattr ( hparams , \"<STR_LIT>\" , <NUM_LIT> ) \n self . max_text_len = getattr ( hparams , \"<STR_LIT>\" , <NUM_LIT> ) \n self . _filter ( ) \n def _filter ( self ) : \n audiopaths_and_text_new = [ ] \n lengths = [ ] \n for audiopath , text , pitch , pitchf , dv in self . audiopaths_and_text : \n if self . min_text_len <= len ( text ) and len ( text ) <= self . max_text_len : \n audiopaths_and_text_new . append ( [ audiopath , text , pitch , pitchf , dv ] ) \n lengths . append ( os . path . getsize ( audiopath ) // ( <NUM_LIT> * self . hop_length ) ) \n self . audiopaths_and_text = audiopaths_and_text_new \n self . lengths = lengths \n def get_sid ( self , sid ) : \n sid = torch . LongTensor ( [ int ( sid ) ] ) \n return sid \n def get_audio_text_pair ( self , audiopath_and_text ) : \n file = audiopath_and_text [ <NUM_LIT> ] \n phone = audiopath_and_text [ <NUM_LIT> ] \n pitch = audiopath_and_text [ <NUM_LIT> ] \n pitchf = audiopath_and_text [ <NUM_LIT> ] \n dv = audiopath_and_text [ <NUM_LIT> ] \n phone , pitch , pitchf = self . get_labels ( phone , pitch , pitchf ) \n spec , wav = self . get_audio ( file ) \n dv = self . get_sid ( dv ) \n len_phone = phone . size ( ) [ <NUM_LIT> ] \n len_spec = spec . size ( ) [ - <NUM_LIT> ] \n if len_phone != len_spec : \n len_min = min ( len_phone , len_spec ) \n len_wav = len_min * self . hop_length \n spec = spec [ : , : len_min ] \n wav = wav [ : , : len_wav ] \n phone = phone [ : len_min , : ] \n pitch = pitch [ : len_min ] \n pitchf = pitchf [ : len_min ] \n return ( spec , wav , phone , pitch , pitchf , dv ) \n def get_labels ( self , phone , pitch , pitchf ) : \n phone = np . load ( phone ) \n phone = np . repeat ( phone , <NUM_LIT> , axis = <NUM_LIT> ) \n pitch = np . load ( pitch ) \n pitchf = np . load ( pitchf ) \n n_num = min ( phone . shape [ <NUM_LIT> ] , <NUM_LIT> ) \n phone = phone [ : n_num , : ] \n pitch = pitch [ : n_num ] \n pitchf = pitchf [ : n_num ] \n phone = torch . FloatTensor ( phone ) \n pitch = torch . LongTensor ( pitch ) \n pitchf = torch . FloatTensor ( pitchf ) \n return phone , pitch , pitchf \n def get_audio ( self , filename ) : \n audio , sampling_rate = load_wav_to_torch ( filename ) \n if sampling_rate != self . sampling_rate : \n raise ValueError ( \n \"<STR_LIT>\" . format ( \n sampling_rate , self . sampling_rate \n ) \n ) \n audio_norm = audio \n audio_norm = audio_norm . unsqueeze ( <NUM_LIT> ) \n spec_filename = filename . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if os . path . exists ( spec_filename ) : \n try : \n spec = torch . load ( spec_filename ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n spec = spectrogram_torch ( \n audio_norm , \n self . filter_length , \n self . hop_length , \n self . win_length , \n center = False , \n ) \n spec = torch . squeeze ( spec , <NUM_LIT> ) \n torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) \n else : \n spec = spectrogram_torch ( \n audio_norm , \n self . filter_length , \n self . hop_length , \n self . win_length , \n center = False , \n ) \n spec = torch . squeeze ( spec , <NUM_LIT> ) \n torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) \n return spec , audio_norm \n def __getitem__ ( self , index ) : \n return self . get_audio_text_pair ( self . audiopaths_and_text [ index ] ) \n def __len__ ( self ) : \n return len ( self . audiopaths_and_text ) \n class TextAudioCollateMultiNSFsid : \n def __init__ ( self , return_ids = False ) : \n self . return_ids = return_ids \n def __call__ ( self , batch ) : \n _ , ids_sorted_decreasing = torch . sort ( \n torch . LongTensor ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) , dim = <NUM_LIT> , descending = True \n )", "output": "max_spec_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n max_wave_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n spec_lengths = torch . LongTensor ( len ( batch ) ) \n wave_lengths = torch . LongTensor ( len ( batch ) ) \n spec_padded = torch . FloatTensor ( len ( batch ) , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . size ( <NUM_LIT> ) , max_spec_len ) \n wave_padded = torch . FloatTensor ( len ( batch ) , <NUM_LIT> , max_wave_len ) \n spec_padded . zero_ ( ) \n wave_padded . zero_ ( ) \n max_phone_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n phone_lengths = torch . LongTensor ( len ( batch ) ) \n phone_padded = torch . FloatTensor ( \n len ( batch ) , max_phone_len , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] \n ) \n pitch_padded = torch . LongTensor ( len ( batch ) , max_phone_len ) \n pitchf_padded = torch . FloatTensor ( len ( batch ) , max_phone_len ) \n phone_padded . zero_ ( ) \n pitch_padded . zero_ ( ) \n pitchf_padded . zero_ ( ) \n sid = torch . LongTensor ( len ( batch ) ) \n for i in range ( len ( ids_sorted_decreasing ) ) : \n row = batch [ ids_sorted_decreasing [ i ] ] \n spec = row [ <NUM_LIT> ] \n spec_padded [ i , : , : spec . size ( <NUM_LIT> ) ] = spec \n spec_lengths [ i ] = spec . size ( <NUM_LIT> ) \n wave = row [ <NUM_LIT> ] \n wave_padded [ i , : , : wave . size ( <NUM_LIT> ) ] = wave \n wave_lengths [ i ] = wave . size ( <NUM_LIT> ) \n phone = row [ <NUM_LIT> ] \n phone_padded [ i , : phone . size ( <NUM_LIT> ) , : ] = phone \n phone_lengths [ i ] = phone . size ( <NUM_LIT> ) \n pitch = row [ <NUM_LIT> ] \n pitch_padded [ i , : pitch . size ( <NUM_LIT> ) ] = pitch \n pitchf = row [ <NUM_LIT> ] \n pitchf_padded [ i , : pitchf . size ( <NUM_LIT> ) ] = pitchf \n sid [ i ] = row [ <NUM_LIT> ] \n return ( \n phone_padded , \n phone_lengths , \n pitch_padded , \n pitchf_padded , \n spec_padded , \n spec_lengths , \n wave_padded , \n wave_lengths , \n sid , \n ) \n class TextAudioLoader ( torch . utils . data . Dataset ) : \n def __init__ ( self , hparams ) : \n self . audiopaths_and_text = load_filepaths_and_text ( hparams . training_files ) \n self . max_wav_value = hparams . max_wav_value \n self . sampling_rate = hparams . sampling_rate \n self . filter_length = hparams . filter_length \n self . hop_length = hparams . hop_length \n self . win_length = hparams . win_length \n self . sampling_rate = hparams . sampling_rate \n self . min_text_len = getattr ( hparams , \"<STR_LIT>\" , <NUM_LIT> ) \n self . max_text_len = getattr ( hparams , \"<STR_LIT>\" , <NUM_LIT> ) \n self . _filter ( ) \n def _filter ( self ) : \n audiopaths_and_text_new = [ ] \n lengths = [ ] \n for entry in self . audiopaths_and_text : \n if len ( entry ) >= <NUM_LIT> : \n audiopath , text , dv = entry [ : <NUM_LIT> ] \n if self . min_text_len <= len ( text ) and len ( text ) <= self . max_text_len : \n audiopaths_and_text_new . append ( [ audiopath , text , dv ] ) \n lengths . append ( os . path . getsize ( audiopath ) // ( <NUM_LIT> * self . hop_length ) ) \n self . audiopaths_and_text = audiopaths_and_text_new \n self . lengths = lengths \n def get_sid ( self , sid ) : \n sid = os . path . basename ( os . path . dirname ( sid ) ) \n try : \n sid = torch . LongTensor ( [ int ( \"<STR_LIT>\" . join ( filter ( str . isdigit , sid ) ) ) ] ) \n except ValueError as error : \n print ( f\"<STR_LIT>\" ) \n sid = torch . LongTensor ( [ <NUM_LIT> ] ) \n return sid \n def get_audio_text_pair ( self , audiopath_and_text ) : \n file = audiopath_and_text [ <NUM_LIT> ] \n phone = audiopath_and_text [ <NUM_LIT> ] \n dv = audiopath_and_text [ <NUM_LIT> ] \n phone = self . get_labels ( phone ) \n spec , wav = self . get_audio ( file ) \n dv = self . get_sid ( dv ) \n len_phone = phone . size ( ) [ <NUM_LIT> ] \n len_spec = spec . size ( ) [ - <NUM_LIT> ] \n if len_phone != len_spec : \n len_min = min ( len_phone , len_spec ) \n len_wav = len_min * self . hop_length \n spec = spec [ : , : len_min ] \n wav = wav [ : , : len_wav ] \n phone = phone [ : len_min , : ] \n return ( spec , wav , phone , dv ) \n def get_labels ( self , phone ) : \n phone = np . load ( phone ) \n phone = np . repeat ( phone , <NUM_LIT> , axis = <NUM_LIT> ) \n n_num = min ( phone . shape [ <NUM_LIT> ] , <NUM_LIT> ) \n phone = phone [ : n_num , : ] \n phone = torch . FloatTensor ( phone ) \n return phone \n def get_audio ( self , filename ) : \n audio , sampling_rate = load_wav_to_torch ( filename ) \n if sampling_rate != self . sampling_rate : \n raise ValueError ( \n \"<STR_LIT>\" . format ( \n sampling_rate , self . sampling_rate \n ) \n ) \n audio_norm = audio \n audio_norm = audio_norm . unsqueeze ( <NUM_LIT> ) \n spec_filename = filename . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if os . path . exists ( spec_filename ) : \n try : \n spec = torch . load ( spec_filename ) \n except Exception as error : \n print ( f\"<STR_LIT>\" ) \n spec = spectrogram_torch ( \n audio_norm , \n self . filter_length , \n self . hop_length , \n self . win_length , \n center = False , \n ) \n spec = torch . squeeze ( spec , <NUM_LIT> ) \n torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) \n else : \n spec = spectrogram_torch ( \n audio_norm , \n self . filter_length , \n self . hop_length , \n self . win_length , \n center = False , \n ) \n spec = torch . squeeze ( spec , <NUM_LIT> ) \n torch . save ( spec , spec_filename , _use_new_zipfile_serialization = False ) \n return spec , audio_norm \n def __getitem__ ( self , index ) : \n return self . get_audio_text_pair ( self . audiopaths_and_text [ index ] ) \n def __len__ ( self ) : \n return len ( self . audiopaths_and_text ) \n class TextAudioCollate : \n def __init__ ( self , return_ids = False ) : \n self . return_ids = return_ids \n def __call__ ( self , batch ) : \n _ , ids_sorted_decreasing = torch . sort ( \n torch . LongTensor ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) , dim = <NUM_LIT> , descending = True \n ) \n max_spec_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n max_wave_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n spec_lengths = torch . LongTensor ( len ( batch ) ) \n wave_lengths = torch . LongTensor ( len ( batch ) ) \n spec_padded = torch . FloatTensor ( len ( batch ) , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . size ( <NUM_LIT> ) , max_spec_len ) \n wave_padded = torch . FloatTensor ( len ( batch ) , <NUM_LIT> , max_wave_len ) \n spec_padded . zero_ ( ) \n wave_padded . zero_ ( ) \n max_phone_len = max ( [ x [ <NUM_LIT> ] . size ( <NUM_LIT> ) for x in batch ] ) \n phone_lengths = torch . LongTensor ( len ( batch ) ) \n phone_padded = torch . FloatTensor ( \n len ( batch ) , max_phone_len , batch [ <NUM_LIT> ] [ <NUM_LIT> ] . shape [ <NUM_LIT> ] \n ) \n phone_padded . zero_ ( ) \n sid = torch . LongTensor ( len ( batch ) ) \n for i in range ( len ( ids_sorted_decreasing ) ) : \n row = batch [ ids_sorted_decreasing [ i ] ] \n spec = row [ <NUM_LIT> ] \n spec_padded [ i , : , : spec . size ( <NUM_LIT> ) ] = spec \n spec_lengths [ i ] = spec . size ( <NUM_LIT> ) \n wave = row [ <NUM_LIT> ] \n wave_padded [ i , : , : wave . size ( <NUM_LIT> ) ] = wave \n wave_lengths [ i ] = wave . size ( <NUM_LIT> ) \n phone = row [ <NUM_LIT> ] \n phone_padded [ i , : phone . size ( <NUM_LIT> ) , : ] = phone \n phone_lengths [ i ] = phone . size ( <NUM_LIT> ) \n sid [ i ] = row [ <NUM_LIT> ] \n return ( \n phone_padded , \n phone_lengths , \n spec_padded , \n spec_lengths , \n wave_padded , \n wave_lengths , \n sid , \n ) \n class DistributedBucketSampler ( torch . utils . data . distributed . DistributedSampler ) : \n def __init__ ( \n self , \n dataset , \n batch_size , \n boundaries , \n num_replicas = None , \n rank = None , \n shuffle = True , \n ) : \n super ( ) . __init__ ( dataset , num_replicas = num_replicas , rank = rank , shuffle = shuffle ) \n self . lengths = dataset . lengths \n self . batch_size = batch_size \n self . boundaries = boundaries \n self . buckets , self . num_samples_per_bucket = self . _create_buckets ( ) \n self . total_size = sum ( self . num_samples_per_bucket ) \n self . num_samples = self . total_size // self . num_replicas \n def _create_buckets ( self ) : \n buckets = [ [ ] for _ in range ( len ( self . boundaries ) - <NUM_LIT> ) ] \n for i in range ( len ( self . lengths ) ) : \n length = self . lengths [ i ] \n idx_bucket = self . _bisect ( length ) \n if idx_bucket != - <NUM_LIT> : \n buckets [ idx_bucket ] . append ( i ) \n for i in range ( len ( buckets ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : \n if len ( buckets [ i ] ) == <NUM_LIT> : \n buckets . pop ( i ) \n self . boundaries . pop ( i + <NUM_LIT> ) \n num_samples_per_bucket = [ ] \n for i in range ( len ( buckets ) ) : \n len_bucket = len ( buckets [ i ] ) \n total_batch_size = self . num_replicas * self . batch_size \n rem = ( \n total_batch_size - ( len_bucket % total_batch_size ) \n ) % total_batch_size \n num_samples_per_bucket . append ( len_bucket + rem ) \n return buckets , num_samples_per_bucket \n def __iter__ ( self ) : \n g = torch . Generator ( ) \n g . manual_seed ( self . epoch ) \n indices = [ ] \n if self . shuffle : \n for bucket in self . buckets : \n indices . append ( torch . randperm ( len ( bucket ) , generator = g ) . tolist ( ) ) \n else : \n for bucket in self . buckets : \n indices . append ( list ( range ( len ( bucket ) ) ) ) \n batches = [ ] \n for i in range ( len ( self . buckets ) ) : \n bucket = self . buckets [ i ] \n len_bucket = len ( bucket ) \n ids_bucket = indices [ i ] \n num_samples_bucket = self . num_samples_per_bucket [ i ] \n rem = num_samples_bucket - len_bucket \n ids_bucket = ( \n ids_bucket \n + ids_bucket * ( rem // len_bucket ) \n + ids_bucket [ : ( rem % len_bucket ) ] \n ) \n ids_bucket = ids_bucket [ self . rank : : self . num_replicas ] \n for j in range ( len ( ids_bucket ) // self . batch_size ) : \n batch = [ \n bucket [ idx ] \n for idx in ids_bucket [ \n j * self . batch_size : ( j + <NUM_LIT> ) * self . batch_size \n ] \n ] \n batches . append ( batch ) \n if self . shuffle : \n batch_ids = torch . randperm ( len ( batches ) , generator = g ) . tolist ( ) \n batches = [ batches [ i ] for i in batch_ids ] \n self . batches = batches \n assert len ( self . batches ) * self . batch_size == self . num_samples \n return iter ( self . batches ) \n def _bisect ( self , x , lo = <NUM_LIT> , hi = None ) : \n if hi is None : \n hi = len ( self . boundaries ) - <NUM_LIT> \n if hi > lo : \n mid = ( hi + lo ) // <NUM_LIT> \n if self . boundaries [ mid ] < x and x <= self . boundaries [ mid + <NUM_LIT> ] : \n return mid \n elif x <= self . boundaries [ mid ] : \n return self . _bisect ( x , lo , mid ) \n else : \n return self . _bisect ( x , mid + <NUM_LIT> , hi ) \n else : \n return - <NUM_LIT> \n def __len__ ( self ) : \n return self . num_samples // self . batch_size"}, {"input": "import os \n import json \n from long_captions . utils import get_clip_token_length \n from long_captions . config import OUTPUT_SUMMARY_PATH , OUTPUT_NEGATIVE_PATH , DATASET_ANNOTATION_DUMP , DATASET_COMPLETE_PATH \n ANNOTATION_PATH = DATASET_ANNOTATION_DUMP \n IDX_TARGET = <NUM_LIT> \n DEBUG = False \n SKIP_EXISTING = False \n NUM_THREADS = <NUM_LIT> \n def ensure_size ( text , max_size = <NUM_LIT> ) : \n text = text . strip ( ) \n assert get_clip_token_length ( text ) <= max_size , f\"<STR_LIT>\" \n return text \n def run_collate ( ) : \n entries = os . listdir ( ANNOTATION_PATH ) \n for e in entries : \n try : \n source_path = os . path . join ( ANNOTATION_PATH , e ) \n summary_path = os . path . join ( OUTPUT_SUMMARY_PATH , e ) \n negative_path = os . path . join ( OUTPUT_NEGATIVE_PATH , e ) \n output_path = os . path . join ( DATASET_COMPLETE_PATH , e ) \n if os . path . exists ( output_path ) : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n if not os . path . exists ( summary_path ) : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n if not os . path . exists ( negative_path ) : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n with open ( source_path ) as jsonf : \n source_dict = json . load ( jsonf ) \n with open ( summary_path ) as jsonf : \n summary_dict = json . load ( jsonf ) \n with open ( negative_path ) as jsonf : \n negative_dict = json . load ( jsonf ) \n if len ( negative_dict ) == <NUM_LIT> : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n continue \n if '<STR_LIT>' in summary_dict : \n del summary_dict [ '<STR_LIT>' ] \n if '<STR_LIT>' in negative_dict : \n del negative_dict [ '<STR_LIT>' ] \n summary_dict = { k : ensure_size ( summary ) for k , summary in summary_dict . items ( ) } \n negative_dict = { \n k : { \n neg_type : [ \n ensure_size ( n . strip ( ) ) for n in sub_negatives \n ] for neg_type , sub_negatives in negatives . items ( ) \n } for k , negatives in negative_dict . items ( ) \n } \n source_dict [ '<STR_LIT>' ] = summary_dict \n source_dict [ '<STR_LIT>' ] = negative_dict", "output": "assert len ( source_dict [ '<STR_LIT>' ] ) == len ( source_dict [ '<STR_LIT>' ] ) \n with open ( output_path , '<STR_LIT>' ) as jsonf : \n json . dump ( source_dict , jsonf ) \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n except Exception as _ : \n print ( '<STR_LIT>' , end = '<STR_LIT>' , flush = True ) \n with open ( '<STR_LIT>' , '<STR_LIT>' ) as fn : \n fn . write ( e + \"<STR_LIT>\" ) \n continue \n if __name__ == '<STR_LIT>' : \n run_collate ( )"}, {"input": "import torch . nn as nn \n import torch \n def build_patch_mlp_projector ( \n input_hidden_size : int , lm_hidden_size : int , num_layers : int \n ) -> nn . Module : \n modules = [ nn . Linear ( input_hidden_size , lm_hidden_size ) ] \n for _ in range ( <NUM_LIT> , num_layers ) : \n modules . append ( nn . GELU ( ) ) \n modules . append ( nn . Linear ( lm_hidden_size , lm_hidden_size ) ) \n return nn . Sequential ( * modules ) \n class _MLPVectorProjector ( nn . Module ) : \n def __init__ ( \n self , input_hidden_size : int , lm_hidden_size : int , num_layers : int , width : int \n ) : \n super ( _MLPVectorProjector , self ) . __init__ ( ) \n self . mlps = nn . ModuleList ( ) \n for _ in range ( width ) :", "output": "mlp = [ nn . Linear ( input_hidden_size , lm_hidden_size ) ] \n for _ in range ( <NUM_LIT> , num_layers ) : \n mlp . append ( nn . GELU ( ) ) \n mlp . append ( nn . Linear ( lm_hidden_size , lm_hidden_size ) ) \n self . mlps . append ( nn . Sequential ( * mlp ) ) \n def forward ( self , x ) : \n return torch . cat ( [ mlp ( x ) for mlp in self . mlps ] , dim = - <NUM_LIT> ) \n def build_mlp_vector_projector ( \n input_hidden_size : int , lm_hidden_size : int , num_layers : int , num_tokens : int \n ) : \n return _MLPVectorProjector ( \n input_hidden_size , lm_hidden_size , num_layers , num_tokens \n )"}, {"input": "import math \n import torch \n from torch import nn \n from torch . nn import functional as F \n from torch . nn import Conv1d \n from torch . nn . utils import remove_weight_norm \n from torch . nn . utils . parametrizations import weight_norm \n from . import commons \n from . commons import init_weights , get_padding \n from . transforms import piecewise_rational_quadratic_transform \n LRELU_SLOPE = <NUM_LIT> \n class LayerNorm ( nn . Module ) : \n def __init__ ( self , channels , eps = <NUM_LIT> ) : \n super ( ) . __init__ ( ) \n self . channels = channels \n self . eps = eps \n self . gamma = nn . Parameter ( torch . ones ( channels ) ) \n self . beta = nn . Parameter ( torch . zeros ( channels ) ) \n def forward ( self , x ) : \n x = x . transpose ( <NUM_LIT> , - <NUM_LIT> ) \n x = F . layer_norm ( x , ( self . channels , ) , self . gamma , self . beta , self . eps ) \n return x . transpose ( <NUM_LIT> , - <NUM_LIT> ) \n class ConvReluNorm ( nn . Module ) : \n def __init__ ( \n self , \n in_channels , \n hidden_channels , \n out_channels , \n kernel_size , \n n_layers , \n p_dropout , \n ) : \n super ( ) . __init__ ( ) \n self . in_channels = in_channels \n self . hidden_channels = hidden_channels \n self . out_channels = out_channels \n self . kernel_size = kernel_size \n self . n_layers = n_layers \n self . p_dropout = p_dropout \n assert n_layers > <NUM_LIT> , \"<STR_LIT>\" \n self . conv_layers = nn . ModuleList ( ) \n self . norm_layers = nn . ModuleList ( ) \n self . conv_layers . append ( \n nn . Conv1d ( \n in_channels , hidden_channels , kernel_size , padding = kernel_size // <NUM_LIT> \n ) \n ) \n self . norm_layers . append ( LayerNorm ( hidden_channels ) ) \n self . relu_drop = nn . Sequential ( nn . ReLU ( ) , nn . Dropout ( p_dropout ) ) \n for _ in range ( n_layers - <NUM_LIT> ) : \n self . conv_layers . append ( \n nn . Conv1d ( \n hidden_channels , \n hidden_channels , \n kernel_size , \n padding = kernel_size // <NUM_LIT> , \n ) \n ) \n self . norm_layers . append ( LayerNorm ( hidden_channels ) ) \n self . proj = nn . Conv1d ( hidden_channels , out_channels , <NUM_LIT> ) \n self . proj . weight . data . zero_ ( ) \n self . proj . bias . data . zero_ ( ) \n def forward ( self , x , x_mask ) : \n x_org = x \n for i in range ( self . n_layers ) : \n x = self . conv_layers [ i ] ( x * x_mask ) \n x = self . norm_layers [ i ] ( x ) \n x = self . relu_drop ( x ) \n x = x_org + self . proj ( x ) \n return x * x_mask \n class DDSConv ( nn . Module ) : \n def __init__ ( self , channels , kernel_size , n_layers , p_dropout = <NUM_LIT> ) : \n super ( ) . __init__ ( ) \n self . channels = channels \n self . kernel_size = kernel_size \n self . n_layers = n_layers \n self . p_dropout = p_dropout \n self . drop = nn . Dropout ( p_dropout ) \n self . convs_sep = nn . ModuleList ( ) \n self . convs_1x1 = nn . ModuleList ( ) \n self . norms_1 = nn . ModuleList ( ) \n self . norms_2 = nn . ModuleList ( ) \n for i in range ( n_layers ) : \n dilation = kernel_size ** i \n padding = ( kernel_size * dilation - dilation ) // <NUM_LIT> \n self . convs_sep . append ( \n nn . Conv1d ( \n channels , \n channels , \n kernel_size , \n groups = channels , \n dilation = dilation , \n padding = padding , \n ) \n ) \n self . convs_1x1 . append ( nn . Conv1d ( channels , channels , <NUM_LIT> ) ) \n self . norms_1 . append ( LayerNorm ( channels ) ) \n self . norms_2 . append ( LayerNorm ( channels ) ) \n def forward ( self , x , x_mask , g = None ) : \n if g is not None : \n x = x + g \n for i in range ( self . n_layers ) : \n y = self . convs_sep [ i ] ( x * x_mask ) \n y = self . norms_1 [ i ] ( y ) \n y = F . gelu ( y ) \n y = self . convs_1x1 [ i ] ( y ) \n y = self . norms_2 [ i ] ( y ) \n y = F . gelu ( y ) \n y = self . drop ( y ) \n x = x + y \n return x * x_mask \n class WN ( torch . nn . Module ) : \n def __init__ ( \n self , \n hidden_channels , \n kernel_size , \n dilation_rate , \n n_layers ,", "output": "gin_channels = <NUM_LIT> , \n p_dropout = <NUM_LIT> , \n ) : \n super ( WN , self ) . __init__ ( ) \n assert kernel_size % <NUM_LIT> == <NUM_LIT> \n self . hidden_channels = hidden_channels \n self . kernel_size = ( kernel_size , ) \n self . dilation_rate = dilation_rate \n self . n_layers = n_layers \n self . gin_channels = gin_channels \n self . p_dropout = p_dropout \n self . in_layers = torch . nn . ModuleList ( ) \n self . res_skip_layers = torch . nn . ModuleList ( ) \n self . drop = nn . Dropout ( p_dropout ) \n if gin_channels != <NUM_LIT> : \n cond_layer = torch . nn . Conv1d ( \n gin_channels , <NUM_LIT> * hidden_channels * n_layers , <NUM_LIT> \n ) \n self . cond_layer = torch . nn . utils . parametrizations . weight_norm ( \n cond_layer , name = \"<STR_LIT>\" \n ) \n for i in range ( n_layers ) : \n dilation = dilation_rate ** i \n padding = int ( ( kernel_size * dilation - dilation ) / <NUM_LIT> ) \n in_layer = torch . nn . Conv1d ( \n hidden_channels , \n <NUM_LIT> * hidden_channels , \n kernel_size , \n dilation = dilation , \n padding = padding , \n ) \n in_layer = torch . nn . utils . parametrizations . weight_norm ( \n in_layer , name = \"<STR_LIT>\" \n ) \n self . in_layers . append ( in_layer ) \n if i < n_layers - <NUM_LIT> : \n res_skip_channels = <NUM_LIT> * hidden_channels \n else : \n res_skip_channels = hidden_channels \n res_skip_layer = torch . nn . Conv1d ( hidden_channels , res_skip_channels , <NUM_LIT> ) \n res_skip_layer = torch . nn . utils . parametrizations . weight_norm ( \n res_skip_layer , name = \"<STR_LIT>\" \n ) \n self . res_skip_layers . append ( res_skip_layer ) \n def forward ( self , x , x_mask , g = None , ** kwargs ) : \n output = torch . zeros_like ( x ) \n n_channels_tensor = torch . IntTensor ( [ self . hidden_channels ] ) \n if g is not None : \n g = self . cond_layer ( g ) \n for i in range ( self . n_layers ) : \n x_in = self . in_layers [ i ] ( x ) \n if g is not None : \n cond_offset = i * <NUM_LIT> * self . hidden_channels \n g_l = g [ : , cond_offset : cond_offset + <NUM_LIT> * self . hidden_channels , : ] \n else : \n g_l = torch . zeros_like ( x_in ) \n acts = commons . fused_add_tanh_sigmoid_multiply ( x_in , g_l , n_channels_tensor ) \n acts = self . drop ( acts ) \n res_skip_acts = self . res_skip_layers [ i ] ( acts ) \n if i < self . n_layers - <NUM_LIT> : \n res_acts = res_skip_acts [ : , : self . hidden_channels , : ] \n x = ( x + res_acts ) * x_mask \n output = output + res_skip_acts [ : , self . hidden_channels : , : ] \n else : \n output = output + res_skip_acts \n return output * x_mask \n def remove_weight_norm ( self ) : \n if self . gin_channels != <NUM_LIT> : \n torch . nn . utils . remove_weight_norm ( self . cond_layer ) \n for l in self . in_layers : \n torch . nn . utils . remove_weight_norm ( l ) \n for l in self . res_skip_layers : \n torch . nn . utils . remove_weight_norm ( l ) \n class ResBlock1 ( torch . nn . Module ) : \n def __init__ ( self , channels , kernel_size = <NUM_LIT> , dilation = ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) ) : \n super ( ResBlock1 , self ) . __init__ ( ) \n self . convs1 = nn . ModuleList ( \n [ \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = dilation [ <NUM_LIT> ] , \n padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , \n ) \n ) , \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = dilation [ <NUM_LIT> ] , \n padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , \n ) \n ) , \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = dilation [ <NUM_LIT> ] , \n padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , \n ) \n ) , \n ] \n ) \n self . convs1 . apply ( init_weights ) \n self . convs2 = nn . ModuleList ( \n [ \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = <NUM_LIT> , \n padding = get_padding ( kernel_size , <NUM_LIT> ) , \n ) \n ) , \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = <NUM_LIT> , \n padding = get_padding ( kernel_size , <NUM_LIT> ) , \n ) \n ) , \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = <NUM_LIT> , \n padding = get_padding ( kernel_size , <NUM_LIT> ) , \n ) \n ) , \n ] \n ) \n self . convs2 . apply ( init_weights ) \n def forward ( self , x , x_mask = None ) : \n for c1 , c2 in zip ( self . convs1 , self . convs2 ) : \n xt = F . leaky_relu ( x , LRELU_SLOPE ) \n if x_mask is not None : \n xt = xt * x_mask \n xt = c1 ( xt ) \n xt = F . leaky_relu ( xt , LRELU_SLOPE ) \n if x_mask is not None : \n xt = xt * x_mask \n xt = c2 ( xt ) \n x = xt + x \n if x_mask is not None : \n x = x * x_mask \n return x \n def remove_weight_norm ( self ) : \n for l in self . convs1 : \n remove_weight_norm ( l ) \n for l in self . convs2 : \n remove_weight_norm ( l ) \n class ResBlock2 ( torch . nn . Module ) : \n def __init__ ( self , channels , kernel_size = <NUM_LIT> , dilation = ( <NUM_LIT> , <NUM_LIT> ) ) : \n super ( ResBlock2 , self ) . __init__ ( ) \n self . convs = nn . ModuleList ( \n [ \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = dilation [ <NUM_LIT> ] , \n padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , \n ) \n ) , \n weight_norm ( \n Conv1d ( \n channels , \n channels , \n kernel_size , \n <NUM_LIT> , \n dilation = dilation [ <NUM_LIT> ] , \n padding = get_padding ( kernel_size , dilation [ <NUM_LIT> ] ) , \n ) \n ) , \n ] \n ) \n self . convs . apply ( init_weights ) \n def forward ( self , x , x_mask = None ) : \n for c in self . convs : \n xt = F . leaky_relu ( x , LRELU_SLOPE ) \n if x_mask is not None : \n xt = xt * x_mask \n xt = c ( xt ) \n x = xt + x \n if x_mask is not None : \n x = x * x_mask \n return x \n def remove_weight_norm ( self ) : \n for l in self . convs : \n remove_weight_norm ( l ) \n class Log ( nn . Module ) : \n def forward ( self , x , x_mask , reverse = False , ** kwargs ) : \n if not reverse : \n y = torch . log ( torch . clamp_min ( x , <NUM_LIT> ) ) * x_mask \n logdet = torch . sum ( - y , [ <NUM_LIT> , <NUM_LIT> ] ) \n return y , logdet \n else : \n x = torch . exp ( x ) * x_mask \n return x \n class Flip ( nn . Module ) : \n def forward ( self , x , * args , reverse = False , ** kwargs ) : \n x = torch . flip ( x , [ <NUM_LIT> ] ) \n if not reverse : \n logdet = torch . zeros ( x . size ( <NUM_LIT> ) ) . to ( dtype = x . dtype , device = x . device ) \n return x , logdet \n else : \n return x \n class ElementwiseAffine ( nn . Module ) : \n def __init__ ( self , channels ) : \n super ( ) . __init__ ( ) \n self . channels = channels \n self . m = nn . Parameter ( torch . zeros ( channels , <NUM_LIT> ) ) \n self . logs = nn . Parameter ( torch . zeros ( channels , <NUM_LIT> ) ) \n def forward ( self , x , x_mask , reverse = False , ** kwargs ) : \n if not reverse : \n y = self . m + torch . exp ( self . logs ) * x \n y = y * x_mask \n logdet = torch . sum ( self . logs * x_mask , [ <NUM_LIT> , <NUM_LIT> ] ) \n return y , logdet \n else : \n x = ( x - self . m ) * torch . exp ( - self . logs ) * x_mask \n return x \n class ResidualCouplingLayer ( nn . Module ) : \n def __init__ ( \n self , \n channels , \n hidden_channels , \n kernel_size , \n dilation_rate , \n n_layers , \n p_dropout = <NUM_LIT> , \n gin_channels = <NUM_LIT> , \n mean_only = False , \n ) : \n assert channels % <NUM_LIT> == <NUM_LIT> , \"<STR_LIT>\" \n super ( ) . __init__ ( ) \n self . channels = channels \n self . hidden_channels = hidden_channels \n self . kernel_size = kernel_size \n self . dilation_rate = dilation_rate \n self . n_layers = n_layers \n self . half_channels = channels // <NUM_LIT> \n self . mean_only = mean_only \n self . pre = nn . Conv1d ( self . half_channels , hidden_channels , <NUM_LIT> ) \n self . enc = WN ( \n hidden_channels , \n kernel_size , \n dilation_rate , \n n_layers , \n p_dropout = p_dropout , \n gin_channels = gin_channels , \n ) \n self . post = nn . Conv1d ( hidden_channels , self . half_channels * ( <NUM_LIT> - mean_only ) , <NUM_LIT> ) \n self . post . weight . data . zero_ ( ) \n self . post . bias . data . zero_ ( ) \n def forward ( self , x , x_mask , g = None , reverse = False ) : \n x0 , x1 = torch . split ( x , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) \n h = self . pre ( x0 ) * x_mask \n h = self . enc ( h , x_mask , g = g ) \n stats = self . post ( h ) * x_mask \n if not self . mean_only : \n m , logs = torch . split ( stats , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) \n else : \n m = stats \n logs = torch . zeros_like ( m ) \n if not reverse : \n x1 = m + x1 * torch . exp ( logs ) * x_mask \n x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) \n logdet = torch . sum ( logs , [ <NUM_LIT> , <NUM_LIT> ] ) \n return x , logdet \n else : \n x1 = ( x1 - m ) * torch . exp ( - logs ) * x_mask \n x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) \n return x \n def remove_weight_norm ( self ) : \n self . enc . remove_weight_norm ( ) \n class ConvFlow ( nn . Module ) : \n def __init__ ( \n self , \n in_channels , \n filter_channels , \n kernel_size , \n n_layers , \n num_bins = <NUM_LIT> , \n tail_bound = <NUM_LIT> , \n ) : \n super ( ) . __init__ ( ) \n self . in_channels = in_channels \n self . filter_channels = filter_channels \n self . kernel_size = kernel_size \n self . n_layers = n_layers \n self . num_bins = num_bins \n self . tail_bound = tail_bound \n self . half_channels = in_channels // <NUM_LIT> \n self . pre = nn . Conv1d ( self . half_channels , filter_channels , <NUM_LIT> ) \n self . convs = DDSConv ( filter_channels , kernel_size , n_layers , p_dropout = <NUM_LIT> ) \n self . proj = nn . Conv1d ( \n filter_channels , self . half_channels * ( num_bins * <NUM_LIT> - <NUM_LIT> ) , <NUM_LIT> \n ) \n self . proj . weight . data . zero_ ( ) \n self . proj . bias . data . zero_ ( ) \n def forward ( self , x , x_mask , g = None , reverse = False ) : \n x0 , x1 = torch . split ( x , [ self . half_channels ] * <NUM_LIT> , <NUM_LIT> ) \n h = self . pre ( x0 ) \n h = self . convs ( h , x_mask , g = g ) \n h = self . proj ( h ) * x_mask \n b , c , t = x0 . shape \n h = h . reshape ( b , c , - <NUM_LIT> , t ) . permute ( <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ) \n unnormalized_widths = h [ ... , : self . num_bins ] / math . sqrt ( self . filter_channels ) \n unnormalized_heights = h [ ... , self . num_bins : <NUM_LIT> * self . num_bins ] / math . sqrt ( \n self . filter_channels \n ) \n unnormalized_derivatives = h [ ... , <NUM_LIT> * self . num_bins : ] \n x1 , logabsdet = piecewise_rational_quadratic_transform ( \n x1 , \n unnormalized_widths , \n unnormalized_heights , \n unnormalized_derivatives , \n inverse = reverse , \n tails = \"<STR_LIT>\" , \n tail_bound = self . tail_bound , \n ) \n x = torch . cat ( [ x0 , x1 ] , <NUM_LIT> ) * x_mask \n logdet = torch . sum ( logabsdet * x_mask , [ <NUM_LIT> , <NUM_LIT> ] ) \n if not reverse : \n return x , logdet \n else : \n return x"}, {"input": "class Persona : \n def accion_recibida ( self , insulto , golpe = None ) : \n if golpe is None : \n print ( f\"<STR_LIT>\" ) \n else : \n print ( f\"<STR_LIT>\" ) \n jesus = Persona ( ) \n jesus . accion_recibida ( \"<STR_LIT>\" ) \n jesus . accion_recibida ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n class MiClase : \n def mi_metodo ( self , * args ) : \n if len ( args ) == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n elif len ( args ) == <NUM_LIT> : \n print ( f\"<STR_LIT>\" ) \n else : \n print ( \"<STR_LIT>\" ) \n clase = MiClase ( )", "output": "clase . mi_metodo ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" )"}, {"input": "from flask import Flask \n from flasgger import Swagger \n def config_docs ( app : Flask ) : \n swagger = Swagger ( \n app , \n template = { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } , \n \"<STR_LIT>\" : [ \n \"<STR_LIT>\" \n ] , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : [ \n { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n ] , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n }", "output": "} \n )"}, {"input": "import os \n import torch \n from collections import OrderedDict \n def extract ( ckpt ) : \n a = ckpt [ \"<STR_LIT>\" ] \n opt = OrderedDict ( ) \n opt [ \"<STR_LIT>\" ] = { } \n for key in a . keys ( ) : \n if \"<STR_LIT>\" in key : \n continue \n opt [ \"<STR_LIT>\" ] [ key ] = a [ key ] \n return opt \n def model_blender ( name , path1 , path2 , ratio ) : \n try : \n message = f\"<STR_LIT>\" \n ckpt1 = torch . load ( path1 , map_location = \"<STR_LIT>\" ) \n ckpt2 = torch . load ( path2 , map_location = \"<STR_LIT>\" ) \n cfg = ckpt1 [ \"<STR_LIT>\" ] \n cfg_f0 = ckpt1 [ \"<STR_LIT>\" ] \n cfg_version = ckpt1 [ \"<STR_LIT>\" ] \n if \"<STR_LIT>\" in ckpt1 : \n ckpt1 = extract ( ckpt1 ) \n else : \n ckpt1 = ckpt1 [ \"<STR_LIT>\" ] \n if \"<STR_LIT>\" in ckpt2 : \n ckpt2 = extract ( ckpt2 ) \n else : \n ckpt2 = ckpt2 [ \"<STR_LIT>\" ] \n if sorted ( list ( ckpt1 . keys ( ) ) ) != sorted ( list ( ckpt2 . keys ( ) ) ) : \n return \"<STR_LIT>\" \n opt = OrderedDict ( ) \n opt [ \"<STR_LIT>\" ] = { } \n for key in ckpt1 . keys ( ) : \n if key == \"<STR_LIT>\" and ckpt1 [ key ] . shape != ckpt2 [ key ] . shape : \n min_shape0 = min ( ckpt1 [ key ] . shape [ <NUM_LIT> ] , ckpt2 [ key ] . shape [ <NUM_LIT> ] ) \n opt [ \"<STR_LIT>\" ] [ key ] = ( \n ratio * ( ckpt1 [ key ] [ : min_shape0 ] . float ( ) ) \n + ( <NUM_LIT> - ratio ) * ( ckpt2 [ key ] [ : min_shape0 ] . float ( ) ) \n ) . half ( ) \n else : \n opt [ \"<STR_LIT>\" ] [ key ] = ( \n ratio * ( ckpt1 [ key ] . float ( ) ) + ( <NUM_LIT> - ratio ) * ( ckpt2 [ key ] . float ( ) ) \n ) . half ( ) \n opt [ \"<STR_LIT>\" ] = cfg \n opt [ \"<STR_LIT>\" ] = message \n opt [ \"<STR_LIT>\" ] = cfg_f0 \n opt [ \"<STR_LIT>\" ] = cfg_version \n opt [ \"<STR_LIT>\" ] = message \n torch . save ( opt , os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" % name ) )", "output": "print ( message ) \n return message , os . path . join ( \"<STR_LIT>\" , \"<STR_LIT>\" % name ) \n except Exception as error : \n print ( error ) \n return error"}, {"input": "import asyncio \n import re \n from contextlib import ( \n AbstractAsyncContextManager , \n AbstractContextManager , \n asynccontextmanager , \n contextmanager , \n ) \n from typing import NewType \n import pytest \n import svcs \n from . fake_factories import ( \n async_bool_cm_factory , \n async_int_factory , \n async_str_gen_factory , \n ) \n from . helpers import Annotated , CloseMe , nop \n from . ifaces import AnotherService , Interface , Service , YetAnotherService \n def test_register_factory_get ( registry , container ) : \n registry . register_factory ( Service , Service ) \n svc = container . get ( Service ) \n assert isinstance ( svc , Service ) \n assert svc is container . get ( Service ) \n def test_register_factory_get_abstract ( registry , container ) : \n registry . register_factory ( Interface , Service ) \n svc = container . get_abstract ( Interface ) \n assert isinstance ( svc , Interface ) \n assert svc is container . get ( Interface ) \n def test_register_value_multiple ( registry , container ) : \n registry . register_value ( Service , <NUM_LIT> ) \n registry . register_value ( AnotherService , <NUM_LIT> ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == container . get ( Service , AnotherService ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == container . get ( Service , AnotherService ) \n S1 = Annotated [ Interface , \"<STR_LIT>\" ] \n S2 = NewType ( \"<STR_LIT>\" , Interface ) \n def test_get_annotated_multiple ( registry , container ) : \n registry . register_factory ( S1 , Service ) \n registry . register_factory ( S2 , AnotherService ) \n assert isinstance ( container . get ( S1 ) , Service ) \n assert isinstance ( container . get ( S2 ) , AnotherService ) \n def test_get_not_found ( container ) : \n with pytest . raises ( svcs . exceptions . ServiceNotFoundError ) as ei : \n container . get ( Service ) \n assert Service is ei . value . args [ <NUM_LIT> ] \n def test_passes_container_bc_name ( registry , container ) : \n def factory ( svcs_container ) : \n return str ( svcs_container . get ( int ) ) \n registry . register_value ( int , <NUM_LIT> ) \n registry . register_factory ( str , factory ) \n assert \"<STR_LIT>\" == container . get ( str ) \n def test_passes_container_bc_annotation ( registry , container ) : \n def factory ( foo : svcs . Container ) : \n return str ( foo . get ( int ) ) \n registry . register_value ( int , <NUM_LIT> ) \n registry . register_factory ( str , factory ) \n assert \"<STR_LIT>\" == container . get ( str ) \n def test_get_enter_false ( registry , container ) : \n entered = False \n @ contextmanager", "output": "def factory ( ) : \n nonlocal entered \n entered = True \n yield <NUM_LIT> \n registry . register_factory ( Service , factory , enter = False ) \n cm = container . get ( Service ) \n assert not entered \n assert isinstance ( cm , AbstractContextManager ) \n with cm as i : \n assert <NUM_LIT> == i \n assert entered \n def test_get_pings ( registry , container , svc ) : \n registry . register_factory ( AnotherService , AnotherService ) \n registry . register_value ( Service , svc , ping = nop ) \n assert [ Service ] == [ ping . _svc_type for ping in container . get_pings ( ) ] \n def test_cleanup_called ( registry , container , close_me ) : \n def factory ( ) : \n yield <NUM_LIT> \n close_me . close ( ) \n registry . register_factory ( Service , factory ) \n container . get ( Service ) \n assert not close_me . is_closed \n container . close ( ) \n assert close_me . is_closed \n assert not container . _instantiated \n assert not container . _on_close \n def test_close_resilient ( container , registry , caplog , close_me ) : \n def factory ( ) : \n yield <NUM_LIT> \n raise Exception \n def factory_no_boom ( ) : \n yield <NUM_LIT> \n close_me . close ( ) \n registry . register_factory ( Service , factory ) \n registry . register_factory ( YetAnotherService , factory_no_boom ) \n assert <NUM_LIT> == container . get ( Service ) \n assert <NUM_LIT> == container . get ( YetAnotherService ) \n assert not close_me . is_closed \n container . close ( ) \n assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name \n assert close_me . is_closed \n def test_none_is_a_valid_factory_result ( registry , container ) : \n i = <NUM_LIT> \n def factory ( ) : \n nonlocal i \n i += <NUM_LIT> \n yield None \n registry . register_factory ( Service , factory ) \n assert None is container . get ( Service ) \n assert None is container . get ( Service ) \n assert <NUM_LIT> == i \n container . close ( ) \n @ pytest . mark . parametrize ( \n \"<STR_LIT>\" , \n [ \n async_int_factory , \n async_str_gen_factory , \n async_bool_cm_factory , \n ] , \n ) \n def test_get_on_async_factory_raises_type_error ( \n registry , container , factory , recwarn \n ) : \n registry . register_factory ( Service , factory ) \n with pytest . raises ( \n TypeError , match = re . escape ( \"<STR_LIT>\" ) \n ) : \n container . get ( Service ) \n if recwarn . list : \n assert ( \n \"<STR_LIT>\" , \n ) == recwarn . pop ( ) . message . args \n def test_local_value_overrides_global_value ( registry , container ) : \n registry . register_value ( int , <NUM_LIT> ) \n assert container . _lazy_local_registry is None \n cm = CloseMe ( ) \n container . register_local_value ( int , <NUM_LIT> , on_registry_close = cm . close ) \n assert container . _lazy_local_registry . _on_close \n assert <NUM_LIT> == container . get ( int ) \n container . close ( ) \n assert not container . _lazy_local_registry . _on_close \n assert cm . is_closed \n def test_local_registry_is_lazy_but_only_once ( container ) : \n assert container . _lazy_local_registry is None \n container . register_local_value ( int , <NUM_LIT> ) \n reg = container . _lazy_local_registry \n container . register_local_value ( int , <NUM_LIT> ) \n assert reg is container . _lazy_local_registry \n @ pytest . mark . asyncio ( ) \n class TestAsync : \n async def test_async_factory ( self , registry , container ) : \n async def factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n return Service ( ) \n registry . register_factory ( Service , factory ) \n svc = await container . aget ( Service ) \n assert isinstance ( svc , Service ) \n assert svc is ( await container . aget ( Service ) ) \n async def test_aget_works_with_sync_factory ( self , registry , container ) : \n registry . register_factory ( Service , Service ) \n assert Service ( ) == ( await container . aget ( Service ) ) \n async def test_aget_works_with_value ( self , registry , container ) : \n registry . register_value ( Service , <NUM_LIT> ) \n registry . register_value ( AnotherService , <NUM_LIT> ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == ( await container . aget ( Service , AnotherService ) ) \n assert [ <NUM_LIT> , <NUM_LIT> ] == ( await container . aget ( Service , AnotherService ) ) \n async def test_aget_abstract_works_with_value ( self , registry , container ) : \n registry . register_value ( int , <NUM_LIT> ) \n registry . register_value ( str , \"<STR_LIT>\" ) \n assert [ <NUM_LIT> , \"<STR_LIT>\" ] == ( await container . aget_abstract ( int , str ) ) \n assert [ <NUM_LIT> , \"<STR_LIT>\" ] == ( await container . aget_abstract ( int , str ) ) \n async def test_aget_enter_false ( self , registry , container ) : \n entered = False \n @ asynccontextmanager \n async def factory ( ) : \n nonlocal entered \n entered = True \n yield <NUM_LIT> \n registry . register_factory ( Service , factory , enter = False ) \n cm = await container . aget ( Service ) \n assert not entered \n assert isinstance ( cm , AbstractAsyncContextManager ) \n async with cm as i : \n assert <NUM_LIT> == i \n assert entered \n async def test_passes_container_bc_name ( self , registry , container ) : \n async def factory ( svcs_container ) : \n return str ( svcs_container . get ( int ) ) \n registry . register_value ( int , <NUM_LIT> ) \n registry . register_factory ( str , factory ) \n assert \"<STR_LIT>\" == await container . aget ( str ) \n async def test_passes_container_bc_annotation ( self , registry , container ) : \n async def factory ( foo : svcs . Container ) : \n return str ( foo . get ( int ) ) \n registry . register_value ( int , <NUM_LIT> ) \n registry . register_factory ( str , factory ) \n assert \"<STR_LIT>\" == await container . aget ( str ) \n async def test_async_cleanup ( self , registry , container , close_me ) : \n async def factory ( ) : \n await asyncio . sleep ( <NUM_LIT> ) \n yield Service ( ) \n await asyncio . sleep ( <NUM_LIT> ) \n await close_me . aclose ( ) \n registry . register_factory ( Service , factory ) \n svc = await container . aget ( Service ) \n assert <NUM_LIT> == len ( container . _on_close ) \n assert Service ( ) == svc \n assert not close_me . is_aclosed \n await container . aclose ( ) \n assert close_me . is_aclosed \n assert not container . _instantiated \n assert not container . _on_close \n @ pytest . mark . asyncio ( ) \n async def test_aclose_resilient ( \n self , container , registry , caplog , close_me \n ) : \n def factory ( ) : \n yield <NUM_LIT> \n raise Exception \n async def async_factory ( ) : \n yield <NUM_LIT> \n raise Exception \n async def factory_no_boom ( ) : \n yield <NUM_LIT> \n await close_me . aclose ( ) \n registry . register_factory ( Service , factory ) \n registry . register_factory ( AnotherService , async_factory ) \n registry . register_factory ( YetAnotherService , factory_no_boom ) \n assert <NUM_LIT> == container . get ( Service ) \n assert <NUM_LIT> == await container . aget ( AnotherService ) \n assert <NUM_LIT> == await container . aget ( YetAnotherService ) \n assert not close_me . is_aclosed \n await container . aclose ( ) \n assert ( \n \"<STR_LIT>\" \n == caplog . records [ <NUM_LIT> ] . svcs_service_name \n ) \n assert \"<STR_LIT>\" == caplog . records [ <NUM_LIT> ] . svcs_service_name \n assert close_me . is_aclosed \n assert not container . _instantiated \n assert not container . _on_close \n async def test_aping ( self , registry , container ) : \n apinged = pinged = False \n async def aping ( svc ) : \n await asyncio . sleep ( <NUM_LIT> ) \n nonlocal apinged \n apinged = True \n def ping ( svc ) : \n nonlocal pinged \n pinged = True \n registry . register_value ( Service , Service ( ) , ping = aping ) \n registry . register_value ( AnotherService , AnotherService ( ) , ping = ping ) \n ( ap , p ) = container . get_pings ( ) \n assert ap . is_async \n assert not p . is_async \n await ap . aping ( ) \n await p . aping ( ) \n assert pinged \n assert apinged \n async def test_none_is_a_valid_factory_result ( self , registry , container ) : \n i = <NUM_LIT> \n async def factory ( ) : \n nonlocal i \n i += <NUM_LIT> \n yield None \n registry . register_factory ( Service , factory ) \n assert None is await container . aget ( Service ) \n assert None is await container . aget ( Service ) \n assert <NUM_LIT> == i \n await container . aclose ( ) \n async def test_local_factory_overrides_global_factory ( \n self , registry , container \n ) : \n cm = CloseMe ( ) \n container . register_local_factory ( \n int , async_int_factory , on_registry_close = cm . aclose \n ) \n registry . register_value ( int , <NUM_LIT> ) \n async with container : \n assert <NUM_LIT> == await container . aget ( int ) \n assert cm . is_aclosed"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . create_table ( '<STR_LIT>' , \n sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , \n sa . Column ( '<STR_LIT>' , sa . INTEGER ( ) , nullable = False ) , \n sa . Column ( '<STR_LIT>' , sa . VARCHAR ( ) , nullable = False ) , \n sa . ForeignKeyConstraint ( [ '<STR_LIT>' ] , [ '<STR_LIT>' ] , ) , \n sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , \n sa . UniqueConstraint ( '<STR_LIT>' )", "output": ") \n def downgrade ( ) -> None : \n op . drop_table ( '<STR_LIT>' )"}, {"input": "from . import * \n import requests \n from flask import request , abort , redirect \n def follow_redirects ( url , max_redirects = <NUM_LIT> ) : \n for _ in range ( max_redirects ) : \n response = requests . head ( url , allow_redirects = False ) \n if response . status_code == <NUM_LIT> : \n return url \n elif <NUM_LIT> <= response . status_code < <NUM_LIT> : \n url = response . headers [ '<STR_LIT>' ] \n else : \n abort ( <NUM_LIT> ) \n abort ( <NUM_LIT> ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) \n def cover_api ( ) : \n req_args = { key : request . args . get ( key ) for key in request . args } \n target_url = '<STR_LIT>' + '<STR_LIT>' . join ( [ f\"<STR_LIT>\" for key in req_args ] ) \n result = requests . get ( target_url , headers = { \"<STR_LIT>\" : \"<STR_LIT>\" } ) \n if result . status_code == <NUM_LIT> : \n return result . content , <NUM_LIT> , { \"<STR_LIT>\" : result . headers [ '<STR_LIT>' ] }", "output": "elif result . status_code == <NUM_LIT> : \n abort ( <NUM_LIT> ) \n else : \n abort ( <NUM_LIT> ) \n @ v1_bp . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n @ cache . cached ( timeout = <NUM_LIT> , key_prefix = make_cache_key ) \n def cover_new ( s_type ) : \n __endpoints__ = [ \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ] \n if s_type not in __endpoints__ : \n abort ( <NUM_LIT> ) \n req_args = { key : request . args . get ( key ) for key in request . args } \n target_url = f'<STR_LIT>' + '<STR_LIT>' . join ( [ f\"<STR_LIT>\" for key in req_args ] ) \n return redirect ( target_url , <NUM_LIT> )"}, {"input": "import werobot \n from config import channel_conf \n from common import const \n from common . log import logger \n from channel . channel import Channel \n from concurrent . futures import ThreadPoolExecutor \n robot = werobot . WeRoBot ( token = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n @ robot . text \n def hello_world ( msg ) :", "output": "logger . info ( '<STR_LIT>' . format ( msg . content , msg . source ) ) \n return WechatServiceAccount ( ) . handle ( msg ) \n class WechatServiceAccount ( Channel ) : \n def startup ( self ) : \n logger . info ( '<STR_LIT>' ) \n robot . config [ '<STR_LIT>' ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) \n robot . config [ \"<STR_LIT>\" ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) \n robot . config [ \"<STR_LIT>\" ] = channel_conf ( const . WECHAT_MP ) . get ( '<STR_LIT>' ) \n robot . config [ '<STR_LIT>' ] = '<STR_LIT>' \n robot . run ( ) \n def handle ( self , msg , count = <NUM_LIT> ) : \n context = { } \n context [ '<STR_LIT>' ] = msg . source \n thread_pool . submit ( self . _do_send , msg . content , context ) \n return \"<STR_LIT>\" \n def _do_send ( self , query , context ) : \n reply_text = super ( ) . build_reply_content ( query , context ) \n logger . info ( '<STR_LIT>' . format ( reply_text ) ) \n client = robot . client \n client . send_text_message ( context [ '<STR_LIT>' ] , reply_text )"}, {"input": "import numpy as np \n import os \n from sklearn . manifold import TSNE \n import matplotlib . pyplot as plt \n import argparse \n width = <NUM_LIT> \n walk_xlim = ( <NUM_LIT> , <NUM_LIT> )", "output": "walk_ylim = ( - <NUM_LIT> , <NUM_LIT> ) \n jump_xlim = ( - <NUM_LIT> , <NUM_LIT> ) \n jump_ylim = ( <NUM_LIT> , <NUM_LIT> ) \n def state_coverage ( points , dim1_range = ( - <NUM_LIT> , <NUM_LIT> ) , dim2_range = ( <NUM_LIT> , <NUM_LIT> ) ) : \n resolution = <NUM_LIT> \n num_bins = int ( <NUM_LIT> / resolution ) \n print ( points . shape , points [ : , <NUM_LIT> ] . min ( ) , points [ : , <NUM_LIT> ] . max ( ) , points [ : , <NUM_LIT> ] . min ( ) , points [ : , <NUM_LIT> ] . max ( ) ) \n points = np . clip ( ( points - np . array ( [ dim1_range [ <NUM_LIT> ] , dim2_range [ <NUM_LIT> ] ] ) ) / np . array ( [ dim1_range [ <NUM_LIT> ] - dim1_range [ <NUM_LIT> ] , dim2_range [ <NUM_LIT> ] - dim2_range [ <NUM_LIT> ] ] ) , <NUM_LIT> , <NUM_LIT> ) \n bins = np . zeros ( ( num_bins , num_bins ) ) \n for point in points : \n bin1 = min ( int ( point [ <NUM_LIT> ] * num_bins ) , num_bins - <NUM_LIT> ) \n bin2 = min ( int ( point [ <NUM_LIT> ] * num_bins ) , num_bins - <NUM_LIT> ) \n bins [ bin1 , bin2 ] += <NUM_LIT> \n score = np . sum ( bins > <NUM_LIT> ) / ( num_bins ** <NUM_LIT> ) \n return bins , score \n parser = argparse . ArgumentParser ( ) \n parser . add_argument ( \"<STR_LIT>\" , type = str , default = \"<STR_LIT>\" ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = False ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) \n parser . add_argument ( '<STR_LIT>' , action = '<STR_LIT>' , default = True ) \n args = parser . parse_args ( ) \n prefix = f\"<STR_LIT>\" + ( \"<STR_LIT>\" if args . jump else \"<STR_LIT>\" ) \n print ( prefix ) \n dir_name = f\"<STR_LIT>\" \n os . makedirs ( dir_name , exist_ok = True ) \n exptid = args . exptid \n data_raw = np . load ( f'<STR_LIT>' , allow_pickle = True ) \n features = data_raw [ : , : - <NUM_LIT> ] \n labels = data_raw [ : , - <NUM_LIT> ] \n print ( features . shape , labels . shape ) \n if args . tsne : \n tsne = TSNE ( n_components = <NUM_LIT> , perplexity = <NUM_LIT> , random_state = <NUM_LIT> , verbose = <NUM_LIT> ) \n tsne_features = tsne . fit_transform ( features ) \n fig , ax = plt . subplots ( ) \n unique_labels = np . unique ( labels ) \n i = <NUM_LIT> \n for label in unique_labels : \n indices = np . where ( labels == label ) \n x = tsne_features [ indices , <NUM_LIT> ] \n y = tsne_features [ indices , <NUM_LIT> ] \n ax . scatter ( x , y , label = label , alpha = <NUM_LIT> , marker = \"<STR_LIT>\" ) \n if i == <NUM_LIT> : \n break \n i += <NUM_LIT> \n ax . legend ( ) \n plt . title ( f'<STR_LIT>' ) \n plt . show ( ) \n if args . histogram : \n feature_t = features [ : , : <NUM_LIT> ] \n fig , axes = plt . subplots ( <NUM_LIT> , <NUM_LIT> , figsize = ( <NUM_LIT> , <NUM_LIT> ) ) \n for i in range ( <NUM_LIT> ) : \n ax = axes [ i // <NUM_LIT> , i % <NUM_LIT> ] \n f0 = feature_t [ labels == i ] + <NUM_LIT> \n uniques , counts = np . unique ( f0 , axis = <NUM_LIT> , return_counts = True ) \n print ( uniques . shape , counts . shape ) \n uniques_dec = uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] * <NUM_LIT> + uniques [ : , <NUM_LIT> ] \n print ( uniques_dec . shape ) \n hist_values = np . repeat ( uniques_dec , counts ) \n ax . hist ( hist_values , bins = len ( uniques_dec ) + <NUM_LIT> ) \n ax . set_xlabel \n ax . set_xlabel ( \"<STR_LIT>\" ) \n ax . set_ylabel ( \"<STR_LIT>\" ) \n ax . set_title ( \"<STR_LIT>\" ) \n if args . traj : \n print ( \"<STR_LIT>\" * width , \"<STR_LIT>\" , \"<STR_LIT>\" * width ) \n def draw_traj ( features , labels , xlim , ylim , xlabel , ylabel ) : \n height = features [ : , <NUM_LIT> ] \n pitch = features [ : , <NUM_LIT> ] \n unique_labels = np . unique ( labels ) \n fig , axes = plt . subplots ( <NUM_LIT> , unique_labels . shape [ <NUM_LIT> ] // <NUM_LIT> , figsize = ( <NUM_LIT> , <NUM_LIT> ) ) \n for i , label in enumerate ( unique_labels ) : \n indices = np . where ( labels == label ) \n x = height [ indices ] \n y = pitch [ indices ] \n ax = axes [ i % <NUM_LIT> , i // <NUM_LIT> ] \n ax . scatter ( x , y , label = label , alpha = <NUM_LIT> , marker = \"<STR_LIT>\" , color = \"<STR_LIT>\" ) \n ax . legend ( ) \n ax . set_xlim ( xlim ) \n ax . set_ylim ( ylim ) \n ax . set_xlabel ( xlabel ) \n ax . set_ylabel ( ylabel ) \n ax . set_title ( prefix ) \n return fig \n if args . jump : \n fig = draw_traj ( features , labels , jump_xlim , jump_ylim , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n else : \n fig = draw_traj ( features , labels , walk_xlim , walk_ylim , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n fig . savefig ( f\"<STR_LIT>\" , bbox_inches = '<STR_LIT>' ) \n if args . coverage : \n print ( \"<STR_LIT>\" * width , \"<STR_LIT>\" , \"<STR_LIT>\" * width ) \n if args . jump : \n bins , score = state_coverage ( features [ : , : <NUM_LIT> ] , dim1_range = jump_xlim , dim2_range = jump_ylim ) \n else : \n bins , score = state_coverage ( features [ : , : <NUM_LIT> ] , dim1_range = walk_xlim , dim2_range = walk_ylim ) \n print ( f\"<STR_LIT>\" ) \n norm_bins = np . clip ( bins / np . max ( bins ) , <NUM_LIT> , <NUM_LIT> ) \n fig , ax = plt . subplots ( ) \n ax . imshow ( norm_bins , cmap = \"<STR_LIT>\" , interpolation = \"<STR_LIT>\" ) \n ax . set_title ( prefix + f\"<STR_LIT>\" ) \n fig . savefig ( f\"<STR_LIT>\" , bbox_inches = '<STR_LIT>' ) \n if args . scatter : \n def scatter ( features , labels , xlim , ylim , xlabel , ylabel ) : \n height = features [ : , <NUM_LIT> ] \n pitch = features [ : , <NUM_LIT> ] \n fig , ax = plt . subplots ( ) \n unique_labels = np . unique ( labels ) \n for i , label in enumerate ( unique_labels ) : \n indices = np . where ( labels == label ) \n x = height [ indices ] \n y = pitch [ indices ] \n ax . scatter ( x [ : ] , y [ : ] , label = label , alpha = <NUM_LIT> , marker = \"<STR_LIT>\" ) \n ax . legend ( ) \n ax . set_xlim ( xlim ) \n ax . set_ylim ( ylim ) \n ax . set_xlabel ( xlabel ) \n ax . set_ylabel ( ylabel ) \n ax . set_title ( prefix ) \n return fig \n if args . jump : \n fig = scatter ( features , labels , jump_xlim , jump_ylim , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n else : \n fig = scatter ( features , labels , walk_xlim , walk_ylim , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n fig . savefig ( f\"<STR_LIT>\" , bbox_inches = '<STR_LIT>' )"}, {"input": "import os , sys \n import json \n from pathlib import Path \n from locale import getdefaultlocale \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n class I18nAuto : \n LANGUAGE_PATH = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def __init__ ( self , language = None ) : \n with open ( \n os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" \n ) as file : \n config = json . load ( file ) \n override = config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n lang_prefix = config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n self . language = lang_prefix \n if override == False : \n language = language or getdefaultlocale ( ) [ <NUM_LIT> ] \n lang_prefix = language [ : <NUM_LIT> ] if language is not None else \"<STR_LIT>\" \n available_languages = self . _get_available_languages ( ) \n matching_languages = [ \n lang for lang in available_languages if lang . startswith ( lang_prefix ) \n ] \n self . language = matching_languages [ <NUM_LIT> ] if matching_languages else \"<STR_LIT>\" \n self . language_map = self . _load_language_list ( ) \n def _load_language_list ( self ) : \n try : \n file_path = Path ( self . LANGUAGE_PATH ) / f\"<STR_LIT>\" \n with open ( file_path , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n return json . load ( file ) \n except FileNotFoundError : \n raise FileNotFoundError (", "output": "f\"<STR_LIT>\" \n ) \n def _get_available_languages ( self ) : \n language_files = [ path . stem for path in Path ( self . LANGUAGE_PATH ) . glob ( \"<STR_LIT>\" ) ] \n return language_files \n def _language_exists ( self , language ) : \n return ( Path ( self . LANGUAGE_PATH ) / f\"<STR_LIT>\" ) . exists ( ) \n def __call__ ( self , key ) : \n return self . language_map . get ( key , key )"}, {"input": "import os \n import requests \n import tarfile \n from tqdm import tqdm \n import logging \n import time \n from densely_captioned_images . dataset . config import DATASET_BASE , MODEL_BASE \n import hashlib \n RESOURCES = { \n '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } , \n '<STR_LIT>' : { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' \n } , \n } \n def check_checksum ( file_download_location , target_checksum ) : \n sha256_hash = hashlib . sha256 ( ) \n with open ( file_download_location , '<STR_LIT>' ) as f : \n for byte_block in iter ( lambda : f . read ( <NUM_LIT> ) , b\"<STR_LIT>\" ) : \n sha256_hash . update ( byte_block ) \n assert sha256_hash . hexdigest ( ) == target_checksum , ( \n f\"<STR_LIT>\" \n ) \n return \n def download_file ( target_dir , file_meta , num_retries = <NUM_LIT> ) : \n url = file_meta [ '<STR_LIT>' ] \n outfile = file_meta [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] \n download = True", "output": "logging . info ( f\"<STR_LIT>\" ) \n retry = num_retries \n exp_backoff = [ <NUM_LIT> ** r for r in reversed ( range ( retry ) ) ] \n pbar = tqdm ( unit = '<STR_LIT>' , unit_scale = True , desc = '<STR_LIT>' . format ( outfile ) ) \n while download and retry > <NUM_LIT> : \n response = None \n with requests . Session ( ) as session : \n try : \n response = session . get ( url , stream = True , timeout = <NUM_LIT> ) \n CHUNK_SIZE = <NUM_LIT> \n total_size = int ( response . headers . get ( '<STR_LIT>' , - <NUM_LIT> ) ) \n pbar . total = total_size \n done = <NUM_LIT> \n with open ( outfile , '<STR_LIT>' ) as f : \n for chunk in response . iter_content ( CHUNK_SIZE ) : \n if chunk : \n f . write ( chunk ) \n if total_size > <NUM_LIT> : \n done += len ( chunk ) \n if total_size < done : \n total_size = done \n pbar . total = total_size \n pbar . update ( len ( chunk ) ) \n break \n except ( \n requests . exceptions . ConnectionError , \n requests . exceptions . ReadTimeout , \n ) : \n retry -= <NUM_LIT> \n pbar . clear ( ) \n if retry > <NUM_LIT> : \n pl = '<STR_LIT>' if retry == <NUM_LIT> else '<STR_LIT>' \n logging . debug ( \n f'<STR_LIT>' \n ) \n time . sleep ( exp_backoff [ retry ] ) \n else : \n logging . error ( '<STR_LIT>' ) \n finally : \n if response : \n response . close ( ) \n if retry <= <NUM_LIT> : \n raise RuntimeError ( '<STR_LIT>' ) \n if download and retry > <NUM_LIT> : \n pbar . update ( done - pbar . n ) \n if done < total_size : \n raise RuntimeError ( \n f'<STR_LIT>' \n f'<STR_LIT>' \n ) \n pbar . close ( ) \n check_checksum ( outfile , file_meta [ '<STR_LIT>' ] ) \n logging . info ( f\"<STR_LIT>\" ) \n os . makedirs ( target_dir , exist_ok = True ) \n with tarfile . open ( outfile , '<STR_LIT>' ) as tar : \n for member in tqdm ( iterable = tar . getmembers ( ) , total = len ( tar . getmembers ( ) ) ) : \n tar . extract ( path = target_dir , member = member ) \n os . unlink ( outfile ) \n def run_downloads ( ) : \n for dsname in [ '<STR_LIT>' ] : \n download_file ( os . path . dirname ( DATASET_BASE ) , RESOURCES [ dsname ] ) \n for modelname in [ '<STR_LIT>' , '<STR_LIT>' ] : \n download_file ( MODEL_BASE , RESOURCES [ modelname ] ) \n if __name__ == '<STR_LIT>' : \n run_downloads ( )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n pass", "output": "def downgrade ( ) -> None : \n with op . batch_alter_table ( '<STR_LIT>' , schema = None ) as batch_op : \n pass"}, {"input": "from mephisto . operations . operator import Operator \n from mephisto . tools . scripts import ( \n task_script , \n build_custom_bundle , \n ) \n from mephisto . abstractions . blueprints . abstract . static_task . static_blueprint import ( \n SharedStaticTaskState , \n ) \n from omegaconf import DictConfig \n from dataclasses import dataclass , field \n import cv2 \n import base64 \n import random \n from PIL import Image \n from mephisto . data_model . qualification import QUAL_NOT_EXIST , QUAL_EXISTS \n from mephisto . utils . qualifications import make_qualification_dict \n from mephisto . operations . hydra_config import build_default_task_config \n from typing import Any , Dict , List , Optional \n import os \n import json \n AREA_CUTOFF = <NUM_LIT> \n NUM_TASKS = <NUM_LIT> \n LOW = <NUM_LIT> \n HIGH = <NUM_LIT> \n BASE_SOURCE_PATH = os . path . expanduser ( \"<STR_LIT>\" ) \n BASE_IMAGE_PATH = os . path . join ( BASE_SOURCE_PATH , '<STR_LIT>' ) \n BASE_MASK_PATH = os . path . join ( BASE_SOURCE_PATH , '<STR_LIT>' )", "output": "PILOT_QUALIFICATION = '<STR_LIT>' \n ALLOWLIST_QUALIFICATION = '<STR_LIT>' \n @ dataclass \n class LongCapsConfig ( build_default_task_config ( \"<STR_LIT>\" ) ) : \n idx_start : int = field ( \n default = LOW , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n idx_end : int = field ( \n default = HIGH , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n is_pilot : Optional [ bool ] = field ( \n default = None , \n metadata = { \"<STR_LIT>\" : \"<STR_LIT>\" } , \n ) \n def build_tasks ( num_tasks , lo , hi ) : \n tasks = [ ] \n all_masks = os . listdir ( BASE_MASK_PATH ) \n use_masks = all_masks [ lo : hi ] \n idx = <NUM_LIT> \n while idx < len ( use_masks ) and len ( tasks ) < num_tasks : \n mask_name = use_masks [ idx ] \n idx += <NUM_LIT> \n img_name = mask_name [ : - len ( '<STR_LIT>' ) ] \n image_path = os . path . join ( BASE_IMAGE_PATH , img_name ) \n with open ( image_path , \"<STR_LIT>\" ) as img_file : \n b64_image = base64 . b64encode ( img_file . read ( ) ) \n image = cv2 . imread ( image_path ) \n image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) \n mask_path = os . path . join ( BASE_MASK_PATH , mask_name ) \n try : \n with open ( mask_path , '<STR_LIT>' ) as mask_file : \n mask_data = json . load ( mask_file ) \n except Exception as e : \n print ( \"<STR_LIT>\" , e ) \n continue \n masks_unrolled = { } \n curr_idx = <NUM_LIT> \n def unroll_mask_into ( curr_mask_layer , target ) : \n nonlocal curr_idx \n curr_mask_layer [ '<STR_LIT>' ] = curr_idx \n curr_idx += <NUM_LIT> \n if curr_mask_layer [ '<STR_LIT>' ] < AREA_CUTOFF : \n return \n curr_mask_layer [ '<STR_LIT>' ] = [ ] \n curr_mask_layer [ '<STR_LIT>' ] = - <NUM_LIT> \n for deeper_mask_layer in sorted ( \n list ( curr_mask_layer [ '<STR_LIT>' ] . values ( ) ) , \n key = lambda x : x [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] * <NUM_LIT> + x [ '<STR_LIT>' ] [ <NUM_LIT> ] [ <NUM_LIT> ] , \n ) : \n unroll_mask_into ( deeper_mask_layer , target ) \n deeper_mask_layer [ '<STR_LIT>' ] = curr_mask_layer [ '<STR_LIT>' ] \n curr_mask_layer [ '<STR_LIT>' ] . append ( deeper_mask_layer [ '<STR_LIT>' ] ) \n del curr_mask_layer [ '<STR_LIT>' ] \n target [ curr_mask_layer [ '<STR_LIT>' ] ] = curr_mask_layer \n ( top , left ) , ( bottom , right ) = curr_mask_layer [ '<STR_LIT>' ] \n assert top < bottom and left < right , \"<STR_LIT>\" \n curr_mask_layer [ '<STR_LIT>' ] = { '<STR_LIT>' : { '<STR_LIT>' : left , '<STR_LIT>' : top } , '<STR_LIT>' : { '<STR_LIT>' : right , '<STR_LIT>' : bottom } } \n curr_mask_layer [ '<STR_LIT>' ] = '<STR_LIT>' \n curr_mask_layer [ '<STR_LIT>' ] = '<STR_LIT>' \n curr_mask_layer [ '<STR_LIT>' ] = <NUM_LIT> \n try : \n for entry in mask_data . values ( ) : \n unroll_mask_into ( entry , masks_unrolled ) \n except AssertionError : \n continue \n image_data = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + b64_image . decode ( '<STR_LIT>' ) , \n \"<STR_LIT>\" : image . shape [ <NUM_LIT> ] , \n \"<STR_LIT>\" : image . shape [ <NUM_LIT> ] , \n } \n tasks . append ( \n { \n \"<STR_LIT>\" : mask_name , \n \"<STR_LIT>\" : masks_unrolled , \n \"<STR_LIT>\" : image_data , \n } \n ) \n print ( len ( tasks ) , len ( all_masks ) , len ( use_masks ) ) \n return tasks \n @ task_script ( config = LongCapsConfig ) \n def main ( operator : Operator , cfg : DictConfig ) -> None : \n shared_state = SharedStaticTaskState ( \n static_task_data = build_tasks ( cfg . num_tasks , cfg . idx_start , cfg . idx_end ) , \n ) \n if cfg . is_pilot is True : \n shared_state . qualifications = [ \n make_qualification_dict ( \n PILOT_QUALIFICATION , \n QUAL_EXISTS , \n None , \n ) , \n make_qualification_dict ( \n ALLOWLIST_QUALIFICATION , \n QUAL_NOT_EXIST , \n None , \n ) , \n ] \n elif cfg . is_pilot is False : \n shared_state . qualifications = [ \n make_qualification_dict ( \n ALLOWLIST_QUALIFICATION , \n QUAL_EXISTS , \n None , \n ) , \n ] \n task_dir = cfg . task_dir \n build_custom_bundle ( \n task_dir , \n force_rebuild = cfg . mephisto . task . force_rebuild , \n ) \n operator . launch_task_run ( cfg . mephisto , shared_state ) \n operator . wait_for_runs_then_shutdown ( skip_input = True , log_rate = <NUM_LIT> ) \n if __name__ == \"<STR_LIT>\" : \n main ( )"}, {"input": "import jwt \n import datetime \n import time \n from flask import jsonify , request \n from common import const \n from config import channel_conf \n class Auth ( ) : \n def __init__ ( self , login ) : \n self . login = login \n super ( Auth , self ) . __init__ ( ) \n @ staticmethod \n def encode_auth_token ( user_id , login_time ) : \n try : \n payload = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : datetime . datetime . utcnow ( ) + datetime . timedelta ( days = <NUM_LIT> , hours = <NUM_LIT> ) , \n '<STR_LIT>' : datetime . datetime . utcnow ( ) , \n '<STR_LIT>' : { \n '<STR_LIT>' : user_id , \n '<STR_LIT>' : login_time \n } \n } \n return jwt . encode ( \n payload , \n channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) , \n algorithm = '<STR_LIT>' \n ) \n except Exception as e : \n return e \n @ staticmethod \n def decode_auth_token ( auth_token ) : \n try : \n payload = jwt . decode ( auth_token , channel_conf ( const . HTTP ) . get ( \n '<STR_LIT>' ) , algorithms = '<STR_LIT>' ) \n if ( '<STR_LIT>' in payload and '<STR_LIT>' in payload [ '<STR_LIT>' ] ) : \n return payload \n else : \n raise jwt . InvalidTokenError \n except jwt . ExpiredSignatureError : \n return '<STR_LIT>' \n except jwt . InvalidTokenError : \n return '<STR_LIT>' \n def authenticate ( password ) : \n authPassword = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' ) \n if ( authPassword != password ) : \n return False \n else : \n login_time = time . strftime ( \"<STR_LIT>\" , time . localtime ( ) ) \n token = Auth . encode_auth_token ( password , login_time ) \n return token \n def identify ( request ) : \n try : \n authPassword = channel_conf ( const . HTTP ) . get ( '<STR_LIT>' )", "output": "if ( not authPassword ) : \n return True \n if ( request is None ) : \n return False \n authorization = request . cookies . get ( '<STR_LIT>' ) \n if ( authorization ) : \n payload = Auth . decode_auth_token ( authorization ) \n if not isinstance ( payload , str ) : \n authPassword = channel_conf ( \n const . HTTP ) . get ( '<STR_LIT>' ) \n password = payload [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if ( password != authPassword ) : \n return False \n else : \n return True \n return False \n except jwt . ExpiredSignatureError : \n return False \n except jwt . InvalidTokenError : \n return False"}, {"input": "from . contact import load_contact \n from . hotreload import load_hotreload \n from . login import load_login \n from . messages import load_messages \n from . register import load_register \n def load_components ( core ) : \n load_contact ( core ) \n load_hotreload ( core ) \n load_login ( core )", "output": "load_messages ( core ) \n load_register ( core )"}, {"input": "import random \n from flask import Flask , request , render_template_string \n from jinja2 import Template \n app = Flask ( __name__ ) \n blacklist = [ \n ]", "output": "@ app . route ( \"<STR_LIT>\" , methods = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] ) \n def index ( ) : \n name = request . args . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if any ( w in name for w in blacklist ) : \n return \"<STR_LIT>\" \n template = \n return render_template_string ( template ) \n if __name__ == \"<STR_LIT>\" : \n app . run ( host = \"<STR_LIT>\" , port = <NUM_LIT> )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op \n from werkzeug . security import generate_password_hash \n revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n table = op . create_table ( '<STR_LIT>' , \n sa . Column ( '<STR_LIT>' , sa . Integer ( ) , nullable = False ) , \n sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , \n sa . Column ( '<STR_LIT>' , sa . String ( length = <NUM_LIT> ) , nullable = False ) , \n sa . PrimaryKeyConstraint ( '<STR_LIT>' ) , \n sa . UniqueConstraint ( '<STR_LIT>' ) \n ) \n op . bulk_insert ( \n table , \n [ { \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : generate_password_hash ( \"<STR_LIT>\" ) } ] ) \n def downgrade ( ) -> None :", "output": "op . drop_table ( '<STR_LIT>' )"}, {"input": "import base64 \n import itertools \n import mutagen . ogg \n import mutagen . oggvorbis \n import mutagen . oggopus \n import mutagen . oggflac \n import mutagen . oggtheora \n import mutagen . oggspeex \n from . import util \n from . file import Artwork , AudioFile , MetadataItem , TAG_MAP_ENTRY \n def get_pictures ( afile , norm_key ) : \n artworks = [ ] \n pics_dat = afile . mfile . get ( \"<STR_LIT>\" , [ ] ) \n mimes = afile . mfile . get ( \"<STR_LIT>\" , [ ] ) \n for dat , mime in itertools . zip_longest ( pics_dat , mimes , fillvalue = \"<STR_LIT>\" ) : \n image_data = base64 . b64decode ( dat . encode ( \"<STR_LIT>\" ) ) \n artworks = Artwork ( image_data ) \n for p in afile . mfile . tags [ '<STR_LIT>' ] : \n pb = util . parse_picture_block ( base64 . standard_b64decode ( p ) ) \n art = Artwork ( pb . data , width = pb . width , height = pb . height , fmt = pb . format ) \n artworks . append ( art ) \n return MetadataItem ( Artwork , None , artworks ) \n def set_pictures ( afile , norm_key , artworks ) : \n if not isinstance ( artworks , MetadataItem ) : \n raise TypeError ( ) \n pics = [ ] \n for i , art in enumerate ( artworks . values ) : \n if any ( v is None for v in ( art . mime , art . width , art . height , art . depth ) ) : \n raise ImportError ( \"<STR_LIT>\" ) \n pic = mutagen . flac . Picture ( ) \n pic . data = art . raw \n pic . type = art . pic_type \n pic . mime = art . mime \n pic . width = art . width \n pic . height = art . height \n pic . depth = art . depth \n pic_data = base64 . b64encode ( pic . write ( ) ) . decode ( '<STR_LIT>' ) \n pics . append ( pic_data ) \n afile . mfile . tags [ '<STR_LIT>' ] = pics \n def rm_pictures ( afile , norm_key ) : \n for k in ( '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ) : \n if k in afile . mfile . tags : \n del afile . mfile . tags [ k ] \n class OggFile ( AudioFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . ogg . OggFileType \n _TAG_MAP = { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n setter = ( '<STR_LIT>' , '<STR_LIT>' ) , \n type = int , sanitizer = util . sanitize_year ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = '<STR_LIT>' , setter = '<STR_LIT>' , \n type = int , sanitizer = util . sanitize_bool ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = get_pictures , setter = set_pictures , \n remover = rm_pictures , \n type = Artwork ) , \n } \n def _ft_setter ( self , key , md_val , appendable = True ) : \n if self . appendable and appendable : \n self . mfile . tags [ key ] = [ str ( v ) for v in md_val . values ] \n else : \n self . mfile . tags [ key ] = str ( md_val . value ) \n class OggFlacFile ( OggFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . oggflac . OggFLAC \n class OggSpeexFile ( OggFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . oggspeex . OggSpeex \n class OggTheoraFile ( OggFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . oggtheora . OggTheora \n class OggVorbisFile ( OggFile ) : \n tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . oggvorbis . OggVorbis \n _TAG_MAP = OggFile . _TAG_MAP . copy ( ) \n _TAG_MAP . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n } ) \n class OggOpusFile ( OggFile ) :", "output": "tag_format = \"<STR_LIT>\" \n mutagen_kls = mutagen . oggopus . OggOpus \n _TAG_MAP = OggFile . _TAG_MAP . copy ( ) \n _TAG_MAP . update ( { \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : '<STR_LIT>' , \n type = str ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n '<STR_LIT>' : TAG_MAP_ENTRY ( getter = lambda afile , norm_key : None , \n type = int ) , \n } )"}, {"input": "from model . model import Model \n from config import model_conf \n from common import const \n from common . log import logger \n import requests \n import time \n sessions = { } \n class YiyanModel ( Model ) : \n def __init__ ( self ) : \n self . acs_token = model_conf ( const . BAIDU ) . get ( '<STR_LIT>' ) \n self . cookie = model_conf ( const . BAIDU ) . get ( '<STR_LIT>' ) \n self . base_url = '<STR_LIT>' \n def reply ( self , query , context = None ) : \n logger . info ( \"<STR_LIT>\" . format ( query ) ) \n user_id = context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) \n context [ '<STR_LIT>' ] = query \n chat_session_id = sessions . get ( user_id ) \n if not chat_session_id : \n self . new_session ( context ) \n sessions [ user_id ] = context [ '<STR_LIT>' ] \n else : \n context [ '<STR_LIT>' ] = chat_session_id \n flag = self . new_chat ( context ) \n if not flag : \n return \"<STR_LIT>\" \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n self . query ( context , <NUM_LIT> , <NUM_LIT> ) \n return context [ '<STR_LIT>' ] \n def new_session ( self , context ) : \n data = { \n \"<STR_LIT>\" : context [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = self . _create_header ( ) , json = data ) \n context [ '<STR_LIT>' ] = res . json ( ) [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n logger . info ( \"<STR_LIT>\" . format ( context [ '<STR_LIT>' ] ) ) \n def new_chat ( self , context ) : \n headers = self . _create_header ( ) \n headers [ '<STR_LIT>' ] = self . acs_token \n data = { \n \"<STR_LIT>\" : context . get ( '<STR_LIT>' ) , \n \"<STR_LIT>\" : context [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : int ( time . time ( ) * <NUM_LIT> ) , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = headers , json = data ) . json ( ) \n if res [ '<STR_LIT>' ] != <NUM_LIT> : \n logger . error ( \"<STR_LIT>\" , res [ '<STR_LIT>' ] ) \n return False \n context [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n context [ '<STR_LIT>' ] = res [ '<STR_LIT>' ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n return True \n def query ( self , context , sentence_id , count ) : \n headers = self . _create_header ( ) \n headers [ '<STR_LIT>' ] = self . acs_token \n data = { \n \"<STR_LIT>\" : context [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : context [ '<STR_LIT>' ] , \n \"<STR_LIT>\" : sentence_id , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : <NUM_LIT> , \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n res = requests . post ( url = self . base_url + '<STR_LIT>' , headers = headers , json = data )", "output": "logger . debug ( \"<STR_LIT>\" . format ( sentence_id , count , res . text ) ) \n res = res . json ( ) \n if res [ '<STR_LIT>' ] [ '<STR_LIT>' ] != '<STR_LIT>' : \n context [ '<STR_LIT>' ] += res [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if res [ '<STR_LIT>' ] [ '<STR_LIT>' ] == <NUM_LIT> : \n return \n if count > <NUM_LIT> : \n return \n time . sleep ( <NUM_LIT> ) \n if not res [ '<STR_LIT>' ] [ '<STR_LIT>' ] : \n return self . query ( context , sentence_id , count + <NUM_LIT> ) \n else : \n return self . query ( context , sentence_id + <NUM_LIT> , count + <NUM_LIT> ) \n def _create_header ( self ) : \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : self . cookie \n } \n return headers"}, {"input": "import json \n import hmac \n import hashlib \n import base64 \n import time \n import requests \n from urllib . parse import quote_plus \n from common import log \n from flask import Flask , request , render_template , make_response \n from common import const \n from common import functions \n from config import channel_conf \n from config import channel_conf_val \n from channel . channel import Channel \n class DingTalkHandler ( ) : \n def __init__ ( self , config ) : \n self . dingtalk_key = config . get ( '<STR_LIT>' ) \n self . dingtalk_secret = config . get ( '<STR_LIT>' ) \n self . dingtalk_token = config . get ( '<STR_LIT>' ) \n self . dingtalk_post_token = config . get ( '<STR_LIT>' ) \n self . access_token = None \n log . info ( \"<STR_LIT>\" . format ( self . dingtalk_key , self . dingtalk_secret , self . dingtalk_token , self . dingtalk_post_token ) ) \n def notify_dingtalk_webhook ( self , data ) : \n timestamp = round ( time . time ( ) * <NUM_LIT> ) \n secret_enc = bytes ( self . dingtalk_secret , encoding = '<STR_LIT>' ) \n string_to_sign = '<STR_LIT>' . format ( timestamp , self . dingtalk_secret ) \n string_to_sign_enc = bytes ( string_to_sign , encoding = '<STR_LIT>' ) \n hmac_code = hmac . new ( secret_enc , string_to_sign_enc , \n digestmod = hashlib . sha256 ) . digest ( ) \n sign = quote_plus ( base64 . b64encode ( hmac_code ) ) \n notify_url = f\"<STR_LIT>\" \n try : \n log . info ( \"<STR_LIT>\" . format ( str ( notify_url ) ) ) \n r = requests . post ( notify_url , json = data ) \n reply = r . json ( ) \n log . info ( \"<STR_LIT>\" . format ( str ( reply ) ) ) \n except Exception as e : \n log . error ( e ) \n def get_token_internal ( self ) : \n access_token_url = '<STR_LIT>' \n try : \n r = requests . post ( access_token_url , json = { \"<STR_LIT>\" : self . dingtalk_key , \"<STR_LIT>\" : self . dingtalk_secret } ) \n except : \n raise Exception ( \"<STR_LIT>\" ) \n data = json . loads ( r . content ) \n access_token = data [ '<STR_LIT>' ] \n expire_in = data [ '<STR_LIT>' ] \n self . access_token = access_token \n self . expire_at = int ( expire_in ) + time . time ( ) \n return self . access_token \n def get_token ( self ) : \n if self . access_token is None or self . expire_at <= time . time ( ) : \n self . get_token_internal ( ) \n return self . access_token \n def get_post_url ( self , data ) : \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n return f\"<STR_LIT>\" \n else : \n return f\"<STR_LIT>\" \n def build_response ( self , reply , data ) : \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n return self . build_oto_response ( reply , data ) \n else : \n return self . build_group_response ( reply , data ) \n def build_oto_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n img_match_prefix = functions . check_prefix ( \n prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robotCode = data [ '<STR_LIT>' ] \n if img_match_prefix and isinstance ( reply , list ) : \n images = \"<STR_LIT>\" \n for url in reply : \n images += f\"<STR_LIT>\" \n reply = images \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" + nick + \"<STR_LIT>\" , \n \"<STR_LIT>\" : images + \"<STR_LIT>\" + \"<STR_LIT>\" + nick \n } ) , \n \"<STR_LIT>\" : robotCode , \n \"<STR_LIT>\" : [ staffid ] \n } \n else : \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : reply \n } ) , \n \"<STR_LIT>\" : robotCode , \n \"<STR_LIT>\" : [ staffid ] \n } \n return resp \n def build_group_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n img_match_prefix = functions . check_prefix ( \n prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robot_code = data [ '<STR_LIT>' ] \n if img_match_prefix and isinstance ( reply , list ) : \n images = \"<STR_LIT>\" \n for url in reply : \n images += f\"<STR_LIT>\" \n reply = images \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : \"<STR_LIT>\" + nick + \"<STR_LIT>\" , \n \"<STR_LIT>\" : images + \"<STR_LIT>\" + \"<STR_LIT>\" + nick \n } ) , \n \"<STR_LIT>\" : robot_code , \n \"<STR_LIT>\" : conversation_id , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n else : \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( { \n \"<STR_LIT>\" : reply + \"<STR_LIT>\" + \"<STR_LIT>\" + nick \n } ) , \n \"<STR_LIT>\" : robot_code , \n \"<STR_LIT>\" : conversation_id , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n return resp \n def build_webhook_response ( self , reply , data ) : \n conversation_id = data [ '<STR_LIT>' ] \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n img_match_prefix = functions . check_prefix ( \n prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) \n nick = data [ '<STR_LIT>' ] \n staffid = data [ '<STR_LIT>' ] \n robotCode = data [ '<STR_LIT>' ] \n if img_match_prefix and isinstance ( reply , list ) : \n images = \"<STR_LIT>\" \n for url in reply : \n images += f\"<STR_LIT>\" \n reply = images \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : \"<STR_LIT>\" + nick + \"<STR_LIT>\" , \n \"<STR_LIT>\" : images + \"<STR_LIT>\" + \"<STR_LIT>\" + nick \n } , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n else : \n resp = { \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : reply", "output": "} , \n \"<STR_LIT>\" : { \n \"<STR_LIT>\" : [ \n staffid \n ] , \n \"<STR_LIT>\" : False \n } \n } \n return resp \n def chat ( self , channel , data ) : \n reply = channel . handle ( data ) \n type = data [ '<STR_LIT>' ] \n if type == \"<STR_LIT>\" : \n reply_json = self . build_response ( reply , data ) \n self . notify_dingtalk ( data , reply_json ) \n else : \n reply_json = self . build_webhook_response ( reply , data ) \n self . notify_dingtalk_webhook ( reply_json ) \n def notify_dingtalk ( self , data , reply_json ) : \n headers = { \n '<STR_LIT>' : '<STR_LIT>' , \n '<STR_LIT>' : self . get_token ( ) \n } \n notify_url = self . get_post_url ( data ) \n try : \n r = requests . post ( notify_url , json = reply_json , headers = headers ) \n resp = r . json ( ) \n log . info ( \"<STR_LIT>\" . format ( str ( resp ) ) ) \n except Exception as e : \n log . error ( e ) \n class DingTalkChannel ( Channel ) : \n def __init__ ( self ) : \n log . info ( \"<STR_LIT>\" ) \n def startup ( self ) : \n http_app . run ( host = '<STR_LIT>' , port = channel_conf ( const . DINGTALK ) . get ( '<STR_LIT>' ) ) \n def handle ( self , data ) : \n reply = \"<STR_LIT>\" \n prompt = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n prompt = prompt . strip ( ) \n if str ( prompt ) != <NUM_LIT> : \n conversation_id = data [ '<STR_LIT>' ] \n sender_id = data [ '<STR_LIT>' ] \n context = dict ( ) \n img_match_prefix = functions . check_prefix ( \n prompt , channel_conf_val ( const . DINGTALK , '<STR_LIT>' ) ) \n if img_match_prefix : \n prompt = prompt . split ( img_match_prefix , <NUM_LIT> ) [ <NUM_LIT> ] . strip ( ) \n context [ '<STR_LIT>' ] = '<STR_LIT>' \n id = sender_id \n context [ '<STR_LIT>' ] = str ( id ) \n reply = super ( ) . build_reply_content ( prompt , context ) \n return reply \n dd = DingTalkChannel ( ) \n handlers = dict ( ) \n robots = channel_conf ( const . DINGTALK ) . get ( '<STR_LIT>' ) \n if robots and len ( robots ) > <NUM_LIT> : \n for robot in robots : \n robot_config = channel_conf ( const . DINGTALK ) . get ( robot ) \n robot_key = robot_config . get ( '<STR_LIT>' ) \n group_name = robot_config . get ( '<STR_LIT>' ) \n handlers [ group_name or robot_key ] = DingTalkHandler ( robot_config ) \n else : \n handlers [ '<STR_LIT>' ] = DingTalkHandler ( channel_conf ( const . DINGTALK ) ) \n http_app = Flask ( __name__ , ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def chat ( ) : \n log . info ( \"<STR_LIT>\" . format ( str ( request . headers ) ) ) \n log . info ( \"<STR_LIT>\" . format ( str ( request . data ) ) ) \n token = request . headers . get ( '<STR_LIT>' ) \n data = json . loads ( request . data ) \n if data : \n content = data [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n if not content : \n return \n code = data [ '<STR_LIT>' ] \n group_name = None \n if '<STR_LIT>' in data : \n group_name = data [ '<STR_LIT>' ] \n handler = handlers . get ( group_name , handlers . get ( code , handlers . get ( '<STR_LIT>' ) ) ) \n if handler . dingtalk_post_token and token != handler . dingtalk_post_token : \n return { '<STR_LIT>' : <NUM_LIT> } \n handler . chat ( dd , data ) \n return { '<STR_LIT>' : <NUM_LIT> } \n return { '<STR_LIT>' : <NUM_LIT> }"}, {"input": "from sqlite3 import Connection \n from fastapi import Depends , FastAPI , Request \n from fastapi . middleware . cors import CORSMiddleware \n from fastapi . staticfiles import StaticFiles \n from fastapi . templating import Jinja2Templates \n from starlette . responses import RedirectResponse \n import uvicorn \n from profyle . application . trace . create import create_trace_selected_table , create_trace_table \n from profyle . application . trace . get import get_all_traces , get_trace_selected , get_trace_by_id \n from profyle . application . trace . store import store_trace_selected \n from profyle . infrastructure . sqlite3 . get_connection import get_connection \n from profyle . infrastructure . sqlite3 . repository import SQLiteTraceRepository \n from profyle . settings import settings \n app = FastAPI ( \n title = \"<STR_LIT>\" , \n version = \"<STR_LIT>\" \n ) \n app . add_middleware ( \n CORSMiddleware , \n allow_origins = [ \"<STR_LIT>\" ] , \n allow_methods = [ \"<STR_LIT>\" ] , \n allow_headers = [ \"<STR_LIT>\" ] , \n ) \n @ app . on_event ( \"<STR_LIT>\" ) \n async def startup_event ( ) : \n db = get_connection ( ) \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n create_trace_table ( repo = sqlite_trace_repo ) \n create_trace_selected_table ( repo = sqlite_trace_repo ) \n STATIC_PATH = ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n app . mount ( \n \"<STR_LIT>\" , \n StaticFiles ( directory = settings . get_path ( * STATIC_PATH ) ) , \n name = \"<STR_LIT>\" \n ) \n app . mount ( \n \"<STR_LIT>\" , \n StaticFiles ( directory = settings . get_viztracer_static_files ( ) , html = True ) , \n name = \"<STR_LIT>\" \n ) \n TEMPLATES_PATH = ( \"<STR_LIT>\" , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n templates = Jinja2Templates ( directory = settings . get_path ( * TEMPLATES_PATH ) ) \n @ app . get ( \"<STR_LIT>\" ) \n async def vizviewer_info ( ) : \n return { \"<STR_LIT>\" : False } \n @ app . get ( \"<STR_LIT>\" ) \n async def file_info ( \n db : Connection = Depends ( get_connection ) , \n ) : \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n trace_id = get_trace_selected ( repo = sqlite_trace_repo ) \n if not trace_id : \n return { } \n trace = get_trace_by_id ( \n trace_id = trace_id , \n repo = sqlite_trace_repo \n )", "output": "if not trace : \n return { } \n return trace . data . get ( \"<STR_LIT>\" ) \n @ app . get ( \"<STR_LIT>\" ) \n async def localtrace ( \n db : Connection = Depends ( get_connection ) , \n ) : \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n trace_id = get_trace_selected ( repo = sqlite_trace_repo ) \n if not trace_id : \n return { } \n trace = get_trace_by_id ( \n trace_id = trace_id , \n repo = sqlite_trace_repo \n ) \n if not trace : \n return { } \n return trace . data \n @ app . get ( \"<STR_LIT>\" ) \n async def index ( ) : \n return RedirectResponse ( \"<STR_LIT>\" ) \n @ app . get ( \"<STR_LIT>\" ) \n async def traces ( \n request : Request , \n db : Connection = Depends ( get_connection ) , \n ) : \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n traces = get_all_traces ( repo = sqlite_trace_repo ) \n return templates . TemplateResponse ( \n name = \"<STR_LIT>\" , \n context = { \n \"<STR_LIT>\" : request , \n \"<STR_LIT>\" : [ trace . dict ( ) for trace in traces ] \n } \n ) \n @ app . get ( \"<STR_LIT>\" ) \n async def get_trace ( \n id : int , \n db : Connection = Depends ( get_connection ) , \n ) : \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n store_trace_selected ( \n trace_id = id , \n repo = sqlite_trace_repo \n ) \n return RedirectResponse ( url = \"<STR_LIT>\" ) \n @ app . delete ( \"<STR_LIT>\" , status_code = <NUM_LIT> ) \n async def delete_trace ( \n id : int , \n db : Connection = Depends ( get_connection ) , \n ) -> None : \n sqlite_trace_repo = SQLiteTraceRepository ( db ) \n sqlite_trace_repo . delete_trace_by_id ( id ) \n async def start_server ( port : int = <NUM_LIT> , host : str = \"<STR_LIT>\" ) : \n config = uvicorn . Config ( app , port = port , log_level = \"<STR_LIT>\" , host = host ) \n server = uvicorn . Server ( config ) \n await server . serve ( )"}, {"input": "from typing import Callable , Dict \n from enum import Enum \n from pathlib import Path \n import json \n CURRENT_FOLDER = Path ( __file__ ) . parent \n DEFAULT_USER_AGENT = \"<STR_LIT>\" \n LITERAL = \"<STR_LIT>\" \n UNSATISFIED = \"<STR_LIT>\" \n EXPRESSION = \"<STR_LIT>\" \n ONEOF = \"<STR_LIT>\" \n ENCLOSE_UNDER = \"<STR_LIT>\" \n ENCLOSE = \"<STR_LIT>\" \n WITH_CONTEXT_VAR = \"<STR_LIT>\" \n JINJA_CONTEXT_VAR = \"<STR_LIT>\" \n FLASK_CONTEXT_VAR = \"<STR_LIT>\" \n REQUIRE_PYTHON3 = \"<STR_LIT>\" \n PLUS = \"<STR_LIT>\" \n MULTIPLY = \"<STR_LIT>\" \n MOD = \"<STR_LIT>\" \n FUNCTION_CALL = \"<STR_LIT>\" \n STRING_CONCAT = \"<STR_LIT>\" \n STRING_CONCATMANY = \"<STR_LIT>\" \n VARIABLE_OF = \"<STR_LIT>\" \n ZERO = \"<STR_LIT>\" \n POSITIVE_INTEGER = \"<STR_LIT>\" \n INTEGER = \"<STR_LIT>\" \n STRING_PERCENT = \"<STR_LIT>\" \n STRING_PERCENT_LOWER_C = \"<STR_LIT>\" \n STRING_UNDERLINE = \"<STR_LIT>\" \n STRING_TWOUNDERLINE = \"<STR_LIT>\" \n STRING_LOWERC = \"<STR_LIT>\" \n STRING_MANY_PERCENT_LOWER_C = \"<STR_LIT>\" \n STRING_MANY_FORMAT_C = \"<STR_LIT>\" \n CHAR = \"<STR_LIT>\" \n STRING = \"<STR_LIT>\" \n FORMULAR_SUM = \"<STR_LIT>\" \n ATTRIBUTE = \"<STR_LIT>\" \n ITEM = \"<STR_LIT>\" \n CLASS_ATTRIBUTE = \"<STR_LIT>\" \n CHAINED_ATTRIBUTE_ITEM = \"<STR_LIT>\" \n BUILTINS_DICT = \"<STR_LIT>\" \n IMPORT_FUNC = \"<STR_LIT>\" \n EVAL_FUNC = \"<STR_LIT>\" \n EVAL = \"<STR_LIT>\" \n CONFIG = \"<STR_LIT>\" \n MODULE_OS = \"<STR_LIT>\" \n OS_POPEN_OBJ = \"<STR_LIT>\" \n OS_POPEN_READ = \"<STR_LIT>\" \n CALLBACK_PREPARE_FULLPAYLOADGEN = \"<STR_LIT>\" \n CALLBACK_GENERATE_FULLPAYLOAD = \"<STR_LIT>\" \n CALLBACK_GENERATE_PAYLOAD = \"<STR_LIT>\" \n CALLBACK_SUBMIT = \"<STR_LIT>\" \n CALLBACK_TEST_FORM_INPUT = \"<STR_LIT>\" \n APICODE_OK = <NUM_LIT> \n APICODE_WRONG_INPUT = <NUM_LIT> \n class DetectMode ( Enum ) : \n FAST = \"<STR_LIT>\" \n ACCURATE = \"<STR_LIT>\" \n class TemplateEnvironment ( Enum ) : \n FLASK = \"<STR_LIT>\" \n JINJA2 = \"<STR_LIT>\" \n class PythonEnvironment ( Enum ) : \n UNKNOWN = \"<STR_LIT>\" \n PYTHON2 = \"<STR_LIT>\" \n PYTHON3 = \"<STR_LIT>\" \n class ReplacedKeywordStrategy ( Enum ) : \n AVOID = \"<STR_LIT>\" \n IGNORE = \"<STR_LIT>\" \n DOUBLETAPPING = \"<STR_LIT>\" \n class AutoFix500Code ( Enum ) : \n ENABLED = \"<STR_LIT>\" \n DISABLED = \"<STR_LIT>\" \n WafFunc = Callable [ [ str ] , bool ] \n SET_STMT_PATTERNS = [ \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ( \"<STR_LIT>\" , \"<STR_LIT>\" ) , \n ] \n DANGEROUS_KEYWORDS = [ \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n UNICODE_INT_CHARCODES = [ \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n [ <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> , <NUM_LIT> ] , \n ] \n with open ( CURRENT_FOLDER / \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as f : \n CHAR_PATTERNS : Dict [ str , Dict [ str , str ] ] = json . load ( f )"}, {"input": "import logging \n import re \n import random \n import string \n from urllib . parse import urlparse \n from typing import Union , Generator , Tuple , List , Set \n from bs4 import BeautifulSoup \n from . form import get_form , parse_forms , Form \n from . requester import HTTPRequester \n from . colorize import colored \n from . wordlist import HTTP_PARAMS_LIST \n logger = logging . getLogger ( \"<STR_LIT>\" ) \n PARAM_CHUNK_SIZE = <NUM_LIT> \n PARAM_MAXIMUM_COUNT = <NUM_LIT> \n def parse_urls ( html : Union [ str , BeautifulSoup ] ) -> list : \n if isinstance ( html , str ) : \n bs_obj = BeautifulSoup ( html , \"<STR_LIT>\" ) \n elif isinstance ( html , BeautifulSoup ) : \n bs_obj = html \n return [ \n element . attrs [ \"<STR_LIT>\" ] for element in bs_obj . select ( \"<STR_LIT>\" ) if \"<STR_LIT>\" in element \n ] \n def burst_respond_params_data ( \n requester : HTTPRequester , url : str , html_str : str \n ) -> Tuple [ List [ str ] , List [ str ] ] : \n words : List [ str ] = ( \n re . findall ( r\"<STR_LIT>\" , html_str ) \n + re . findall ( r\"<STR_LIT>\" , html_str ) \n + HTTP_PARAMS_LIST \n ) \n words = list ( set ( words ) ) \n random . shuffle ( words ) \n if len ( words ) > PARAM_CHUNK_SIZE * <NUM_LIT> : \n logger . warning ( \"<STR_LIT>\" , len ( words ) ) \n return [ ] , [ ] \n logger . warning ( \"<STR_LIT>\" , len ( words ) ) \n respond_get_params : Set [ str ] = set ( ) \n respond_post_params : Set [ str ] = set ( ) \n for i in range ( <NUM_LIT> , len ( words ) , PARAM_CHUNK_SIZE ) : \n words_chunk = words [ i : i + PARAM_CHUNK_SIZE ] \n for _ in range ( <NUM_LIT> ) : \n params = { \n k : \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase + string . digits , k = <NUM_LIT> ) ) \n for k in words_chunk \n } \n data = { \n k : \"<STR_LIT>\" . join ( random . choices ( string . ascii_lowercase + string . digits , k = <NUM_LIT> ) ) \n for k in words_chunk \n } \n resp = requester . request ( method = \"<STR_LIT>\" , url = url , params = params ) \n if resp : \n respond_get_params |= set ( \n k for k , v in params . items ( ) if v in resp . text \n ) \n resp = requester . request ( method = \"<STR_LIT>\" , url = url , data = data ) \n if resp : \n respond_post_params |= set ( k for k , v in data . items ( ) if v in resp . text ) \n break \n return list ( respond_get_params ) , list ( respond_post_params ) \n def yield_form ( \n requester : HTTPRequester , start_url : str \n ) -> Generator [ Tuple [ str , List [ Form ] ] , None , None ] : \n found = False \n targets = [ \n start_url , \n ] \n visited = set ( )", "output": "logger . warning ( \"<STR_LIT>\" ) \n while targets : \n target_url = targets . pop ( <NUM_LIT> ) \n if target_url in visited : \n continue \n visited . add ( target_url ) \n resp = requester . request ( method = \"<STR_LIT>\" , url = target_url ) \n if resp is None : \n logger . warning ( \"<STR_LIT>\" , target_url ) \n continue \n html = BeautifulSoup ( resp . text , \"<STR_LIT>\" ) \n forms = parse_forms ( target_url , html ) \n if forms : \n yield target_url , forms \n found = True \n respond_get_params , respond_post_params = burst_respond_params_data ( \n requester , target_url , resp . text \n ) \n if respond_get_params and len ( respond_get_params ) < PARAM_MAXIMUM_COUNT : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( respond_get_params ) ) , \n ) \n yield target_url , [ \n get_form ( \n action = urlparse ( target_url ) . path , \n inputs = respond_get_params , \n method = \"<STR_LIT>\" , \n ) \n ] \n found = True \n if respond_post_params and len ( respond_post_params ) < PARAM_MAXIMUM_COUNT : \n logger . warning ( \n \"<STR_LIT>\" , \n colored ( \"<STR_LIT>\" , repr ( respond_post_params ) ) , \n ) \n yield target_url , [ \n get_form ( \n action = urlparse ( target_url ) . path , \n inputs = respond_post_params , \n method = \"<STR_LIT>\" , \n ) \n ] \n found = True \n targets += parse_urls ( html ) \n if not found : \n logger . warning ( \"<STR_LIT>\" )"}, {"input": "from legged_gym . envs import AnymalCRoughCfg , AnymalCRoughCfgPPO \n class AnymalBRoughCfg ( AnymalCRoughCfg ) : \n class asset ( AnymalCRoughCfg . asset ) : \n file = '<STR_LIT>' \n foot_name = '<STR_LIT>' \n class rewards ( AnymalCRoughCfg . rewards ) : \n class scales ( AnymalCRoughCfg . rewards . scales ) : \n pass \n class AnymalBRoughCfgPPO ( AnymalCRoughCfgPPO ) :", "output": "class runner ( AnymalCRoughCfgPPO . runner ) : \n run_name = '<STR_LIT>' \n experiment_name = '<STR_LIT>' \n load_run = - <NUM_LIT>"}, {"input": "import json \n import logging \n import shutil \n import subprocess \n import urllib \n import zipfile \n import favicon . favicon as favicon \n from bs4 import BeautifulSoup \n from feedi . requests import USER_AGENT , requests \n logger = logging . getLogger ( __name__ ) \n def get_favicon ( url , html = None ) : \n \"<STR_LIT>\" \n url_parts = urllib . parse . urlparse ( url ) \n url = f'<STR_LIT>' \n try : \n if not html : \n favicons = favicon . get ( url , headers = { '<STR_LIT>' : USER_AGENT } , timeout = <NUM_LIT> ) \n else : \n favicons = sorted ( favicon . tags ( url , html ) , \n key = lambda i : i . width + i . height , reverse = True ) \n except Exception : \n logger . exception ( \"<STR_LIT>\" , url ) \n return \n ico_format = [ f for f in favicons if f . format == '<STR_LIT>' ] \n if ico_format : \n return ico_format [ <NUM_LIT> ] . url \n return favicons [ <NUM_LIT> ] . url if favicons else None \n class CachingRequestsMixin : \n def __init__ ( self ) : \n self . response_cache = { } \n def request ( self , url ) : \n if url in self . response_cache : \n logger . debug ( \"<STR_LIT>\" , url ) \n return self . response_cache [ url ] \n logger . debug ( \"<STR_LIT>\" , url ) \n content = requests . get ( url ) . content", "output": "self . response_cache [ url ] = content \n return content \n def fetch_meta ( self , url , * tags ) : \n soup = BeautifulSoup ( self . request ( url ) , '<STR_LIT>' ) \n return extract_meta ( soup , * tags ) \n def extract_meta ( soup , * tags ) : \n for tag in tags : \n for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n meta_tag = soup . find ( \"<STR_LIT>\" , { attr : tag } , content = True ) \n if meta_tag : \n return meta_tag [ '<STR_LIT>' ] \n def all_meta ( soup ) : \n result = { } \n for attr in [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] : \n for meta_tag in soup . find_all ( \"<STR_LIT>\" , { attr : True } , content = True ) : \n result [ meta_tag [ attr ] ] = meta_tag [ '<STR_LIT>' ] \n return result \n def extract_links ( url , html ) : \n soup = BeautifulSoup ( html , '<STR_LIT>' ) \n links = soup . find_all ( lambda tag : tag . name == '<STR_LIT>' and tag . text and '<STR_LIT>' in tag ) \n return [ ( make_absolute ( url , a [ '<STR_LIT>' ] ) , a . text ) for a in links ] \n def make_absolute ( url , path ) : \n \"<STR_LIT>\" \n if not urllib . parse . urlparse ( path ) . netloc : \n path = urllib . parse . urljoin ( url , path ) \n return path \n def extract ( url = None , html = None ) : \n if url : \n html = requests . get ( url ) . content \n elif not html : \n raise ValueError ( '<STR_LIT>' ) \n r = subprocess . run ( [ \"<STR_LIT>\" , \"<STR_LIT>\" , url ] , input = html , \n capture_output = True , check = True ) \n article = json . loads ( r . stdout ) \n soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) \n LAZY_DATA_ATTRS = [ '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' , '<STR_LIT>' ] \n for data_attr in LAZY_DATA_ATTRS : \n for img in soup . findAll ( '<STR_LIT>' , attrs = { data_attr : True } ) : \n img . attrs = { '<STR_LIT>' : img [ data_attr ] } \n for iframe in soup . findAll ( '<STR_LIT>' , height = True ) : \n del iframe [ '<STR_LIT>' ] \n article [ '<STR_LIT>' ] = str ( soup ) \n return article \n def compress ( outfilename , article ) : \n soup = BeautifulSoup ( article [ '<STR_LIT>' ] , '<STR_LIT>' ) \n with zipfile . ZipFile ( outfilename , '<STR_LIT>' , compression = zipfile . ZIP_DEFLATED ) as zip : \n for img in soup . findAll ( '<STR_LIT>' ) : \n img_url = img [ '<STR_LIT>' ] \n img_filename = '<STR_LIT>' + img [ '<STR_LIT>' ] . split ( '<STR_LIT>' ) [ - <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n img [ '<STR_LIT>' ] = img_filename \n with requests . get ( img_url , stream = True ) as img_src , zip . open ( img_filename , mode = '<STR_LIT>' ) as img_dest : \n shutil . copyfileobj ( img_src . raw , img_dest ) \n zip . writestr ( '<STR_LIT>' , str ( soup ) )"}, {"input": "from profyle . fastapi import ProfyleMiddleware \n from tests . unit . repository import InMemoryTraceRepository \n def test_should_trace_all_requests ( fastapi_client , fastapi_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n fastapi_app . add_middleware ( \n ProfyleMiddleware , \n trace_repo = trace_repo \n ) \n fastapi_client . post ( \"<STR_LIT>\" ) \n fastapi_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n def test_should_trace_filtered_requests ( fastapi_client , fastapi_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n fastapi_app . add_middleware ( \n ProfyleMiddleware , \n pattern = \"<STR_LIT>\" , \n trace_repo = trace_repo \n )", "output": "fastapi_client . post ( \"<STR_LIT>\" ) \n fastapi_client . get ( \"<STR_LIT>\" ) \n fastapi_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT> \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n assert trace_repo . traces [ <NUM_LIT> ] . name == \"<STR_LIT>\" \n def test_should_no_trace_if_disabled ( fastapi_client , fastapi_app ) : \n trace_repo = InMemoryTraceRepository ( ) \n fastapi_app . add_middleware ( \n ProfyleMiddleware , \n enabled = False , \n trace_repo = InMemoryTraceRepository ( ) \n ) \n fastapi_client . post ( \"<STR_LIT>\" ) \n fastapi_client . get ( \"<STR_LIT>\" ) \n assert len ( trace_repo . traces ) == <NUM_LIT>"}, {"input": "import json \n import hmac \n import hashlib \n import base64 \n import time \n import requests \n from urllib . parse import quote_plus \n from common . log import logger \n from flask import Flask , request , render_template , make_response \n from config import conf \n from bridge . bridge import Bridge \n from channel . channel import Channel \n from urllib import request as url_request \n from concurrent . futures import ThreadPoolExecutor \n class FeiShuChannel ( Channel ) : \n def __init__ ( self ) : \n self . app_id = conf ( ) . get ( '<STR_LIT>' ) \n self . app_secret = conf ( ) . get ( '<STR_LIT>' ) \n self . verification_token = conf ( ) . get ( '<STR_LIT>' ) \n self . host = conf ( ) . get ( '<STR_LIT>' ) \n self . port = conf ( ) . get ( '<STR_LIT>' ) \n logger . info ( \"<STR_LIT>\" . format ( \n self . app_id , self . app_secret , self . verification_token , self . host , self . port ) ) \n def startup ( self ) : \n http_app . run ( host = self . host , port = self . port ) \n def get_tenant_access_token ( self ) : \n url = \"<STR_LIT>\" \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" \n } \n req_body = { \n \"<STR_LIT>\" : self . app_id , \n \"<STR_LIT>\" : self . app_secret \n } \n data = bytes ( json . dumps ( req_body ) , encoding = '<STR_LIT>' ) \n req = url_request . Request ( url = url , data = data , \n headers = headers , method = '<STR_LIT>' ) \n try : \n response = url_request . urlopen ( req ) \n except Exception as e : \n print ( e . read ( ) . decode ( ) ) \n return \"<STR_LIT>\" \n rsp_body = response . read ( ) . decode ( '<STR_LIT>' ) \n rsp_dict = json . loads ( rsp_body ) \n code = rsp_dict . get ( \"<STR_LIT>\" , - <NUM_LIT> ) \n if code != <NUM_LIT> : \n print ( \"<STR_LIT>\" , code ) \n return \"<STR_LIT>\" \n return rsp_dict . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def notify_feishu ( self , token , receive_type , receive_id , at_id , answer ) :", "output": "url = \"<STR_LIT>\" \n params = { \"<STR_LIT>\" : receive_type } \n text = answer . lstrip ( ) \n msgContent = { \n \"<STR_LIT>\" : text , \n } \n req = { \n \"<STR_LIT>\" : receive_id , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n \"<STR_LIT>\" : json . dumps ( msgContent ) , \n } \n payload = json . dumps ( req ) \n headers = { \n \"<STR_LIT>\" : \"<STR_LIT>\" + token , \n \"<STR_LIT>\" : \"<STR_LIT>\" , \n } \n response = requests . request ( \n \"<STR_LIT>\" , url , params = params , headers = headers , data = payload \n ) \n def handle ( self , message ) : \n event = message [ \"<STR_LIT>\" ] \n msg = event [ \"<STR_LIT>\" ] \n messageId = msg [ \"<STR_LIT>\" ] \n chat_type = msg [ \"<STR_LIT>\" ] \n sender_id = event [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n prompt = json . loads ( msg [ \"<STR_LIT>\" ] ) [ \"<STR_LIT>\" ] \n prompt = prompt . replace ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n message_type = msg [ \"<STR_LIT>\" ] \n if message_type != \"<STR_LIT>\" : \n return { '<STR_LIT>' : <NUM_LIT> } \n if chat_type == \"<STR_LIT>\" : \n mentions = msg [ \"<STR_LIT>\" ] \n if not mentions : \n return { '<STR_LIT>' : <NUM_LIT> } \n receive_type = \"<STR_LIT>\" \n receive_id = msg . get ( \"<STR_LIT>\" ) \n at_id = sender_id \n elif chat_type == \"<STR_LIT>\" : \n receive_type = \"<STR_LIT>\" \n receive_id = sender_id \n at_id = None \n access_token = self . get_tenant_access_token ( ) \n if access_token == \"<STR_LIT>\" : \n logger . error ( \"<STR_LIT>\" ) \n return { '<STR_LIT>' : <NUM_LIT> } \n context = dict ( ) \n context [ '<STR_LIT>' ] = str ( sender_id ) \n reply = self . build_reply_content ( prompt , context ) \n self . notify_feishu ( access_token , receive_type , \n receive_id , at_id , reply ) \n return { '<STR_LIT>' : <NUM_LIT> } \n def handle_request_url_verify ( self , post_obj ) : \n challenge = post_obj . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n logger . info ( \"<STR_LIT>\" . format ( challenge ) ) \n return { '<STR_LIT>' : challenge } \n def build_reply_content ( self , query , context = None ) : \n return Bridge ( ) . fetch_reply_content ( query , context ) \n thread_pool = ThreadPoolExecutor ( max_workers = <NUM_LIT> ) \n http_app = Flask ( __name__ ) \n @ http_app . route ( \"<STR_LIT>\" , methods = [ '<STR_LIT>' ] ) \n def chat ( ) : \n feishu = FeiShuChannel ( ) \n logger . info ( \"<STR_LIT>\" . format ( str ( request . data ) ) ) \n obj = json . loads ( request . data ) \n if not obj : \n return { '<STR_LIT>' : <NUM_LIT> } \n headers = obj . get ( \"<STR_LIT>\" ) \n if not headers : \n return { '<STR_LIT>' : <NUM_LIT> } \n t = obj . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if \"<STR_LIT>\" == t : \n return feishu . handle_request_url_verify ( obj ) \n elif headers . get ( \"<STR_LIT>\" , None ) == \"<STR_LIT>\" : \n return feishu . handle ( obj ) \n return { '<STR_LIT>' : <NUM_LIT> }"}, {"input": "import os , sys \n import json \n import gradio as gr \n from assets . i18n . i18n import I18nAuto \n now_dir = os . getcwd ( ) \n sys . path . append ( now_dir ) \n i18n = I18nAuto ( ) \n config_file = os . path . join ( now_dir , \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def get_language_settings ( ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file ) \n if config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] == False : \n return \"<STR_LIT>\" \n else : \n return config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] \n def save_lang_settings ( selected_language ) : \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n config = json . load ( file )", "output": "if selected_language == \"<STR_LIT>\" : \n config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = False \n else : \n config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = True \n config [ \"<STR_LIT>\" ] [ \"<STR_LIT>\" ] = selected_language \n gr . Info ( \"<STR_LIT>\" ) \n with open ( config_file , \"<STR_LIT>\" , encoding = \"<STR_LIT>\" ) as file : \n json . dump ( config , file , indent = <NUM_LIT> ) \n def lang_tab ( ) : \n with gr . Column ( ) : \n selected_language = gr . Dropdown ( \n label = i18n ( \"<STR_LIT>\" ) , \n info = i18n ( \n \"<STR_LIT>\" \n ) , \n value = get_language_settings ( ) , \n choices = [ \"<STR_LIT>\" ] \n + i18n . _get_available_languages ( ) , \n interactive = True , \n ) \n selected_language . change ( \n fn = save_lang_settings , \n inputs = [ selected_language ] , \n outputs = [ ] , \n )"}, {"input": "import requests \n from tqdm import tqdm \n def download_file_from_google_drive ( id , destination ) :", "output": "def get_confirm_token ( response ) : \n if '<STR_LIT>' in response . text : \n key = response . text . split ( '<STR_LIT>' ) [ <NUM_LIT> ] . split ( '<STR_LIT>' ) [ <NUM_LIT> ] \n return key \n return None \n def save_response_content ( response , destination ) : \n CHUNK_SIZE = <NUM_LIT> * <NUM_LIT> \n with open ( destination , \"<STR_LIT>\" ) as f : \n for chunk in tqdm ( response . iter_content ( CHUNK_SIZE ) ) : \n if chunk : \n f . write ( chunk ) \n URL = \"<STR_LIT>\" \n session = requests . Session ( ) \n response = session . get ( URL , params = { '<STR_LIT>' : id } , stream = True ) \n token = get_confirm_token ( response ) \n if token : \n params = { '<STR_LIT>' : id , '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : token } \n response = session . get ( URL , params = params , stream = True ) \n save_response_content ( response , destination ) \n if __name__ == \"<STR_LIT>\" : \n import sys \n if len ( sys . argv ) != <NUM_LIT> : \n print ( \"<STR_LIT>\" ) \n else : \n file_id = sys . argv [ <NUM_LIT> ] \n destination = sys . argv [ <NUM_LIT> ] \n download_file_from_google_drive ( file_id , destination )"}, {"input": "import sys \n sys . path . append ( '<STR_LIT>' ) \n import os \n import traceback \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( )", "output": "from KeyManagement import reliableKey \n import openai \n reliableKey . token = \"<STR_LIT>\" \n openai . api_key = reliableKey . get_key ( \"<STR_LIT>\" , os . getenv ( \"<STR_LIT>\" ) ) \n openai . error . AuthenticationError = reliableKey . AuthenticationError \n questions = [ \"<STR_LIT>\" , \"<STR_LIT>\" ] \n for question in questions : \n try : \n chat_completion = openai . ChatCompletion . create ( model = \"<STR_LIT>\" , messages = [ { \"<STR_LIT>\" : \"<STR_LIT>\" , \"<STR_LIT>\" : question } ] ) \n print ( chat_completion ) \n except : \n traceback . print_exc ( ) \n continue"}, {"input": "from flask import Flask , render_template , request , redirect , url_for \n app = Flask ( __name__ ) \n data = [ \n { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : \"<STR_LIT>\" } , \n { \"<STR_LIT>\" : <NUM_LIT> , \"<STR_LIT>\" : \"<STR_LIT>\" } \n ] \n @ app . route ( '<STR_LIT>' ) \n def index ( ) : \n return render_template ( \"<STR_LIT>\" , datos = data ) \n @ app . route ( '<STR_LIT>' , methods = [ '<STR_LIT>' ] ) \n def add_element ( ) : \n if request . method == '<STR_LIT>' : \n nuevo_nombre = request . form [ '<STR_LIT>' ] \n nuevo_id = max ( dato [ '<STR_LIT>' ] for dato in data ) + <NUM_LIT> \n data . append ( { \"<STR_LIT>\" : nuevo_id , \"<STR_LIT>\" : nuevo_nombre } )", "output": "return redirect ( url_for ( '<STR_LIT>' ) ) \n if __name__ == '<STR_LIT>' : \n app . run ( debug = True )"}, {"input": "from typing import List , Optional , Tuple , Union \n import torch \n import torch . nn as nn \n from torch . nn import CrossEntropyLoss \n from transformers import ( \n AutoConfig , \n AutoModelForCausalLM , \n MistralConfig , \n MistralModel , \n MistralForCausalLM , \n ) \n from transformers . modeling_outputs import CausalLMOutputWithPast \n from multi_token . language_models . base_model import ( \n LMMMetaModel , \n LMMMetaForCausalLM , \n ) \n class MistralLMMConfig ( MistralConfig ) : \n model_type = \"<STR_LIT>\" \n class MistralLMMModel ( LMMMetaModel , MistralModel ) : \n config_class = MistralLMMConfig \n def __init__ ( self , config : MistralLMMConfig ) : \n super ( MistralLMMModel , self ) . __init__ ( config ) \n class MistralLMMForCausalLM ( MistralForCausalLM , LMMMetaForCausalLM ) : \n config_class = MistralLMMConfig \n def __init__ ( self , config ) : \n super ( MistralForCausalLM , self ) . __init__ ( config ) \n self . model = MistralLMMModel ( config ) \n self . vocab_size = config . vocab_size \n self . lm_head = nn . Linear ( config . hidden_size , config . vocab_size , bias = False ) \n self . modalities = None \n self . post_init ( ) \n def get_model ( self ) -> \"<STR_LIT>\" : \n return self . model \n def forward ( \n self , \n input_ids : torch . LongTensor = None , \n attention_mask : Optional [ torch . Tensor ] = None , \n position_ids : Optional [ torch . LongTensor ] = None , \n past_key_values : Optional [ List [ torch . FloatTensor ] ] = None , \n inputs_embeds : Optional [ torch . FloatTensor ] = None , \n labels : Optional [ torch . LongTensor ] = None , \n use_cache : Optional [ bool ] = None , \n output_attentions : Optional [ bool ] = None , \n output_hidden_states : Optional [ bool ] = None , \n return_dict : Optional [ bool ] = None , \n ** kwargs \n ) -> Union [ Tuple , CausalLMOutputWithPast ] : \n output_attentions = ( \n output_attentions \n if output_attentions is not None \n else self . config . output_attentions \n ) \n output_hidden_states = ( \n output_hidden_states \n if output_hidden_states is not None \n else self . config . output_hidden_states \n ) \n return_dict = ( \n return_dict if return_dict is not None else self . config . use_return_dict \n ) \n ( \n input_ids , \n attention_mask , \n past_key_values , \n inputs_embeds , \n labels , \n ) = self . prepare_inputs_labels_for_multimodal (", "output": "input_ids , attention_mask , past_key_values , labels , ** kwargs \n ) \n outputs = self . model ( \n input_ids = input_ids , \n attention_mask = attention_mask , \n position_ids = position_ids , \n past_key_values = past_key_values , \n inputs_embeds = inputs_embeds , \n use_cache = use_cache , \n output_attentions = output_attentions , \n output_hidden_states = output_hidden_states , \n return_dict = return_dict , \n ) \n hidden_states = outputs [ <NUM_LIT> ] \n logits = self . lm_head ( hidden_states ) \n logits = logits . float ( ) \n loss = None \n if labels is not None : \n shift_logits = logits [ ... , : - <NUM_LIT> , : ] . contiguous ( ) \n shift_labels = labels [ ... , <NUM_LIT> : ] . contiguous ( ) \n loss_fct = CrossEntropyLoss ( ) \n shift_logits = shift_logits . view ( - <NUM_LIT> , self . config . vocab_size ) \n shift_labels = shift_labels . view ( - <NUM_LIT> ) \n shift_labels = shift_labels . to ( shift_logits . device ) \n loss = loss_fct ( shift_logits , shift_labels ) \n if not return_dict : \n output = ( logits , ) + outputs [ <NUM_LIT> : ] \n return ( loss , ) + output if loss is not None else output \n return CausalLMOutputWithPast ( \n loss = loss , \n logits = logits , \n past_key_values = outputs . past_key_values , \n hidden_states = outputs . hidden_states , \n attentions = outputs . attentions , \n ) \n def prepare_inputs_for_generation ( \n self , \n input_ids , \n past_key_values = None , \n attention_mask = None , \n inputs_embeds = None , \n modality_inputs = None , \n ** kwargs \n ) : \n if past_key_values : \n input_ids = input_ids [ : , - <NUM_LIT> : ] \n if inputs_embeds is not None : \n raise ValueError ( \"<STR_LIT>\" ) \n model_inputs = { \n \"<STR_LIT>\" : input_ids , \n \"<STR_LIT>\" : None , \n \"<STR_LIT>\" : past_key_values , \n \"<STR_LIT>\" : kwargs . get ( \"<STR_LIT>\" ) , \n \"<STR_LIT>\" : attention_mask , \n ** ( modality_inputs or { } ) , \n } \n return model_inputs \n AutoConfig . register ( \"<STR_LIT>\" , MistralLMMConfig ) \n AutoModelForCausalLM . register ( MistralLMMConfig , MistralLMMForCausalLM )"}, {"input": "from model . model import Model \n from config import model_conf , common_conf_val \n from common import const \n from common import log \n import openai \n import time \n user_session = dict ( ) \n class ChatGPTModel ( Model ) : \n def __init__ ( self ) : \n openai . api_key = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n api_base = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n if api_base : \n openai . api_base = api_base \n proxy = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n if proxy : \n openai . proxy = proxy \n log . info ( \"<STR_LIT>\" . format ( \n api_base , proxy ) ) \n def reply ( self , query , context = None ) : \n if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n from_user_id = context [ '<STR_LIT>' ] \n clear_memory_commands = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) \n if query in clear_memory_commands : \n Session . clear_session ( from_user_id ) \n return '<STR_LIT>' \n new_query = Session . build_session_query ( query , from_user_id ) \n log . debug ( \"<STR_LIT>\" . format ( new_query ) ) \n reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) \n return reply_content \n elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : \n return self . create_img ( query , <NUM_LIT> ) \n def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : \n try : \n response = openai . ChatCompletion . create ( \n model = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" ) or \"<STR_LIT>\" , \n messages = query , \n temperature = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n frequency_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n presence_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) \n ) \n reply_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n used_token = response [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n log . debug ( response ) \n log . info ( \"<STR_LIT>\" , reply_content ) \n if reply_content : \n Session . save_session ( query , reply_content , user_id , used_token ) \n return response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] [ '<STR_LIT>' ] \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) \n else : \n return \"<STR_LIT>\" \n except openai . error . APIConnectionError as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n except openai . error . Timeout as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n return \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n Session . clear_session ( user_id ) \n return \"<STR_LIT>\" \n async def reply_text_stream ( self , query , context , retry_count = <NUM_LIT> ) : \n try :", "output": "user_id = context [ '<STR_LIT>' ] \n new_query = Session . build_session_query ( query , user_id ) \n res = openai . ChatCompletion . create ( \n model = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" ) or \"<STR_LIT>\" , \n messages = new_query , \n temperature = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n frequency_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n presence_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n stream = True \n ) \n full_response = \"<STR_LIT>\" \n for chunk in res : \n log . debug ( chunk ) \n if ( chunk [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] == \"<STR_LIT>\" ) : \n break \n chunk_message = chunk [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] . get ( \"<STR_LIT>\" ) \n if ( chunk_message ) : \n full_response += chunk_message \n yield False , full_response \n Session . save_session ( query , full_response , user_id ) \n log . info ( \"<STR_LIT>\" , full_response ) \n yield True , full_response \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n yield True , self . reply_text_stream ( query , user_id , retry_count + <NUM_LIT> ) \n else : \n yield True , \"<STR_LIT>\" \n except openai . error . APIConnectionError as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n yield True , \"<STR_LIT>\" \n except openai . error . Timeout as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n yield True , \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n Session . clear_session ( user_id ) \n yield True , \"<STR_LIT>\" \n def create_img ( self , query , retry_count = <NUM_LIT> ) : \n try : \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n response = openai . Image . create ( \n prompt = query , \n n = <NUM_LIT> , \n size = \"<STR_LIT>\" \n ) \n image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n log . info ( \"<STR_LIT>\" . format ( image_url ) ) \n return [ image_url ] \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n return self . reply_text ( query , retry_count + <NUM_LIT> ) \n else : \n return \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n return None \n class Session ( object ) : \n @ staticmethod \n def build_session_query ( query , user_id ) : \n session = user_session . get ( user_id , [ ] ) \n if len ( session ) == <NUM_LIT> : \n system_prompt = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n system_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : system_prompt } \n session . append ( system_item ) \n user_session [ user_id ] = session \n user_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : query } \n session . append ( user_item ) \n return session \n @ staticmethod \n def save_session ( query , answer , user_id , used_tokens = <NUM_LIT> ) : \n max_tokens = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n max_history_num = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' , None ) \n if not max_tokens or max_tokens > <NUM_LIT> : \n max_tokens = <NUM_LIT> \n session = user_session . get ( user_id ) \n if session : \n gpt_item = { '<STR_LIT>' : '<STR_LIT>' , '<STR_LIT>' : answer } \n session . append ( gpt_item ) \n if used_tokens > max_tokens and len ( session ) >= <NUM_LIT> : \n session . pop ( <NUM_LIT> ) \n session . pop ( <NUM_LIT> ) \n if max_history_num is not None : \n while len ( session ) > max_history_num * <NUM_LIT> + <NUM_LIT> : \n session . pop ( <NUM_LIT> ) \n session . pop ( <NUM_LIT> ) \n @ staticmethod \n def clear_session ( user_id ) : \n user_session [ user_id ] = [ ]"}, {"input": "from model . model import Model \n from config import model_conf , common_conf_val \n from common import const \n from common import log \n import openai \n import time \n user_session = dict ( ) \n class OpenAIModel ( Model ) : \n def __init__ ( self ) : \n openai . api_key = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n api_base = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n if api_base : \n openai . api_base = api_base \n log . info ( \"<STR_LIT>\" . format ( openai . api_base ) ) \n self . model = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' , '<STR_LIT>' ) \n proxy = model_conf ( const . OPEN_AI ) . get ( '<STR_LIT>' ) \n if proxy : \n openai . proxy = proxy \n def reply ( self , query , context = None ) : \n if not context or not context . get ( '<STR_LIT>' ) or context . get ( '<STR_LIT>' ) == '<STR_LIT>' : \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n from_user_id = context [ '<STR_LIT>' ] \n clear_memory_commands = common_conf_val ( '<STR_LIT>' , [ '<STR_LIT>' ] ) \n if query in clear_memory_commands : \n Session . clear_session ( from_user_id ) \n return '<STR_LIT>' \n new_query = Session . build_session_query ( query , from_user_id ) \n log . debug ( \"<STR_LIT>\" . format ( new_query ) ) \n if context . get ( '<STR_LIT>' ) : \n return self . reply_text_stream ( query , new_query , from_user_id ) \n reply_content = self . reply_text ( new_query , from_user_id , <NUM_LIT> ) \n log . debug ( \"<STR_LIT>\" . format ( new_query , from_user_id , reply_content ) ) \n if reply_content and query : \n Session . save_session ( query , reply_content , from_user_id ) \n return reply_content \n elif context . get ( '<STR_LIT>' , None ) == '<STR_LIT>' : \n return self . create_img ( query , <NUM_LIT> ) \n def reply_text ( self , query , user_id , retry_count = <NUM_LIT> ) : \n try : \n response = openai . Completion . create ( \n model = self . model , \n prompt = query , \n temperature = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n frequency_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n presence_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n stop = [ \"<STR_LIT>\" ] \n ) \n res_content = response . choices [ <NUM_LIT> ] [ '<STR_LIT>' ] . strip ( ) . replace ( '<STR_LIT>' , '<STR_LIT>' ) \n log . info ( \"<STR_LIT>\" . format ( res_content ) ) \n return res_content \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n return self . reply_text ( query , user_id , retry_count + <NUM_LIT> ) \n else : \n return \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n Session . clear_session ( user_id ) \n return \"<STR_LIT>\" \n async def reply_text_stream ( self , query , context , retry_count = <NUM_LIT> ) : \n try : \n user_id = context [ '<STR_LIT>' ] \n new_query = Session . build_session_query ( query , user_id ) \n res = openai . Completion . create ( \n model = \"<STR_LIT>\" , \n prompt = new_query , \n temperature = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n max_tokens = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n frequency_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n presence_penalty = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , <NUM_LIT> ) , \n stream = True \n ) \n full_response = \"<STR_LIT>\" \n for chunk in res : \n log . debug ( chunk ) \n if ( chunk [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] == \"<STR_LIT>\" ) : \n break \n chunk_message = chunk [ '<STR_LIT>' ] [ <NUM_LIT> ] . get ( \"<STR_LIT>\" ) \n if ( chunk_message ) : \n full_response += chunk_message \n yield False , full_response \n Session . save_session ( query , full_response , user_id ) \n log . info ( \"<STR_LIT>\" , full_response ) \n yield True , full_response \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n yield True , self . reply_text_stream ( query , user_id , retry_count + <NUM_LIT> )", "output": "else : \n yield True , \"<STR_LIT>\" \n except openai . error . APIConnectionError as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n yield True , \"<STR_LIT>\" \n except openai . error . Timeout as e : \n log . warn ( e ) \n log . warn ( \"<STR_LIT>\" ) \n yield True , \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n Session . clear_session ( user_id ) \n yield True , \"<STR_LIT>\" \n def _process_reply_stream ( \n self , \n query : str , \n reply : dict , \n user_id : str \n ) -> str : \n full_response = \"<STR_LIT>\" \n for response in reply : \n if response . get ( \"<STR_LIT>\" ) is None or len ( response [ \"<STR_LIT>\" ] ) == <NUM_LIT> : \n raise Exception ( \"<STR_LIT>\" ) \n if response [ \"<STR_LIT>\" ] [ <NUM_LIT> ] . get ( \"<STR_LIT>\" ) is not None : \n break \n if response [ \"<STR_LIT>\" ] [ <NUM_LIT> ] . get ( \"<STR_LIT>\" ) is None : \n raise Exception ( \"<STR_LIT>\" ) \n if response [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] == \"<STR_LIT>\" : \n break \n yield response [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n full_response += response [ \"<STR_LIT>\" ] [ <NUM_LIT> ] [ \"<STR_LIT>\" ] \n if query and full_response : \n Session . save_session ( query , full_response , user_id ) \n def create_img ( self , query , retry_count = <NUM_LIT> ) : \n try : \n log . info ( \"<STR_LIT>\" . format ( query ) ) \n response = openai . Image . create ( \n prompt = query , \n n = <NUM_LIT> , \n size = \"<STR_LIT>\" \n ) \n image_url = response [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n log . info ( \"<STR_LIT>\" . format ( image_url ) ) \n return [ image_url ] \n except openai . error . RateLimitError as e : \n log . warn ( e ) \n if retry_count < <NUM_LIT> : \n time . sleep ( <NUM_LIT> ) \n log . warn ( \"<STR_LIT>\" . format ( retry_count + <NUM_LIT> ) ) \n return self . reply_text ( query , retry_count + <NUM_LIT> ) \n else : \n return \"<STR_LIT>\" \n except Exception as e : \n log . exception ( e ) \n return None \n class Session ( object ) : \n @ staticmethod \n def build_session_query ( query , user_id ) : \n prompt = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n if prompt : \n prompt += \"<STR_LIT>\" \n session = user_session . get ( user_id , None ) \n if session : \n for conversation in session : \n prompt += \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" + conversation [ \"<STR_LIT>\" ] + \"<STR_LIT>\" \n prompt += \"<STR_LIT>\" + query + \"<STR_LIT>\" \n return prompt \n else : \n return prompt + \"<STR_LIT>\" + query + \"<STR_LIT>\" \n @ staticmethod \n def save_session ( query , answer , user_id ) : \n max_tokens = model_conf ( const . OPEN_AI ) . get ( \"<STR_LIT>\" ) \n if not max_tokens : \n max_tokens = <NUM_LIT> \n conversation = dict ( ) \n conversation [ \"<STR_LIT>\" ] = query \n conversation [ \"<STR_LIT>\" ] = answer \n session = user_session . get ( user_id ) \n log . debug ( conversation ) \n log . debug ( session ) \n if session : \n session . append ( conversation ) \n else : \n queue = list ( ) \n queue . append ( conversation ) \n user_session [ user_id ] = queue \n Session . discard_exceed_conversation ( user_session [ user_id ] , max_tokens ) \n @ staticmethod \n def discard_exceed_conversation ( session , max_tokens ) : \n count = <NUM_LIT> \n count_list = list ( ) \n for i in range ( len ( session ) - <NUM_LIT> , - <NUM_LIT> , - <NUM_LIT> ) : \n history_conv = session [ i ] \n count += len ( history_conv [ \"<STR_LIT>\" ] ) + len ( history_conv [ \"<STR_LIT>\" ] ) \n count_list . append ( count ) \n for c in count_list : \n if c > max_tokens : \n session . pop ( <NUM_LIT> ) \n @ staticmethod \n def clear_session ( user_id ) : \n user_session [ user_id ] = [ ]"}, {"input": "from __future__ import annotations \n from collections . abc import Callable \n from contextlib import suppress \n from typing import Any , overload \n from aiohttp import web \n import svcs \n from . _core import ( \n _KEY_CONTAINER , \n _KEY_REGISTRY , \n T1 , \n T2 , \n T3 , \n T4 , \n T5 , \n T6 , \n T7 , \n T8 , \n T9 , \n T10 , \n ) \n try : \n _AIOHTTP_KEY_REGISTRY = web . AppKey ( _KEY_REGISTRY , svcs . Registry ) \n except ( \n AttributeError \n ) : \n _AIOHTTP_KEY_REGISTRY = _KEY_REGISTRY \n _AIOHTTP_KEY_CONTAINER = _KEY_CONTAINER \n def svcs_from ( request : web . Request ) -> svcs . Container : \n return request [ _AIOHTTP_KEY_CONTAINER ] \n def get_registry ( app : web . Application ) -> svcs . Registry : \n return app [ _AIOHTTP_KEY_REGISTRY ] \n def init_app ( \n app : web . Application , \n * , \n registry : svcs . Registry | None = None , \n middleware_pos : int = <NUM_LIT> , \n ) -> web . Application : \n app [ _AIOHTTP_KEY_REGISTRY ] = registry or svcs . Registry ( ) \n app . middlewares . insert ( middleware_pos , svcs_middleware ) \n app . on_cleanup . append ( aclose_registry ) \n return app \n @ web . middleware \n async def svcs_middleware ( \n request : web . Request , handler : Callable \n ) -> web . Response : \n async with svcs . Container ( request . app [ _AIOHTTP_KEY_REGISTRY ] ) as container : \n request [ _AIOHTTP_KEY_CONTAINER ] = container \n return await handler ( request ) \n def register_value ( \n app : web . Application , \n svc_type : type , \n value : object , \n * , \n enter : bool = False , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n get_registry ( app ) . register_value ( \n svc_type , \n value , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n def register_factory ( \n app : web . Application , \n svc_type : type , \n factory : Callable , \n * , \n enter : bool = True , \n ping : Callable | None = None , \n on_registry_close : Callable | None = None , \n ) -> None : \n get_registry ( app ) . register_factory ( \n svc_type , \n factory , \n enter = enter , \n ping = ping , \n on_registry_close = on_registry_close , \n ) \n async def aclose_registry ( app : web . Application ) -> None : \n with suppress ( KeyError ) : \n await get_registry ( app ) . aclose ( ) \n def get_pings ( request : web . Request ) -> list [ svcs . ServicePing ] : \n return svcs_from ( request ) . get_pings ( ) \n async def aget_abstract ( request : web . Request , * svc_types : type ) -> Any : \n return await svcs_from ( request ) . aget_abstract ( * svc_types ) \n @ overload \n async def aget ( request : web . Request , svc_type : type [ T1 ] , / ) -> T1 : ... \n @ overload \n async def aget ( \n request : web . Request , svc_type1 : type [ T1 ] , svc_type2 : type [ T2 ] , / \n ) -> tuple [ T1 , T2 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 ] : ...", "output": "@ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 ] : ... \n @ overload \n async def aget ( \n request : web . Request , \n svc_type1 : type [ T1 ] , \n svc_type2 : type [ T2 ] , \n svc_type3 : type [ T3 ] , \n svc_type4 : type [ T4 ] , \n svc_type5 : type [ T5 ] , \n svc_type6 : type [ T6 ] , \n svc_type7 : type [ T7 ] , \n svc_type8 : type [ T8 ] , \n svc_type9 : type [ T9 ] , \n svc_type10 : type [ T10 ] , \n / , \n ) -> tuple [ T1 , T2 , T3 , T4 , T5 , T6 , T7 , T8 , T9 , T10 ] : ... \n async def aget ( request : web . Request , * svc_types : type ) -> object : \n return await svcs_from ( request ) . aget ( * svc_types )"}, {"input": "import random \n def metodo_burbuja ( lista ) : \n n = len ( lista ) \n cambio = True \n while cambio : \n cambio = False \n for i in range ( n - <NUM_LIT> ) : \n if lista [ i ] > lista [ i + <NUM_LIT> ] : \n lista [ i ] , lista [ i + <NUM_LIT> ] = lista [ i + <NUM_LIT> ] , lista [ i ]", "output": "cambio = True \n return lista \n lista_aleatoria = [ random . randint ( <NUM_LIT> , <NUM_LIT> ) for _ in range ( <NUM_LIT> ) ] \n print ( lista_aleatoria ) \n print ( metodo_burbuja ( lista_aleatoria ) )"}, {"input": "import sys \n sys . path . append ( \"<STR_LIT>\" ) \n from fenjing . form import get_form \n from fenjing . requester import HTTPRequester \n from fenjing . submitter import FormSubmitter \n import fenjing \n import logging \n import os \n from fenjing import FullPayloadGen , const , options \n import unittest \n import random \n import jinja2 \n fenjing . full_payload_gen . logger . setLevel ( logging . ERROR ) \n fenjing . payload_gen . logger . setLevel ( logging . ERROR ) \n VULUNSERVER_ADDR = os . environ . get ( \"<STR_LIT>\" , \"<STR_LIT>\" ) \n def get_full_payload_gen ( \n blacklist , \n detect_mode = fenjing . const . DetectMode . ACCURATE , \n environment = fenjing . const . TemplateEnvironment . FLASK , \n ) : \n return FullPayloadGen ( \n lambda x : all ( word not in x for word in blacklist ) , \n options = options . Options ( detect_mode = detect_mode , environment = environment ) , \n ) \n class FullPayloadGenTestCaseSimple ( unittest . TestCase ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n self . subm = FormSubmitter ( \n url = VULUNSERVER_ADDR , \n form = get_form ( action = \"<STR_LIT>\" , inputs = [ \"<STR_LIT>\" ] , method = \"<STR_LIT>\" ) , \n target_field = \"<STR_LIT>\" , \n requester = HTTPRequester ( interval = <NUM_LIT> ) , \n ) \n def test_string ( self ) : \n strings = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n for string in strings : \n payload , _ = self . full_payload_gen . generate ( const . STRING , string ) \n assert payload is not None \n resp = self . subm . submit ( payload ) \n assert resp is not None \n self . assertIn ( string , resp . text ) \n for word in self . blacklist : \n self . assertNotIn ( word , payload ) \n def test_os_popen_read ( self ) : \n payload , _ = self . full_payload_gen . generate ( \n const . OS_POPEN_READ , \"<STR_LIT>\" \n ) \n assert payload is not None \n resp = self . subm . submit ( payload ) \n assert resp is not None \n self . assertIn ( \"<STR_LIT>\" , resp . text ) \n for word in self . blacklist : \n self . assertNotIn ( word , payload ) \n class FullPayloadGenTestCaseHard ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseHard2 ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" ,", "output": "\"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseStringFormat1 ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseStringFormat2 ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseSubs ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseMul ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklist = [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] \n self . full_payload_gen = get_full_payload_gen ( self . blacklist ) \n class FullPayloadGenTestCaseRandom ( FullPayloadGenTestCaseSimple ) : \n def setUp ( self ) -> None : \n super ( ) . setUp ( ) \n self . blacklists = [ \n random . sample ( \n [ \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n '<STR_LIT>' , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n \"<STR_LIT>\" , \n ] , \n k = <NUM_LIT> , \n ) \n for _ in range ( <NUM_LIT> ) \n ] \n self . full_payload_gens = [ \n get_full_payload_gen ( \n blacklist , \n detect_mode = random . choice ( \n [ \n fenjing . const . DetectMode . ACCURATE , \n fenjing . const . DetectMode . FAST , \n ] \n ) , \n environment = fenjing . const . TemplateEnvironment . JINJA2 , \n ) \n for blacklist in self . blacklists \n ] \n def test_os_popen_read ( self ) : \n for full_payload_gen , blacklist in zip ( self . full_payload_gens , self . blacklists ) : \n payload , _ = full_payload_gen . generate ( \n const . OS_POPEN_READ , \"<STR_LIT>\" \n ) \n assert payload is not None , repr ( blacklist ) \n try : \n result = jinja2 . Template ( payload ) . render ( ) \n except Exception as exc : \n raise RuntimeError ( repr ( blacklist ) ) from exc \n assert \"<STR_LIT>\" in result , repr ( blacklist )"}, {"input": "from flask import Flask \n import time \n import openai \n import sys \n import traceback \n import dotenv \n from dotenv import load_dotenv \n load_dotenv ( ) \n sys . path . append ( '<STR_LIT>' ) \n import openai", "output": "from main import reliableGPT \n import os \n openai . api_type = \"<STR_LIT>\" \n openai . api_base = os . getenv ( \"<STR_LIT>\" ) \n openai . api_version = \"<STR_LIT>\" \n openai . api_key = os . getenv ( \"<STR_LIT>\" ) \n def logging_fn ( * args , ** kwargs ) : \n pass \n openai . Embedding . create = reliableGPT ( \n openai . Embedding . create , \n user_email = \"<STR_LIT>\" , \n backup_openai_key = os . getenv ( '<STR_LIT>' ) ) \n app = Flask ( __name__ ) \n @ app . route ( \"<STR_LIT>\" ) \n def test_fn ( ) : \n print ( \"<STR_LIT>\" ) \n try : \n text_string = \"<STR_LIT>\" * <NUM_LIT> \n embeddings = openai . Embedding . create ( engine = \"<STR_LIT>\" , \n input = text_string ) [ '<STR_LIT>' ] [ <NUM_LIT> ] [ '<STR_LIT>' ] \n return embeddings \n except : \n traceback . print_exc ( ) \n return \"<STR_LIT>\" , <NUM_LIT> \n @ app . route ( '<STR_LIT>' ) \n def index ( ) : \n return '<STR_LIT>' \n if __name__ == \"<STR_LIT>\" : \n from waitress import serve \n serve ( app , host = \"<STR_LIT>\" , port = <NUM_LIT> , threads = <NUM_LIT> )"}, {"input": "from typing import Sequence , Union \n import sqlalchemy as sa \n from alembic import op", "output": "revision : str = '<STR_LIT>' \n down_revision : Union [ str , None ] = '<STR_LIT>' \n branch_labels : Union [ str , Sequence [ str ] , None ] = None \n depends_on : Union [ str , Sequence [ str ] , None ] = None \n def upgrade ( ) -> None : \n op . drop_index ( '<STR_LIT>' , table_name = '<STR_LIT>' ) \n with op . batch_alter_table ( \"<STR_LIT>\" ) as batch_op : \n batch_op . drop_column ( '<STR_LIT>' ) \n def downgrade ( ) -> None : \n op . add_column ( '<STR_LIT>' , sa . Column ( '<STR_LIT>' , sa . TIMESTAMP ( ) , nullable = True ) ) \n op . create_index ( '<STR_LIT>' , '<STR_LIT>' , [ '<STR_LIT>' ] , unique = False )"}]